Running code quality checks on PyBIRD AI codebase...
=========================================

=== BLACK FORMATTING CHECKS ===
Checking formatting with Black: pybirdai/utils/
-----------------------------------
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/datapoint_test_run/generate_technical_test_report.py	2025-08-02 18:37:08.466191+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/datapoint_test_run/generate_technical_test_report.py	2025-09-21 17:07:34.465699+00:00
@@ -17,10 +17,11 @@
 """
 !todo():
     show more basic view of the test (table test to for datapoint that have test)
 
 """
+
 
 def generate_technical_test_report(json_data):
     html_template = """
 <html lang="en">
 <head>
@@ -118,12 +119,18 @@
 </body>
 </html>"""
 
     # Read JSON data
     packages_html = "\n                ".join([f"<li>{pkg}</li>" for pkg in json_data["platform_info"]["packages"]])
-    passed_tests_html = "\n                ".join([f"<li class='passed-test'>{test}</li>" for test in json_data["test_results"]["passed"]])
-    failed_tests = f"<li class='passed-test'>{"None"}</li>" if not json_data["test_results"]["failed"] else "\n".join([f"<li class='failed-test'>{test}</li>" for test in json_data["test_results"]["failed"]])
+    passed_tests_html = "\n                ".join(
+        [f"<li class='passed-test'>{test}</li>" for test in json_data["test_results"]["passed"]]
+    )
+    failed_tests = (
+        f"<li class='passed-test'>{"None"}</li>"
+        if not json_data["test_results"]["failed"]
+        else "\n".join([f"<li class='failed-test'>{test}</li>" for test in json_data["test_results"]["failed"]])
+    )
 
     # Format HTML
     html_content = html_template.format(
         datapoint_value=json_data["test_information"]["datapoint_value"],
         regulatory_template_id=json_data["test_information"]["regulatory_template_id"],
@@ -133,9 +140,9 @@
         packages=packages_html,
         cachedir=json_data["paths"]["cachedir"],
         rootdir=json_data["paths"]["rootdir"],
         configfile=json_data["paths"]["configfile"],
         passed_tests=passed_tests_html,
-        failed_tests=failed_tests
+        failed_tests=failed_tests,
     )
 
     return html_content
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/datapoint_test_run/generate_technical_test_report.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/datapoint_test_run/generate_test_url.py	2025-09-15 13:18:11.411315+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/datapoint_test_run/generate_test_url.py	2025-09-21 17:07:34.483858+00:00
@@ -15,73 +15,75 @@
 import json
 import os
 import gzip
 from urllib.parse import urlencode
 
+
 def main():
 
     # Get all JSON files
-    PATH = os.path.join("tests","test_results","json")
-    json_files = [f for f in os.listdir(PATH) if f.endswith('.json') and f.startswith('2025')]
+    PATH = os.path.join("tests", "test_results", "json")
+    json_files = [f for f in os.listdir(PATH) if f.endswith(".json") and f.startswith("2025")]
     json_files.sort()
 
     # Collect all test data
     all_tests = []
     common_platform_info = None
     common_paths = None
 
     for filename in json_files:
-        with open(os.path.join(PATH,filename), 'r') as f:
+        with open(os.path.join(PATH, filename), "r") as f:
             data = json.load(f)
 
         # Extract common data from first file
         if common_platform_info is None:
-            common_platform_info = data.get('platform_info', {})
-            common_paths = data.get('paths', {})
+            common_platform_info = data.get("platform_info", {})
+            common_paths = data.get("paths", {})
 
         # Create compact test entry
         compact_test = {
-            't': data['timestamp'],
-            'i': {
-                'v': data['test_information']['datapoint_value'],
-                'r': data['test_information']['regulatory_template_id'],
-                'd': data['test_information']['datapoint_suffix'],
-                's': data['test_information']['scenario_name']
+            "t": data["timestamp"],
+            "i": {
+                "v": data["test_information"]["datapoint_value"],
+                "r": data["test_information"]["regulatory_template_id"],
+                "d": data["test_information"]["datapoint_suffix"],
+                "s": data["test_information"]["scenario_name"],
             },
-            'p': data['test_results']['passed'],
-            'f': data['test_results']['failed'],
+            "p": data["test_results"]["passed"],
+            "f": data["test_results"]["failed"],
         }
 
         # Only include failure details if there are failures
-        if data['test_results']['details']['failures']:
-            compact_test['fd'] = data['test_results']['details']['failures']
+        if data["test_results"]["details"]["failures"]:
+            compact_test["fd"] = data["test_results"]["details"]["failures"]
 
         # Only include stdout/stderr if not empty
-        if data['test_results']['details']['captured_stdout']:
-            compact_test['so'] = data['test_results']['details']['captured_stdout']
-        if data['test_results']['details']['captured_stderr']:
-            compact_test['se'] = data['test_results']['details']['captured_stderr']
+        if data["test_results"]["details"]["captured_stdout"]:
+            compact_test["so"] = data["test_results"]["details"]["captured_stdout"]
+        if data["test_results"]["details"]["captured_stderr"]:
+            compact_test["se"] = data["test_results"]["details"]["captured_stderr"]
 
         all_tests.append(compact_test)
 
     # Create combined data structure
     combined_data = {
-        'v': 1,  # Version for future compatibility
-        'pi': common_platform_info,
-        'pa': common_paths,
-        't': all_tests
+        "v": 1,  # Version for future compatibility
+        "pi": common_platform_info,
+        "pa": common_paths,
+        "t": all_tests,
     }
 
     # Convert to JSON and compress
-    json_str = json.dumps(combined_data, separators=(',', ':'))
+    json_str = json.dumps(combined_data, separators=(",", ":"))
     compressed = gzip.compress(json_str.encode())
     encoded = base64.urlsafe_b64encode(compressed).decode()
 
     # Generate the URL
     base_url = "index.html"
-    params = {'d': encoded}
+    params = {"d": encoded}
     query_string = urlencode(params)
 
     print(f"Please see the test report at https://freebird-test-result.github.io?{query_string}")
 
+
 if __name__ == "__main__":
     main()
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/datapoint_test_run/generate_test_url.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/datapoint_test_run/generator_delete_fixtures.py	2025-09-18 06:30:12.730646+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/datapoint_test_run/generator_delete_fixtures.py	2025-09-21 17:07:34.490669+00:00
@@ -16,19 +16,19 @@
 
 import re
 import logging
 from pathlib import Path
 
-def return_logger(__file_name__:str):
+
+def return_logger(__file_name__: str):
     return logging.getLogger(__file_name__)
 
+
 # Set up logging
-logging.basicConfig(
-    level=logging.INFO,
-    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
-)
+logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s")
 logger = logging.getLogger()
+
 
 def process_sql_file(file_path: str):
     """
     Process SQL file and convert insert/update statements to deletes
     Args:
@@ -41,22 +41,22 @@
             logger.debug("Successfully read SQL file")
     except Exception as e:
         logger.error(f"Error reading SQL file: {str(e)}")
         raise
 
-    def convert_into_info(line:str):
+    def convert_into_info(line: str):
         """
         Convert INSERT statement to DELETE statement
         Args:
             line (str): INSERT SQL statement
         Returns:
             str: Converted DELETE statement
         """
         try:
             # Parse the INSERT statement
             table_columns, values = line.strip("INSERT INTO ").rstrip(";\n").split(" VALUES")
-            values = eval(values.replace("NULL","None"))
+            values = eval(values.replace("NULL", "None"))
             table_name, columns = table_columns.rstrip(")").split("(")
             columns = columns.split(",")
             rowid_column = columns[0]
             rowid_value = values[0]
             # Create DELETE statement
@@ -66,20 +66,17 @@
             raise
 
     # Process each line in the file
     result_data = list()
     for line in data:
-        if not line.strip(): continue
+        if not line.strip():
+            continue
         try:
             match line:
                 case _ if "UPDATE" in line:
                     # Handle UPDATE statements by setting values to NULL
-                    result_data.append(re.sub(
-                       r"(?==).*(?>WHERE)",
-                       "=NULL WHERE",
-                       line
-                   ).replace("\n\n","\n"))
+                    result_data.append(re.sub(r"(?==).*(?>WHERE)", "=NULL WHERE", line).replace("\n\n", "\n"))
                     logger.debug("Processed UPDATE statement")
                 case _ if "INSERT INTO" in line:
                     # Convert INSERT statements to DELETE statements
                     result_data.append(convert_into_info(line))
                     logger.debug("Processed INSERT statement")
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/datapoint_test_run/generator_delete_fixtures.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/bird_ecb_website_fetcher.py	2025-09-15 13:18:11.409393+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/bird_ecb_website_fetcher.py	2025-09-21 17:07:34.494424+00:00
@@ -21,10 +21,11 @@
 import requests
 import zipfile
 import os
 import io
 
+
 class BirdEcbClient:
     """Client for the European Central Bank's Bird API."""
 
     BASE_URL = "https://bird.ecb.europa.eu/excel/tree"
 
@@ -33,11 +34,11 @@
         self.params = {
             "includeMappingContent": False,
             "includeRenderingContent": False,
             "includeTransformationContent": False,
             "onlyCurrentlyValidMetadata": False,
-            "format": "csv"
+            "format": "csv",
         }
         self.tree_root_ids = None
         self.tree_root_type = None
 
     def set_tree_root_ids(self, ids):
@@ -120,18 +121,15 @@
             raise ValueError("Tree root IDs must be specified using set_tree_root_ids()")
 
         if not self.tree_root_type:
             raise ValueError("Tree root type must be specified using set_tree_root_type()")
 
-        all_params = {
-            "treeRootIds": self.tree_root_ids,
-            "treeRootType": self.tree_root_type,
-            **self.params
-        }
+        all_params = {"treeRootIds": self.tree_root_ids, "treeRootType": self.tree_root_type, **self.params}
 
         query_string = urlencode(all_params)
         return f"{self.BASE_URL}?{query_string}"
+
 
 class BirdEcbWebsiteClient:
     def __init__(self):
         """Initialize the Bird ECB Website client."""
         self.client = BirdEcbClient()
@@ -146,22 +144,28 @@
         # Save response to temporary ZIP file
         with open(RESPONSE_ZIP, "wb") as f:
             f.write(response.content)
 
         # Extract contents and clean up
-        with zipfile.ZipFile(RESPONSE_ZIP, 'r') as zip_ref:
+        with zipfile.ZipFile(RESPONSE_ZIP, "r") as zip_ref:
             for file in zip_ref.infolist():
                 zip_ref.extract(file, path_to_results)
 
         os.remove(RESPONSE_ZIP)
         return path_to_results
 
-
-    def request_and_save(self, tree_root_ids, tree_root_type="FRAMEWORK", output_dir="results/csv",
-                         format_type="csv", include_mapping_content=False,
-                         include_rendering_content=False, include_transformation_content=False,
-                         only_currently_valid_metadata=False):
+    def request_and_save(
+        self,
+        tree_root_ids,
+        tree_root_type="FRAMEWORK",
+        output_dir="results/csv",
+        format_type="csv",
+        include_mapping_content=False,
+        include_rendering_content=False,
+        include_transformation_content=False,
+        only_currently_valid_metadata=False,
+    ):
         """Request data from the Bird ECB API and save it to a file.
 
         Args:
             tree_root_ids (str or list): The tree root ID(s) to query (e.g., 'ANCRDT').
             tree_root_type (str): The type of tree root (e.g., 'FRAMEWORK', 'CUBE').
@@ -203,16 +207,17 @@
         # Save response to temporary ZIP file
         with open(RESPONSE_ZIP, "wb") as f:
             f.write(response.content)
 
         # Extract contents and clean up
-        with zipfile.ZipFile(RESPONSE_ZIP, 'r') as zip_ref:
+        with zipfile.ZipFile(RESPONSE_ZIP, "r") as zip_ref:
             for file in zip_ref.infolist():
                 zip_ref.extract(file, path_to_results)
 
         os.remove(RESPONSE_ZIP)
         return path_to_results
+
 
 def main():
     client = BirdEcbWebsiteClient()
     output_dir = client.request_and_save(
         tree_root_ids="ANCRDT",
@@ -220,11 +225,12 @@
         output_dir="resources/technical_export",
         format_type="csv",
         include_mapping_content=False,
         include_rendering_content=False,
         include_transformation_content=False,
-        only_currently_valid_metadata=False
+        only_currently_valid_metadata=False,
     )
     print(f"Results saved to: {output_dir}")
 
+
 if __name__ == "__main__":
     main()
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/bird_ecb_website_fetcher.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/clone_mode/export_with_ids.py	2025-09-15 13:18:11.409667+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/clone_mode/export_with_ids.py	2025-09-21 17:07:34.498754+00:00
@@ -25,134 +25,136 @@
 import re
 
 
 def clean_whitespace(text):
     """Clean whitespace from text values"""
-    return re.sub(r'\s+', ' ', str(text).replace('\r', '').replace('\n', ' ')) if text else text
+    return re.sub(r"\s+", " ", str(text).replace("\r", "").replace("\n", " ")) if text else text
 
 
 def export_database_to_csv_with_ids(output_path=None):
     """
     Export database to CSV files, including ID fields for models that use Django's auto-generated primary key.
-    
+
     Args:
         output_path: Path for the output zip file. If None, uses default location.
-    
+
     Returns:
         Path to the created zip file
     """
     # Default output path
     if output_path is None:
-        results_dir = os.path.join(settings.BASE_DIR, 'results')
+        results_dir = os.path.join(settings.BASE_DIR, "results")
         os.makedirs(results_dir, exist_ok=True)
-        output_path = os.path.join(results_dir, 'database_export_with_ids.zip')
-    
+        output_path = os.path.join(results_dir, "database_export_with_ids.zip")
+
     # Get all model classes from bird_meta_data_model
     valid_table_names = set()
     model_map = {}  # Store model classes for reference
     for name, obj in inspect.getmembers(bird_meta_data_model):
         if inspect.isclass(obj) and issubclass(obj, models.Model) and obj != models.Model:
             valid_table_names.add(obj._meta.db_table)
             model_map[obj._meta.db_table] = obj
-    
-    with zipfile.ZipFile(output_path, 'w') as zip_file:
+
+    with zipfile.ZipFile(output_path, "w") as zip_file:
         # Get all table names from SQLite and sort them
         with connection.cursor() as cursor:
-            cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name NOT LIKE 'sqlite_%' AND name NOT LIKE 'django_%' ORDER BY name")
+            cursor.execute(
+                "SELECT name FROM sqlite_master WHERE type='table' AND name NOT LIKE 'sqlite_%' AND name NOT LIKE 'django_%' ORDER BY name"
+            )
             tables = cursor.fetchall()
-        
+
         # Export each table to a CSV file
         for table in tables:
             table_name = table[0]
-            
+
             if table_name in valid_table_names:
                 # Get the model class for this table
                 model_class = model_map[table_name]
-                
+
                 # Check if model has an explicit primary key
-                has_explicit_pk = any(field.primary_key for field in model_class._meta.fields if field.name != 'id')
-                
+                has_explicit_pk = any(field.primary_key for field in model_class._meta.fields if field.name != "id")
+
                 # Get fields in the order they're defined in the model
                 fields = model_class._meta.fields
                 headers = []
                 db_headers = []
-                
+
                 # If model uses Django's auto ID and has no explicit PK, include the ID field
                 if not has_explicit_pk:
-                    headers.append('ID')
-                    db_headers.append('id')
-                
+                    headers.append("ID")
+                    db_headers.append("id")
+
                 for field in fields:
                     # Skip the id field if we already added it or if there's an explicit PK
-                    if field.name == 'id' and has_explicit_pk:
+                    if field.name == "id" and has_explicit_pk:
                         continue
-                    elif field.name == 'id' and not has_explicit_pk:
+                    elif field.name == "id" and not has_explicit_pk:
                         # We already added it above
                         continue
-                    
+
                     headers.append(field.name.upper())  # Convert header to uppercase
                     # If it's a foreign key, append _id for the actual DB column
                     if isinstance(field, models.ForeignKey):
                         db_headers.append(f"{field.name}_id")
                     else:
                         db_headers.append(field.name)
-                
+
                 # Create CSV in memory
                 csv_content = []
-                csv_content.append(','.join(headers))
-                
+                csv_content.append(",".join(headers))
+
                 # Get data with escaped column names and ordered by primary key
                 with connection.cursor() as cursor:
-                    escaped_headers = [f'"{h}"' if h == 'order' else h for h in db_headers]
+                    escaped_headers = [f'"{h}"' if h == "order" else h for h in db_headers]
                     # Get primary key column name - table_name is validated against valid_table_names set
                     if table_name not in valid_table_names:
                         continue  # Skip unsafe table names
                     # SQLite PRAGMA doesn't support parameterized queries, but table name is validated above
                     cursor.execute(f"PRAGMA table_info({table_name})")
                     table_info = cursor.fetchall()
                     pk_columns = []
-                    
+
                     # Collect all primary key columns for composite keys
                     for col in table_info:
                         if col[5] == 1:  # 5 is the index for pk flag in table_info
                             pk_columns.append(col[1])  # 1 is the index for column name
-                    
+
                     # Build ORDER BY clause
                     if pk_columns:
                         order_by = f"ORDER BY {', '.join(pk_columns)}"
                     else:
                         # If no primary key, sort by id if it exists, otherwise by all columns
-                        if 'id' in db_headers:
+                        if "id" in db_headers:
                             order_by = "ORDER BY id"
                         else:
                             order_by = f"ORDER BY {', '.join(escaped_headers)}"
-                    
+
                     # Table name validated above, column names come from model field definitions
                     cursor.execute(f"SELECT {','.join(escaped_headers)} FROM {table_name} {order_by}")
                     rows = cursor.fetchall()
-                    
+
                     for row in rows:
                         # Convert all values to strings and handle None values
-                        csv_row = [str(clean_whitespace(val)) if val is not None else '' for val in row]
+                        csv_row = [str(clean_whitespace(val)) if val is not None else "" for val in row]
                         # Escape commas and quotes in values
                         processed_row = []
                         for val in csv_row:
-                            if ',' in val or '"' in val:
+                            if "," in val or '"' in val:
                                 escaped_val = val.replace('"', '""')
                                 processed_row.append(f'"{escaped_val}"')
                             else:
                                 processed_row.append(val)
-                        csv_content.append(','.join(processed_row))
-                
+                        csv_content.append(",".join(processed_row))
+
                 # Determine CSV filename
                 csv_filename = f"bird_{table_name.replace('pybirdai_', '')}.csv"
-                
+
                 # Add CSV content to zip file
-                csv_data = '\n'.join(csv_content)
+                csv_data = "\n".join(csv_content)
                 zip_file.writestr(csv_filename, csv_data)
-                
+
                 print(f"Exported {table_name} to {csv_filename} ({len(rows)} rows)")
                 if not has_explicit_pk:
                     print(f"  Note: Included ID field for {table_name} (uses Django auto-generated primary key)")
-    
+
     print(f"\nExport complete. Zip file created at: {output_path}")
-    return output_path
\ No newline at end of file
+    return output_path
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/clone_mode/export_with_ids.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/clone_mode/clone_mode_column_index.py	2025-08-02 18:37:08.464925+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/clone_mode/clone_mode_column_index.py	2025-09-21 17:07:34.536034+00:00
@@ -7,10 +7,11 @@
 #
 # SPDX-License-Identifier: EPL-2.0
 #
 # Contributors:
 #    Neil Mackenzie - initial API and implementation
+
 
 class ColumnIndexes(object):
 
     maintenance_agency_id = 0
     maintenance_agency_code = 1
@@ -76,19 +77,16 @@
     domain_domain_data_type = 5
     domain_code = 6
     domain_facet_id = 7
     domain_domain_is_reference = 8
 
-
     member_maintenance_agency = 0
     member_member_id_index = 1
     member_member_code_index = 2
     member_member_name_index = 3
     member_domain_id_index = 4
     member_member_descriptions = 5
-
-
 
     subdomain_maintenance_agency_id = 0
     subdomain_subdomain_id_index = 1
     subdomain_subdomain_name = 2
     subdomain_domain_id_index = 3
@@ -101,11 +99,10 @@
     subdomain_enumeration_member_id_index = 0
     subdomain_enumeration_subdomain_id_index = 1
     subdomain_enumeration_valid_from = 2
     subdomain_enumeration_valid_to_index = 3
     subdomain_enumeration_order = 4
-
 
     cube_structure_maintenance_agency = 0
     cube_structure_id_index = 1
     cube_structure_name_index = 2
     cube_structure_code_index = 3
@@ -137,11 +134,10 @@
     combination_version = 4
     combination_valid_from = 5
     combination_combination_valid_to = 6
     combination_metric = 7
 
-
     combination_item_combination_id = 0
     combination_item_variable_id = 1
     combination_item_subdomain_id = 2
     combination_variable_set = 3
     combination_member_id = 4
@@ -202,11 +198,10 @@
     ordinate_item_member_id = 2
     ordinate_item_member_hierarchy_id = 3
     ordinate_item_member_hierarchy_valid_from = 4
     ordinate_item_starting_member_id = 5
     ordinate_item_is_starting_member_included = 6
-
 
     cell_positions_cell_id = 0
     cell_positions_axis_ordinate_id = 1
 
     member_hierarchy_maintenance_agency = 0
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/database_setup_first_use.py	2025-09-15 13:18:11.410821+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/database_setup_first_use.py	2025-09-21 17:07:34.535018+00:00
@@ -20,10 +20,11 @@
 import ast
 
 # Create a logger
 logger = logging.getLogger(__name__)
 
+
 class DjangoSetup:
     _initialized = False
 
     @classmethod
     def configure_django(cls):
@@ -31,38 +32,39 @@
         if cls._initialized:
             return
 
         try:
             # Set up Django settings module for birds_nest in parent directory
-            project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '../..'))
+            project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "../.."))
             sys.path.insert(0, project_root)
-            os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'birds_nest.settings')
+            os.environ.setdefault("DJANGO_SETTINGS_MODULE", "birds_nest.settings")
 
             # This allows us to use Django models without running the server
             django.setup()
 
-            logger.info("Django configured successfully with settings module: %s",
-                       os.environ['DJANGO_SETTINGS_MODULE'])
+            logger.info("Django configured successfully with settings module: %s", os.environ["DJANGO_SETTINGS_MODULE"])
             cls._initialized = True
         except Exception as e:
             logger.error(f"Django configuration failed: {str(e)}")
             raise
 
+
 class RunDatabaseSetup(AppConfig):
     DjangoSetup.configure_django()
     from pybirdai.entry_points.create_django_models import RunCreateDjangoModels
-    app_config = RunCreateDjangoModels('pybirdai', 'birds_nest')
+
+    app_config = RunCreateDjangoModels("pybirdai", "birds_nest")
     app_config.ready()
 
     # File paths - Define paths relative to where the script is executed
     initial_migration_file = "pybirdai/migrations/0001_initial.py"
     db_file = "db.sqlite3"
     pybirdai_admin_path = "pybirdai/admin.py"
     pybirdai_meta_data_model_path = "pybirdai/bird_meta_data_model.py"
     results_admin_path = "results/database_configuration_files/admin.py"
-    pybirdai_models_path = "pybirdai/bird_data_model.py" # Target file
-    results_models_path = "results/database_configuration_files/models.py" # Source file
+    pybirdai_models_path = "pybirdai/bird_data_model.py"  # Target file
+    results_models_path = "results/database_configuration_files/models.py"  # Source file
 
     # --- Cleanup steps ---
 
     # Remove initial migration file - Strict interpretation: must exist to remove
     logger.info(f"Attempting to remove initial migration file: {initial_migration_file}")
@@ -72,11 +74,10 @@
             logger.info(f"Successfully removed {initial_migration_file}")
         except OSError as e:
             logger.error(f"Error removing file {initial_migration_file}: {e}")
             # Raising specific error for removal failure
             raise RuntimeError(f"Failed to remove file {initial_migration_file}") from e
-
 
     # Remove database file - Strict interpretation: must exist to remove
     logger.info(f"Attempting to remove database file: {db_file}")
     if os.path.exists(db_file):
         try:
@@ -123,18 +124,19 @@
     except IOError as e:
         logger.error(f"Error reading or writing files for {pybirdai_admin_path} update: {e}")
         # Raising specific error for file operation failure
         raise RuntimeError(f"Failed to update {pybirdai_admin_path}") from e
 
-
     # --- Update bird_data_model.py (models.py) ---
 
     logger.info(f"Updating {pybirdai_models_path}...")
     # Check if results file for reading (results/models.py) exists
     if not os.path.exists(results_models_path):
         logger.error(f"Results models file not found: {results_models_path}")
-        raise FileNotFoundError(f"Source file '{results_models_path}' not found. Cannot read results models.py content.")
+        raise FileNotFoundError(
+            f"Source file '{results_models_path}' not found. Cannot read results models.py content."
+        )
 
     try:
         # Read content from results file
         with open(results_models_path, "r") as f_read_results:
             results_models_str = f_read_results.read()
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/clone_mode/clone_mode_column_index.py
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/database_setup_first_use.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/clone_repo_service.py	2025-09-15 13:18:11.410467+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/clone_repo_service.py	2025-09-21 17:07:34.544657+00:00
@@ -19,15 +19,12 @@
 import time
 
 # Set up logging configuration to write to both file and console
 logging.basicConfig(
     level=logging.INFO,
-    format='%(asctime)s - %(levelname)s - %(message)s',
-    handlers=[
-        logging.FileHandler('clone_repo_setup.log'),
-        logging.StreamHandler()
-    ]
+    format="%(asctime)s - %(levelname)s - %(message)s",
+    handlers=[logging.FileHandler("clone_repo_setup.log"), logging.StreamHandler()],
 )
 logger = logging.getLogger(__name__)
 
 # Base directory path for the birds_nest project
 BASE = f"birds_nest{os.sep}"
@@ -36,36 +33,36 @@
 # should be copied to target folders, with optional file filtering functions
 REPO_MAPPING = {
     # Database export files with specific filtering rules
     f"export{os.sep}database_export_ldm": {
         f"resources{os.sep}admin": (lambda file: file.startswith("auth_")),  # Only auth-related files
-        f"resources{os.sep}bird": (lambda file: file.startswith("bird_")),   # Only bird-related files
-        f"resources{os.sep}technical_export": (lambda file: True)            # All files
+        f"resources{os.sep}bird": (lambda file: file.startswith("bird_")),  # Only bird-related files
+        f"resources{os.sep}technical_export": (lambda file: True),  # All files
     },
     # Join configuration files
     "joins_configuration": {
-        f"resources{os.sep}joins_configuration": (lambda file: True),        # All files
+        f"resources{os.sep}joins_configuration": (lambda file: True),  # All files
     },
     # Initial correction files
     f"birds_nest{os.sep}resources{os.sep}extra_variables": {
-        f"resources{os.sep}extra_variables": (lambda file: True),            # All files
+        f"resources{os.sep}extra_variables": (lambda file: True),  # All files
     },
     # Derivation files from birds_nest resources
     f"birds_nest{os.sep}resources{os.sep}derivation_files": {
-        f"resources{os.sep}derivation_files": (lambda file: True),           # All files
+        f"resources{os.sep}derivation_files": (lambda file: True),  # All files
     },
     # LDM (Logical Data Model) files from birds_nest resources
     f"birds_nest{os.sep}resources{os.sep}ldm": {
-        f"resources{os.sep}ldm": (lambda file: True),                        # All files
+        f"resources{os.sep}ldm": (lambda file: True),  # All files
     },
     # Test files from birds_nest
     f"birds_nest{os.sep}tests": {
-        "tests": (lambda file: True),                                        # All files
+        "tests": (lambda file: True),  # All files
     },
     # Additional mapping for IL files
     f"birds_nest{os.sep}resources{os.sep}il": {
-        f"resources{os.sep}il": (lambda file: True),                         # All files
+        f"resources{os.sep}il": (lambda file: True),  # All files
     },
     # Filter code files
     f"birds_nest{os.sep}pybirdai{os.sep}process_steps{os.sep}filter_code": {
         f"pybirdai{os.sep}process_steps{os.sep}filter_code": (lambda file: True),  # Only Python files
     },
@@ -73,12 +70,13 @@
     f"birds_nest{os.sep}results{os.sep}generated_python_filters": {
         f"results{os.sep}generated_python_filters": (lambda file: True),
     },
     f"birds_nest{os.sep}results{os.sep}generated_python_joins": {
         f"results{os.sep}generated_python_joins": (lambda file: True),
-    }
+    },
 }
+
 
 class CloneRepoService:
     """
     Enhanced service class for cloning a repository and setting up files according to the mapping configuration.
     Handles downloading, extracting, organizing files, cleanup operations, and supports authentication.
@@ -106,11 +104,11 @@
 
     def _get_authenticated_headers(self):
         """Get headers with authentication if token is provided."""
         headers = {}
         if self.token:
-            headers['Authorization'] = f'Bearer {self.token}'
+            headers["Authorization"] = f"Bearer {self.token}"
         return headers
 
     def _ensure_directory_exists(self, path):
         """Ensure a directory exists, creating it if necessary."""
         os.makedirs(path, exist_ok=True)
@@ -140,11 +138,16 @@
                     logger.error(f"Failed to delete {file_path}: {e}")
             logger.info(f"Cleared directory: {path}")
         else:
             logger.debug(f"Directory does not exist, skipping clear: {path}")
 
-    def clone_repo(self, base_url:str="https://github.com/regcommunity/FreeBIRD", destination_path: str = "FreeBIRD", branch: str = "main"):
+    def clone_repo(
+        self,
+        base_url: str = "https://github.com/regcommunity/FreeBIRD",
+        destination_path: str = "FreeBIRD",
+        branch: str = "main",
+    ):
         """
         Download and extract a repository from GitHub as a ZIP file.
 
         Args:
             base_url (str): The base URL of the GitHub repository
@@ -210,11 +213,10 @@
             logger.error("Could not find extracted repository folder")
             print("Could not find extracted repository folder")
             return
 
         logger.info(f"Found extracted folder: {extracted_folder}")
-
 
         # Process each mapping in REPO_MAPPING
         for source_folder, target_mappings in REPO_MAPPING.items():
             source_path = os.path.join(extracted_folder, source_folder)
             logger.debug(f"Processing source folder: {source_path}")
@@ -269,25 +271,27 @@
         logger.info(f"Removing fetched files from {destination_path}")
         shutil.rmtree(destination_path)
         end_time = time.time()
         logger.info(f"Fetched files removed in {end_time - start_time:.2f} seconds")
 
+
 def main():
     """
     Main function that orchestrates the complete repository cloning and setup process.
     Executes clone, setup, and cleanup operations in sequence.
     """
     start_time = time.time()
     logger.info("Starting CloneRepoService execution")
 
     # Create service instance and execute the complete workflow
     service = CloneRepoService()
-    service.clone_repo()        # Download and extract repository
-    service.setup_files()       # Organize files according to mapping
+    service.clone_repo()  # Download and extract repository
+    service.setup_files()  # Organize files according to mapping
     service.remove_fetched_files()  # Clean up downloaded files
 
     end_time = time.time()
     logger.info(f"CloneRepoService execution completed in {end_time - start_time:.2f} seconds")
+
 
 # Entry point for script execution
 if __name__ == "__main__":
     main()
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/clone_repo_service.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/logger_factory.py	2025-08-02 18:37:08.468485+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/logger_factory.py	2025-09-21 17:07:34.562194+00:00
@@ -12,14 +12,13 @@
 #
 
 import logging
 from pathlib import Path
 
-def return_logger(__file_name__:str):
+
+def return_logger(__file_name__: str):
     return logging.getLogger(__file_name__)
 
+
 # Set up logging
-logging.basicConfig(
-    level=logging.INFO,
-    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
-)
+logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s")
 logger = logging.getLogger()
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/datapoint_test_run/parser_for_tests.py	2025-08-02 18:37:08.467334+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/datapoint_test_run/parser_for_tests.py	2025-09-21 17:07:34.562409+00:00
@@ -14,18 +14,17 @@
 import json
 import sys
 import logging
 from pathlib import Path
 
-def return_logger(__file_name__:str):
+
+def return_logger(__file_name__: str):
     return logging.getLogger(__file_name__)
 
+
 # Set up logging
-logging.basicConfig(
-    level=logging.INFO,
-    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
-)
+logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s")
 logger = logging.getLogger()
 from datetime import datetime
 
 FAILURE_STRING = "=================================== FAILURES ==================================="
 
@@ -65,31 +64,19 @@
             "timestamp": datetime.now().isoformat(),
             "test_information": {
                 "datapoint_value": self.dp_value,
                 "regulatory_template_id": self.reg_tid,
                 "datapoint_suffix": self.dp_suffix,
-                "scenario_name": self.scenario_name
+                "scenario_name": self.scenario_name,
             },
-            'platform_info': {
-                "os": "",
-                "packages": "",
-                "python": ""
-            },
-            'paths': {
-                "cachedir": "",
-                "rootdir": "",
-                "configfile": ""
-            },
-            'test_results': {
+            "platform_info": {"os": "", "packages": "", "python": ""},
+            "paths": {"cachedir": "", "rootdir": "", "configfile": ""},
+            "test_results": {
                 "passed": [],
                 "failed": [],
-                "details": {
-                    "failures": [],
-                    "captured_stdout": [],
-                    "captured_stderr": []
-                }
-            }
+                "details": {"failures": [], "captured_stdout": [], "captured_stderr": []},
+            },
         }
 
     def read_output_file(self):
         """
         Read the pytest output file content.
@@ -97,11 +84,11 @@
         Raises:
             Exception: If the file cannot be read
         """
         logger.debug("Reading pytest output file")
         try:
-            with open(self.output_file_path, 'r') as f:
+            with open(self.output_file_path, "r") as f:
                 self.output_content = f.read()
         except Exception as e:
             logger.error(f"Error reading output file: {str(e)}")
             raise
 
@@ -113,17 +100,16 @@
             line (str): The line containing platform information
 
         Returns:
             bool: True if platform info was parsed, False otherwise
         """
-        if line.startswith('platform'):
+        if line.startswith("platform"):
             try:
-                platform_parts = line.split('--')
-                self.result['platform_info']['os'] = platform_parts[0].replace('platform', '').strip()
-                self.result['platform_info']['python'] = platform_parts[2].strip()
-                self.result['platform_info']['packages'] = list(map(str.strip,
-                    platform_parts[1].split(',')))
+                platform_parts = line.split("--")
+                self.result["platform_info"]["os"] = platform_parts[0].replace("platform", "").strip()
+                self.result["platform_info"]["python"] = platform_parts[2].strip()
+                self.result["platform_info"]["packages"] = list(map(str.strip, platform_parts[1].split(",")))
             except Exception as e:
                 logger.error(f"Error parsing platform info: {str(e)}")
             return True
         return False
 
@@ -135,19 +121,15 @@
             line (str): The line containing path information
 
         Returns:
             bool: True if path info was parsed, False otherwise
         """
-        path_prefixes = {
-            'cachedir:': 'cachedir',
-            'rootdir:': 'rootdir',
-            'configfile:': 'configfile'
-        }
+        path_prefixes = {"cachedir:": "cachedir", "rootdir:": "rootdir", "configfile:": "configfile"}
 
         for prefix, key in path_prefixes.items():
             if line.startswith(prefix):
-                self.result['paths'][key] = line.replace(prefix, '').strip()
+                self.result["paths"][key] = line.replace(prefix, "").strip()
                 return True
         return False
 
     def _parse_test_result(self, line):
         """
@@ -157,13 +139,13 @@
             line (str): The line containing test result information
 
         Returns:
             bool: True if test result was parsed, False otherwise
         """
-        if '::' in line:
-            test_name = line.split('::')[1].split()[0]
-            status_mapping = {k: self.result['test_results'][k.lower()] for k in ["PASSED", "FAILED"]}
+        if "::" in line:
+            test_name = line.split("::")[1].split()[0]
+            status_mapping = {k: self.result["test_results"][k.lower()] for k in ["PASSED", "FAILED"]}
 
             for status, test_list in status_mapping.items():
                 if status in line and test_name not in test_list:
                     test_list.append(test_name)
                     logger.debug(f"Test {test_name} {status.lower()}")
@@ -180,23 +162,23 @@
 
         Returns:
             dict: Dictionary of failure details by test name
         """
         logger.debug("Processing test failures")
-        failures = {k: [] for k in self.result['test_results']['failed']}
+        failures = {k: [] for k in self.result["test_results"]["failed"]}
         i = start_index + 1
         key_to_assign = ""
 
         while i < len(lines):
             line = lines[i].strip()
-            for test_name in self.result['test_results']['failed']:
+            for test_name in self.result["test_results"]["failed"]:
                 if f"def {test_name}" in line:
                     key_to_assign = test_name
 
-            if line.startswith('E '):
-                failures[key_to_assign].append(line.replace('E ', ''))
-            elif '====' in line:  # End of failures section
+            if line.startswith("E "):
+                failures[key_to_assign].append(line.replace("E ", ""))
+            elif "====" in line:  # End of failures section
                 break
             i += 1
 
         return failures
 
@@ -208,11 +190,11 @@
             str: JSON string of the parsed result
         """
         if not self.output_content:
             self.read_output_file()
 
-        lines = self.output_content.split('\n')
+        lines = self.output_content.split("\n")
 
         logger.debug("Processing test output lines")
         for index, line in enumerate(lines):
             line = line.strip()
 
@@ -230,11 +212,11 @@
             self._parse_test_result(line)
 
             # Parse failure details
             if line == FAILURE_STRING:
                 failures = self._parse_failure_details(lines, index)
-                self.result['test_results']['details']['failures'] = failures
+                self.result["test_results"]["details"]["failures"] = failures
 
         logger.debug("Finished parsing pytest output")
         return json.dumps(self.result, indent=2)
 
 
@@ -265,7 +247,7 @@
     except Exception as e:
         logger.error(f"Error running script: {str(e)}")
         raise
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     main()
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/logger_factory.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/licensing_code.py	2025-09-15 13:18:11.412617+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/licensing_code.py	2025-09-21 17:07:34.564085+00:00
@@ -52,11 +52,11 @@
 for path in save_path_for_correction:
     with open(path) as f:
         data = f.read()
         file_data = bss_line_comment_python + "\n" + data
 
-    with open(path,"w") as f:
+    with open(path, "w") as f:
         f.write(file_data)
 
 html_line_comment = """<!--
 # coding=UTF-8
 # Copyright (c) 2025 Bird Software Solutions Ltd
@@ -85,7 +85,7 @@
 for path in save_path_for_correction:
     with open(path) as f:
         data = f.read()
         file_data = html_line_comment + "\n" + data
 
-    with open(path,"w") as f:
+    with open(path, "w") as f:
         f.write(file_data)
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/datapoint_test_run/parser_for_tests.py
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/licensing_code.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/export_db.py	2025-09-18 09:56:06.416286+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/export_db.py	2025-09-21 17:07:34.601959+00:00
@@ -22,57 +22,60 @@
 import traceback
 
 # Configure logging
 logging.basicConfig(
     level=logging.INFO,
-    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
-    handlers=[
-        logging.FileHandler("visualization_service.log"),
-        logging.StreamHandler()
-    ]
+    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
+    handlers=[logging.FileHandler("visualization_service.log"), logging.StreamHandler()],
 )
 
 logger = logging.getLogger(__name__)
+
 
 class DjangoSetup:
     @staticmethod
     def configure_django():
         """Configure Django settings without starting the application"""
         if not settings.configured:
             # Set up Django settings module for birds_nest in parent directory
-            project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '../..'))
+            project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "../.."))
             sys.path.insert(0, project_root)
-            os.environ['DJANGO_SETTINGS_MODULE'] = 'birds_nest.settings'
-            logger.info("Configuring Django with settings module: %s", os.environ['DJANGO_SETTINGS_MODULE'])
+            os.environ["DJANGO_SETTINGS_MODULE"] = "birds_nest.settings"
+            logger.info("Configuring Django with settings module: %s", os.environ["DJANGO_SETTINGS_MODULE"])
             django.setup()
             logger.debug("Django setup complete")
+
 
 def _export_database_to_csv_logic():
     import re
     from pybirdai.models import bird_meta_data_model
     from pybirdai.models import bird_data_model
     from django.db import transaction, connection
     from django.http import HttpResponse, JsonResponse, HttpResponseBadRequest
+
     def clean_whitespace(text):
-        return re.sub(r'\s+', ' ', str(text).replace('\r', '').replace('\n', ' ')) if text else text
+        return re.sub(r"\s+", " ", str(text).replace("\r", "").replace("\n", " ")) if text else text
+
     # Create a zip file path in results directory
-    results_dir = os.path.join(settings.BASE_DIR, 'results')
+    results_dir = os.path.join(settings.BASE_DIR, "results")
     os.makedirs(results_dir, exist_ok=True)
-    zip_file_path = os.path.join(results_dir, 'database_export.zip')
+    zip_file_path = os.path.join(results_dir, "database_export.zip")
 
     # Get all model classes from bird_meta_data_model
     valid_table_names = set()
     model_map = {}  # Store model classes for reference
     for name, obj in inspect.getmembers(bird_meta_data_model):
         if inspect.isclass(obj) and issubclass(obj, models.Model) and obj != models.Model:
             valid_table_names.add(obj._meta.db_table)
             model_map[obj._meta.db_table] = obj
 
-    with zipfile.ZipFile(zip_file_path, 'w') as zip_file:
+    with zipfile.ZipFile(zip_file_path, "w") as zip_file:
         # Get all table names from SQLite and sort them
         with connection.cursor() as cursor:
-            cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name NOT LIKE 'sqlite_%' AND name NOT LIKE 'django_%' ORDER BY name")
+            cursor.execute(
+                "SELECT name FROM sqlite_master WHERE type='table' AND name NOT LIKE 'sqlite_%' AND name NOT LIKE 'django_%' ORDER BY name"
+            )
             tables = cursor.fetchall()
 
         # Export each table to a CSV file
         for table in tables:
             is_meta_data_table = False
@@ -82,27 +85,27 @@
                 is_meta_data_table = True
                 # Get the model class for this table
                 model_class = model_map[table_name]
 
                 # Check if model has an explicit primary key
-                has_explicit_pk = any(field.primary_key for field in model_class._meta.fields if field.name != 'id')
+                has_explicit_pk = any(field.primary_key for field in model_class._meta.fields if field.name != "id")
 
                 # Get fields in the order they're defined in the model
                 fields = model_class._meta.fields
                 headers = []
                 db_headers = []
 
                 # If model uses Django's auto ID and has no explicit PK, include the ID field
                 if not has_explicit_pk:
-                    headers.append('ID')
-                    db_headers.append('id')
+                    headers.append("ID")
+                    db_headers.append("id")
 
                 for field in fields:
                     # Skip the id field if we already added it or if there's an explicit PK
-                    if field.name == 'id' and has_explicit_pk:
+                    if field.name == "id" and has_explicit_pk:
                         continue
-                    elif field.name == 'id' and not has_explicit_pk:
+                    elif field.name == "id" and not has_explicit_pk:
                         # We already added it above
                         continue
                     headers.append(field.name.upper())  # Convert header to uppercase
                     # If it's a foreign key, append _id for the actual DB column
                     if isinstance(field, models.ForeignKey):
@@ -110,15 +113,15 @@
                     else:
                         db_headers.append(field.name)
 
                 # Create CSV in memory
                 csv_content = []
-                csv_content.append(','.join(headers))
+                csv_content.append(",".join(headers))
 
                 # Get data with escaped column names and ordered by primary key
                 with connection.cursor() as cursor:
-                    escaped_headers = [f'"{h}"' if h == 'order' else h for h in db_headers]
+                    escaped_headers = [f'"{h}"' if h == "order" else h for h in db_headers]
                     # Get primary key column name - validate table name against our whitelist
                     if table_name not in valid_table_names:
                         continue
                     # Use parameterized query for table info - note: SQLite PRAGMA doesn't support parameters
                     # but we validate table_name against valid_table_names whitelist above
@@ -134,11 +137,11 @@
                     # Build ORDER BY clause - handle composite keys and sort by all columns for consistency
                     if pk_columns:
                         order_by = f"ORDER BY {', '.join(pk_columns)}"
                     else:
                         # If no primary key, sort by id if it exists, otherwise by all columns
-                        if 'id' in db_headers:
+                        if "id" in db_headers:
                             order_by = "ORDER BY id"
                         else:
                             order_by = f"ORDER BY {', '.join(escaped_headers)}"
 
                     # Build the query - table_name is already validated against whitelist
@@ -147,20 +150,20 @@
                     cursor.execute(query)
                     rows = cursor.fetchall()
 
                     for row in rows:
                         # Convert all values to strings and handle None values
-                        csv_row = [str(clean_whitespace(val)) if val is not None else '' for val in row]
+                        csv_row = [str(clean_whitespace(val)) if val is not None else "" for val in row]
                         # Escape commas and quotes in values
                         processed_row = []
                         for val in csv_row:
-                            if ',' in val or '"' in val:
+                            if "," in val or '"' in val:
                                 escaped_val = val.replace('"', '""')
                                 processed_row.append(f'"{escaped_val}"')
                             else:
                                 processed_row.append(val)
-                        csv_content.append(','.join(processed_row))
+                        csv_content.append(",".join(processed_row))
             else:
                 continue
             #     # Fallback for tables without models : does not work because of Aorta Models
             #     with connection.cursor() as cursor:
             #         # Get column names
@@ -196,21 +199,22 @@
             #                     processed_row.append(val)
             #             csv_content.append(','.join(processed_row))
 
             # Add CSV to zip file
             if is_meta_data_table:
-                zip_file.writestr(f"{table_name.replace('pybirdai_', '')}.csv", '\n'.join(csv_content))
+                zip_file.writestr(f"{table_name.replace('pybirdai_', '')}.csv", "\n".join(csv_content))
             else:
-                zip_file.writestr(f"{table_name.replace('pybirdai_', 'bird_')}.csv", '\n'.join(csv_content))
+                zip_file.writestr(f"{table_name.replace('pybirdai_', 'bird_')}.csv", "\n".join(csv_content))
 
     # Unzip the file in the database_export folder
-    extract_dir = os.path.join(results_dir, 'database_export')
+    extract_dir = os.path.join(results_dir, "database_export")
     os.makedirs(extract_dir, exist_ok=True)
 
-    with zipfile.ZipFile(zip_file_path, 'r') as zip_file:
+    with zipfile.ZipFile(zip_file_path, "r") as zip_file:
         zip_file.extractall(extract_dir)
 
     return zip_file_path, extract_dir
 
-if __name__ == '__main__':
+
+if __name__ == "__main__":
     DjangoSetup.configure_django()
     _export_database_to_csv_logic()
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/export_db.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/member_hierarchy_editor/django_hierarchy_integration.py	2025-09-15 13:18:11.413071+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/member_hierarchy_editor/django_hierarchy_integration.py	2025-09-21 17:07:34.635433+00:00
@@ -10,25 +10,30 @@
 # Contributors:
 #    Benjamin Arfa - initial API and implementation
 #
 import json
 from typing import Dict, List
-from ...models.bird_meta_data_model import (
-    MEMBER, MEMBER_HIERARCHY, DOMAIN
-)
+from ...models.bird_meta_data_model import MEMBER, MEMBER_HIERARCHY, DOMAIN
 from .django_model_converter import DjangoModelConverter
 
 BOX_WIDTH = 300
 BOX_HEIGHT = 120
 
 
 class HierarchyNodeDTO:
     """Data Transfer Object for hierarchy nodes"""
 
-    def __init__(self, id_: str = "My Identifier", x: int = 0, y: int = 0,
-                 width: int = BOX_WIDTH, height: int = BOX_HEIGHT,
-                 name: str = "My Name", text: str = "My Description"):
+    def __init__(
+        self,
+        id_: str = "My Identifier",
+        x: int = 0,
+        y: int = 0,
+        width: int = BOX_WIDTH,
+        height: int = BOX_HEIGHT,
+        name: str = "My Name",
+        text: str = "My Description",
+    ):
         self.id_ = id_
         self.x = x
         self.y = y
         self.width = width
         self.height = height
@@ -37,30 +42,30 @@
 
     @classmethod
     def from_dict(cls, dict_data):
         """Create HierarchyNodeDTO from dictionary data"""
         return cls(
-            id_=str(dict_data.get('id', '')),
-            x=dict_data.get('x', 0),
-            y=dict_data.get('y', 0),
-            width=dict_data.get('width', BOX_WIDTH),
-            height=dict_data.get('height', BOX_HEIGHT),
-            name=dict_data.get('name', ''),
-            text=dict_data.get('text', '')
+            id_=str(dict_data.get("id", "")),
+            x=dict_data.get("x", 0),
+            y=dict_data.get("y", 0),
+            width=dict_data.get("width", BOX_WIDTH),
+            height=dict_data.get("height", BOX_HEIGHT),
+            name=dict_data.get("name", ""),
+            text=dict_data.get("text", ""),
         )
 
     @property
     def to_dict(self) -> dict:
         """Convert to dictionary format expected by visualization tool"""
         return {
-            'id': self.id_,
-            'x': self.x,
-            'y': self.y,
-            'width': self.width,
-            'height': self.height,
-            'name': self.name,
-            'text': self.text
+            "id": self.id_,
+            "x": self.x,
+            "y": self.y,
+            "width": self.width,
+            "height": self.height,
+            "name": self.name,
+            "text": self.text,
         }
 
     @property
     def to_json(self) -> str:
         return json.dumps(self.to_dict)
@@ -69,27 +74,21 @@
 class HierarchyArrowDTO:
     """Data Transfer Object for hierarchy arrows"""
 
     def __init__(self, from_: str = "", to_: str = ""):
         self.from_ = from_  # child
-        self.to_ = to_      # parent
+        self.to_ = to_  # parent
 
     @classmethod
     def from_dict(cls, dict_data):
         """Create HierarchyArrowDTO from dictionary data"""
-        return cls(
-            from_=str(dict_data.get("from", "")),
-            to_=str(dict_data.get("to", ""))
-        )
+        return cls(from_=str(dict_data.get("from", "")), to_=str(dict_data.get("to", "")))
 
     @property
     def to_dict(self) -> dict:
         """Convert to dictionary format expected by visualization tool"""
-        return {
-            "from": self.from_,
-            "to": self.to_
-        }
+        return {"from": self.from_, "to": self.to_}
 
     @property
     def to_json(self) -> str:
         return json.dumps(self.to_dict)
 
@@ -108,13 +107,11 @@
         """Get all hierarchies as a dictionary"""
         all_hierarchies = {}
         hierarchies = MEMBER_HIERARCHY.objects.all()
 
         for hierarchy in hierarchies:
-            all_hierarchies[hierarchy.member_hierarchy_id] = self.get_hierarchy_by_id(
-                hierarchy.member_hierarchy_id
-            )
+            all_hierarchies[hierarchy.member_hierarchy_id] = self.get_hierarchy_by_id(hierarchy.member_hierarchy_id)
 
         return all_hierarchies
 
     def get_hierarchies_by_domain(self, domain_id: str) -> Dict[str, Dict]:
         """Get all hierarchies for a specific domain"""
@@ -122,13 +119,11 @@
             domain = DOMAIN.objects.get(domain_id=domain_id)
             hierarchies = MEMBER_HIERARCHY.objects.filter(domain_id=domain)
 
             result = {}
             for hierarchy in hierarchies:
-                result[hierarchy.member_hierarchy_id] = self.get_hierarchy_by_id(
-                    hierarchy.member_hierarchy_id
-                )
+                result[hierarchy.member_hierarchy_id] = self.get_hierarchy_by_id(hierarchy.member_hierarchy_id)
 
             return result
         except DOMAIN.DoesNotExist:
             return {}
 
@@ -142,19 +137,19 @@
 
         return self.converter.visualization_to_django_nodes(hierarchy_id, visualization_data)
 
     def get_available_hierarchies(self) -> List[Dict]:
         """Get a summary of all available hierarchies"""
-        hierarchies = MEMBER_HIERARCHY.objects.select_related('domain_id').all()
+        hierarchies = MEMBER_HIERARCHY.objects.select_related("domain_id").all()
 
         return [
             {
-                'member_hierarchy_id': h.member_hierarchy_id,
-                'name': h.name,
-                'domain_id': h.domain_id.domain_id if h.domain_id else None,
-                'description': h.description,
-                'is_main_hierarchy': getattr(h, 'is_main_hierarchy', False)
+                "member_hierarchy_id": h.member_hierarchy_id,
+                "name": h.name,
+                "domain_id": h.domain_id.domain_id if h.domain_id else None,
+                "description": h.description,
+                "is_main_hierarchy": getattr(h, "is_main_hierarchy", False),
             }
             for h in hierarchies
         ]
 
     def get_domain_members(self, domain_id: str) -> List[Dict]:
@@ -163,14 +158,14 @@
             domain = DOMAIN.objects.get(domain_id=domain_id)
             members = MEMBER.objects.filter(domain_id=domain)
 
             return [
                 {
-                    'member_id': m.member_id,
-                    'name': m.name or m.member_id,
-                    'code': m.code or m.member_id,
-                    'description': m.description or ""
+                    "member_id": m.member_id,
+                    "name": m.name or m.member_id,
+                    "code": m.code or m.member_id,
+                    "description": m.description or "",
                 }
                 for m in members
             ]
         except DOMAIN.DoesNotExist:
             return []
@@ -194,17 +189,10 @@
     integration = get_hierarchy_integration()
 
     # Validate structure first
     is_valid, errors = integration.converter.validate_hierarchy_structure(visualization_data)
     if not is_valid:
-        return {
-            'success': False,
-            'message': 'Hierarchy validation failed',
-            'errors': errors
-        }
+        return {"success": False, "message": "Hierarchy validation failed", "errors": errors}
 
     success = integration.save_hierarchy_from_visualization(hierarchy_id, visualization_data)
 
-    return {
-        'success': success,
-        'message': 'Hierarchy saved successfully' if success else 'Failed to save hierarchy'
-    }
+    return {"success": success, "message": "Hierarchy saved successfully" if success else "Failed to save hierarchy"}
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/member_hierarchy_editor/django_hierarchy_integration.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/derived_fields_extractor.py	2025-09-18 09:56:06.416277+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/derived_fields_extractor.py	2025-09-21 17:07:34.651631+00:00
@@ -27,13 +27,11 @@
     @staticmethod
     def configure_django():
         """Configure Django settings without starting the application"""
         if not settings.configured:
             # Set up Django settings module for birds_nest in parent directory
-            project_root = os.path.abspath(
-                os.path.join(os.path.dirname(__file__), "../..")
-            )
+            project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "../.."))
             sys.path.insert(0, project_root)
             os.environ["DJANGO_SETTINGS_MODULE"] = "birds_nest.settings"
             django.setup()
 
 
@@ -60,29 +58,17 @@
                 if isinstance(item, ast.FunctionDef):
                     # Check if it has decorators
                     for decorator in item.decorator_list:
                         # Check for lineage decorator
                         if (
-                            (
-                                isinstance(decorator, ast.Name)
-                                and decorator.id == "lineage"
-                            )
-                            or (
-                                isinstance(decorator, ast.Attribute)
-                                and decorator.attr == "lineage"
-                            )
+                            (isinstance(decorator, ast.Name) and decorator.id == "lineage")
+                            or (isinstance(decorator, ast.Attribute) and decorator.attr == "lineage")
                             or (
                                 isinstance(decorator, ast.Call)
                                 and (
-                                    (
-                                        isinstance(decorator.func, ast.Name)
-                                        and decorator.func.id == "lineage"
-                                    )
-                                    or (
-                                        isinstance(decorator.func, ast.Attribute)
-                                        and decorator.func.attr == "lineage"
-                                    )
+                                    (isinstance(decorator.func, ast.Name) and decorator.func.id == "lineage")
+                                    or (isinstance(decorator.func, ast.Attribute) and decorator.func.attr == "lineage")
                                 )
                             )
                         ):
                             lineage_properties.append(item.name)
                             break
@@ -102,13 +88,11 @@
     """Generate AST representation of the extracted classes"""
     DjangoSetup.configure_django()
 
     # Create module node
     module = ast.Module(body=[], type_ignores=[])
-    import_sttmt_1 = ast.ImportFrom(
-        module="django.db", names=[ast.alias(name="models")], level=0
-    )
+    import_sttmt_1 = ast.ImportFrom(module="django.db", names=[ast.alias(name="models")], level=0)
     import_sttmt_2 = ast.ImportFrom(
         module="pybirdai.annotations.decorators",
         names=[ast.alias(name="lineage")],
         level=0,
     )
@@ -164,21 +148,17 @@
     """Check if the file already has lineage imports and decorators"""
     with open(file_path, "r") as f:
         content = f.read()
 
     # Check for lineage import and @lineage decorator
-    has_lineage_import = (
-        "from pybirdai.annotations.decorators import lineage" in content
-    )
+    has_lineage_import = "from pybirdai.annotations.decorators import lineage" in content
     has_lineage_decorator = "@lineage" in content
 
     return has_lineage_import and has_lineage_decorator
 
 
-def merge_derived_fields_into_original_model(
-    bird_data_model_path, lineage_classes_ast_path
-):
+def merge_derived_fields_into_original_model(bird_data_model_path, lineage_classes_ast_path):
     """
     Merge derived fields from derived_field_configuration.py into the original bird_data_model.py.
 
     This function:
     1. Checks if the original file has already been modified (has @lineage imports/decorators)
@@ -196,13 +176,11 @@
     """
     logger = logging.getLogger(__name__)
 
     # Check if file has already been modified
     if check_if_file_already_modified(bird_data_model_path):
-        logger.info(
-            "File already contains @lineage decorators and imports, skipping modification"
-        )
+        logger.info("File already contains @lineage decorators and imports, skipping modification")
         return False
 
     # Parse both files
     with open(bird_data_model_path, "r") as f:
         original_content = f.read()
@@ -222,29 +200,17 @@
             # Find all properties with @lineage decorator
             for item in node.body:
                 if isinstance(item, ast.FunctionDef):
                     for decorator in item.decorator_list:
                         if (
-                            (
-                                isinstance(decorator, ast.Name)
-                                and decorator.id == "lineage"
-                            )
-                            or (
-                                isinstance(decorator, ast.Attribute)
-                                and decorator.attr == "lineage"
-                            )
+                            (isinstance(decorator, ast.Name) and decorator.id == "lineage")
+                            or (isinstance(decorator, ast.Attribute) and decorator.attr == "lineage")
                             or (
                                 isinstance(decorator, ast.Call)
                                 and (
-                                    (
-                                        isinstance(decorator.func, ast.Name)
-                                        and decorator.func.id == "lineage"
-                                    )
-                                    or (
-                                        isinstance(decorator.func, ast.Attribute)
-                                        and decorator.func.attr == "lineage"
-                                    )
+                                    (isinstance(decorator.func, ast.Name) and decorator.func.id == "lineage")
+                                    or (isinstance(decorator.func, ast.Attribute) and decorator.func.attr == "lineage")
                                 )
                             )
                         ):
                             derived_properties.append(item)
                             break
@@ -353,13 +319,11 @@
     #     f.write(ast.unparse(ast_module))
 
     # print(f"Extracted {len(lineage_classes)} classes with lineage properties")
     print("Output written to derived_field_configuration.py")
     model_file_path = f"pybirdai{os.sep}models{os.sep}bird_data_model.py"
-    derived_fields_file_path = (
-        f"resources{os.sep}derivation_files{os.sep}derived_field_configuration.py"
-    )
+    derived_fields_file_path = f"resources{os.sep}derivation_files{os.sep}derived_field_configuration.py"
 
     merge_derived_fields_into_original_model(model_file_path, derived_fields_file_path)
 
 
 if __name__ == "__main__":
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/derived_fields_extractor.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/datapoint_test_run/run_tests.py	2025-09-15 13:18:11.411451+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/datapoint_test_run/run_tests.py	2025-09-21 17:07:34.671361+00:00
@@ -26,15 +26,15 @@
 import logging
 from pathlib import Path
 
 
 # Define safe directory for configuration files
-SAFE_CONFIG_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', '..'))
+SAFE_CONFIG_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..", ".."))
 
 # Force UTF-8 encoding for stdout on Windows to handle Unicode characters
-if sys.platform == 'win32':
-    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
+if sys.platform == "win32":
+    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding="utf-8")
 
 # Define constants
 # Base directories
 TESTS_DIR = "tests"
 PYBIRDAI_DIR = "pybirdai"
@@ -58,23 +58,23 @@
 DEFAULT_DP_VALUE = 83491250
 DEFAULT_REG_TID = "F_05_01_REF_FINREP_3_0"
 DEFAULT_DP_SUFFIX = "152589_REF"
 
 
-def return_logger(__file_name__:str):
+def return_logger(__file_name__: str):
     return logging.getLogger(__file_name__)
 
+
 # Set up logging
-logging.basicConfig(
-    level=logging.INFO,
-    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
-)
+logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s")
 logger = logging.getLogger()
+
 
 class FakeArgs(object):
     def __init__(self):
         pass
+
 
 class RegulatoryTemplateTestRunner:
     """
     A class to generate and run test code for regulatory templates.
 
@@ -88,23 +88,34 @@
     def __init__(self, parser_: bool = True):
         """Initialize the test runner with command line arguments."""
         # Set up command line argument parsing
         self.args = FakeArgs()
         if parser_:
-            self.parser = argparse.ArgumentParser(description='Generate and run test code for regulatory templates')
-            self.parser.add_argument('--uv', type=str, default=DEFAULT_UV,
-                        help=f'run with astral/uv as backend (default: {DEFAULT_UV})')
-            self.parser.add_argument('--dp-value', type=int, default=DEFAULT_DP_VALUE,
-                        help=f'Datapoint value to test (default: {DEFAULT_DP_VALUE})')
-            self.parser.add_argument('--reg-tid', type=str, default=DEFAULT_REG_TID,
-                        help=f'Regulatory template ID (default: {DEFAULT_REG_TID})')
-            self.parser.add_argument('--dp-suffix', type=str, default=DEFAULT_DP_SUFFIX,
-                        help=f'Suffix for datapoint and cell IDs (default: {DEFAULT_DP_SUFFIX})')
-            self.parser.add_argument('--config-file', type=str,
-                        help='JSON configuration file for multiple tests')
-            self.parser.add_argument('--scenario', type=str,
-                        help='Specific scenario to run (if not all scenarios)')
+            self.parser = argparse.ArgumentParser(description="Generate and run test code for regulatory templates")
+            self.parser.add_argument(
+                "--uv", type=str, default=DEFAULT_UV, help=f"run with astral/uv as backend (default: {DEFAULT_UV})"
+            )
+            self.parser.add_argument(
+                "--dp-value",
+                type=int,
+                default=DEFAULT_DP_VALUE,
+                help=f"Datapoint value to test (default: {DEFAULT_DP_VALUE})",
+            )
+            self.parser.add_argument(
+                "--reg-tid",
+                type=str,
+                default=DEFAULT_REG_TID,
+                help=f"Regulatory template ID (default: {DEFAULT_REG_TID})",
+            )
+            self.parser.add_argument(
+                "--dp-suffix",
+                type=str,
+                default=DEFAULT_DP_SUFFIX,
+                help=f"Suffix for datapoint and cell IDs (default: {DEFAULT_DP_SUFFIX})",
+            )
+            self.parser.add_argument("--config-file", type=str, help="JSON configuration file for multiple tests")
+            self.parser.add_argument("--scenario", type=str, help="Specific scenario to run (if not all scenarios)")
 
             self.args = self.parser.parse_args()
 
     def get_file_paths(self, reg_tid: str, dp_suffix: str) -> tuple:
         """
@@ -122,11 +133,13 @@
         result_filename = f"{timestamp}__test_results_{cell_class.lower()}"
         txt_path = os.path.join(TEST_RESULTS_TXT_FOLDER, result_filename)
         json_path = os.path.join(TEST_RESULTS_JSON_FOLDER, result_filename)
         return txt_path, json_path
 
-    def setup_subprocess_commands(self, use_uv: bool, scenario: str, dp_value: str, reg_tid: str, dp_suffix: str) -> tuple:
+    def setup_subprocess_commands(
+        self, use_uv: bool, scenario: str, dp_value: str, reg_tid: str, dp_suffix: str
+    ) -> tuple:
         """
         Configure subprocess commands for test execution.
 
         Args:
             use_uv: Whether to use UV as backend
@@ -142,25 +155,33 @@
 
         test_generation = subprocess_list.copy()
         test_runs = subprocess_list.copy()
         test_results_conversion = subprocess_list.copy()
 
-        test_generation.extend([
-            GENERATOR_FILE_PATH,
-            "--dp-value", str(dp_value),
-            "--reg-tid", reg_tid,
-            "--dp-suffix", dp_suffix,
-            "--scenario", scenario
-        ])
-
-        extension = ["-m","pytest", "-v"] if not use_uv else ["pytest", "-v"]
+        test_generation.extend(
+            [
+                GENERATOR_FILE_PATH,
+                "--dp-value",
+                str(dp_value),
+                "--reg-tid",
+                reg_tid,
+                "--dp-suffix",
+                dp_suffix,
+                "--scenario",
+                scenario,
+            ]
+        )
+
+        extension = ["-m", "pytest", "-v"] if not use_uv else ["pytest", "-v"]
         test_runs.extend(extension)
         test_results_conversion.extend([PARSER_FILE_PATH])
 
         return test_generation, test_runs, test_results_conversion
 
-    def load_sql_fixture(self, connection: sqlite3.Connection, cursor: sqlite3.Cursor, file_path: str, is_delete: bool=False) -> bool:
+    def load_sql_fixture(
+        self, connection: sqlite3.Connection, cursor: sqlite3.Cursor, file_path: str, is_delete: bool = False
+    ) -> bool:
         """
         Load SQL fixtures from file.
 
         Args:
             connection: SQLite connection
@@ -170,11 +191,11 @@
 
         Returns:
             Boolean indicating success
         """
         try:
-            with open(file_path, 'r') as sql_file:
+            with open(file_path, "r") as sql_file:
                 sql_script = sql_file.read()
                 cursor.executescript(sql_script)
             connection.commit()
             return True
         except Exception as e:
@@ -218,22 +239,22 @@
 
         Returns:
             Boolean indicating success
         """
         try:
-            with open(json_path, 'r') as json_file:
+            with open(json_path, "r") as json_file:
                 test_data = json.load(json_file)
 
             print("\n" + "=" * 80)
             print(f"TEST RESULTS FOR SCENARIO: {scenario}")
             print(f"Template ID: {reg_tid}")
             print(f"Datapoint: {dp_suffix}")
             print(f"Value: {dp_value}")
             print("=" * 80)
 
-            passed = test_data.get('test_results', {}).get('passed', [])
-            failed = test_data.get('test_results', {}).get('failed', [])
+            passed = test_data.get("test_results", {}).get("passed", [])
+            failed = test_data.get("test_results", {}).get("failed", [])
 
             print(f"\nPASSED TESTS ({len(passed)}):")
             for test in passed:
                 print(f"  ✓ {test}")
 
@@ -249,12 +270,20 @@
             return True
         except Exception as e:
             logger.error(f"Failed to read and print test results: {str(e)}")
             return False
 
-    def process_scenario(self, connection: sqlite3.Connection, cursor: sqlite3.Cursor,
-                        scenario_path: str, reg_tid: str, dp_suffix: str, dp_value: str, use_uv: bool):
+    def process_scenario(
+        self,
+        connection: sqlite3.Connection,
+        cursor: sqlite3.Cursor,
+        scenario_path: str,
+        reg_tid: str,
+        dp_suffix: str,
+        dp_value: str,
+        use_uv: bool,
+    ):
         """
         Process a single test scenario.
 
         Args:
             connection: SQLite connection
@@ -296,36 +325,27 @@
         logger.debug("Test generator completed successfully")
 
         # Run tests
         logger.debug("Running pytest...")
         txt_output_path = f"{txt_path_stub}__{scenario_path}.txt"
-        test_path = os.path.join(TESTS_DIR,
-            f"test_cell_{reg_tid}_{dp_suffix}__{scenario_path}.py".lower())
-        if not self.execute_test_process(test_runs+[test_path], txt_output_path):
+        test_path = os.path.join(TESTS_DIR, f"test_cell_{reg_tid}_{dp_suffix}__{scenario_path}.py".lower())
+        if not self.execute_test_process(test_runs + [test_path], txt_output_path):
             return
         logger.debug("Pytest completed successfully")
 
         # Process results
         logger.debug("Processing test results...")
         json_output_path = f"{json_path_stub}__{scenario_path}.json"
-        result_args = [
-            txt_output_path,
-            str(dp_value),
-            reg_tid,
-            dp_suffix,
-            scenario_path
-        ]
+        result_args = [txt_output_path, str(dp_value), reg_tid, dp_suffix, scenario_path]
         if not self.execute_test_process(test_results_conversion + result_args, json_output_path):
             return
         logger.debug("Test results processed successfully")
 
         # Display results
         self.display_test_results(json_output_path, scenario_path, reg_tid, dp_suffix, dp_value)
 
         logger.debug(f"Finished scenario: {scenario_path} from {reg_tid} at datapoint {dp_suffix}")
-
-    
 
     def get_safe_config_path(self, user_config_path: str) -> str:
         """
         Validates and constructs a safe absolute path for a config file.
         Returns safe config path if valid; raises ValueError if unsafe.
@@ -351,65 +371,76 @@
         Returns:
             Configuration dictionary or None if failed
         """
         try:
             safe_path = self.get_safe_config_path(config_path)
-            with open(safe_path, 'r') as f:
+            with open(safe_path, "r") as f:
                 return json.load(f)
         except Exception as e:
             logger.error(f"Failed to load config file: {str(e)}")
             return None
-        
-    def run_tests_from_config(self, config_path: str, use_uv: bool=False):
+
+    def run_tests_from_config(self, config_path: str, use_uv: bool = False):
         """
         Run tests based on a configuration file.
 
         Args:
             config_path: Path to config file
             use_uv: Whether to use UV as backend
         """
-  
-    
+
         config = self.load_config_file(config_path)
         if not config:
             logger.error("Invalid or missing configuration file.")
             return
 
         connection = sqlite3.connect("db.sqlite3")
         cursor = connection.cursor()
 
         # Process each test configuration
-        for test_config in config.get('tests', []):
-            reg_tid = test_config.get('reg_tid')
-            dp_suffix = test_config.get('dp_suffix')
-            dp_value = test_config.get('dp_value')
-            scenario = test_config.get('scenario')
+        for test_config in config.get("tests", []):
+            reg_tid = test_config.get("reg_tid")
+            dp_suffix = test_config.get("dp_suffix")
+            dp_value = test_config.get("dp_value")
+            scenario = test_config.get("scenario")
 
             if not all([reg_tid, dp_suffix, dp_value]):
                 logger.warning(f"Skipping incomplete test configuration: {test_config}")
                 continue
 
             if scenario:
                 # Run specific scenario
                 self.process_scenario(connection, cursor, scenario, reg_tid, dp_suffix, str(dp_value), use_uv)
             else:
                 # Run all scenarios for this template/datapoint
-                test_data_scenario_path = f"tests{os.sep}fixtures{os.sep}templates{os.sep}{reg_tid}{os.sep}{dp_suffix}{os.sep}"
+                test_data_scenario_path = (
+                    f"tests{os.sep}fixtures{os.sep}templates{os.sep}{reg_tid}{os.sep}{dp_suffix}{os.sep}"
+                )
                 try:
                     for scenario_path in os.listdir(test_data_scenario_path):
                         if ".py" in scenario_path:
                             continue
-                        self.process_scenario(connection, cursor, scenario_path, reg_tid, dp_suffix, str(dp_value), use_uv)
+                        self.process_scenario(
+                            connection, cursor, scenario_path, reg_tid, dp_suffix, str(dp_value), use_uv
+                        )
                 except Exception as e:
                     logger.error(f"Error processing scenarios: {str(e)}")
 
         cursor.close()
         connection.close()
         from pybirdai.utils.datapoint_test_run.generate_test_url import main
+
         main()
 
-    def run_tests(self, reg_tid: str="", dp_suffix: str="", dp_value: str="", use_uv: bool=False, specific_scenario: str=None):
+    def run_tests(
+        self,
+        reg_tid: str = "",
+        dp_suffix: str = "",
+        dp_value: str = "",
+        use_uv: bool = False,
+        specific_scenario: str = None,
+    ):
         """
         Main function to run all test scenarios.
 
         Args:
             reg_tid: Regulatory template ID
@@ -422,33 +453,17 @@
         cursor = connection.cursor()
 
         test_data_scenario_path = f"tests{os.sep}fixtures{os.sep}templates{os.sep}{reg_tid}{os.sep}{dp_suffix}{os.sep}"
 
         if specific_scenario:
-            self.process_scenario(
-                connection,
-                cursor,
-                specific_scenario,
-                reg_tid,
-                dp_suffix,
-                dp_value,
-                use_uv
-            )
+            self.process_scenario(connection, cursor, specific_scenario, reg_tid, dp_suffix, dp_value, use_uv)
         else:
             for scenario_path in os.listdir(test_data_scenario_path):
                 if ".py" in scenario_path:
                     continue
 
-                self.process_scenario(
-                    connection,
-                    cursor,
-                    scenario_path,
-                    reg_tid,
-                    dp_suffix,
-                    dp_value,
-                    use_uv
-                )
+                self.process_scenario(connection, cursor, scenario_path, reg_tid, dp_suffix, dp_value, use_uv)
         cursor.close()
         connection.close()
 
     def main(self):
         """
@@ -469,11 +484,7 @@
         if self.args.config_file:
             self.run_tests_from_config(self.args.config_file, eval(self.args.uv))
         else:
             # Run with command line arguments
             self.run_tests(
-                self.args.reg_tid,
-                self.args.dp_suffix,
-                str(self.args.dp_value),
-                eval(self.args.uv),
-                self.args.scenario
+                self.args.reg_tid, self.args.dp_suffix, str(self.args.dp_value), eval(self.args.uv), self.args.scenario
             )
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/datapoint_test_run/run_tests.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/model_meta_data.py	2025-08-02 18:37:08.469739+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/model_meta_data.py	2025-09-21 17:07:34.684131+00:00
@@ -11,17 +11,19 @@
 #    Neil Mackenzie - initial API and implementation
 import django
 from django.apps import AppConfig
 from django.db.models.fields.related import ForeignKey
 
+
 class ModelMetaDataUtils(AppConfig):
-    
-    path = '/workspaces/efbt/bird/birdseed_creator/birds_nest'
+
+    path = "/workspaces/efbt/bird/birdseed_creator/birds_nest"
 
     def ready(self):
-        
+
         from django.apps import apps
+
         model_list = apps.get_models()
 
         for model in model_list:
 
             print(f"{model._meta.app_label}  -> {model.__name__}")
@@ -44,8 +46,9 @@
         pass
 
     def print_table_meta_data(self):
         pass
 
-if __name__ == '__main__':
+
+if __name__ == "__main__":
     django.setup()
-    ModelMetaDataUtils('pybirdai','birds_nest').ready()
+    ModelMetaDataUtils("pybirdai", "birds_nest").ready()
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/model_meta_data.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/mapping_library.py	2025-08-02 18:37:08.468643+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/mapping_library.py	2025-09-21 17:07:34.721404+00:00
@@ -18,15 +18,19 @@
     MEMBER,
     MEMBER_MAPPING_ITEM,
     MAPPING_DEFINITION,
     MAPPING_TO_CUBE,
     VARIABLE,
-    VARIABLE_MAPPING_ITEM
+    VARIABLE_MAPPING_ITEM,
 )
+
 logger = logging.getLogger(__name__)
 
-def update_member_mapping_item(member_mapping: MEMBER_MAPPING_ITEM, member_mapping_row: str, variable: VARIABLE, member: MEMBER, is_source_str: str) -> MEMBER_MAPPING_ITEM:
+
+def update_member_mapping_item(
+    member_mapping: MEMBER_MAPPING_ITEM, member_mapping_row: str, variable: VARIABLE, member: MEMBER, is_source_str: str
+) -> MEMBER_MAPPING_ITEM:
     """Updates or creates a member mapping item.
 
     Args:
         member_mapping: The member mapping object
         member_mapping_row: Row identifier
@@ -40,31 +44,32 @@
     logger.debug(f"Updating member mapping item: {member_mapping_row}")
     mapping_item, created = MEMBER_MAPPING_ITEM.objects.update_or_create(
         member_mapping_id=member_mapping,
         member_mapping_row=member_mapping_row,
         variable_id=variable,
-        defaults={
-            'member_id': member,
-            'is_source': is_source_str
-        }
+        defaults={"member_id": member, "is_source": is_source_str},
     )
     return mapping_item
 
-def get_filtered_var_items(variable_mapping_id: str) -> Tuple[List[VARIABLE_MAPPING_ITEM], List[VARIABLE_MAPPING_ITEM], List[VARIABLE_MAPPING_ITEM]]:
+
+def get_filtered_var_items(
+    variable_mapping_id: str,
+) -> Tuple[List[VARIABLE_MAPPING_ITEM], List[VARIABLE_MAPPING_ITEM], List[VARIABLE_MAPPING_ITEM]]:
     """Gets filtered variable items for a given mapping ID.
 
     Args:
         variable_mapping_id: ID of the variable mapping to filter
 
     Returns:
         Tuple containing lists of all items, source items and target items
     """
- #   logger.debug(f"Getting filtered variable items for mapping ID: {variable_mapping_id}")
+    #   logger.debug(f"Getting filtered variable items for mapping ID: {variable_mapping_id}")
     var_items = VARIABLE_MAPPING_ITEM.objects.filter(variable_mapping_id=variable_mapping_id)
-    source_vars = [item for item in var_items if item.is_source.lower() == 'true']
-    target_vars = [item for item in var_items if item.is_source.lower() != 'true']
+    source_vars = [item for item in var_items if item.is_source.lower() == "true"]
+    target_vars = [item for item in var_items if item.is_source.lower() != "true"]
     return var_items, source_vars, target_vars
+
 
 def build_mapping_results(mapping_definitions: List[MAPPING_DEFINITION]) -> Dict[str, Dict[str, Any]]:
     """Builds mapping results dictionary from mapping definitions.
 
     Args:
@@ -74,11 +79,10 @@
         Dictionary containing mapping results
     """
     logger.debug("Building mapping results")
     results = {}
     for map_def in mapping_definitions:
-
 
         if not map_def.member_mapping_id:
             continue
 
         if map_def.variable_mapping_id:
@@ -88,30 +92,33 @@
 
         if map_def.mapping_id not in results:
             results[map_def.mapping_id] = {
                 "variable_mapping_id": map_def.variable_mapping_id.code if map_def.variable_mapping_id else None,
                 "has_member_mapping": True,
-                "member_mapping_id": {
-                    "code": map_def.member_mapping_id.code,
-                    "items": []
-                }
+                "member_mapping_id": {"code": map_def.member_mapping_id.code, "items": []},
             }
     return results
 
+
 def get_source_target_vars(var_items: List[VARIABLE_MAPPING_ITEM]) -> Dict[str, List[str]]:
     """Gets source and target variables from variable items.
 
     Args:
         var_items: List of variable mapping items
 
     Returns:
         Dictionary with source and target variable lists
     """
     logger.debug("Getting source and target variables")
-    source_vars = [f"{item.variable_id.name} ({item.variable_id.code})" for item in var_items if item.is_source.lower() == 'true']
-    target_vars = [f"{item.variable_id.name} ({item.variable_id.code})" for item in var_items if item.is_source.lower() != 'true']
-    return {"source":source_vars, "target":target_vars}
+    source_vars = [
+        f"{item.variable_id.name} ({item.variable_id.code})" for item in var_items if item.is_source.lower() == "true"
+    ]
+    target_vars = [
+        f"{item.variable_id.name} ({item.variable_id.code})" for item in var_items if item.is_source.lower() != "true"
+    ]
+    return {"source": source_vars, "target": target_vars}
+
 
 def initialize_unique_set(member_mapping_items: List[MEMBER_MAPPING_ITEM]) -> Dict[str, Set[str]]:
     """Initializes unique set of member mappings.
 
     Args:
@@ -123,15 +130,18 @@
     logger.debug("Initializing unique set")
     unique_set = {}
     for item in member_mapping_items:
         vars_ = f"{item.variable_id.name} ({item.variable_id.code})"
         if vars_ not in unique_set:
-            unique_set[vars_] = {elt.member_id: f"{elt.name} ({elt.code})"
-            for elt in MEMBER.objects.filter(domain_id=item.variable_id.domain_id)}
+            unique_set[vars_] = {
+                elt.member_id: f"{elt.name} ({elt.code})"
+                for elt in MEMBER.objects.filter(domain_id=item.variable_id.domain_id)
+            }
     return unique_set
 
-def build_temp_items(member_mapping_items: List[MEMBER_MAPPING_ITEM], unique_set:dict) -> Dict[str, Dict[str, Any]]:
+
+def build_temp_items(member_mapping_items: List[MEMBER_MAPPING_ITEM], unique_set: dict) -> Dict[str, Dict[str, Any]]:
     """Builds temporary items dictionary from member mappings.
 
     Args:
         member_mapping_items: List of member mapping items
         unique_set: Dictionary of unique variable sets
@@ -141,20 +151,27 @@
     """
     logger.debug("Building temporary items")
     temp_items = {}
     for item in member_mapping_items:
         if item.member_mapping_row not in temp_items:
-            temp_items[item.member_mapping_row] = {'has_source': False, 'has_target': False, 'items': {k:"None (None)" for k in unique_set}}
+            temp_items[item.member_mapping_row] = {
+                "has_source": False,
+                "has_target": False,
+                "items": {k: "None (None)" for k in unique_set},
+            }
 
         vars_ = f"{item.variable_id.name} ({item.variable_id.code})"
         member_ = f"{item.member_id.name} ({item.member_id.code})"
 
-        temp_items[item.member_mapping_row]['has_source' if item.is_source.lower() == 'true' else 'has_target'] = True
-        temp_items[item.member_mapping_row]['items'][vars_] = member_
+        temp_items[item.member_mapping_row]["has_source" if item.is_source.lower() == "true" else "has_target"] = True
+        temp_items[item.member_mapping_row]["items"][vars_] = member_
     return temp_items
 
-def process_member_mappings(member_mapping_items: List[MEMBER_MAPPING_ITEM], var_items: List[VARIABLE_MAPPING_ITEM]) -> Tuple[Dict[str, Dict[str, Any]], Dict[str, Set[str]], Dict[str, List[str]]]:
+
+def process_member_mappings(
+    member_mapping_items: List[MEMBER_MAPPING_ITEM], var_items: List[VARIABLE_MAPPING_ITEM]
+) -> Tuple[Dict[str, Dict[str, Any]], Dict[str, Set[str]], Dict[str, List[str]]]:
     """Processes member mappings to create temporary items and unique sets.
 
     Args:
         member_mapping_items: List of member mapping items
         var_items: List of variable mapping items
@@ -163,13 +180,14 @@
         Tuple of temporary items, unique sets and source/target variables
     """
     logger.debug("Processing member mappings")
     source_target = get_source_target_vars(var_items)
     unique_set = initialize_unique_set(member_mapping_items)
-    temp_items = build_temp_items(member_mapping_items,unique_set)
+    temp_items = build_temp_items(member_mapping_items, unique_set)
 
     return temp_items, unique_set, source_target
+
 
 def create_table_data(serialized_items: Dict[str, Dict[str, str]], columns_of_table: List[str]) -> Dict[str, Any]:
     """Creates table data structure from serialized items.
 
     Args:
@@ -178,22 +196,20 @@
 
     Returns:
         Dictionary containing table data structure
     """
     logger.debug("Creating table data")
-    table_data = {
-        'headers': ["row_id"]+columns_of_table,
-        'rows': []
-    }
+    table_data = {"headers": ["row_id"] + columns_of_table, "rows": []}
     for row_id, row_data in serialized_items.items():
-        table_row = {"row_id":int(row_id)}
+        table_row = {"row_id": int(row_id)}
         table_row.update(row_data)
-        table_data['rows'].append(table_row)
+        table_data["rows"].append(table_row)
 
     # Sort the rows by row_id
     table_data["rows"] = sorted(table_data["rows"], key=lambda x: x["row_id"])
     return table_data
+
 
 def cascade_member_mapping_changes(member_mapping_item: MEMBER_MAPPING_ITEM) -> None:
     """Cascades changes from a new member mapping item through related mapping objects.
 
     Creates new:
@@ -206,12 +222,13 @@
     """
     # Create mapping definition
     mapping_def = MAPPING_DEFINITION.objects.create(
         member_mapping_id=member_mapping_item.member_mapping_id,
         name=f"Generated mapping for {member_mapping_item.member_mapping_row}",
-        code=f"GEN_MAP_{member_mapping_item.member_mapping_row}"
+        code=f"GEN_MAP_{member_mapping_item.member_mapping_row}",
     )
+
 
 def add_variable_to_mapping(mapping_id: str, variable_code: str, is_source_str: str) -> VARIABLE:
     """Adds a variable to an existing mapping.
 
     Args:
@@ -225,17 +242,18 @@
     logger.debug(f"Adding variable to mapping: {variable_code}")
     mapping_def = MAPPING_DEFINITION.objects.get(code=mapping_id)
     variable = VARIABLE.objects.get(code=variable_code)
 
     VARIABLE_MAPPING_ITEM.objects.create(
-        variable_mapping_id=mapping_def.variable_mapping_id,
-        variable_id=variable,
-        is_source=is_source_str
+        variable_mapping_id=mapping_def.variable_mapping_id, variable_id=variable, is_source=is_source_str
     )
     return variable
 
-def process_related_mappings(member_mapping: MEMBER_MAPPING_ITEM, mapping_def: MAPPING_DEFINITION, member_mapping_row: str) -> None:
+
+def process_related_mappings(
+    member_mapping: MEMBER_MAPPING_ITEM, mapping_def: MAPPING_DEFINITION, member_mapping_row: str
+) -> None:
     """Processes mappings related to a member mapping.
 
     Args:
         member_mapping: Member mapping object
         mapping_def: Mapping definition object
@@ -244,23 +262,20 @@
     logger.debug(f"Processing related mappings for row: {member_mapping_row}")
     related_mappings = MAPPING_DEFINITION.objects.filter(member_mapping_id=member_mapping)
     for rel_mapping in related_mappings:
         if rel_mapping != mapping_def:
             existing_items = MEMBER_MAPPING_ITEM.objects.filter(
-                member_mapping_id=member_mapping,
-                member_mapping_row=member_mapping_row
+                member_mapping_id=member_mapping, member_mapping_row=member_mapping_row
             )
             for item in existing_items:
                 MEMBER_MAPPING_ITEM.objects.update_or_create(
                     member_mapping_id=rel_mapping.member_mapping_id,
                     member_mapping_row=member_mapping_row,
                     variable_id=item.variable_id,
-                    defaults={
-                        'member_id': item.member_id,
-                        'is_source': item.is_source
-                    }
+                    defaults={"member_id": item.member_id, "is_source": item.is_source},
                 )
+
 
 def create_or_update_member(member_id: str, variable: VARIABLE, domain: Any) -> MEMBER:
     """Creates or updates a member object.
 
     Args:
@@ -274,16 +289,13 @@
     logger.debug(f"Creating or updating member: {member_id}")
     try:
         member = MEMBER.objects.get(code=member_id, domain_id=domain)
     except MEMBER.DoesNotExist:
         logger.info(f"Creating new member: {member_id}")
-        member = MEMBER.objects.create(
-            code=member_id,
-            name=member_id,
-            domain_id=domain
-        )
+        member = MEMBER.objects.create(code=member_id, name=member_id, domain_id=domain)
     return member
+
 
 def process_mapping_chain(variable: VARIABLE, mapping_def: MAPPING_DEFINITION) -> None:
     """Process a chain of related mappings starting from a variable.
 
     Args:
@@ -293,14 +305,16 @@
     logger.debug(f"Processing mapping chain for variable: {variable.code}")
     var_items = get_filtered_var_items(mapping_def.variable_mapping_id.variable_mapping_id)
     source_target = get_source_target_vars(var_items[0])
 
     for target_var in source_target["target"]:
-        cascade_member_mapping_changes(MEMBER_MAPPING_ITEM.objects.filter(
-            variable_id__code=target_var.split("(")[1].strip(")"),
-            is_source="true"
-        ).first())
+        cascade_member_mapping_changes(
+            MEMBER_MAPPING_ITEM.objects.filter(
+                variable_id__code=target_var.split("(")[1].strip(")"), is_source="true"
+            ).first()
+        )
+
 
 def get_source_variables():
     source_variables = {}
     for v in VARIABLE.objects.all():
         if v.maintenance_agency_id:
@@ -308,23 +322,21 @@
                 domain = v.domain_id
                 domain_members = {}
                 members = MEMBER.objects.filter(domain_id=domain)
                 if len(members):
                     for m in members:
-                        domain_members[m.member_id] = {
-                            'code': m.code,
-                            'name': m.name
-                        }
+                        domain_members[m.member_id] = {"code": m.code, "name": m.name}
                     source_variables[v.variable_id] = {
-                        'domain': {
-                            'id': domain.domain_id,
-                            'code': domain.code,
-                            'name': domain.name,
-                            'members': domain_members
+                        "domain": {
+                            "id": domain.domain_id,
+                            "code": domain.code,
+                            "name": domain.name,
+                            "members": domain_members,
                         }
                     }
     return source_variables
+
 
 # Get all available variables from reference framework - FINREF_REF
 def get_reference_variables():
     reference_variables = {}
     for v in VARIABLE.objects.all():
@@ -333,18 +345,15 @@
                 domain = v.domain_id
                 domain_members = {}
                 members = MEMBER.objects.filter(domain_id=domain)
                 if len(members):
                     for m in members:
-                        domain_members[m.member_id] = {
-                            'code': m.code,
-                            'name': m.name
-                        }
+                        domain_members[m.member_id] = {"code": m.code, "name": m.name}
                     reference_variables[v.variable_id] = {
-                        'domain': {
-                            'id': domain.domain_id,
-                            'code': domain.code,
-                            'name': domain.name,
-                            'members': domain_members
+                        "domain": {
+                            "id": domain.domain_id,
+                            "code": domain.code,
+                            "name": domain.name,
+                            "members": domain_members,
                         }
                     }
     return reference_variables
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/mapping_library.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/member_hierarchy_editor/django_model_converter.py	2025-09-15 13:18:11.413361+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/member_hierarchy_editor/django_model_converter.py	2025-09-21 17:07:34.724740+00:00
@@ -25,20 +25,20 @@
             hierarchy = MEMBER_HIERARCHY.objects.get(member_hierarchy_id=hierarchy_id)
 
             # Clear existing nodes for this hierarchy
             MEMBER_HIERARCHY_NODE.objects.filter(member_hierarchy_id=hierarchy).delete()
 
-            boxes = visualization_data.get('boxes', [])
-            arrows = visualization_data.get('arrows', [])
+            boxes = visualization_data.get("boxes", [])
+            arrows = visualization_data.get("arrows", [])
 
             # Build parent-child relationships from arrows
             parent_map = {}
             children_map = {}
 
             for arrow in arrows:
-                child_id = arrow['from']
-                parent_id = arrow['to']
+                child_id = arrow["from"]
+                parent_id = arrow["to"]
                 parent_map[child_id] = parent_id
 
                 if parent_id not in children_map:
                     children_map[parent_id] = []
                 children_map[parent_id].append(child_id)
@@ -79,29 +79,29 @@
 
             # Create new nodes
             created_nodes = []
             for box in boxes:
                 try:
-                    member = MEMBER.objects.get(member_id=box['id'])
+                    member = MEMBER.objects.get(member_id=box["id"])
                     parent_member = None
 
-                    if box['id'] in parent_map:
+                    if box["id"] in parent_map:
                         try:
-                            parent_member = MEMBER.objects.get(member_id=parent_map[box['id']])
+                            parent_member = MEMBER.objects.get(member_id=parent_map[box["id"]])
                         except MEMBER.DoesNotExist:
                             continue  # Skip if parent member doesn't exist
 
-                    level = calculate_level(box['id'])
-                    comparator, operator = determine_node_properties(box['id'])
+                    level = calculate_level(box["id"])
+                    comparator, operator = determine_node_properties(box["id"])
 
                     node = MEMBER_HIERARCHY_NODE.objects.create(
                         member_hierarchy_id=hierarchy,
                         member_id=member,
                         parent_member_id=parent_member,
                         level=level,
                         comparator=comparator,
-                        operator=operator
+                        operator=operator,
                     )
                     created_nodes.append(node)
 
                 except MEMBER.DoesNotExist:
                     continue  # Skip invalid members
@@ -120,13 +120,15 @@
             hierarchy = MEMBER_HIERARCHY.objects.get(member_hierarchy_id=hierarchy_id)
         except MEMBER_HIERARCHY.DoesNotExist:
             return {"boxes": [], "arrows": [], "nextId": 1, "error": "Hierarchy not found"}
 
         # Get all nodes for this hierarchy
-        hierarchy_nodes = MEMBER_HIERARCHY_NODE.objects.filter(
-            member_hierarchy_id=hierarchy
-        ).select_related('member_id', 'parent_member_id').order_by('level', 'member_id__member_id')
+        hierarchy_nodes = (
+            MEMBER_HIERARCHY_NODE.objects.filter(member_hierarchy_id=hierarchy)
+            .select_related("member_id", "parent_member_id")
+            .order_by("level", "member_id__member_id")
+        )
 
         if not hierarchy_nodes.exists():
             return {"boxes": [], "arrows": [], "nextId": 1}
 
         boxes = []
@@ -165,73 +167,70 @@
                 x = start_x + (i * node_spacing_x)
                 y = start_y + ((level - 1) * level_spacing_y)
 
                 # Create box
                 box = {
-                    'id': member.member_id if member else f"unknown_{node_row.id}",
-                    'x': x,
-                    'y': y,
-                    'width': BOX_WIDTH,
-                    'height': BOX_HEIGHT,
-                    'name': name,
-                    'text': description
+                    "id": member.member_id if member else f"unknown_{node_row.id}",
+                    "x": x,
+                    "y": y,
+                    "width": BOX_WIDTH,
+                    "height": BOX_HEIGHT,
+                    "name": name,
+                    "text": description,
                 }
                 boxes.append(box)
 
                 # Create arrow if there's a parent
                 if node_row.parent_member_id:
                     arrow = {
-                        'from': member.member_id if member else f"unknown_{node_row.id}",
-                        'to': node_row.parent_member_id.member_id
+                        "from": member.member_id if member else f"unknown_{node_row.id}",
+                        "to": node_row.parent_member_id.member_id,
                     }
                     arrows.append(arrow)
 
         # Calculate next available ID
         next_id = len(boxes) + 1
 
         # Get allowed members for this hierarchy's domain
         allowed_members = {}
         if hierarchy.domain_id:
             domain_members = MEMBER.objects.filter(domain_id=hierarchy.domain_id)
-            allowed_members = {
-                member.member_id: member.name or member.member_id
-                for member in domain_members
-            }
+            allowed_members = {member.member_id: member.name or member.member_id for member in domain_members}
 
         return {
             "boxes": boxes,
             "arrows": arrows,
             "nextId": next_id,
             "hierarchy_info": {
                 "id": hierarchy_id,
                 "name": hierarchy.name or hierarchy_id,
                 "description": hierarchy.description or "",
                 "domain": hierarchy.domain_id.domain_id if hierarchy.domain_id else "",
-                "allowed_members": allowed_members
-            }
+                "allowed_members": allowed_members,
+            },
         }
 
     def validate_hierarchy_structure(self, visualization_data: dict) -> Tuple[bool, List[str]]:
         """Validate the hierarchy structure for logical consistency"""
-        boxes = visualization_data.get('boxes', [])
-        arrows = visualization_data.get('arrows', [])
+        boxes = visualization_data.get("boxes", [])
+        arrows = visualization_data.get("arrows", [])
 
         errors = []
 
         # Check for basic structure
         if not boxes:
             errors.append("Hierarchy must contain at least one member")
             return False, errors
 
         # Build graph structure
-        all_nodes = {box['id'] for box in boxes}
+        all_nodes = {box["id"] for box in boxes}
         parent_map = {}
         children_map = {}
 
         for arrow in arrows:
-            child_id = arrow['from']
-            parent_id = arrow['to']
+            child_id = arrow["from"]
+            parent_id = arrow["to"]
 
             # Validate that both nodes exist
             if child_id not in all_nodes:
                 errors.append(f"Arrow references non-existent child node: {child_id}")
                 continue
@@ -292,33 +291,31 @@
         try:
             hierarchy = MEMBER_HIERARCHY.objects.get(member_hierarchy_id=hierarchy_id)
             nodes = MEMBER_HIERARCHY_NODE.objects.filter(member_hierarchy_id=hierarchy)
 
             total_nodes = nodes.count()
-            levels = nodes.values_list('level', flat=True).distinct()
+            levels = nodes.values_list("level", flat=True).distinct()
             max_level = max(levels) if levels else 0
 
             # Count nodes by level
             level_counts = {}
             for level in levels:
                 level_counts[level] = nodes.filter(level=level).count()
 
             # Count by node types
             root_nodes = nodes.filter(parent_member_id__isnull=True).count()
-            leaf_nodes = nodes.exclude(
-                member_id__in=nodes.values_list('parent_member_id', flat=True)
-            ).count()
+            leaf_nodes = nodes.exclude(member_id__in=nodes.values_list("parent_member_id", flat=True)).count()
             intermediate_nodes = total_nodes - root_nodes - leaf_nodes
 
             return {
-                'total_nodes': total_nodes,
-                'max_level': max_level,
-                'level_counts': level_counts,
-                'root_nodes': root_nodes,
-                'leaf_nodes': leaf_nodes,
-                'intermediate_nodes': intermediate_nodes,
-                'hierarchy_name': hierarchy.name,
-                'domain': hierarchy.domain_id.domain_id if hierarchy.domain_id else None
+                "total_nodes": total_nodes,
+                "max_level": max_level,
+                "level_counts": level_counts,
+                "root_nodes": root_nodes,
+                "leaf_nodes": leaf_nodes,
+                "intermediate_nodes": intermediate_nodes,
+                "hierarchy_name": hierarchy.name,
+                "domain": hierarchy.domain_id.domain_id if hierarchy.domain_id else None,
             }
 
         except MEMBER_HIERARCHY.DoesNotExist:
-            return {'error': 'Hierarchy not found'}
+            return {"error": "Hierarchy not found"}
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/member_hierarchy_editor/django_model_converter.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/member_hierarchy_editor/from_member_hierarchy_node_to_visualisation.py	2025-09-18 10:08:06.430715+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/member_hierarchy_editor/from_member_hierarchy_node_to_visualisation.py	2025-09-21 17:07:34.732341+00:00
@@ -18,10 +18,11 @@
 from collections import defaultdict
 
 BOX_WIDTH = 300
 BOX_HEIGHT = 120
 
+
 @dataclass
 class HierarchyNodeDTO:
     id_: str = "My Identifier"
     x: int = 0
     y: int = 0
@@ -42,63 +43,57 @@
          'height': 120,
          'name': 'Concept 1',
          'text': 'Enter description here...'}
         """
         return cls(
-            id_=str(dict_data.get('id', '')),
-            x=dict_data.get('x', 0),
-            y=dict_data.get('y', 0),
-            width=dict_data.get('width', BOX_WIDTH),
-            height=dict_data.get('height', BOX_HEIGHT),
-            name=dict_data.get('name', ''),
-            text=dict_data.get('text', '')
+            id_=str(dict_data.get("id", "")),
+            x=dict_data.get("x", 0),
+            y=dict_data.get("y", 0),
+            width=dict_data.get("width", BOX_WIDTH),
+            height=dict_data.get("height", BOX_HEIGHT),
+            name=dict_data.get("name", ""),
+            text=dict_data.get("text", ""),
         )
 
     @property
     def to_dict(self) -> dict:
         """
         Convert to dictionary format expected by visualization tool
         """
         return {
-            'id': self.id_,
-            'x': self.x,
-            'y': self.y,
-            'width': self.width,
-            'height': self.height,
-            'name': self.name,
-            'text': self.text
+            "id": self.id_,
+            "x": self.x,
+            "y": self.y,
+            "width": self.width,
+            "height": self.height,
+            "name": self.name,
+            "text": self.text,
         }
 
     @property
     def to_json(self) -> str:
         return json.dumps(self.to_dict)
 
 
 @dataclass
 class HierarchyArrowDTO:
     from_: str = ""  # child
-    to_: str = ""    # parent
+    to_: str = ""  # parent
 
     @classmethod
     def from_dict(cls, dict_data):
         """
         Create HierarchyArrowDTO from dictionary data
         """
-        return cls(
-            from_=str(dict_data.get("from", "")),
-            to_=str(dict_data.get("to", ""))
-        )
+        return cls(from_=str(dict_data.get("from", "")), to_=str(dict_data.get("to", "")))
 
     @property
     def to_dict(self) -> dict:
         """
         Convert to dictionary format expected by visualization tool
         """
-        return {
-            "from": self.from_,
-            "to": self.to_
-        }
+        return {"from": self.from_, "to": self.to_}
 
     @property
     def to_json(self) -> str:
         return json.dumps(self.to_dict)
 
@@ -106,33 +101,29 @@
 class MemberHierarchyIntegration:
     """
     Integration class to convert member hierarchy CSV data into visualization JSON format
     """
 
-    def __init__(self, members_data: List[Dict], hierarchies_data: List[Dict],
-                 hierarchy_nodes_data: List[Dict]):
+    def __init__(self, members_data: List[Dict], hierarchies_data: List[Dict], hierarchy_nodes_data: List[Dict]):
         self.members_data = members_data
         self.hierarchies_data = hierarchies_data
         self.hierarchy_nodes_data = hierarchy_nodes_data
 
     def get_hierarchy_by_id(self, hierarchy_id: str) -> Dict:
         """
         Convert a specific member hierarchy to visualization format
         """
         # Filter nodes for this hierarchy
-        hierarchy_nodes = [
-            node for node in self.hierarchy_nodes_data
-            if node['MEMBER_HIERARCHY_ID'] == hierarchy_id
-        ]
+        hierarchy_nodes = [node for node in self.hierarchy_nodes_data if node["MEMBER_HIERARCHY_ID"] == hierarchy_id]
 
         if not hierarchy_nodes:
             return {"boxes": [], "arrows": [], "nextId": 1}
 
         # Get hierarchy info
         hierarchy_info = None
         for hierarchy in self.hierarchies_data:
-            if hierarchy['MEMBER_HIERARCHY_ID'] == hierarchy_id:
+            if hierarchy["MEMBER_HIERARCHY_ID"] == hierarchy_id:
                 hierarchy_info = hierarchy
                 break
 
         # Create nodes
         nodes = []
@@ -145,57 +136,50 @@
         start_y = 200
 
         # Group nodes by level for positioning
         levels = defaultdict(list)
         for node in hierarchy_nodes:
-            level = int(node['LEVEL']) if node['LEVEL'] else 1
+            level = int(node["LEVEL"]) if node["LEVEL"] else 1
             levels[level].append(node)
 
         for level, level_nodes in levels.items():
             # Sort nodes by MEMBER_ID
-            level_nodes.sort(key=lambda x: x['MEMBER_ID'])
+            level_nodes.sort(key=lambda x: x["MEMBER_ID"])
             nodes_in_level = len(level_nodes)
 
             for i, node_row in enumerate(level_nodes):
                 # Get member information
                 member_info = None
                 for member in self.members_data:
-                    if member['MEMBER_ID'] == node_row['MEMBER_ID']:
+                    if member["MEMBER_ID"] == node_row["MEMBER_ID"]:
                         member_info = member
                         break
 
                 if member_info:
-                    name = member_info['NAME']
-                    description = member_info['DESCRIPTION'] if isinstance(member_info['DESCRIPTION'], str) else name
-                    code = member_info['CODE']
+                    name = member_info["NAME"]
+                    description = member_info["DESCRIPTION"] if isinstance(member_info["DESCRIPTION"], str) else name
+                    code = member_info["CODE"]
                 else:
-                    name = node_row['MEMBER_ID']
+                    name = node_row["MEMBER_ID"]
                     description = "No description available"
                     code = ""
 
                 # Calculate position
                 x = start_x + (i * node_spacing_x)
                 y = start_y + ((level - 1) * level_spacing_y)
 
                 text = description
 
                 node = HierarchyNodeDTO(
-                    id_=node_row['MEMBER_ID'],
-                    x=x,
-                    y=y,
-                    width=BOX_WIDTH,
-                    height=BOX_HEIGHT,
-                    name=name,
-                    text=text
+                    id_=node_row["MEMBER_ID"], x=x, y=y, width=BOX_WIDTH, height=BOX_HEIGHT, name=name, text=text
                 )
                 nodes.append(node)
 
                 # Create arrow if there's a parent
-                if node_row['PARENT_MEMBER_ID'] is not None and node_row['PARENT_MEMBER_ID'] != '':
+                if node_row["PARENT_MEMBER_ID"] is not None and node_row["PARENT_MEMBER_ID"] != "":
                     arrow = HierarchyArrowDTO(
-                        from_=node_row['MEMBER_ID'],  # parent points to child
-                        to_=node_row['PARENT_MEMBER_ID']
+                        from_=node_row["MEMBER_ID"], to_=node_row["PARENT_MEMBER_ID"]  # parent points to child
                     )
                     arrows.append(arrow)
 
         # Convert to dict format
         boxes = [node.to_dict for node in nodes]
@@ -205,34 +189,34 @@
         next_id = len(boxes) + 1
 
         # Create allowed_members dictionary
         allowed_members = {}
         if hierarchy_info is not None:
-            domain_id = hierarchy_info['DOMAIN_ID']
+            domain_id = hierarchy_info["DOMAIN_ID"]
             for member in self.members_data:
-                if member['DOMAIN_ID'] == domain_id:
-                    allowed_members[member['MEMBER_ID']] = member['NAME']
+                if member["DOMAIN_ID"] == domain_id:
+                    allowed_members[member["MEMBER_ID"]] = member["NAME"]
 
         return {
             "boxes": boxes,
             "arrows": arrow_dicts,
             "nextId": next_id,
             "hierarchy_info": {
                 "id": hierarchy_id,
-                "name": hierarchy_info['NAME'] if hierarchy_info is not None else hierarchy_id,
-                "description": hierarchy_info['DESCRIPTION'] if hierarchy_info is not None else "",
-                "domain": hierarchy_info['DOMAIN_ID'] if hierarchy_info is not None else "",
-                "allowed_members": allowed_members
-            }
+                "name": hierarchy_info["NAME"] if hierarchy_info is not None else hierarchy_id,
+                "description": hierarchy_info["DESCRIPTION"] if hierarchy_info is not None else "",
+                "domain": hierarchy_info["DOMAIN_ID"] if hierarchy_info is not None else "",
+                "allowed_members": allowed_members,
+            },
         }
 
     def get_all_hierarchies(self) -> Dict[str, Dict]:
         """
         Get all hierarchies as a dictionary
         """
         all_hierarchies = {}
-        hierarchy_ids = list(set(node['MEMBER_HIERARCHY_ID'] for node in self.hierarchy_nodes_data))
+        hierarchy_ids = list(set(node["MEMBER_HIERARCHY_ID"] for node in self.hierarchy_nodes_data))
 
         for hierarchy_id in hierarchy_ids:
             all_hierarchies[hierarchy_id] = self.get_hierarchy_by_id(hierarchy_id)
 
         return all_hierarchies
@@ -240,13 +224,13 @@
     def get_hierarchies_by_domain(self, domain_id: str) -> Dict[str, Dict]:
         """
         Get all hierarchies for a specific domain
         """
         domain_hierarchies = [
-            hierarchy['MEMBER_HIERARCHY_ID']
+            hierarchy["MEMBER_HIERARCHY_ID"]
             for hierarchy in self.hierarchies_data
-            if hierarchy['DOMAIN_ID'] == domain_id
+            if hierarchy["DOMAIN_ID"] == domain_id
         ]
 
         result = {}
         for hierarchy_id in domain_hierarchies:
             result[hierarchy_id] = self.get_hierarchy_by_id(hierarchy_id)
@@ -256,38 +240,39 @@
     def save_hierarchy_json(self, hierarchy_id: str, output_path: str):
         """
         Save a specific hierarchy to JSON file
         """
         hierarchy_data = self.get_hierarchy_by_id(hierarchy_id)
-        with open(output_path, 'w') as f:
+        with open(output_path, "w") as f:
             json.dump(hierarchy_data, f, indent=2)
 
     def get_available_hierarchies(self) -> List[Dict]:
         """
         Get a summary of all available hierarchies
         """
         return [
             {
-                'MEMBER_HIERARCHY_ID': hierarchy['MEMBER_HIERARCHY_ID'],
-                'NAME': hierarchy['NAME'],
-                'DOMAIN_ID': hierarchy['DOMAIN_ID'],
-                'DESCRIPTION': hierarchy['DESCRIPTION'],
-                'IS_MAIN_HIERARCHY': hierarchy.get('IS_MAIN_HIERARCHY', '')
+                "MEMBER_HIERARCHY_ID": hierarchy["MEMBER_HIERARCHY_ID"],
+                "NAME": hierarchy["NAME"],
+                "DOMAIN_ID": hierarchy["DOMAIN_ID"],
+                "DESCRIPTION": hierarchy["DESCRIPTION"],
+                "IS_MAIN_HIERARCHY": hierarchy.get("IS_MAIN_HIERARCHY", ""),
             }
             for hierarchy in self.hierarchies_data
         ]
 
 
 # Convenience functions for easy usage
 def load_member_hierarchy_data(entities_path: str = "entities/") -> MemberHierarchyIntegration:
     """
     Load member hierarchy data from CSV files and return integration object
     """
+
     def read_csv_as_dict_list(filepath: str) -> List[Dict]:
         """Read CSV file and return as list of dictionaries"""
         data = []
-        with open(filepath, 'r', newline='', encoding='utf-8') as f:
+        with open(filepath, "r", newline="", encoding="utf-8") as f:
             reader = csv.DictReader(f)
             for row in reader:
                 data.append(row)
         return data
 
@@ -296,12 +281,11 @@
     member_hierarchy_nodes = read_csv_as_dict_list(f"{entities_path}member_hierarchy_node.csv")
 
     return MemberHierarchyIntegration(members, member_hierarchies, member_hierarchy_nodes)
 
 
-def generate_hierarchy_json(hierarchy_id: str, output_path: str = "hierarchy.json",
-                          entities_path: str = "entities/"):
+def generate_hierarchy_json(hierarchy_id: str, output_path: str = "hierarchy.json", entities_path: str = "entities/"):
     """
     Generate hierarchy JSON for a specific hierarchy ID
     """
     integration = load_member_hierarchy_data(entities_path)
     integration.save_hierarchy_json(hierarchy_id, output_path)
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/member_hierarchy_editor/from_member_hierarchy_node_to_visualisation.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/member_hierarchy_editor/from_visualisation_to_member_hierarchy_node.py	2025-09-15 13:18:11.414036+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/member_hierarchy_editor/from_visualisation_to_member_hierarchy_node.py	2025-09-21 17:07:34.733532+00:00
@@ -12,62 +12,69 @@
 #
 import json
 import sys
 from typing import Dict, List, Set, Tuple
 
+
 def load_hierarchy_json(file_path: str) -> dict:
     """Load the hierarchy JSON file."""
-    with open(file_path, 'r') as f:
+    with open(file_path, "r") as f:
         return json.load(f)
+
 
 def build_hierarchy_graph(arrows: List[dict]) -> Tuple[Dict[str, Set[str]], Dict[str, Set[str]]]:
     """Build parent-child relationships from arrows."""
     children_map = {}  # parent -> set of children
-    parent_map = {}    # child -> set of parents
-    
+    parent_map = {}  # child -> set of parents
+
     for arrow in arrows:
-        child = arrow['from']
-        parent = arrow['to']
-        
+        child = arrow["from"]
+        parent = arrow["to"]
+
         if parent not in children_map:
             children_map[parent] = set()
         children_map[parent].add(child)
-        
+
         if child not in parent_map:
             parent_map[child] = set()
         parent_map[child].add(parent)
-    
+
     return children_map, parent_map
+
 
 def find_root_nodes(all_nodes: Set[str], parent_map: Dict[str, Set[str]]) -> Set[str]:
     """Find nodes that have no parents (root nodes)."""
     return {node for node in all_nodes if node not in parent_map}
 
+
 def calculate_levels(root_nodes: Set[str], children_map: Dict[str, Set[str]]) -> Dict[str, int]:
     """Calculate the level of each node starting from 1."""
     levels = {}
-    
+
     def dfs(node, level):
         if node in levels:
             levels[node] = max(levels[node], level)
         else:
             levels[node] = level
-        
+
         if node in children_map:
             for child in children_map[node]:
                 dfs(child, level + 1)
-    
+
     for root in root_nodes:
         dfs(root, 1)
-    
+
     return levels
 
-def determine_node_properties(node: str, children_map: Dict[str, Set[str]], parent_map: Dict[str, Set[str]]) -> Tuple[str, str]:
+
+def determine_node_properties(
+    node: str, children_map: Dict[str, Set[str]], parent_map: Dict[str, Set[str]]
+) -> Tuple[str, str]:
     """Determine comparator and operator based on node type."""
     has_children = node in children_map and len(children_map[node]) > 0
     has_parents = node in parent_map and len(parent_map[node]) > 0
-    
+
     if has_children and not has_parents:
         # Rule 1: parent and not a child
         return "=", ""
     elif has_children and has_parents:
         # Rule 2: parent and child
@@ -77,136 +84,140 @@
         return "", "+"
     else:
         # Default case
         return "", ""
 
+
 def get_node_name(node_id: str, boxes: List[dict]) -> str:
     """Get the name of a node from the boxes list."""
     for box in boxes:
-        if box['id'] == node_id:
-            return box.get('name', node_id)
+        if box["id"] == node_id:
+            return box.get("name", node_id)
     return node_id
+
 
 def convert_to_member_hierarchy_nodes(hierarchy_data: dict) -> List[dict]:
     """Convert hierarchy data to MEMBER_HIERARCHY_NODE format."""
-    boxes = hierarchy_data.get('boxes', [])
-    arrows = hierarchy_data.get('arrows', [])
-    
+    boxes = hierarchy_data.get("boxes", [])
+    arrows = hierarchy_data.get("arrows", [])
+
     # Get all unique node IDs
     all_nodes = set()
     for box in boxes:
-        all_nodes.add(box['id'])
-    
+        all_nodes.add(box["id"])
+
     # Build hierarchy relationships
     children_map, parent_map = build_hierarchy_graph(arrows)
-    
+
     # Find root nodes and calculate levels
     root_nodes = find_root_nodes(all_nodes, parent_map)
     levels = calculate_levels(root_nodes, children_map)
-    
+
     # Create MEMBER_HIERARCHY_NODE entries
     member_nodes = []
-    
+
     for node in all_nodes:
         # Get node properties
         comparator, operator = determine_node_properties(node, children_map, parent_map)
         level = levels.get(node, 1)
         name = get_node_name(node, boxes)
-        
+
         # Create the member hierarchy node
         member_node = {
             "member_code": node,
             "member_name": name,
             "parent_member_code": None,
             "level": level,
             "comparator": comparator,
-            "operator": operator
+            "operator": operator,
         }
-        
+
         # Set parent (if any) - use the first parent if multiple exist
         if node in parent_map and parent_map[node]:
             member_node["parent_member_code"] = list(parent_map[node])[0]
-        
+
         member_nodes.append(member_node)
-    
+
     # Sort by level and then by member_code for consistent output
-    member_nodes.sort(key=lambda x: (x['level'], x['member_code']))
-    
+    member_nodes.sort(key=lambda x: (x["level"], x["member_code"]))
+
     return member_nodes
+
 
 def save_member_hierarchy_nodes(member_nodes: List[dict], output_file: str):
     """Save the MEMBER_HIERARCHY_NODE data to a JSON file."""
-    output_data = {
-        "MEMBER_HIERARCHY_NODE": member_nodes
-    }
-    
-    with open(output_file, 'w') as f:
+    output_data = {"MEMBER_HIERARCHY_NODE": member_nodes}
+
+    with open(output_file, "w") as f:
         json.dump(output_data, f, indent=2)
+
 
 def print_summary(member_nodes: List[dict]):
     """Print a summary of the conversion."""
     print(f"Converted {len(member_nodes)} nodes to MEMBER_HIERARCHY_NODE format")
     print("\nLevel distribution:")
     level_counts = {}
     for node in member_nodes:
-        level = node['level']
+        level = node["level"]
         level_counts[level] = level_counts.get(level, 0) + 1
-    
+
     for level in sorted(level_counts.keys()):
         print(f"  Level {level}: {level_counts[level]} nodes")
-    
+
     print("\nNode type distribution:")
     type_counts = {"Root (=, '')": 0, "Intermediate (=, +)": 0, "Leaf ('', +)": 0}
     for node in member_nodes:
-        comp = node['comparator']
-        op = node['operator']
+        comp = node["comparator"]
+        op = node["operator"]
         if comp == "=" and op == "":
             type_counts["Root (=, '')"] += 1
         elif comp == "=" and op == "+":
             type_counts["Intermediate (=, +)"] += 1
         elif comp == "" and op == "+":
             type_counts["Leaf ('', +)"] += 1
-    
+
     for type_name, count in type_counts.items():
         print(f"  {type_name}: {count} nodes")
+
 
 def main():
     """Main function to run the conversion."""
     input_file = "generated_hierarchy.json"
     output_file = "member_hierarchy_nodes.json"
-    
+
     if len(sys.argv) > 1:
         input_file = sys.argv[1]
     if len(sys.argv) > 2:
         output_file = sys.argv[2]
-    
+
     try:
         # Load the hierarchy data
         print(f"Loading hierarchy from {input_file}...")
         hierarchy_data = load_hierarchy_json(input_file)
-        
+
         # Convert to MEMBER_HIERARCHY_NODE format
         print("Converting to MEMBER_HIERARCHY_NODE format...")
         member_nodes = convert_to_member_hierarchy_nodes(hierarchy_data)
-        
+
         # Save the result
         print(f"Saving to {output_file}...")
         save_member_hierarchy_nodes(member_nodes, output_file)
-        
+
         # Print summary
         print_summary(member_nodes)
-        
+
         print(f"\nConversion completed successfully!")
         print(f"Output saved to: {output_file}")
-        
+
     except FileNotFoundError:
         print(f"Error: Input file '{input_file}' not found.")
         sys.exit(1)
     except json.JSONDecodeError as e:
         print(f"Error: Invalid JSON in input file - {e}")
         sys.exit(1)
     except Exception as e:
         print(f"Error: {e}")
         sys.exit(1)
 
+
 if __name__ == "__main__":
-    main()
\ No newline at end of file
+    main()
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/member_hierarchy_editor/from_visualisation_to_member_hierarchy_node.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/datapoint_test_run/generator_for_tests.py	2025-09-02 15:09:38.725301+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/datapoint_test_run/generator_for_tests.py	2025-09-21 17:07:34.736821+00:00
@@ -16,18 +16,17 @@
 import ast
 import argparse
 import logging
 from pathlib import Path
 
-def return_logger(__file_name__:str):
+
+def return_logger(__file_name__: str):
     return logging.getLogger(__file_name__)
 
+
 # Set up logging
-logging.basicConfig(
-    level=logging.INFO,
-    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
-)
+logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s")
 logger = logging.getLogger()
 
 
 class TestCodeGenerator:
     """
@@ -43,70 +42,80 @@
         Set up the Django environment by configuring the settings module.
 
         This method sets the DJANGO_SETTINGS_MODULE environment variable and imports Django settings.
         It detects whether we're running under uv or regular Python and handles accordingly.
         """
-        os.environ['DJANGO_SETTINGS_MODULE'] = 'birds_nest.settings'
-        
+        os.environ["DJANGO_SETTINGS_MODULE"] = "birds_nest.settings"
+
         # Check if we're running under uv
         is_uv_environment = (
-            os.getenv('UV_PROJECT_ENVIRONMENT') is not None or
-            os.getenv('VIRTUAL_ENV') and 'uv' in os.getenv('VIRTUAL_ENV', '') or
-            'uv' in os.getenv('_', '')  # Check the command that started the process
-        )
-        
+            os.getenv("UV_PROJECT_ENVIRONMENT") is not None
+            or os.getenv("VIRTUAL_ENV")
+            and "uv" in os.getenv("VIRTUAL_ENV", "")
+            or "uv" in os.getenv("_", "")  # Check the command that started the process
+        )
+
         try:
             if is_uv_environment:
                 logger.info("Detected uv environment - Django should be available via uv dependencies")
             else:
                 logger.info("Detected standard Python environment - using virtual environment or system Python")
                 # For regular Python venv, we might need to ensure we're in the right directory
                 # and that the virtual environment is activated
-                venv_path = os.getenv('VIRTUAL_ENV')
+                venv_path = os.getenv("VIRTUAL_ENV")
                 if venv_path:
                     logger.info(f"Using virtual environment: {venv_path}")
                 else:
                     logger.warning("No virtual environment detected - using system Python")
-            
+
             from django.conf import settings
+
             logger.info("Successfully imported Django settings")
-            
+
         except ImportError as e:
             logger.error(f"Failed to import Django settings: {e}")
             logger.error("Django import failed. Environment details:")
             logger.error(f"  - UV_PROJECT_ENVIRONMENT: {os.getenv('UV_PROJECT_ENVIRONMENT')}")
             logger.error(f"  - VIRTUAL_ENV: {os.getenv('VIRTUAL_ENV')}")
             logger.error(f"  - Python executable: {sys.executable}")
             logger.error(f"  - Current working directory: {os.getcwd()}")
-            
+
             if is_uv_environment:
                 logger.error("Running under uv but Django not available.")
                 logger.error("Ensure this script is called with 'uv run python script.py'")
             else:
                 logger.error("Running under regular Python but Django not available.")
                 logger.error("Ensure Django is installed: pip install django")
                 logger.error("Or activate the virtual environment with Django installed")
-            
+
             raise RuntimeError("Django not available in current Python environment") from e
 
     @staticmethod
     def parse_arguments():
         """
         Parse command line arguments for test code generation.
 
         Returns:
             argparse.Namespace: Parsed command line arguments.
         """
-        parser = argparse.ArgumentParser(description='Generate test code for regulatory templates')
-        parser.add_argument('--dp-value', type=int, default=83491250,
-                          help='Datapoint value to test (default: 83491250)')
-        parser.add_argument('--reg-tid', type=str, default="F_05_01_REF_FINREP_3_0",
-                          help='Regulatory template ID (default: F_05_01_REF_FINREP_3_0)')
-        parser.add_argument('--dp-suffix', type=str, default="152589_REF",
-                          help='Suffix for datapoint and cell IDs (default: 152589_REF)')
-        parser.add_argument('--scenario', type=str, default="base",
-                          help='Scenario name (default: base)')
+        parser = argparse.ArgumentParser(description="Generate test code for regulatory templates")
+        parser.add_argument(
+            "--dp-value", type=int, default=83491250, help="Datapoint value to test (default: 83491250)"
+        )
+        parser.add_argument(
+            "--reg-tid",
+            type=str,
+            default="F_05_01_REF_FINREP_3_0",
+            help="Regulatory template ID (default: F_05_01_REF_FINREP_3_0)",
+        )
+        parser.add_argument(
+            "--dp-suffix",
+            type=str,
+            default="152589_REF",
+            help="Suffix for datapoint and cell IDs (default: 152589_REF)",
+        )
+        parser.add_argument("--scenario", type=str, default="base", help="Scenario name (default: base)")
 
         return parser.parse_args()
 
     @staticmethod
     def create_import_statements(cell_class):
@@ -119,62 +128,50 @@
         Returns:
             str: Generated import statements as a string.
         """
         module = ast.Module(
             body=[
-                ast.Import(names=[ast.alias(name='os', asname=None)]),
-                ast.Import(names=[ast.alias(name='logging', asname=None)]),
+                ast.Import(names=[ast.alias(name="os", asname=None)]),
+                ast.Import(names=[ast.alias(name="logging", asname=None)]),
                 ast.Assign(
                     targets=[
                         ast.Subscript(
                             value=ast.Attribute(
-                                value=ast.Name(id='os', ctx=ast.Load()),
-                                attr='environ',
-                                ctx=ast.Load()
+                                value=ast.Name(id="os", ctx=ast.Load()), attr="environ", ctx=ast.Load()
                             ),
-                            slice=ast.Constant(value='DJANGO_SETTINGS_MODULE'),
-                            ctx=ast.Store()
+                            slice=ast.Constant(value="DJANGO_SETTINGS_MODULE"),
+                            ctx=ast.Store(),
                         )
                     ],
-                    value=ast.Constant(value='birds_nest.settings')
-                ),
+                    value=ast.Constant(value="birds_nest.settings"),
+                ),
+                ast.ImportFrom(module="django.conf", names=[ast.alias(name="settings", asname=None)], level=0),
+                ast.Import(names=[ast.alias(name="pytest", asname=None)]),
                 ast.ImportFrom(
-                    module='django.conf',
-                    names=[ast.alias(name='settings', asname=None)],
-                    level=0
-                ),
-                ast.Import(names=[ast.alias(name='pytest', asname=None)]),
+                    module="django.core.wsgi", names=[ast.alias(name="get_wsgi_application", asname=None)], level=0
+                ),
+                ast.Assign(
+                    targets=[ast.Name(id="application", ctx=ast.Store())],
+                    value=ast.Call(func=ast.Name(id="get_wsgi_application", ctx=ast.Load()), args=[], keywords=[]),
+                ),
                 ast.ImportFrom(
-                    module='django.core.wsgi',
-                    names=[ast.alias(name='get_wsgi_application', asname=None)],
-                    level=0
-                ),
-                ast.Assign(
-                    targets=[ast.Name(id='application', ctx=ast.Store())],
-                    value=ast.Call(
-                        func=ast.Name(id='get_wsgi_application', ctx=ast.Load()),
-                        args=[],
-                        keywords=[]
-                    )
+                    module="pybirdai.entry_points.execute_datapoint",
+                    names=[ast.alias(name="RunExecuteDataPoint", asname=None)],
+                    level=0,
                 ),
                 ast.ImportFrom(
-                    module='pybirdai.entry_points.execute_datapoint',
-                    names=[ast.alias(name='RunExecuteDataPoint', asname=None)],
-                    level=0
+                    module="pybirdai.process_steps.pybird.execute_datapoint",
+                    names=[ast.alias(name="ExecuteDataPoint", asname=None)],
+                    level=0,
                 ),
                 ast.ImportFrom(
-                    module='pybirdai.process_steps.pybird.execute_datapoint',
-                    names=[ast.alias(name='ExecuteDataPoint', asname=None)],
-                    level=0
-                ),
-                ast.ImportFrom(
-                    module='pybirdai.process_steps.filter_code.report_cells',
+                    module="pybirdai.process_steps.filter_code.report_cells",
                     names=[ast.alias(name=cell_class, asname=None)],
-                    level=0
-                )
+                    level=0,
+                ),
             ],
-            type_ignores=[]
+            type_ignores=[],
         )
 
         return ast.unparse(ast.fix_missing_locations(module))
 
     @staticmethod
@@ -190,67 +187,64 @@
             str: Generated test functions as a string.
         """
         test_functions = ast.Module(
             body=[
                 ast.FunctionDef(
-                    name='test_execute_datapoint',
+                    name="test_execute_datapoint",
                     args=ast.arguments(
                         posonlyargs=[],
-                        args=[
-                            ast.arg(arg='value', annotation=ast.Name(id='int', ctx=ast.Load()))
-                        ],
+                        args=[ast.arg(arg="value", annotation=ast.Name(id="int", ctx=ast.Load()))],
                         kwonlyargs=[],
                         kw_defaults=[],
-                        defaults=[ast.Constant(value=datapoint_value)]
+                        defaults=[ast.Constant(value=datapoint_value)],
                     ),
                     body=[
                         ast.Assign(
-                            targets=[ast.Name(id='data_point_id', ctx=ast.Store())],
-                            value=ast.Constant(value=datapoint_id)
+                            targets=[ast.Name(id="data_point_id", ctx=ast.Store())],
+                            value=ast.Constant(value=datapoint_id),
                         ),
                         ast.Assign(
-                            targets=[ast.Name(id='result', ctx=ast.Store())],
-                            value=ast.Call(
-                                func=ast.Attribute(
-                                    value=ast.Name(id='RunExecuteDataPoint', ctx=ast.Load()),
-                                    attr='run_execute_data_point',
-                                    ctx=ast.Load()
-                                ),
-                                args=[ast.Name(id='data_point_id', ctx=ast.Load())],
-                                keywords=[]
-                            )
+                            targets=[ast.Name(id="result", ctx=ast.Store())],
+                            value=ast.Call(
+                                func=ast.Attribute(
+                                    value=ast.Name(id="RunExecuteDataPoint", ctx=ast.Load()),
+                                    attr="run_execute_data_point",
+                                    ctx=ast.Load(),
+                                ),
+                                args=[ast.Name(id="data_point_id", ctx=ast.Load())],
+                                keywords=[],
+                            ),
                         ),
                         ast.Expr(
                             value=ast.Call(
                                 func=ast.Attribute(
-                                    value=ast.Name(id='ExecuteDataPoint', ctx=ast.Load()),
-                                    attr='delete_lineage_data',
-                                    ctx=ast.Load()
-                                ),
-                                args=[],
-                                keywords=[]
+                                    value=ast.Name(id="ExecuteDataPoint", ctx=ast.Load()),
+                                    attr="delete_lineage_data",
+                                    ctx=ast.Load(),
+                                ),
+                                args=[],
+                                keywords=[],
                             )
                         ),
                         ast.Assert(
                             test=ast.Compare(
-                                left=ast.Name(id='result', ctx=ast.Load()),
+                                left=ast.Name(id="result", ctx=ast.Load()),
                                 ops=[ast.Eq()],
                                 comparators=[
                                     ast.Call(
-                                        func=ast.Name(id='str', ctx=ast.Load()),
-                                        args=[ast.Name(id='value', ctx=ast.Load())],
-                                        keywords=[]
+                                        func=ast.Name(id="str", ctx=ast.Load()),
+                                        args=[ast.Name(id="value", ctx=ast.Load())],
+                                        keywords=[],
                                     )
-                                ]
-                            )
-                        )
+                                ],
+                            )
+                        ),
                     ],
-                    decorator_list=[]
+                    decorator_list=[],
                 )
-
             ],
-            type_ignores=[]
+            type_ignores=[],
         )
 
         return ast.unparse(ast.fix_missing_locations(test_functions))
 
     @staticmethod
@@ -266,139 +260,110 @@
             str: Generated additional test functions as a string.
         """
         test_functions_additional = ast.Module(
             body=[
                 ast.FunctionDef(
-                    name='test_cell_metric',
-                    args=ast.arguments(
-                        posonlyargs=[],
-                        args=[],
-                        kwonlyargs=[],
-                        kw_defaults=[],
-                        defaults=[]
-                    ),
+                    name="test_cell_metric",
+                    args=ast.arguments(posonlyargs=[], args=[], kwonlyargs=[], kw_defaults=[], defaults=[]),
                     body=[
                         ast.Assign(
-                            targets=[ast.Name(id='cell', ctx=ast.Store())],
-                            value=ast.Call(
-                                func=ast.Name(id=cell_class, ctx=ast.Load()),
-                                args=[],
-                                keywords=[]
-                            )
+                            targets=[ast.Name(id="cell", ctx=ast.Store())],
+                            value=ast.Call(func=ast.Name(id=cell_class, ctx=ast.Load()), args=[], keywords=[]),
                         ),
                         ast.Expr(
                             value=ast.Call(
                                 func=ast.Attribute(
-                                    value=ast.Name(id='cell', ctx=ast.Load()),
-                                    attr='init',
-                                    ctx=ast.Load()
-                                ),
-                                args=[],
-                                keywords=[]
+                                    value=ast.Name(id="cell", ctx=ast.Load()), attr="init", ctx=ast.Load()
+                                ),
+                                args=[],
+                                keywords=[],
                             )
                         ),
                         ast.Assign(
-                            targets=[ast.Name(id='result', ctx=ast.Store())],
-                            value=ast.Call(
-                                func=ast.Attribute(
-                                    value=ast.Name(id='cell', ctx=ast.Load()),
-                                    attr='metric_value',
-                                    ctx=ast.Load()
-                                ),
-                                args=[],
-                                keywords=[]
-                            )
+                            targets=[ast.Name(id="result", ctx=ast.Store())],
+                            value=ast.Call(
+                                func=ast.Attribute(
+                                    value=ast.Name(id="cell", ctx=ast.Load()), attr="metric_value", ctx=ast.Load()
+                                ),
+                                args=[],
+                                keywords=[],
+                            ),
                         ),
                         ast.Expr(
                             value=ast.Call(
                                 func=ast.Attribute(
-                                    value=ast.Name(id='ExecuteDataPoint', ctx=ast.Load()),
-                                    attr='delete_lineage_data',
-                                    ctx=ast.Load()
-                                ),
-                                args=[],
-                                keywords=[]
+                                    value=ast.Name(id="ExecuteDataPoint", ctx=ast.Load()),
+                                    attr="delete_lineage_data",
+                                    ctx=ast.Load(),
+                                ),
+                                args=[],
+                                keywords=[],
                             )
                         ),
                         ast.Assert(
                             test=ast.Call(
-                                func=ast.Name(id='isinstance', ctx=ast.Load()),
+                                func=ast.Name(id="isinstance", ctx=ast.Load()),
                                 args=[
-                                    ast.Name(id='result', ctx=ast.Load()),
+                                    ast.Name(id="result", ctx=ast.Load()),
                                     ast.Tuple(
-                                        elts=[
-                                            ast.Name(id='int', ctx=ast.Load()),
-                                            ast.Name(id='float', ctx=ast.Load())
-                                        ],
-                                        ctx=ast.Load()
-                                    )
+                                        elts=[ast.Name(id="int", ctx=ast.Load()), ast.Name(id="float", ctx=ast.Load())],
+                                        ctx=ast.Load(),
+                                    ),
                                 ],
-                                keywords=[]
-                            )
-                        )
+                                keywords=[],
+                            )
+                        ),
                     ],
-                    decorator_list=[]
+                    decorator_list=[],
                 ),
                 ast.FunctionDef(
-                    name='test_cell_filter',
-                    args=ast.arguments(
-                        posonlyargs=[],
-                        args=[],
-                        kwonlyargs=[],
-                        kw_defaults=[],
-                        defaults=[]
-                    ),
+                    name="test_cell_filter",
+                    args=ast.arguments(posonlyargs=[], args=[], kwonlyargs=[], kw_defaults=[], defaults=[]),
                     body=[
                         ast.Assign(
-                            targets=[ast.Name(id='cell', ctx=ast.Store())],
-                            value=ast.Call(
-                                func=ast.Name(id=cell_class, ctx=ast.Load()),
-                                args=[],
-                                keywords=[]
-                            )
+                            targets=[ast.Name(id="cell", ctx=ast.Store())],
+                            value=ast.Call(func=ast.Name(id=cell_class, ctx=ast.Load()), args=[], keywords=[]),
                         ),
                         ast.Expr(
                             value=ast.Call(
                                 func=ast.Attribute(
-                                    value=ast.Name(id='cell', ctx=ast.Load()),
-                                    attr='init',
-                                    ctx=ast.Load()
-                                ),
-                                args=[],
-                                keywords=[]
+                                    value=ast.Name(id="cell", ctx=ast.Load()), attr="init", ctx=ast.Load()
+                                ),
+                                args=[],
+                                keywords=[],
                             )
                         ),
                         ast.Expr(
                             value=ast.Call(
                                 func=ast.Attribute(
-                                    value=ast.Name(id='ExecuteDataPoint', ctx=ast.Load()),
-                                    attr='delete_lineage_data',
-                                    ctx=ast.Load()
-                                ),
-                                args=[],
-                                keywords=[]
+                                    value=ast.Name(id="ExecuteDataPoint", ctx=ast.Load()),
+                                    attr="delete_lineage_data",
+                                    ctx=ast.Load(),
+                                ),
+                                args=[],
+                                keywords=[],
                             )
                         ),
                         ast.Assert(
                             test=ast.Call(
-                                func=ast.Name(id='isinstance', ctx=ast.Load()),
+                                func=ast.Name(id="isinstance", ctx=ast.Load()),
                                 args=[
                                     ast.Attribute(
-                                        value=ast.Name(id='cell', ctx=ast.Load()),
-                                        attr=f'{regulatory_template_id}s',
-                                        ctx=ast.Load()
+                                        value=ast.Name(id="cell", ctx=ast.Load()),
+                                        attr=f"{regulatory_template_id}s",
+                                        ctx=ast.Load(),
                                     ),
-                                    ast.Name(id='list', ctx=ast.Load())
+                                    ast.Name(id="list", ctx=ast.Load()),
                                 ],
-                                keywords=[]
-                            )
-                        )
+                                keywords=[],
+                            )
+                        ),
                     ],
-                    decorator_list=[]
-                )
+                    decorator_list=[],
+                ),
             ],
-            type_ignores=[]
+            type_ignores=[],
         )
 
         return ast.unparse(ast.fix_missing_locations(test_functions_additional))
 
     @staticmethod
@@ -410,13 +375,13 @@
             output_file (str): Path to the output file.
             import_code (str): Import statements code.
             test_code (str): Test functions code.
             logger (logging.Logger): Logger instance for logging debug messages.
         """
-        with open(output_file, 'w') as f:
+        with open(output_file, "w") as f:
             f.write(import_code)
-            f.write('\n\n')
+            f.write("\n\n")
             f.write(test_code)
 
         logger.debug(f"Saved generated code to {output_file}")
 
     @classmethod
@@ -425,11 +390,11 @@
         Generate test code based on command line arguments.
 
         This method parses command line arguments, initializes variables,
         generates code components, and saves the generated code to a file.
         """
-        logger = return_logger(str(Path(__file__).resolve()).rsplit("/",1)[-1])
+        logger = return_logger(str(Path(__file__).resolve()).rsplit("/", 1)[-1])
 
         # Parse command line arguments
         args = cls.parse_arguments()
         logger.debug(f"Running with arguments: {args}")
 
@@ -447,15 +412,15 @@
         logger.debug("Generated import code")
 
         test_code = cls.create_test_functions(datapoint_value, datapoint_id)
         logger.debug("Generated test functions")
 
-        #test_code_additional = cls.create_additional_test_functions(cell_class, regulatory_template_id)
-        #logger.debug("Generated additional test functions")
+        # test_code_additional = cls.create_additional_test_functions(cell_class, regulatory_template_id)
+        # logger.debug("Generated additional test functions")
 
         # Save generated code
-        output_file = os.path.join('tests', f'test_{cell_class.lower()}__{scenario_name}.py')
+        output_file = os.path.join("tests", f"test_{cell_class.lower()}__{scenario_name}.py")
         cls.save_generated_code(output_file, import_code, test_code, logger)
 
 
 def main():
     """
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/datapoint_test_run/generator_for_tests.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/secure_error_handling.py	2025-09-15 13:18:11.414430+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/secure_error_handling.py	2025-09-21 17:07:34.758814+00:00
@@ -17,248 +17,244 @@
 from django.db import DatabaseError, IntegrityError, OperationalError
 from django.contrib import messages
 
 logger = logging.getLogger(__name__)
 
+
 class SecureErrorHandler:
     """
     Utility class for secure error handling that prevents information disclosure.
     """
-    
+
     # Generic error messages that don't reveal implementation details
     GENERIC_MESSAGES = {
-        'database_error': 'A database error occurred. Please try again later.',
-        'file_error': 'File operation failed. Please check the file and try again.',
-        'validation_error': 'Invalid input provided. Please correct and try again.',
-        'permission_error': 'Access denied. You do not have permission to perform this operation.',
-        'not_found': 'The requested resource was not found.',
-        'internal_error': 'An internal error occurred. Please try again later.',
-        'upload_error': 'File upload failed. Please check the file and try again.',
-        'processing_error': 'Processing failed. Please try again later.',
-        'configuration_error': 'Configuration error. Please contact the administrator.',
-        'timeout_error': 'The operation timed out. Please try again.',
+        "database_error": "A database error occurred. Please try again later.",
+        "file_error": "File operation failed. Please check the file and try again.",
+        "validation_error": "Invalid input provided. Please correct and try again.",
+        "permission_error": "Access denied. You do not have permission to perform this operation.",
+        "not_found": "The requested resource was not found.",
+        "internal_error": "An internal error occurred. Please try again later.",
+        "upload_error": "File upload failed. Please check the file and try again.",
+        "processing_error": "Processing failed. Please try again later.",
+        "configuration_error": "Configuration error. Please contact the administrator.",
+        "timeout_error": "The operation timed out. Please try again.",
     }
-    
-    @staticmethod
-    def handle_exception(exception: Exception, context: Optional[str] = None, 
-                        request=None) -> Dict[str, Any]:
+
+    @staticmethod
+    def handle_exception(exception: Exception, context: Optional[str] = None, request=None) -> Dict[str, Any]:
         """
         Handle exception securely by logging full details internally and returning safe message.
-        
+
         Args:
             exception: The exception that occurred
             context: Optional context description for logging
             request: Django request object (optional)
-            
+
         Returns:
             Dict with safe error information for user
         """
         # Log full exception details for developers (not visible to users)
         log_message = f"Exception in {context or 'unknown context'}: {str(exception)}"
         logger.error(log_message, exc_info=True)
-        
+
         # Determine appropriate user-safe message based on exception type
         if isinstance(exception, (DatabaseError, IntegrityError, OperationalError)):
-            user_message = SecureErrorHandler.GENERIC_MESSAGES['database_error']
+            user_message = SecureErrorHandler.GENERIC_MESSAGES["database_error"]
         elif isinstance(exception, FileNotFoundError):
-            user_message = SecureErrorHandler.GENERIC_MESSAGES['file_error']
+            user_message = SecureErrorHandler.GENERIC_MESSAGES["file_error"]
         elif isinstance(exception, PermissionError):
-            user_message = SecureErrorHandler.GENERIC_MESSAGES['permission_error']
+            user_message = SecureErrorHandler.GENERIC_MESSAGES["permission_error"]
         elif isinstance(exception, ValueError):
-            user_message = SecureErrorHandler.GENERIC_MESSAGES['validation_error']
+            user_message = SecureErrorHandler.GENERIC_MESSAGES["validation_error"]
         elif isinstance(exception, TimeoutError):
-            user_message = SecureErrorHandler.GENERIC_MESSAGES['timeout_error']
+            user_message = SecureErrorHandler.GENERIC_MESSAGES["timeout_error"]
         else:
-            user_message = SecureErrorHandler.GENERIC_MESSAGES['internal_error']
-        
+            user_message = SecureErrorHandler.GENERIC_MESSAGES["internal_error"]
+
         return {
-            'status': 'error',
-            'message': user_message,
-            'error_id': id(exception)  # Can be used for support ticket reference
+            "status": "error",
+            "message": user_message,
+            "error_id": id(exception),  # Can be used for support ticket reference
         }
-    
-    @staticmethod
-    def secure_json_response(exception: Exception, context: Optional[str] = None,
-                           request=None) -> JsonResponse:
+
+    @staticmethod
+    def secure_json_response(exception: Exception, context: Optional[str] = None, request=None) -> JsonResponse:
         """
         Return a secure JSON error response.
-        
+
         Args:
             exception: The exception that occurred
             context: Optional context description
             request: Django request object (optional)
-            
+
         Returns:
             JsonResponse with safe error message
         """
         error_data = SecureErrorHandler.handle_exception(exception, context, request)
         return JsonResponse(error_data, status=500)
-    
-    @staticmethod
-    def secure_http_response(exception: Exception, context: Optional[str] = None,
-                           request=None) -> HttpResponseServerError:
+
+    @staticmethod
+    def secure_http_response(
+        exception: Exception, context: Optional[str] = None, request=None
+    ) -> HttpResponseServerError:
         """
         Return a secure HTTP error response.
-        
+
         Args:
             exception: The exception that occurred
             context: Optional context description
             request: Django request object (optional)
-            
+
         Returns:
             HttpResponseServerError with safe error message
         """
         error_data = SecureErrorHandler.handle_exception(exception, context, request)
-        return HttpResponseServerError(error_data['message'])
-    
+        return HttpResponseServerError(error_data["message"])
+
     @staticmethod
     def secure_message(request, exception: Exception, context: Optional[str] = None):
         """
         Add a secure error message to Django's messages framework.
-        
+
         Args:
             request: Django request object
             exception: The exception that occurred
             context: Optional context description
         """
         error_data = SecureErrorHandler.handle_exception(exception, context, request)
-        messages.error(request, error_data['message'])
-    
-    @staticmethod
-    def log_security_event(event_type: str, details: str, request=None, 
-                          severity: str = 'WARNING'):
+        messages.error(request, error_data["message"])
+
+    @staticmethod
+    def log_security_event(event_type: str, details: str, request=None, severity: str = "WARNING"):
         """
         Log security-related events for monitoring and alerting.
-        
+
         Args:
             event_type: Type of security event (e.g., 'path_traversal_attempt')
             details: Details of the event
             request: Django request object (optional)
             severity: Log severity level
         """
         log_message = f"SECURITY EVENT [{event_type}]: {details}"
-        
+
         if request:
             log_message += f" - IP: {request.META.get('REMOTE_ADDR', 'unknown')}"
             log_message += f" - User: {getattr(request.user, 'username', 'anonymous')}"
-        
+
         getattr(logger, severity.lower())(log_message)
-    
+
     @staticmethod
     def sanitize_user_input_error(user_input: str, max_length: int = 50) -> str:
         """
         Sanitize user input for safe inclusion in error messages.
-        
+
         Args:
             user_input: User-provided input
             max_length: Maximum length to include in error message
-            
+
         Returns:
             Sanitized input safe for error messages
         """
         if not user_input:
             return "[empty]"
-        
+
         # Truncate and remove potentially dangerous characters
         safe_input = str(user_input)[:max_length]
         # Remove HTML/script tags and path separators
-        safe_input = safe_input.replace('<', '&lt;').replace('>', '&gt;')
-        safe_input = safe_input.replace('/', '_').replace('\\', '_')
-        safe_input = safe_input.replace('..', '_')
-        
+        safe_input = safe_input.replace("<", "&lt;").replace(">", "&gt;")
+        safe_input = safe_input.replace("/", "_").replace("\\", "_")
+        safe_input = safe_input.replace("..", "_")
+
         return safe_input
 
+
 class DatabaseErrorHandler:
     """
     Specialized handler for database-related errors.
     """
-    
-    @staticmethod
-    def handle_database_error(exception: Exception, operation: str = "database operation",
-                            request=None) -> Dict[str, Any]:
+
+    @staticmethod
+    def handle_database_error(
+        exception: Exception, operation: str = "database operation", request=None
+    ) -> Dict[str, Any]:
         """
         Handle database errors securely.
-        
+
         Args:
             exception: Database exception
             operation: Description of the database operation
             request: Django request object (optional)
-            
+
         Returns:
             Safe error response data
         """
         # Log detailed database error for developers
-        logger.error(f"Database error during {operation}: {str(exception)}", 
-                    exc_info=True)
-        
+        logger.error(f"Database error during {operation}: {str(exception)}", exc_info=True)
+
         # Return generic message to prevent database schema disclosure
         return {
-            'status': 'error',
-            'message': 'Database operation failed. Please try again later.',
-            'operation': operation.replace(str(exception), '[redacted]')
+            "status": "error",
+            "message": "Database operation failed. Please try again later.",
+            "operation": operation.replace(str(exception), "[redacted]"),
         }
-    
-    @staticmethod
-    def handle_integrity_error(exception: IntegrityError, context: str = "operation",
-                             request=None) -> Dict[str, Any]:
+
+    @staticmethod
+    def handle_integrity_error(exception: IntegrityError, context: str = "operation", request=None) -> Dict[str, Any]:
         """
         Handle database integrity errors (like unique constraint violations).
-        
+
         Args:
             exception: IntegrityError exception
             context: Context of the operation
             request: Django request object (optional)
-            
+
         Returns:
             Safe error response data
         """
-        logger.error(f"Integrity error during {context}: {str(exception)}", 
-                    exc_info=True)
-        
+        logger.error(f"Integrity error during {context}: {str(exception)}", exc_info=True)
+
         # Don't reveal constraint names or table structures
         return {
-            'status': 'error',
-            'message': 'Data validation failed. The operation conflicts with existing data.',
-            'context': context
+            "status": "error",
+            "message": "Data validation failed. The operation conflicts with existing data.",
+            "context": context,
         }
 
+
 class FileOperationErrorHandler:
     """
     Specialized handler for file operation errors.
     """
-    
-    @staticmethod
-    def handle_file_error(exception: Exception, operation: str = "file operation",
-                         filename: Optional[str] = None, request=None) -> Dict[str, Any]:
+
+    @staticmethod
+    def handle_file_error(
+        exception: Exception, operation: str = "file operation", filename: Optional[str] = None, request=None
+    ) -> Dict[str, Any]:
         """
         Handle file operation errors securely.
-        
+
         Args:
             exception: File operation exception
             operation: Description of the file operation
             filename: Optional filename (will be sanitized)
             request: Django request object (optional)
-            
+
         Returns:
             Safe error response data
         """
         # Log detailed error for developers (with full path if needed)
         log_msg = f"File error during {operation}: {str(exception)}"
         if filename:
             log_msg += f" - File: {filename}"
         logger.error(log_msg, exc_info=True)
-        
+
         # Return safe message without revealing file paths
         safe_filename = SecureErrorHandler.sanitize_user_input_error(filename) if filename else "file"
-        
+
         if isinstance(exception, FileNotFoundError):
             message = f"The requested {safe_filename} was not found."
         elif isinstance(exception, PermissionError):
             message = f"Access denied to {safe_filename}."
         else:
             message = f"File operation failed for {safe_filename}."
-        
-        return {
-            'status': 'error',
-            'message': message,
-            'operation': operation
-        }
\ No newline at end of file
+
+        return {"status": "error", "message": message, "operation": operation}
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/secure_error_handling.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/secure_file_utils.py	2025-09-15 13:18:11.414510+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/secure_file_utils.py	2025-09-21 17:07:34.780967+00:00
@@ -14,170 +14,173 @@
 import os
 import re
 from pathlib import Path
 from typing import Optional, Set
 
+
 class SecureFileValidator:
     """
     Utility class for secure file operations with path traversal protection.
     """
-    
+
     # Default allowed file extensions for different use cases
-    DOCUMENT_EXTENSIONS = {'.csv', '.xml', '.xsd', '.txt', '.json', '.dmd', '.html', '.md'}
-    IMAGE_EXTENSIONS = {'.jpg', '.jpeg', '.png', '.gif', '.svg', '.webp'}
+    DOCUMENT_EXTENSIONS = {".csv", ".xml", ".xsd", ".txt", ".json", ".dmd", ".html", ".md"}
+    IMAGE_EXTENSIONS = {".jpg", ".jpeg", ".png", ".gif", ".svg", ".webp"}
     ALL_SAFE_EXTENSIONS = DOCUMENT_EXTENSIONS | IMAGE_EXTENSIONS
-    
+
     # Maximum file size (50MB by default)
     DEFAULT_MAX_FILE_SIZE = 50 * 1024 * 1024
-    
+
     @staticmethod
     def sanitize_filename(filename: str, allowed_extensions: Optional[Set[str]] = None) -> Optional[str]:
         """
         Sanitize filename to prevent path traversal attacks and validate extension.
-        
+
         Args:
             filename: Original filename from user input
             allowed_extensions: Set of allowed file extensions (default: DOCUMENT_EXTENSIONS)
-            
+
         Returns:
             Safe filename or None if invalid
         """
         if not filename:
             return None
-        
+
         if allowed_extensions is None:
             allowed_extensions = SecureFileValidator.DOCUMENT_EXTENSIONS
-            
+
         # Get just the filename part (remove any path components)
         safe_name = os.path.basename(filename)
-        
+
         # Remove path traversal sequences
-        safe_name = safe_name.replace('..', '').replace('/', '').replace('\\', '')
-        
+        safe_name = safe_name.replace("..", "").replace("/", "").replace("\\", "")
+
         # Remove or replace dangerous characters
-        safe_name = re.sub(r'[<>:"|?*\x00-\x1f]', '', safe_name)
-        
+        safe_name = re.sub(r'[<>:"|?*\x00-\x1f]', "", safe_name)
+
         # Ensure filename is not empty and has reasonable length
         if not safe_name or len(safe_name) > 255:
             return None
-        
+
         # Validate filename format (alphanumeric, dots, hyphens, underscores only)
-        if not re.match(r'^[a-zA-Z0-9._-]+$', safe_name):
+        if not re.match(r"^[a-zA-Z0-9._-]+$", safe_name):
             return None
-            
+
         # Check file extension
         _, ext = os.path.splitext(safe_name.lower())
         if ext not in allowed_extensions:
             return None
-            
+
         return safe_name
-    
+
     @staticmethod
     def validate_path_safety(file_path: Path, base_directory: Path) -> bool:
         """
         Validate that a file path is safe and within the expected directory boundaries.
-        
+
         Args:
             file_path: The file path to validate
             base_directory: The base directory that should contain the file
-            
+
         Returns:
             True if path is safe, False otherwise
         """
         try:
             # Resolve both paths to handle any symlinks or relative components
             resolved_file_path = file_path.resolve()
             resolved_base_dir = base_directory.resolve()
-            
+
             # Check if the file path is within the base directory
             return resolved_file_path.is_relative_to(resolved_base_dir)
-            
+
         except (OSError, ValueError):
             return False
-    
+
     @staticmethod
-    def get_safe_file_path(base_directory: str | Path, filename: str, 
-                          allowed_extensions: Optional[Set[str]] = None) -> Optional[Path]:
+    def get_safe_file_path(
+        base_directory: str | Path, filename: str, allowed_extensions: Optional[Set[str]] = None
+    ) -> Optional[Path]:
         """
         Construct a safe file path by validating the filename and ensuring it's within bounds.
-        
+
         Args:
             base_directory: Base directory path
             filename: User-provided filename
             allowed_extensions: Set of allowed file extensions
-            
+
         Returns:
             Safe Path object or None if invalid
         """
         # Sanitize the filename
         safe_filename = SecureFileValidator.sanitize_filename(filename, allowed_extensions)
         if not safe_filename:
             return None
-        
+
         # Create base directory path
         base_path = Path(base_directory)
-        
+
         # Construct the file path
         file_path = base_path / safe_filename
-        
+
         # Validate the path is safe
         if not SecureFileValidator.validate_path_safety(file_path, base_path):
             return None
-            
+
         return file_path
-    
+
     @staticmethod
     def validate_file_size(file_size: int, max_size: int = DEFAULT_MAX_FILE_SIZE) -> bool:
         """
         Validate file size to prevent DoS attacks.
-        
+
         Args:
             file_size: Size of the file in bytes
             max_size: Maximum allowed file size in bytes
-            
+
         Returns:
             True if file size is acceptable
         """
         return 0 < file_size <= max_size
-    
+
     @staticmethod
-    def create_secure_upload_path(base_directory: str | Path, original_filename: str,
-                                 allowed_extensions: Optional[Set[str]] = None) -> tuple[Optional[Path], Optional[str]]:
+    def create_secure_upload_path(
+        base_directory: str | Path, original_filename: str, allowed_extensions: Optional[Set[str]] = None
+    ) -> tuple[Optional[Path], Optional[str]]:
         """
         Create a secure upload path with automatic filename collision handling.
-        
+
         Args:
             base_directory: Base directory for uploads
             original_filename: Original filename from user
             allowed_extensions: Set of allowed file extensions
-            
+
         Returns:
             Tuple of (safe_file_path, safe_filename) or (None, None) if invalid
         """
         safe_filename = SecureFileValidator.sanitize_filename(original_filename, allowed_extensions)
         if not safe_filename:
             return None, None
-        
+
         base_path = Path(base_directory)
-        
+
         # Handle filename collisions by adding counter
         original_safe_name = safe_filename
         counter = 1
         while True:
             file_path = base_path / safe_filename
-            
+
             # Validate the path is safe
             if not SecureFileValidator.validate_path_safety(file_path, base_path):
                 return None, None
-            
+
             # If file doesn't exist, we can use this name
             if not file_path.exists():
                 return file_path, safe_filename
-            
+
             # Generate new filename with counter
             name, ext = os.path.splitext(original_safe_name)
             safe_filename = f"{name}_{counter}{ext}"
             counter += 1
-            
+
             # Prevent infinite loop
             if counter > 1000:
-                return None, None
\ No newline at end of file
+                return None, None
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/secure_file_utils.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/member_hierarchy_editor/test_integration.py	2025-09-15 13:18:11.414315+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/member_hierarchy_editor/test_integration.py	2025-09-21 17:07:34.783186+00:00
@@ -23,218 +23,190 @@
 
     def setUp(self):
         """Set up test data"""
         # Create test domain
         self.domain = DOMAIN.objects.create(
-            domain_id="TEST_DOMAIN",
-            name="Test Domain",
-            description="Test domain for hierarchy testing"
+            domain_id="TEST_DOMAIN", name="Test Domain", description="Test domain for hierarchy testing"
         )
 
         # Create test hierarchy
         self.hierarchy = MEMBER_HIERARCHY.objects.create(
             member_hierarchy_id="TEST_HIERARCHY",
             name="Test Hierarchy",
             description="Test hierarchy for testing",
-            domain_id=self.domain
+            domain_id=self.domain,
         )
 
         # Create test members
         self.member1 = MEMBER.objects.create(
-            member_id="MEMBER_001",
-            name="Root Member",
-            description="Root level member",
-            domain_id=self.domain
+            member_id="MEMBER_001", name="Root Member", description="Root level member", domain_id=self.domain
         )
 
         self.member2 = MEMBER.objects.create(
-            member_id="MEMBER_002",
-            name="Child Member 1",
-            description="First child member",
-            domain_id=self.domain
+            member_id="MEMBER_002", name="Child Member 1", description="First child member", domain_id=self.domain
         )
 
         self.member3 = MEMBER.objects.create(
-            member_id="MEMBER_003",
-            name="Child Member 2",
-            description="Second child member",
-            domain_id=self.domain
+            member_id="MEMBER_003", name="Child Member 2", description="Second child member", domain_id=self.domain
         )
 
         # Create hierarchy nodes
         self.root_node = MEMBER_HIERARCHY_NODE.objects.create(
-            member_hierarchy_id=self.hierarchy,
-            member_id=self.member1,
-            level=1,
-            comparator="=",
-            operator=""
+            member_hierarchy_id=self.hierarchy, member_id=self.member1, level=1, comparator="=", operator=""
         )
 
         self.child_node1 = MEMBER_HIERARCHY_NODE.objects.create(
             member_hierarchy_id=self.hierarchy,
             member_id=self.member2,
             parent_member_id=self.member1,
             level=2,
             comparator="",
-            operator="+"
+            operator="+",
         )
 
         self.child_node2 = MEMBER_HIERARCHY_NODE.objects.create(
             member_hierarchy_id=self.hierarchy,
             member_id=self.member3,
             parent_member_id=self.member1,
             level=2,
             comparator="",
-            operator="+"
+            operator="+",
         )
 
         self.integration = DjangoMemberHierarchyIntegration()
         self.converter = DjangoModelConverter()
 
     def test_get_hierarchy_by_id(self):
         """Test converting Django models to visualization format"""
         result = self.integration.get_hierarchy_by_id("TEST_HIERARCHY")
 
-        self.assertIn('boxes', result)
-        self.assertIn('arrows', result)
-        self.assertIn('hierarchy_info', result)
+        self.assertIn("boxes", result)
+        self.assertIn("arrows", result)
+        self.assertIn("hierarchy_info", result)
 
         # Should have 3 boxes
-        self.assertEqual(len(result['boxes']), 3)
+        self.assertEqual(len(result["boxes"]), 3)
 
         # Should have 2 arrows (child1->root, child2->root)
-        self.assertEqual(len(result['arrows']), 2)
+        self.assertEqual(len(result["arrows"]), 2)
 
         # Check hierarchy info
-        hierarchy_info = result['hierarchy_info']
-        self.assertEqual(hierarchy_info['id'], 'TEST_HIERARCHY')
-        self.assertEqual(hierarchy_info['name'], 'Test Hierarchy')
-        self.assertEqual(hierarchy_info['domain'], 'TEST_DOMAIN')
+        hierarchy_info = result["hierarchy_info"]
+        self.assertEqual(hierarchy_info["id"], "TEST_HIERARCHY")
+        self.assertEqual(hierarchy_info["name"], "Test Hierarchy")
+        self.assertEqual(hierarchy_info["domain"], "TEST_DOMAIN")
 
     def test_save_hierarchy_from_visualization(self):
         """Test saving visualization data back to Django models"""
         visualization_data = {
-            'boxes': [
+            "boxes": [
                 {
-                    'id': 'MEMBER_001',
-                    'x': 100,
-                    'y': 100,
-                    'width': 300,
-                    'height': 120,
-                    'name': 'Root Member',
-                    'text': 'Updated description'
+                    "id": "MEMBER_001",
+                    "x": 100,
+                    "y": 100,
+                    "width": 300,
+                    "height": 120,
+                    "name": "Root Member",
+                    "text": "Updated description",
                 },
                 {
-                    'id': 'MEMBER_002',
-                    'x': 200,
-                    'y': 300,
-                    'width': 300,
-                    'height': 120,
-                    'name': 'Child Member 1',
-                    'text': 'Child description'
-                }
+                    "id": "MEMBER_002",
+                    "x": 200,
+                    "y": 300,
+                    "width": 300,
+                    "height": 120,
+                    "name": "Child Member 1",
+                    "text": "Child description",
+                },
             ],
-            'arrows': [
-                {
-                    'from': 'MEMBER_002',
-                    'to': 'MEMBER_001'
-                }
-            ]
+            "arrows": [{"from": "MEMBER_002", "to": "MEMBER_001"}],
         }
 
-        success = self.integration.save_hierarchy_from_visualization(
-            "TEST_HIERARCHY",
-            visualization_data
-        )
+        success = self.integration.save_hierarchy_from_visualization("TEST_HIERARCHY", visualization_data)
 
         self.assertTrue(success)
 
         # Verify nodes were recreated correctly
-        nodes = MEMBER_HIERARCHY_NODE.objects.filter(
-            member_hierarchy_id=self.hierarchy
-        ).order_by('level', 'member_id__member_id')
+        nodes = MEMBER_HIERARCHY_NODE.objects.filter(member_hierarchy_id=self.hierarchy).order_by(
+            "level", "member_id__member_id"
+        )
 
         self.assertEqual(nodes.count(), 2)
 
         # Check root node
         root = nodes.filter(level=1).first()
-        self.assertEqual(root.member_id.member_id, 'MEMBER_001')
-        self.assertEqual(root.comparator, '=')
-        self.assertEqual(root.operator, '')
+        self.assertEqual(root.member_id.member_id, "MEMBER_001")
+        self.assertEqual(root.comparator, "=")
+        self.assertEqual(root.operator, "")
         self.assertIsNone(root.parent_member_id)
 
         # Check child node
         child = nodes.filter(level=2).first()
-        self.assertEqual(child.member_id.member_id, 'MEMBER_002')
-        self.assertEqual(child.comparator, '')
-        self.assertEqual(child.operator, '+')
-        self.assertEqual(child.parent_member_id.member_id, 'MEMBER_001')
+        self.assertEqual(child.member_id.member_id, "MEMBER_002")
+        self.assertEqual(child.comparator, "")
+        self.assertEqual(child.operator, "+")
+        self.assertEqual(child.parent_member_id.member_id, "MEMBER_001")
 
     def test_validate_hierarchy_structure(self):
         """Test hierarchy structure validation"""
         # Valid structure
         valid_data = {
-            'boxes': [
-                {'id': 'MEMBER_001', 'x': 0, 'y': 0, 'width': 300, 'height': 120, 'name': 'Root', 'text': ''},
-                {'id': 'MEMBER_002', 'x': 0, 'y': 200, 'width': 300, 'height': 120, 'name': 'Child', 'text': ''}
+            "boxes": [
+                {"id": "MEMBER_001", "x": 0, "y": 0, "width": 300, "height": 120, "name": "Root", "text": ""},
+                {"id": "MEMBER_002", "x": 0, "y": 200, "width": 300, "height": 120, "name": "Child", "text": ""},
             ],
-            'arrows': [
-                {'from': 'MEMBER_002', 'to': 'MEMBER_001'}
-            ]
+            "arrows": [{"from": "MEMBER_002", "to": "MEMBER_001"}],
         }
 
         is_valid, errors = self.converter.validate_hierarchy_structure(valid_data)
         self.assertTrue(is_valid)
         self.assertEqual(len([e for e in errors if e.startswith("Error:")]), 0)
 
         # Invalid structure - circular reference
         circular_data = {
-            'boxes': [
-                {'id': 'MEMBER_001', 'x': 0, 'y': 0, 'width': 300, 'height': 120, 'name': 'A', 'text': ''},
-                {'id': 'MEMBER_002', 'x': 0, 'y': 200, 'width': 300, 'height': 120, 'name': 'B', 'text': ''}
+            "boxes": [
+                {"id": "MEMBER_001", "x": 0, "y": 0, "width": 300, "height": 120, "name": "A", "text": ""},
+                {"id": "MEMBER_002", "x": 0, "y": 200, "width": 300, "height": 120, "name": "B", "text": ""},
             ],
-            'arrows': [
-                {'from': 'MEMBER_001', 'to': 'MEMBER_002'},
-                {'from': 'MEMBER_002', 'to': 'MEMBER_001'}
-            ]
+            "arrows": [{"from": "MEMBER_001", "to": "MEMBER_002"}, {"from": "MEMBER_002", "to": "MEMBER_001"}],
         }
 
         is_valid, errors = self.converter.validate_hierarchy_structure(circular_data)
         self.assertFalse(is_valid)
-        self.assertTrue(any('circular' in error.lower() for error in errors))
+        self.assertTrue(any("circular" in error.lower() for error in errors))
 
     def test_get_available_hierarchies(self):
         """Test getting list of available hierarchies"""
         hierarchies = self.integration.get_available_hierarchies()
 
         self.assertEqual(len(hierarchies), 1)
         hierarchy = hierarchies[0]
 
-        self.assertEqual(hierarchy['member_hierarchy_id'], 'TEST_HIERARCHY')
-        self.assertEqual(hierarchy['name'], 'Test Hierarchy')
-        self.assertEqual(hierarchy['domain_id'], 'TEST_DOMAIN')
+        self.assertEqual(hierarchy["member_hierarchy_id"], "TEST_HIERARCHY")
+        self.assertEqual(hierarchy["name"], "Test Hierarchy")
+        self.assertEqual(hierarchy["domain_id"], "TEST_DOMAIN")
 
     def test_get_domain_members(self):
         """Test getting members for a domain"""
-        members = self.integration.get_domain_members('TEST_DOMAIN')
+        members = self.integration.get_domain_members("TEST_DOMAIN")
 
         self.assertEqual(len(members), 3)
-        member_ids = [m['member_id'] for m in members]
-        self.assertIn('MEMBER_001', member_ids)
-        self.assertIn('MEMBER_002', member_ids)
-        self.assertIn('MEMBER_003', member_ids)
+        member_ids = [m["member_id"] for m in members]
+        self.assertIn("MEMBER_001", member_ids)
+        self.assertIn("MEMBER_002", member_ids)
+        self.assertIn("MEMBER_003", member_ids)
 
     def test_get_hierarchy_statistics(self):
         """Test getting hierarchy statistics"""
-        stats = self.converter.get_hierarchy_statistics('TEST_HIERARCHY')
-
-        self.assertEqual(stats['total_nodes'], 3)
-        self.assertEqual(stats['max_level'], 2)
-        self.assertEqual(stats['root_nodes'], 1)
-        self.assertEqual(stats['leaf_nodes'], 2)
-        self.assertEqual(stats['intermediate_nodes'], 0)
-        self.assertEqual(stats['hierarchy_name'], 'Test Hierarchy')
-        self.assertEqual(stats['domain'], 'TEST_DOMAIN')
-
-
-if __name__ == '__main__':
+        stats = self.converter.get_hierarchy_statistics("TEST_HIERARCHY")
+
+        self.assertEqual(stats["total_nodes"], 3)
+        self.assertEqual(stats["max_level"], 2)
+        self.assertEqual(stats["root_nodes"], 1)
+        self.assertEqual(stats["leaf_nodes"], 2)
+        self.assertEqual(stats["intermediate_nodes"], 0)
+        self.assertEqual(stats["hierarchy_name"], "Test Hierarchy")
+        self.assertEqual(stats["domain"], "TEST_DOMAIN")
+
+
+if __name__ == "__main__":
     unittest.main()
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/member_hierarchy_editor/test_integration.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/github_file_fetcher.py	2025-09-15 13:18:11.412253+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/github_file_fetcher.py	2025-09-21 17:07:34.787624+00:00
@@ -15,18 +15,16 @@
 import logging
 
 # Configure logging
 logging.basicConfig(
     level=logging.INFO,
-    format='%(asctime)s - %(levelname)s - %(message)s',
-    handlers=[
-        logging.FileHandler('github_fetcher.log'),
-        logging.StreamHandler()
-    ]
+    format="%(asctime)s - %(levelname)s - %(message)s",
+    handlers=[logging.FileHandler("github_fetcher.log"), logging.StreamHandler()],
 )
 logger = logging.getLogger(__name__)
 
+
 class GitHubFileFetcher:
     def __init__(self, base_url):
         """
         Initialize GitHub file fetcher with repository URL.
 
@@ -35,11 +33,11 @@
         """
         logger.info(f"Initializing GitHubFileFetcher with URL: {base_url}")
 
         self.base_url = base_url
         # Parse the GitHub URL to extract owner and repository name
-        parts = base_url.replace('https://github.com/', '').split('/')
+        parts = base_url.replace("https://github.com/", "").split("/")
         self.owner = parts[0]
         self.repo = parts[1]
         # Construct the GitHub API base URL
         self.api_base = f"https://api.github.com/repos/{self.owner}/{self.repo}"
         # Dictionary to cache file information
@@ -100,11 +98,11 @@
         """
         try:
             response = requests.get(raw_url)
             response.raise_for_status()
 
-            with open(local_path, 'wb') as f:
+            with open(local_path, "wb") as f:
                 f.write(response.content)
 
             # logger.info(f"Successfully downloaded file to: {local_path}")
             return True
         except requests.exceptions.RequestException as e:
@@ -147,11 +145,11 @@
         # Construct the GitHub tree URL for the specific folder
         commit_info_url = f"https://github.com/{self.owner}/{self.repo}/tree/{branch}/birds_nest/{folder_path}"
         logger.debug(f"Fetching commit info from URL: {commit_info_url}")
 
         # Make HTTP request to GitHub with JSON headers
-        response = requests.get(commit_info_url, headers={'Accept': 'application/json'})
+        response = requests.get(commit_info_url, headers={"Accept": "application/json"})
         response.raise_for_status()
 
         # Parse the response JSON to extract payload data
         payload_data = response.json()["payload"]
         tree_items = payload_data["tree"]["items"]
@@ -187,11 +185,11 @@
         # Construct result structure
         result = {
             "tree": tree_items,
             "fileTree": file_tree,
             "refInfo": payload_data.get("refInfo", {}),
-            "repo": payload_data.get("repo", {})
+            "repo": payload_data.get("repo", {}),
         }
 
         # Cache the result for this folder path
         self.files[folder_path] = result
         logger.info(f"Cached commit info for folder: {folder_path}")
@@ -239,32 +237,32 @@
 
         Returns:
             str or None: Local file path if saved, file content if not saved, None on error
         """
         # Validate that this is actually a file
-        if file_info.get('type') != 'file':
+        if file_info.get("type") != "file":
             logger.warning(f"Attempted to download non-file item: {file_info.get('name', 'Unknown')}")
             return None
 
         # Get the download URL from file info
-        download_url = file_info.get('download_url')
+        download_url = file_info.get("download_url")
         if not download_url:
             logger.warning(f"No download URL found for file: {file_info.get('name', 'Unknown')}")
             return None
 
-        file_name = file_info.get('name', 'Unknown')
+        file_name = file_info.get("name", "Unknown")
         logger.info(f"Downloading file: {file_name}")
 
         try:
             # Download the file content
             response = requests.get(download_url)
             response.raise_for_status()
 
             if local_path:
                 # Save to local file system
                 logger.debug(f"Saving file to: {local_path}")
-                with open(local_path, 'wb') as f:
+                with open(local_path, "wb") as f:
                     f.write(response.content)
                 logger.info(f"Successfully saved file: {local_path}")
                 return local_path
             else:
                 # Return file content as text
@@ -287,24 +285,24 @@
 
         # Get list of files and directories in the current folder
         files = self.fetch_files(folder_path)
 
         for item in files:
-            name = item.get('name', 'Unknown')
-            item_type = item.get('type', 'Unknown')
-
-            if item_type == 'dir':
+            name = item.get("name", "Unknown")
+            item_type = item.get("type", "Unknown")
+
+            if item_type == "dir":
                 # Process subdirectory recursively
                 sub_folder_path = f"{folder_path}/{name}" if folder_path else name
                 local_sub_path = os.path.join(local_base_path, name)
 
                 logger.debug(f"Creating directory: {local_sub_path}")
                 self._ensure_directory_exists(local_sub_path)
 
                 # Recursive call for subdirectory
                 self.fetch_directory_recursively(sub_folder_path, local_sub_path)
-            elif item_type == 'file':
+            elif item_type == "file":
                 # Download individual file
                 local_file_path = os.path.join(local_base_path, name)
                 logger.debug(f"Downloading file: {name} to {local_file_path}")
                 self.download_file(item, local_file_path)
 
@@ -315,11 +313,11 @@
         # Fetch files from the database export directory
         files = self.fetch_files("export/database_export_ldm")
         logger.info(f"Found {len(files)} database export files")
 
         for file_info in files:
-            name = file_info.get('name', 'Unknown')
+            name = file_info.get("name", "Unknown")
             logger.debug(f"Processing database export file: {name}")
 
             # Categorize files based on their names
             if "bird" in name:
                 folder = "bird"
@@ -342,24 +340,26 @@
                 logger.info(f"Successfully saved database export file: {local_file_path}")
             else:
                 logger.warning(f"Failed to download database export file: {name}")
 
     def fetch_test_fixture(self, folder_data, path_downloaded):
-        file_tree = folder_data.get('fileTree', {})
+        file_tree = folder_data.get("fileTree", {})
         logger.debug(f"Processing file tree with {len(file_tree)} directories")
-        it_ = sum(list(map(lambda item_data : item_data.get("items", []), file_tree.values())), [])
+        it_ = sum(list(map(lambda item_data: item_data.get("items", []), file_tree.values())), [])
         success = False
 
         for item in it_:
-            right_content_type = item['contentType'] == 'file'
-            right_path = f"pybirdai{os.sep}tests" in os.path.join("pybirdai", item['path'].replace(f'birds_nest{os.sep}', ''))
+            right_content_type = item["contentType"] == "file"
+            right_path = f"pybirdai{os.sep}tests" in os.path.join(
+                "pybirdai", item["path"].replace(f"birds_nest{os.sep}", "")
+            )
 
             if not (right_content_type and right_path):
                 continue
 
-            file_path = item['path']
-            relative_path = file_path.replace(f'birds_nest/', '').replace(f'birds_nest{os.sep}', '')
+            file_path = item["path"]
+            relative_path = file_path.replace(f"birds_nest/", "").replace(f"birds_nest{os.sep}", "")
             local_file_path = relative_path
 
             if local_file_path in path_downloaded:
                 continue
 
@@ -367,11 +367,11 @@
 
             # Create directory structure if it doesn't exist
             self._ensure_directory_exists(os.path.dirname(local_file_path))
 
             # Only download Python and SQL files
-            if not local_file_path.endswith(('.py', '.sql', ".json")):
+            if not local_file_path.endswith((".py", ".sql", ".json")):
                 continue
 
             logger.debug(f"Downloading test fixture: {relative_path}")
 
             # Use the reusable download method
@@ -382,11 +382,11 @@
                 path_downloaded.add(local_file_path)
                 logger.info(f"Successfully downloaded test fixture: {local_file_path}")
             else:
                 logger.error(f"Failed to download test fixture {file_path}")
 
-    def fetch_test_fixtures(self, base_url:str=""):
+    def fetch_test_fixtures(self, base_url: str = ""):
         """
         Fetch test fixtures and templates from the repository.
 
         Args:
             base_url (str): Base URL (currently unused but kept for compatibility)
@@ -400,23 +400,23 @@
         path_downloaded = set()
         for folder_data in self.files.values():
             self.fetch_test_fixture(folder_data, path_downloaded)
 
     def fetch_derivation_model_file(
-            self,
-            remote_dir = "birds_nest/pybirdai",
-            # remote_dir = "birds_nest/models/pybirdai",
-            remote_file_name = "bird_data_model.py",
-            local_target_dir = f"resources{os.sep}derivation_implementation",
-            local_target_file_name = "bird_data_model_with_derivation.py"
+        self,
+        remote_dir="birds_nest/pybirdai",
+        # remote_dir = "birds_nest/models/pybirdai",
+        remote_file_name="bird_data_model.py",
+        local_target_dir=f"resources{os.sep}derivation_implementation",
+        local_target_file_name="bird_data_model_with_derivation.py",
     ):
         """Fetches the derivation model file from the specified remote directory"""
         # Fetch contents of the directory containing the target file
         files_in_dir = self.fetch_files(remote_dir)
         found_file_info = None
         for item in files_in_dir:
-            if item.get('name') == remote_file_name and item.get('type') == 'file':
+            if item.get("name") == remote_file_name and item.get("type") == "file":
                 found_file_info = item
                 break
 
         if found_file_info:
             # Construct the local path
@@ -433,13 +433,14 @@
         else:
             logger.warning(f"File {remote_file_name} not found in {remote_dir}")
             print(f"File {remote_file_name} not found in {remote_dir}")
 
     def fetch_filter_code(
-            self,
-            remote_dir = "birds_nest/pybirdai",
-            local_target_dir = f"birds_nest{os.sep}pybirdai{os.sep}process_steps{os.sep}filter_code"):
+        self,
+        remote_dir="birds_nest/pybirdai",
+        local_target_dir=f"birds_nest{os.sep}pybirdai{os.sep}process_steps{os.sep}filter_code",
+    ):
         """Fetches the derivation model file from the specified remote directory"""
 
         files_in_dir = self.fetch_files(remote_dir)
         for remote_file_name in files_in_dir:
             local_file_path = os.path.join(local_target_dir, remote_file_name)
@@ -453,14 +454,16 @@
             else:
                 logger.error(f"Failed to download {remote_file_name}")
 
         return 0
 
-    def fetch_report_template_htmls(self,
-                                   remote_dir="birds_nest/results/generated_html",
-                                   fallback_remote_dir="birds_nest/pybirdai/templates/pybirdai",
-                                   local_target_dir="pybirdai/templates/pybirdai"):
+    def fetch_report_template_htmls(
+        self,
+        remote_dir="birds_nest/results/generated_html",
+        fallback_remote_dir="birds_nest/pybirdai/templates/pybirdai",
+        local_target_dir="pybirdai/templates/pybirdai",
+    ):
         """
         Fetch report template HTML files containing REF_FINREP from GitHub repository.
         First tries the primary remote directory, then falls back to secondary if no templates found.
 
         Args:
@@ -474,29 +477,33 @@
         self._ensure_directory_exists(local_target_dir)
 
         # First try to fetch from the primary directory
         files = self.fetch_files(remote_dir)
         downloaded_count = 0
-        
+
         # Check if there are any FINREP HTML files in the primary directory
-        finrep_files = [f for f in files if f.get('type') == 'file' and 
-                        f.get('name', '').endswith('.html') and 
-                        'FINREP' in f.get('name', '')]
-        
+        finrep_files = [
+            f
+            for f in files
+            if f.get("type") == "file" and f.get("name", "").endswith(".html") and "FINREP" in f.get("name", "")
+        ]
+
         # If no FINREP files found in primary, try fallback directory
         if not finrep_files:
             logger.info(f"No FINREP templates found in {remote_dir}, trying fallback directory: {fallback_remote_dir}")
             files = self.fetch_files(fallback_remote_dir)
-            finrep_files = [f for f in files if f.get('type') == 'file' and 
-                           f.get('name', '').endswith('.html') and 
-                           'FINREP' in f.get('name', '')]
+            finrep_files = [
+                f
+                for f in files
+                if f.get("type") == "file" and f.get("name", "").endswith(".html") and "FINREP" in f.get("name", "")
+            ]
             if finrep_files:
                 logger.info(f"Found {len(finrep_files)} FINREP templates in fallback directory")
 
         # Download the FINREP files
         for file_info in finrep_files:
-            name = file_info.get('name', '')
+            name = file_info.get("name", "")
             logger.debug(f"Processing report template: {name}")
 
             # Download the file
             local_file_path = os.path.join(local_target_dir, name)
             result = self.download_file(file_info, local_file_path)
@@ -508,33 +515,31 @@
                 logger.warning(f"Failed to download report template: {name}")
 
         logger.info(f"Downloaded {downloaded_count} REF_FINREP report templates")
         return downloaded_count
 
+
 def main():
     """Main function to orchestrate the file fetching process"""
     logger.info("Starting GitHub file fetching process")
 
     # Initialize the fetcher with the FreeBIRD repository
     fetcher = GitHubFileFetcher("https://github.com/regcommunity/FreeBIRD")
-
 
     logger.info("STEP 1: Fetching specific derivation model file")
 
     fetcher.fetch_derivation_model_file(
         "birds_nest/pybirdai",
         # "birds_nest/models/pybirdai",
         "bird_data_model.py",
         f"resources{os.sep}derivation_implementation",
-        "bird_data_model_with_derivation.py"
+        "bird_data_model_with_derivation.py",
     )
-
 
     logger.info("STEP 2: Fetching database export files")
     fetcher.fetch_database_export_files()
 
-
     logger.info("STEP 3: Fetching test fixtures and templates")
     fetcher.fetch_test_fixtures()
 
     logger.info("STEP 4: Fetching test templates")
     fetcher.fetch_filter_code()
@@ -543,7 +548,8 @@
     fetcher.fetch_report_template_htmls()
 
     logger.info("File fetching process completed successfully!")
     print("File fetching process completed!")
 
+
 if __name__ == "__main__":
     main()
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/github_file_fetcher.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/utils_views.py	2025-08-02 18:37:08.470822+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/utils_views.py	2025-09-21 17:07:34.801845+00:00
@@ -14,43 +14,47 @@
 import os
 import json
 import logging
 from django.conf import settings
 
+
 def ensure_results_directory():
     """Ensure the test results directory exists and return its path."""
-    results_dir = os.path.join(settings.BASE_DIR, 'tests', 'test_results', 'json')
+    results_dir = os.path.join(settings.BASE_DIR, "tests", "test_results", "json")
 
     # Create a directory for the results if it doesn't exist
     if not os.path.exists(results_dir):
         os.makedirs(results_dir)
 
     return results_dir
 
+
 def process_test_results_files(results_dir):
     """Process all JSON test result files in the specified directory."""
     templates = dict()
 
     try:
-        json_files = [f for f in os.listdir(results_dir) if f.endswith('.json')]
+        json_files = [f for f in os.listdir(results_dir) if f.endswith(".json")]
         for filename in json_files:
             data = load_test_data(results_dir, filename)
             if data:
                 process_test_data(data, templates)
     except Exception as e:
         logging.error(f"Error reading test results: {e}")
 
     return templates
 
+
 def load_test_data(results_dir, filename):
     """Load test data from a JSON file."""
     try:
         with open(os.path.join(results_dir, filename)) as f:
             return json.load(f)
     except Exception as e:
         logging.error(f"Error loading test data from {filename}: {e}")
         return None
+
 
 def process_test_data(data, templates):
     """Process a single test data file and update the templates dictionary."""
     template_id = data["test_information"]["regulatory_template_id"]
     template_obj = get_or_create_template(templates, template_id)
@@ -61,63 +65,51 @@
     scenario_name = data["test_information"]["scenario_name"]
     scenario_obj = get_or_create_scenario(datapoint_obj, scenario_name)
 
     update_scenario_with_tests(scenario_obj, data["test_results"])
 
+
 def get_or_create_template(templates, template_id):
     """Get or create a template in the templates dictionary."""
     if template_id not in templates:
-        templates[template_id] = {
-            'name': template_id,
-            'datapoints': []
-        }
+        templates[template_id] = {"name": template_id, "datapoints": []}
     return templates[template_id]
+
 
 def get_or_create_datapoint(template_obj, datapoint_suffix):
     """Get or create a datapoint in the template's datapoints list."""
-    for dp in template_obj['datapoints']:
-        if dp['name'] == datapoint_suffix:
+    for dp in template_obj["datapoints"]:
+        if dp["name"] == datapoint_suffix:
             return dp
 
-    datapoint_obj = {
-        "name": datapoint_suffix,
-        "scenarios": []
-    }
-    template_obj['datapoints'].append(datapoint_obj)
+    datapoint_obj = {"name": datapoint_suffix, "scenarios": []}
+    template_obj["datapoints"].append(datapoint_obj)
     return datapoint_obj
+
 
 def get_or_create_scenario(datapoint_obj, scenario_name):
     """Get or create a scenario in the datapoint's scenarios list."""
     for sc in datapoint_obj["scenarios"]:
         if sc["name"] == scenario_name:
             return sc
 
-    scenario_obj = {
-        "name": scenario_name,
-        "tests": [],
-        "passed": None
-    }
+    scenario_obj = {"name": scenario_name, "tests": [], "passed": None}
     datapoint_obj["scenarios"].append(scenario_obj)
     return scenario_obj
+
 
 def update_scenario_with_tests(scenario_obj, test_results):
     """Update the scenario with test results."""
     tests = []
 
     # Process passed tests
     for test_name in test_results.get("passed", []):
-        tests.append({
-            "name": test_name,
-            "passed": True
-        })
+        tests.append({"name": test_name, "passed": True})
 
     # Process failed tests
     for test_name in test_results.get("failed", []):
-        tests.append({
-            "name": test_name,
-            "passed": False
-        })
+        tests.append({"name": test_name, "passed": False})
 
     scenario_obj["tests"] = tests
     if tests and all(test["passed"] for test in tests):
         scenario_obj["passed"] = True
     elif tests:
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/utils_views.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/speed_improvements_initial_migration/derived_fields_extractor.py	2025-09-18 09:56:06.416277+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/speed_improvements_initial_migration/derived_fields_extractor.py	2025-09-21 17:07:34.845247+00:00
@@ -27,13 +27,11 @@
     @staticmethod
     def configure_django():
         """Configure Django settings without starting the application"""
         if not settings.configured:
             # Set up Django settings module for birds_nest in parent directory
-            project_root = os.path.abspath(
-                os.path.join(os.path.dirname(__file__), "../..")
-            )
+            project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "../.."))
             sys.path.insert(0, project_root)
             os.environ["DJANGO_SETTINGS_MODULE"] = "birds_nest.settings"
             django.setup()
 
 
@@ -60,29 +58,17 @@
                 if isinstance(item, ast.FunctionDef):
                     # Check if it has decorators
                     for decorator in item.decorator_list:
                         # Check for lineage decorator
                         if (
-                            (
-                                isinstance(decorator, ast.Name)
-                                and decorator.id == "lineage"
-                            )
-                            or (
-                                isinstance(decorator, ast.Attribute)
-                                and decorator.attr == "lineage"
-                            )
+                            (isinstance(decorator, ast.Name) and decorator.id == "lineage")
+                            or (isinstance(decorator, ast.Attribute) and decorator.attr == "lineage")
                             or (
                                 isinstance(decorator, ast.Call)
                                 and (
-                                    (
-                                        isinstance(decorator.func, ast.Name)
-                                        and decorator.func.id == "lineage"
-                                    )
-                                    or (
-                                        isinstance(decorator.func, ast.Attribute)
-                                        and decorator.func.attr == "lineage"
-                                    )
+                                    (isinstance(decorator.func, ast.Name) and decorator.func.id == "lineage")
+                                    or (isinstance(decorator.func, ast.Attribute) and decorator.func.attr == "lineage")
                                 )
                             )
                         ):
                             lineage_properties.append(item.name)
                             break
@@ -102,13 +88,11 @@
     """Generate AST representation of the extracted classes"""
     DjangoSetup.configure_django()
 
     # Create module node
     module = ast.Module(body=[], type_ignores=[])
-    import_sttmt_1 = ast.ImportFrom(
-        module="django.db", names=[ast.alias(name="models")], level=0
-    )
+    import_sttmt_1 = ast.ImportFrom(module="django.db", names=[ast.alias(name="models")], level=0)
     import_sttmt_2 = ast.ImportFrom(
         module="pybirdai.annotations.decorators",
         names=[ast.alias(name="lineage")],
         level=0,
     )
@@ -164,21 +148,17 @@
     """Check if the file already has lineage imports and decorators"""
     with open(file_path, "r") as f:
         content = f.read()
 
     # Check for lineage import and @lineage decorator
-    has_lineage_import = (
-        "from pybirdai.annotations.decorators import lineage" in content
-    )
+    has_lineage_import = "from pybirdai.annotations.decorators import lineage" in content
     has_lineage_decorator = "@lineage" in content
 
     return has_lineage_import and has_lineage_decorator
 
 
-def merge_derived_fields_into_original_model(
-    bird_data_model_path, lineage_classes_ast_path
-):
+def merge_derived_fields_into_original_model(bird_data_model_path, lineage_classes_ast_path):
     """
     Merge derived fields from derived_field_configuration.py into the original bird_data_model.py.
 
     This function:
     1. Checks if the original file has already been modified (has @lineage imports/decorators)
@@ -196,13 +176,11 @@
     """
     logger = logging.getLogger(__name__)
 
     # Check if file has already been modified
     if check_if_file_already_modified(bird_data_model_path):
-        logger.info(
-            "File already contains @lineage decorators and imports, skipping modification"
-        )
+        logger.info("File already contains @lineage decorators and imports, skipping modification")
         return False
 
     # Parse both files
     with open(bird_data_model_path, "r") as f:
         original_content = f.read()
@@ -222,29 +200,17 @@
             # Find all properties with @lineage decorator
             for item in node.body:
                 if isinstance(item, ast.FunctionDef):
                     for decorator in item.decorator_list:
                         if (
-                            (
-                                isinstance(decorator, ast.Name)
-                                and decorator.id == "lineage"
-                            )
-                            or (
-                                isinstance(decorator, ast.Attribute)
-                                and decorator.attr == "lineage"
-                            )
+                            (isinstance(decorator, ast.Name) and decorator.id == "lineage")
+                            or (isinstance(decorator, ast.Attribute) and decorator.attr == "lineage")
                             or (
                                 isinstance(decorator, ast.Call)
                                 and (
-                                    (
-                                        isinstance(decorator.func, ast.Name)
-                                        and decorator.func.id == "lineage"
-                                    )
-                                    or (
-                                        isinstance(decorator.func, ast.Attribute)
-                                        and decorator.func.attr == "lineage"
-                                    )
+                                    (isinstance(decorator.func, ast.Name) and decorator.func.id == "lineage")
+                                    or (isinstance(decorator.func, ast.Attribute) and decorator.func.attr == "lineage")
                                 )
                             )
                         ):
                             derived_properties.append(item)
                             break
@@ -353,13 +319,11 @@
     #     f.write(ast.unparse(ast_module))
 
     # print(f"Extracted {len(lineage_classes)} classes with lineage properties")
     print("Output written to derived_field_configuration.py")
     model_file_path = f"pybirdai{os.sep}models{os.sep}bird_data_model.py"
-    derived_fields_file_path = (
-        f"resources{os.sep}derivation_files{os.sep}derived_field_configuration.py"
-    )
+    derived_fields_file_path = f"resources{os.sep}derivation_files{os.sep}derived_field_configuration.py"
 
     merge_derived_fields_into_original_model(model_file_path, derived_fields_file_path)
 
 
 if __name__ == "__main__":
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/speed_improvements_initial_migration/derived_fields_extractor.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/speed_improvements_initial_migration/artifact_fetcher.py	2025-09-15 13:18:11.415179+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/speed_improvements_initial_migration/artifact_fetcher.py	2025-09-21 17:07:34.848421+00:00
@@ -20,11 +20,11 @@
 import io
 from collections import defaultdict
 import os
 
 # Configure logging
-logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
+logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
 logger = logging.getLogger(__name__)
 
 REPO_OWNER = "benjamin-arfa"
 REPO_NAME = "database_generator_django_service"
 BASELINK = f"https://api.github.com/repos/{REPO_OWNER}/{REPO_NAME}/actions/artifacts"
@@ -35,10 +35,11 @@
     id: int
     repository_id: int
     head_repository_id: int
     head_branch: str
     head_sha: str
+
 
 @dataclass
 class Artifact:
     id: int
     node_id: str
@@ -51,34 +52,31 @@
     created_at: str
     updated_at: str
     expires_at: str
     workflow_run: WorkflowRun
 
+
 class ArtifactFetcher:
     def __init__(self, token: str):
         self.token = token
-        self.headers = {
-            "Authorization": f"Bearer {token}",
-            "Accept": "application/vnd.github+json"
-        }
+        self.headers = {"Authorization": f"Bearer {token}", "Accept": "application/vnd.github+json"}
         logger.info("ArtifactFetcher initialized")
 
     def get_artifacts(self, repo_url: str) -> List[Artifact]:
         logger.info(f"Fetching artifacts from: {repo_url}")
         response = requests.get(repo_url, headers=self.headers)
         logger.info(f"API response status: {response.status_code}")
 
         data = response.json()
-        artifacts_count = len(data.get('artifacts', []))
+        artifacts_count = len(data.get("artifacts", []))
         logger.info(f"Found {artifacts_count} artifacts")
 
         artifacts = []
-        for artifact_data in data.get('artifacts', []):
-            workflow_run = WorkflowRun(**artifact_data['workflow_run'])
+        for artifact_data in data.get("artifacts", []):
+            workflow_run = WorkflowRun(**artifact_data["workflow_run"])
             artifact = Artifact(
-                workflow_run=workflow_run,
-                **{k: v for k, v in artifact_data.items() if k != 'workflow_run'}
+                workflow_run=workflow_run, **{k: v for k, v in artifact_data.items() if k != "workflow_run"}
             )
             artifacts.append(artifact)
             logger.debug(f"Processed artifact: {artifact.name} (ID: {artifact.id})")
 
         return artifacts
@@ -99,14 +97,14 @@
             with zipfile.ZipFile(io.BytesIO(zip_content)) as zip_file:
                 file_list = zip_file.namelist()
                 logger.debug(f"Files in zip: {file_list}")
 
                 for file_name in file_list:
-                    if not file_name.endswith('/'):  # Skip directories
+                    if not file_name.endswith("/"):  # Skip directories
                         try:
                             with zip_file.open(file_name) as file:
-                                content = file.read().decode('utf-8')
+                                content = file.read().decode("utf-8")
                                 files_content[file_name] = content
                                 logger.debug(f"Extracted file: {file_name} ({len(content)} chars)")
                         except UnicodeDecodeError:
                             # Skip binary files
                             logger.debug(f"Skipping binary file: {file_name}")
@@ -118,10 +116,11 @@
         except Exception as e:
             logger.error(f"Error extracting zip: {e}")
 
         return files_content
 
+
 class PreconfiguredDatabaseFetcher(ArtifactFetcher):
     def __init__(self, token: str, repo_url: str = BASELINK):
         super().__init__(token)
         self.repo_url = repo_url
         logger.info(f"PreconfiguredDatabaseFetcher initialized with repo: {repo_url}")
@@ -129,11 +128,11 @@
     def _compare_files(self, local_file_path: str, remote_file_content: str) -> bool:
         """Compare a local file with a file content from zip archive."""
         logger.debug(f"Comparing local file: {local_file_path}")
         try:
             # Read local file
-            with open(local_file_path, 'r') as f:
+            with open(local_file_path, "r") as f:
                 local_content = f.read()
 
             match = local_content == remote_file_content
             logger.debug(f"File comparison result for {local_file_path}: {match}")
             return match
@@ -153,12 +152,15 @@
                 return True
         except Exception as e:
             logger.error(f"Error extracting zip: {e}")
             return False
 
-    def fetch(self, bird_data_model_path: str = f"pybirdai{os.sep}models{os.sep}bird_data_model.py",
-              bird_meta_data_model_path: str = f"pybirdai{os.sep}models{os.sep}bird_meta_data_model.py") -> Optional[bytes]:
+    def fetch(
+        self,
+        bird_data_model_path: str = f"pybirdai{os.sep}models{os.sep}bird_data_model.py",
+        bird_meta_data_model_path: str = f"pybirdai{os.sep}models{os.sep}bird_meta_data_model.py",
+    ) -> Optional[bytes]:
         """
         Fetch the db.sqlite3 artifact if the specified model files match those in the workflow run.
         """
         logger.info("Starting database fetch process")
         logger.info(f"Looking for matching files: {bird_data_model_path}, {bird_meta_data_model_path}")
@@ -207,11 +209,11 @@
                 bird_data_files = self.extract_zip_files_in_memory(bird_data_zip)
 
                 # Find the actual file content (it might be nested in folders)
                 bird_data_content = None
                 for file_path, content in bird_data_files.items():
-                    if file_path.endswith('bird_data_model.py'):
+                    if file_path.endswith("bird_data_model.py"):
                         bird_data_content = content
                         break
 
                 if bird_data_content is None:
                     logger.warning(f"bird_data_model.py not found in artifact zip for workflow {workflow_id}")
@@ -225,11 +227,11 @@
                 bird_meta_files = self.extract_zip_files_in_memory(bird_meta_zip)
 
                 # Find the actual file content (it might be nested in folders)
                 bird_meta_content = None
                 for file_path, content in bird_meta_files.items():
-                    if file_path.endswith('bird_meta_data_model.py'):
+                    if file_path.endswith("bird_meta_data_model.py"):
                         bird_meta_content = content
                         break
 
                 if bird_meta_content is None:
                     logger.warning(f"bird_meta_data_model.py not found in artifact zip for workflow {workflow_id}")
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/speed_improvements_initial_migration/artifact_fetcher.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/visualisation_service.py	2025-09-18 09:56:06.416284+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/visualisation_service.py	2025-09-21 17:07:34.933065+00:00
@@ -20,98 +20,98 @@
 import logging
 
 # Configure logging
 logging.basicConfig(
     level=logging.INFO,
-    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
-    handlers=[
-        logging.FileHandler("visualization_service.log"),
-        logging.StreamHandler()
-    ]
+    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
+    handlers=[logging.FileHandler("visualization_service.log"), logging.StreamHandler()],
 )
 
 logger = logging.getLogger(__name__)
+
 
 class DjangoSetup:
     @staticmethod
     def configure_django():
         """Configure Django settings without starting the application"""
         if not settings.configured:
             # Set up Django settings module for birds_nest in parent directory
-            project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '../..'))
+            project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "../.."))
             sys.path.insert(0, project_root)
-            os.environ['DJANGO_SETTINGS_MODULE'] = 'birds_nest.settings'
-            logger.info("Configuring Django with settings module: %s", os.environ['DJANGO_SETTINGS_MODULE'])
+            os.environ["DJANGO_SETTINGS_MODULE"] = "birds_nest.settings"
+            logger.info("Configuring Django with settings module: %s", os.environ["DJANGO_SETTINGS_MODULE"])
             django.setup()
             logger.debug("Django setup complete")
+
 
 class DatabaseConnector:
 
     @staticmethod
     def get_cube_links_for_cube(cube_id):
         """Get all cube links that involve a specific cube (either as primary or foreign)"""
         logger.info("Getting cube links for cube_id: %s", cube_id)
         DjangoSetup.configure_django()
         from pybirdai.models.bird_meta_data_model import CUBE_LINK
+
         links = CUBE_LINK.objects.filter(
-            models.Q(primary_cube_id=cube_id) |
-            models.Q(foreign_cube_id=cube_id)
-        ).select_related('primary_cube_id', 'foreign_cube_id')
+            models.Q(primary_cube_id=cube_id) | models.Q(foreign_cube_id=cube_id)
+        ).select_related("primary_cube_id", "foreign_cube_id")
         logger.debug("Found %d cube links for cube_id: %s", len(links), cube_id)
         return links
 
     @staticmethod
     def get_all_cube_links():
         """Get cube structure item links for a specific cube link"""
         logger.info("Getting all cube links")
         DjangoSetup.configure_django()
         from pybirdai.models.bird_meta_data_model import CUBE_LINK
+
         links = CUBE_LINK.objects.all()
         logger.debug("Found %d total cube links", len(links))
         return links
 
     @staticmethod
     def get_cube_structure_item_links(cube_link):
         """Get cube structure item links for a specific cube link"""
         logger.info("Getting cube structure item links for cube_link_id: %s", cube_link.cube_link_id)
         DjangoSetup.configure_django()
         from pybirdai.models.bird_meta_data_model import CUBE_STRUCTURE_ITEM_LINK
+
         links = CUBE_STRUCTURE_ITEM_LINK.objects.select_related(
-            'primary_cube_variable_code',
-            'foreign_cube_variable_code',
-            'cube_link_id'
+            "primary_cube_variable_code", "foreign_cube_variable_code", "cube_link_id"
         ).filter(cube_link_id=cube_link)
         logger.debug("Found %d structure item links for cube_link_id: %s", len(links), cube_link.cube_link_id)
         return links
 
     @classmethod
     def get_linked_cube_structure_items(cls, cube_link):
         """Get quadruples of linked cube structure items"""
         logger.info("Building linked cube structure items for cube_link_id: %s", cube_link.cube_link_id)
         DjangoSetup.configure_django()
-        from pybirdai.models.bird_meta_data_model import CUBE_STRUCTURE_ITEM_LINK,CUBE_STRUCTURE,CUBE_STRUCTURE_ITEM
-
+        from pybirdai.models.bird_meta_data_model import CUBE_STRUCTURE_ITEM_LINK, CUBE_STRUCTURE, CUBE_STRUCTURE_ITEM
 
         linked_items = []
         structure_item_links = cls.get_cube_structure_item_links(cube_link)
 
         for link in structure_item_links:
-            linked_items.append((
-                link.cube_link_id.primary_cube_id,
-                link.primary_cube_variable_code,
-                link.cube_link_id.foreign_cube_id,
-                link.foreign_cube_variable_code
-            ))
+            linked_items.append(
+                (
+                    link.cube_link_id.primary_cube_id,
+                    link.primary_cube_variable_code,
+                    link.cube_link_id.foreign_cube_id,
+                    link.foreign_cube_variable_code,
+                )
+            )
 
         logger.debug("Generated %d linked cube structure items", len(linked_items))
         return linked_items
 
     @staticmethod
     def create_visualization_json(linked_cube_structure_items):
 
         DjangoSetup.configure_django()
-        from pybirdai.models.bird_meta_data_model import CUBE_STRUCTURE_ITEM_LINK,CUBE_STRUCTURE,CUBE_STRUCTURE_ITEM
+        from pybirdai.models.bird_meta_data_model import CUBE_STRUCTURE_ITEM_LINK, CUBE_STRUCTURE, CUBE_STRUCTURE_ITEM
 
         """Create JSON structure for visualization"""
         logger.info("Creating visualization JSON from %d linked items", len(linked_cube_structure_items))
         nodes = {}
         edges = []
@@ -119,107 +119,104 @@
         for primary_cube, primary_item, foreign_cube, foreign_item in linked_cube_structure_items:
             # Add nodes
             if primary_cube.cube_id not in nodes:
                 logger.debug("Adding primary cube node: %s", primary_cube.name)
                 nodes[primary_cube.cube_id] = {
-                    'id': primary_cube.cube_id,
-                    'name': primary_cube.name,
-                    'code': primary_cube.cube_id,
-                    'items': [],
-                    'is_source': True
+                    "id": primary_cube.cube_id,
+                    "name": primary_cube.name,
+                    "code": primary_cube.cube_id,
+                    "items": [],
+                    "is_source": True,
                 }
-            nodes[primary_cube.cube_id]['items'].append({
-                'code': primary_item.variable_id.variable_id,
-                'name': primary_item.description
-            })
+            nodes[primary_cube.cube_id]["items"].append(
+                {"code": primary_item.variable_id.variable_id, "name": primary_item.description}
+            )
 
             if foreign_cube.cube_id not in nodes:
                 logger.debug("Adding foreign cube node: %s", foreign_cube.name)
                 nodes[foreign_cube.cube_id] = {
-                    'id': foreign_cube.cube_id,
-                    'name': foreign_cube.name,
-                    'code': foreign_cube.cube_id,
-                    'items': [],
-                    'is_source': False
+                    "id": foreign_cube.cube_id,
+                    "name": foreign_cube.name,
+                    "code": foreign_cube.cube_id,
+                    "items": [],
+                    "is_source": False,
                 }
-            nodes[foreign_cube.cube_id]['items'].append({
-                'code': foreign_item.variable_id.variable_id,
-                'name': foreign_item.description
-            })
+            nodes[foreign_cube.cube_id]["items"].append(
+                {"code": foreign_item.variable_id.variable_id, "name": foreign_item.description}
+            )
 
             # Add edges
             logger.debug("Adding edge: %s -> %s", primary_cube.name, foreign_cube.name)
-            edges.append({
-                'source': primary_cube.name,
-                'target': foreign_cube.name,
-                'sourceItem': primary_item.variable_id.variable_id,
-                'targetItem': foreign_item.variable_id.variable_id,
-                'linkType': "primary" # Default linkType since we no longer have link object
-            })
-
-
+            edges.append(
+                {
+                    "source": primary_cube.name,
+                    "target": foreign_cube.name,
+                    "sourceItem": primary_item.variable_id.variable_id,
+                    "targetItem": foreign_item.variable_id.variable_id,
+                    "linkType": "primary",  # Default linkType since we no longer have link object
+                }
+            )
 
             # Change for #1533
             #
             foreign_structure = CUBE_STRUCTURE.objects.get(cube=foreign_cube)
-            items = CUBE_STRUCTURE_ITEM.objects.all().filter(cube_structure_id = foreign_structure)
-            items_as_node = {tuple(_.values()) for _ in nodes[foreign_cube.cube_id]['items']}
+            items = CUBE_STRUCTURE_ITEM.objects.all().filter(cube_structure_id=foreign_structure)
+            items_as_node = {tuple(_.values()) for _ in nodes[foreign_cube.cube_id]["items"]}
 
             for item in items:
-                if (item.variable_id.variable_id,item.description) not in items_as_node:
-                    nodes[foreign_cube.cube_id]['items'].append({
-                        'code': item.variable_id.variable_id,
-                        'name': item.description
-                    })
-
-        json_data = {
-            'nodes': list(nodes.values()),
-            'edges': edges
-        }
-        logger.info("Created visualization JSON with %d nodes and %d edges",
-                   len(json_data['nodes']), len(json_data['edges']))
+                if (item.variable_id.variable_id, item.description) not in items_as_node:
+                    nodes[foreign_cube.cube_id]["items"].append(
+                        {"code": item.variable_id.variable_id, "name": item.description}
+                    )
+
+        json_data = {"nodes": list(nodes.values()), "edges": edges}
+        logger.info(
+            "Created visualization JSON with %d nodes and %d edges", len(json_data["nodes"]), len(json_data["edges"])
+        )
         return json_data
+
 
 def return_line_break_at_23_char(string):
     if len(string) > 23:
         chunks = []
         for i in range(0, len(string), 23):
-            chunks.append(string[i:i+23])
+            chunks.append(string[i : i + 23])
         return "\n".join(chunks)
     return string
+
 
 class NetworkGraphGenerationService:
     @staticmethod
     def create_graph(json_data, file_name="", in_md=False):
         """Create a Mermaid chart visualization from JSON data"""
         logger.info("Creating graph visualization for file: %s", file_name)
         # Begin building the Mermaid flowchart definition
         mermaid_chart = "```mermaid\ngraph LR\n"  # Changed to TB (top to bottom)
-        mermaid_chart += "    direction LR\n"     # Explicitly set direction
+        mermaid_chart += "    direction LR\n"  # Explicitly set direction
 
         # Organize nodes by type
         source_cubes = []
         source_items = []
         target_items = []
         target_cubes = []
 
         logger.debug("Organizing nodes by type")
         # Group nodes by type first
-        for node in json_data['nodes']:
-            is_source = node.get('is_source', any(edge['source'] == node['name'] for edge in json_data['edges']))
-            node_id = "cube_" + ''.join(c if c.isalnum() else '_' for c in node['name'])
+        for node in json_data["nodes"]:
+            is_source = node.get("is_source", any(edge["source"] == node["name"] for edge in json_data["edges"]))
+            node_id = "cube_" + "".join(c if c.isalnum() else "_" for c in node["name"])
 
             if is_source:
-                logger.debug("Adding source cube: %s", node['name'])
+                logger.debug("Adding source cube: %s", node["name"])
                 source_cubes.append((node_id, node))
-                for item in node['items']:
+                for item in node["items"]:
                     item_id = f"{node_id}_{item['code']}"
                     source_items.append((item_id, item, node_id))
             else:
-                logger.debug("Adding target cube: %s", node['name'])
+                logger.debug("Adding target cube: %s", node["name"])
                 target_cubes.append((node_id, node))
-                for item in node['items']:
+                for item in node["items"]:
                     item_id = f"{node_id}_{item['code']}"
                     target_items.append((item_id, item, node_id))
 
         logger.debug("Adding source cubes to chart: %d cubes", len(source_cubes))
         # Add source cubes and items in subgraphs
@@ -259,15 +256,15 @@
         # Connect target items to target cubes
         for item_id, item, node_id in target_items:
             if f"    {item_id} --> {node_id};\n" not in mermaid_chart:
                 mermaid_chart += f"    {item_id} --> {node_id};\n"
 
-        logger.debug("Adding cross connections between items: %d edges", len(json_data['edges']))
+        logger.debug("Adding cross connections between items: %d edges", len(json_data["edges"]))
         # Add cross connections between items
-        for edge in json_data['edges']:
-            source_id = "cube_" + ''.join(c if c.isalnum() else '_' for c in edge['source'])
-            target_id = "cube_" + ''.join(c if c.isalnum() else '_' for c in edge['target'])
+        for edge in json_data["edges"]:
+            source_id = "cube_" + "".join(c if c.isalnum() else "_" for c in edge["source"])
+            target_id = "cube_" + "".join(c if c.isalnum() else "_" for c in edge["target"])
 
             source_item_id = f"{source_id}_{edge['sourceItem']}"
             target_item_id = f"{target_id}_{edge['targetItem']}"
 
             # Connect source item to target item with a dashed line
@@ -326,38 +323,41 @@
         # Add a title section before the Mermaid chart
         # title = f"# Mapping Visualization: {json_data['nodes'][1]['name']} to {json_data['nodes'][0]['name']}\n\n"
 
         # Complete markdown content
         logger.debug("Preparing final output content")
-        markdown_content = mermaid_chart.replace("```mermaid","").replace("```","")
-
-        html_content = f"""
+        markdown_content = mermaid_chart.replace("```mermaid", "").replace("```", "")
+
+        html_content = (
+            f"""
         <!doctype html>
         <html lang="en">
           <body>
           <h1 style="font-family: Arial, sans-serif; color: #333; margin: 20px 0; text-align: center;">{file_name.replace('.html', '')}</h1>
             <pre class="mermaid">
 {markdown_content}
             </pre>
             <script type="module">
               import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11/dist/mermaid.esm.min.mjs';
-              """+"""mermaid.initialize({
+              """
+            + """mermaid.initialize({
                 maxTextSize: 140000
               });
             </script>
           </body>
         </html>
         """
+        )
 
         # Save to file
         output_folder = "results/generated_linking_visualisations/"
         output_file = file_name
 
         logger.info("Saving visualization to file: %s%s", output_folder, output_file)
         try:
             os.makedirs(output_folder, exist_ok=True)
-            with open(output_folder+output_file, 'w') as f:
+            with open(output_folder + output_file, "w") as f:
                 f.write(html_content)
             logger.info("File saved successfully")
         except Exception as e:
             logger.error("Error saving file: %s", str(e))
             raise
@@ -376,12 +376,15 @@
         join_identifier: Optional filter for join identifiers
 
     Returns:
         The file path of the generated visualization
     """
-    logger.info("Processing cube visualization for cube_id: %s, join_identifier: %s",
-                cube_id, join_identifier if join_identifier else "None")
+    logger.info(
+        "Processing cube visualization for cube_id: %s, join_identifier: %s",
+        cube_id,
+        join_identifier if join_identifier else "None",
+    )
 
     # Get all cube links for this cube
     all_cube_links = DatabaseConnector.get_cube_links_for_cube(cube_id)
     logger.info("Found %d total cube links for cube_id: %s", len(all_cube_links), cube_id)
 
@@ -399,29 +402,27 @@
 
     # Process the filtered cube links
     json_list = []
     for cube_link in cube_links:
         logger.debug("Processing cube_link: %s", cube_link.cube_link_id)
-        linked_cube_structure_items = DatabaseConnector.get_linked_cube_structure_items(
-            cube_link)
+        linked_cube_structure_items = DatabaseConnector.get_linked_cube_structure_items(cube_link)
         json_list.append(DatabaseConnector.create_visualization_json(linked_cube_structure_items))
 
     # Merge all the JSONs for this join identifier
-    merged_json = {'nodes': [], 'edges': []}
+    merged_json = {"nodes": [], "edges": []}
     for json_data in json_list:
-        merged_json['nodes'].extend(json_data['nodes'])
-        merged_json['edges'].extend(json_data['edges'])
-
-    logger.info("Merged JSON contains %d nodes and %d edges",
-               len(merged_json['nodes']), len(merged_json['edges']))
+        merged_json["nodes"].extend(json_data["nodes"])
+        merged_json["edges"].extend(json_data["edges"])
+
+    logger.info("Merged JSON contains %d nodes and %d edges", len(merged_json["nodes"]), len(merged_json["edges"]))
 
     # Generate a filename that includes the join identifier
     if join_identifier is None:
         mermaid_file_name = f"{cube_id}_no_join_identifier.html"
     else:
         # Sanitize join identifier for filename
-        safe_join_id = ''.join(c if c.isalnum() else '_' for c in join_identifier)
+        safe_join_id = "".join(c if c.isalnum() else "_" for c in join_identifier)
         mermaid_file_name = f"{cube_id}_{safe_join_id}.html"
 
     logger.info("Generated filename: %s", mermaid_file_name)
 
     # Create the visualization using the identifier-specific filename
@@ -436,12 +437,15 @@
         sys.exit(1)
 
     cube_id = sys.argv[1]
     join_identifier = sys.argv[2] if len(sys.argv) == 3 else None
 
-    logger.info("Command line arguments - cube_id: %s, join_identifier: %s",
-               cube_id, join_identifier if join_identifier else "None")
+    logger.info(
+        "Command line arguments - cube_id: %s, join_identifier: %s",
+        cube_id,
+        join_identifier if join_identifier else "None",
+    )
 
     try:
         html_content = process_cube_visualization(cube_id, join_identifier)
         logger.info("Visualization generation completed successfully")
     except Exception as e:
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/visualisation_service.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/utils.py	2025-09-18 09:25:16.502382+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/utils.py	2025-09-21 17:07:34.944730+00:00
@@ -9,35 +9,35 @@
 #
 # Contributors:
 #    Neil Mackenzie - initial API and implementation
 #
 
-'''
+"""
 Created on 22 Jan 2022
 
 @author: Neil
-'''
+"""
 
 import unicodedata
 
 from pybirdai.regdna import ELReference
 
 
 class Utils(object):
-    '''
+    """
     Documentation for Utils
-    '''
+    """
 
     @classmethod
     def unique_value(cls, the_enum, adapted_value):
-        '''
-            if the adapted value already exists in the enum then
-            append it with _x2
-            if the that string appended with _x2 already exists,
-            then append with_x3 instead
-            if that exists then _x4 etc.
-        '''
+        """
+        if the adapted value already exists in the enum then
+        append it with _x2
+        if the that string appended with _x2 already exists,
+        then append with_x3 instead
+        if that exists then _x4 etc.
+        """
         new_adapted_value = adapted_value
         if Utils.contains_literal(the_enum.eLiterals, adapted_value):
             new_adapted_value = adapted_value + "_x2"
         counter = 1
         finished = False
@@ -48,185 +48,347 @@
         # it would be better if BIRD addressed this repetition.
         # it is particularly noticeable in NUTS and NACE codes.
         # this high limit increases the processing time from under 1 minute
         # to a few minutes for the full BIRD data model.
         limit = 32
-        while ((counter < limit) and not (finished)):
+        while (counter < limit) and not (finished):
             counter = counter + 1
             if Utils.contains_literal(the_enum.eLiterals, adapted_value + "_x" + str(counter)):
-                new_adapted_value = adapted_value + "_x" + str(counter+1)
+                new_adapted_value = adapted_value + "_x" + str(counter + 1)
             else:
                 finished = True
 
         return new_adapted_value
 
     @classmethod
     def unique_name(cls, the_enum, enum_used_name):
-        '''
+        """
         if the adapted name already exists in the enum then append it with _x2
         if the that string appended with _x2 already exists, then append with_x3 instead
         if that exists then _x4 etc.
-        '''
+        """
         new_adapted_name = enum_used_name
         counter = 1
         finished = False
         limit = 32
         if Utils.contains_name(the_enum.eLiterals, enum_used_name):
             new_adapted_name = enum_used_name + "_x2"
 
         while (counter < limit) and not finished:
             counter = counter + 1
             if Utils.contains_name(the_enum.eLiterals, enum_used_name + "_x" + str(counter)):
-                new_adapted_name = enum_used_name + "_x" + str(counter+1)
+                new_adapted_name = enum_used_name + "_x" + str(counter + 1)
             else:
                 finished = True
 
         return new_adapted_name
 
     @classmethod
     def make_valid_id_for_literal(cls, input_string):
-        '''
+        """
         Tranlate text to be a valid id, without special characters, and following
         the rules for valid id's in regdna
-        '''
-        amended_input_string = input_string.replace('  ', ' ').replace(' ', '_').replace(')', '_').replace('(', '_') \
-            .replace(',', '_').replace('\'', '_').replace('\n', '_').replace('\r', '_').replace('\'t', '_').replace('new', 'New') \
-            .replace('\\', '_').replace('/', '_').replace(':', '_') \
-            .replace('+', '_').replace('.', '_').replace('?', '_').replace('\'', '_').replace('>', '_gt') \
-            .replace('<', '_lt').replace('\"', '_').replace(';', '_').replace('$', '_').replace('=', '_eq').replace('#', '_') \
-            .replace('&', '_').replace('%', '_').replace('[', '_').replace(']', '_').replace('?', '_').replace('–', '_').replace('__', '_').replace('__', '_') \
-            .replace(chr(0x2019), '_').replace(chr(65533), '_').replace(chr(0x00A8), '_').replace(chr(0x00A9), '_')  \
-            .replace(chr(0x00A4), '_').replace(chr(0x00B6), '_').replace(chr(0x00D0), '_').replace(chr(0x00BA), '_') \
-            .replace(chr(0x2020), '_').replace(chr(0x00B5), '_').replace(chr(0x20AC), '_').replace(chr(0x00B4), '_') \
-            .replace(chr(0x0192), '_').replace(chr(0x00B2), '_').replace(chr(0x00BF), '_').replace(chr(0x00B0), '_') \
-            .replace(chr(0x00A6), '_').replace(chr(0x203A), '_').replace(chr(0x00A2), '_').replace(chr(0x2122), '_') \
-            .replace(chr(0x00B1), '_').replace(chr(0x00B9), '_').replace(chr(0x00AE), '_').replace(chr(0x2014), '_') \
-            .replace(chr(0x02DC), '_').replace(chr(0x201E), '_').replace(chr(0x2026), '_').replace(chr(0x00BF), '_') \
-            .replace(chr(0x00BB), '_').replace(chr(0x00AB), '_').replace(chr(0x2022), '_').replace(chr(0x00AC), '_') \
-            .replace(chr(0x2021), '_').replace(chr(0x00A5), '_').replace(chr(0x201E), '_').replace(chr(0x201C), '_') \
-            .replace(chr(0x00AF), '_').replace(chr(0x201D), '_').replace(chr(0x00A3), '_').replace(chr(0x2030), '_') \
-            .replace(chr(0x00BD), '_').replace(chr(0x00BC), '_').replace(chr(0x00BE), '_').replace(chr(0x00A1), '_') \
-            .replace(chr(0x2018), '_').replace(chr(0x0060), '_').replace(chr(0x00B4), '_').replace(chr(0x2026), '_') \
-            .replace(chr(0x200B), '_').replace(chr(0x202F), '_').replace(chr(0x205F), '_').replace(chr(0x3000), '_') \
-            .replace(chr(0x2000), '_').replace(chr(0x2001), '_').replace(chr(0x2002), '_').replace(chr(0x2003), '_') \
-            .replace(chr(0x2004), '_').replace(chr(0x2005), '_').replace(chr(0x2006), '_').replace(chr(0x2007), '_') \
-            .replace(chr(0x2008), '_').replace(chr(0x2009), '_').replace(chr(0x200A), '_').replace(chr(0x00A0), '_') \
-            .replace(chr(0x0027), '_').replace(chr(0x2019), '_').replace(chr(0x2018), '_').replace(chr(0x201A), '_').replace(chr(0x00B7), '_')
-
-
-        return_string = Utils.replace_acutes_graves_and_circumflexes(
-            amended_input_string).replace('\'', '_')
+        """
+        amended_input_string = (
+            input_string.replace("  ", " ")
+            .replace(" ", "_")
+            .replace(")", "_")
+            .replace("(", "_")
+            .replace(",", "_")
+            .replace("'", "_")
+            .replace("\n", "_")
+            .replace("\r", "_")
+            .replace("'t", "_")
+            .replace("new", "New")
+            .replace("\\", "_")
+            .replace("/", "_")
+            .replace(":", "_")
+            .replace("+", "_")
+            .replace(".", "_")
+            .replace("?", "_")
+            .replace("'", "_")
+            .replace(">", "_gt")
+            .replace("<", "_lt")
+            .replace('"', "_")
+            .replace(";", "_")
+            .replace("$", "_")
+            .replace("=", "_eq")
+            .replace("#", "_")
+            .replace("&", "_")
+            .replace("%", "_")
+            .replace("[", "_")
+            .replace("]", "_")
+            .replace("?", "_")
+            .replace("–", "_")
+            .replace("__", "_")
+            .replace("__", "_")
+            .replace(chr(0x2019), "_")
+            .replace(chr(65533), "_")
+            .replace(chr(0x00A8), "_")
+            .replace(chr(0x00A9), "_")
+            .replace(chr(0x00A4), "_")
+            .replace(chr(0x00B6), "_")
+            .replace(chr(0x00D0), "_")
+            .replace(chr(0x00BA), "_")
+            .replace(chr(0x2020), "_")
+            .replace(chr(0x00B5), "_")
+            .replace(chr(0x20AC), "_")
+            .replace(chr(0x00B4), "_")
+            .replace(chr(0x0192), "_")
+            .replace(chr(0x00B2), "_")
+            .replace(chr(0x00BF), "_")
+            .replace(chr(0x00B0), "_")
+            .replace(chr(0x00A6), "_")
+            .replace(chr(0x203A), "_")
+            .replace(chr(0x00A2), "_")
+            .replace(chr(0x2122), "_")
+            .replace(chr(0x00B1), "_")
+            .replace(chr(0x00B9), "_")
+            .replace(chr(0x00AE), "_")
+            .replace(chr(0x2014), "_")
+            .replace(chr(0x02DC), "_")
+            .replace(chr(0x201E), "_")
+            .replace(chr(0x2026), "_")
+            .replace(chr(0x00BF), "_")
+            .replace(chr(0x00BB), "_")
+            .replace(chr(0x00AB), "_")
+            .replace(chr(0x2022), "_")
+            .replace(chr(0x00AC), "_")
+            .replace(chr(0x2021), "_")
+            .replace(chr(0x00A5), "_")
+            .replace(chr(0x201E), "_")
+            .replace(chr(0x201C), "_")
+            .replace(chr(0x00AF), "_")
+            .replace(chr(0x201D), "_")
+            .replace(chr(0x00A3), "_")
+            .replace(chr(0x2030), "_")
+            .replace(chr(0x00BD), "_")
+            .replace(chr(0x00BC), "_")
+            .replace(chr(0x00BE), "_")
+            .replace(chr(0x00A1), "_")
+            .replace(chr(0x2018), "_")
+            .replace(chr(0x0060), "_")
+            .replace(chr(0x00B4), "_")
+            .replace(chr(0x2026), "_")
+            .replace(chr(0x200B), "_")
+            .replace(chr(0x202F), "_")
+            .replace(chr(0x205F), "_")
+            .replace(chr(0x3000), "_")
+            .replace(chr(0x2000), "_")
+            .replace(chr(0x2001), "_")
+            .replace(chr(0x2002), "_")
+            .replace(chr(0x2003), "_")
+            .replace(chr(0x2004), "_")
+            .replace(chr(0x2005), "_")
+            .replace(chr(0x2006), "_")
+            .replace(chr(0x2007), "_")
+            .replace(chr(0x2008), "_")
+            .replace(chr(0x2009), "_")
+            .replace(chr(0x200A), "_")
+            .replace(chr(0x00A0), "_")
+            .replace(chr(0x0027), "_")
+            .replace(chr(0x2019), "_")
+            .replace(chr(0x2018), "_")
+            .replace(chr(0x201A), "_")
+            .replace(chr(0x00B7), "_")
+        )
+
+        return_string = Utils.replace_acutes_graves_and_circumflexes(amended_input_string).replace("'", "_")
 
         if return_string == "op":
             return_string = "_op"
         return return_string
 
     @classmethod
     def make_valid_id(cls, input_string):
-        '''
+        """
         Tranlate text to be a valid id, without special characters, and following
         the rules for valid id's in regdna
-        '''
+        """
 
         # we do not allow id's to start with  number, if it does then we prepend with an underscore
         if len(input_string) > 0:
-            if ((input_string[0] >= '0') and (input_string[0] <= '9')):
+            if (input_string[0] >= "0") and (input_string[0] <= "9"):
                 input_string = "_" + input_string
         # we replace special characters not allowed in id's with an underscore
-        amended_input_string = input_string.replace('  ', ' ').replace(' ', '_').replace(')', '_').replace('(', '_') \
-            .replace(',', '_').replace('\'', '_').replace('\n', '_').replace('\r', '_').replace('\'t', '_').replace('new', 'New') \
-            .replace('\\', '_').replace('/', '_').replace('-', '_').replace(':', '_') \
-            .replace('+', '_').replace('.', '_').replace('?', '_').replace('\'', '_').replace('>', '_gt') \
-            .replace('<', '_lt').replace('\"', '_').replace(';', '_').replace('$', '_').replace('=', '_eq').replace('#', '_') \
-            .replace('&', '_').replace('%', '_').replace('[', '_').replace(']', '_').replace('?', '_').replace('–', '_').replace('__', '_').replace('__', '_') \
-            .replace(chr(0x2019), '_').replace(chr(65533), '_').replace(chr(0x00A8), '_').replace(chr(0x00A9), '_')  \
-            .replace(chr(0x00A4), '_').replace(chr(0x00B6), '_').replace(chr(0x00D0), '_').replace(chr(0x00BA), '_') \
-            .replace(chr(0x2020), '_').replace(chr(0x00B5), '_').replace(chr(0x20AC), '_').replace(chr(0x00B4), '_') \
-            .replace(chr(0x0192), '_').replace(chr(0x00B2), '_').replace(chr(0x00BF), '_').replace(chr(0x00B0), '_') \
-            .replace(chr(0x00A6), '_').replace(chr(0x203A), '_').replace(chr(0x00A2), '_').replace(chr(0x2122), '_') \
-            .replace(chr(0x00B1), '_').replace(chr(0x00B9), '_').replace(chr(0x00AE), '_').replace(chr(0x2014), '_') \
-            .replace(chr(0x02DC), '_').replace(chr(0x201E), '_').replace(chr(0x2026), '_').replace(chr(0x00BF), '_') \
-            .replace(chr(0x00BB), '_').replace(chr(0x00AB), '_').replace(chr(0x2022), '_').replace(chr(0x00AC), '_') \
-            .replace(chr(0x2021), '_').replace(chr(0x00A5), '_').replace(chr(0x201E), '_').replace(chr(0x201C), '_') \
-            .replace(chr(0x00AF), '_').replace(chr(0x201D), '_').replace(chr(0x00A3), '_').replace(chr(0x2030), '_') \
-            .replace(chr(0x00BD), '_').replace(chr(0x00BC), '_').replace(chr(0x00BE), '_').replace(chr(0x00A1), '_') \
-            .replace(chr(0x2018), '_').replace(chr(0x0060), '_').replace(chr(0x00B4), '_').replace(chr(0x2026), '_') \
-            .replace(chr(0x200B), '_').replace(chr(0x202F), '_').replace(chr(0x205F), '_').replace(chr(0x3000), '_') \
-            .replace(chr(0x2000), '_').replace(chr(0x2001), '_').replace(chr(0x2002), '_').replace(chr(0x2003), '_') \
-            .replace(chr(0x2004), '_').replace(chr(0x2005), '_').replace(chr(0x2006), '_').replace(chr(0x2007), '_') \
-            .replace(chr(0x2008), '_').replace(chr(0x2009), '_').replace(chr(0x200A), '_').replace(chr(0x00A0), '_') \
-            .replace(chr(0x0027), '_').replace(chr(0x2019), '_').replace(chr(0x2018), '_').replace(chr(0x201A), '_').replace(chr(0x00B7), '_')
-
-
-        return_string = Utils.replace_acutes_graves_and_circumflexes(
-            amended_input_string).replace('\'', '_')
+        amended_input_string = (
+            input_string.replace("  ", " ")
+            .replace(" ", "_")
+            .replace(")", "_")
+            .replace("(", "_")
+            .replace(",", "_")
+            .replace("'", "_")
+            .replace("\n", "_")
+            .replace("\r", "_")
+            .replace("'t", "_")
+            .replace("new", "New")
+            .replace("\\", "_")
+            .replace("/", "_")
+            .replace("-", "_")
+            .replace(":", "_")
+            .replace("+", "_")
+            .replace(".", "_")
+            .replace("?", "_")
+            .replace("'", "_")
+            .replace(">", "_gt")
+            .replace("<", "_lt")
+            .replace('"', "_")
+            .replace(";", "_")
+            .replace("$", "_")
+            .replace("=", "_eq")
+            .replace("#", "_")
+            .replace("&", "_")
+            .replace("%", "_")
+            .replace("[", "_")
+            .replace("]", "_")
+            .replace("?", "_")
+            .replace("–", "_")
+            .replace("__", "_")
+            .replace("__", "_")
+            .replace(chr(0x2019), "_")
+            .replace(chr(65533), "_")
+            .replace(chr(0x00A8), "_")
+            .replace(chr(0x00A9), "_")
+            .replace(chr(0x00A4), "_")
+            .replace(chr(0x00B6), "_")
+            .replace(chr(0x00D0), "_")
+            .replace(chr(0x00BA), "_")
+            .replace(chr(0x2020), "_")
+            .replace(chr(0x00B5), "_")
+            .replace(chr(0x20AC), "_")
+            .replace(chr(0x00B4), "_")
+            .replace(chr(0x0192), "_")
+            .replace(chr(0x00B2), "_")
+            .replace(chr(0x00BF), "_")
+            .replace(chr(0x00B0), "_")
+            .replace(chr(0x00A6), "_")
+            .replace(chr(0x203A), "_")
+            .replace(chr(0x00A2), "_")
+            .replace(chr(0x2122), "_")
+            .replace(chr(0x00B1), "_")
+            .replace(chr(0x00B9), "_")
+            .replace(chr(0x00AE), "_")
+            .replace(chr(0x2014), "_")
+            .replace(chr(0x02DC), "_")
+            .replace(chr(0x201E), "_")
+            .replace(chr(0x2026), "_")
+            .replace(chr(0x00BF), "_")
+            .replace(chr(0x00BB), "_")
+            .replace(chr(0x00AB), "_")
+            .replace(chr(0x2022), "_")
+            .replace(chr(0x00AC), "_")
+            .replace(chr(0x2021), "_")
+            .replace(chr(0x00A5), "_")
+            .replace(chr(0x201E), "_")
+            .replace(chr(0x201C), "_")
+            .replace(chr(0x00AF), "_")
+            .replace(chr(0x201D), "_")
+            .replace(chr(0x00A3), "_")
+            .replace(chr(0x2030), "_")
+            .replace(chr(0x00BD), "_")
+            .replace(chr(0x00BC), "_")
+            .replace(chr(0x00BE), "_")
+            .replace(chr(0x00A1), "_")
+            .replace(chr(0x2018), "_")
+            .replace(chr(0x0060), "_")
+            .replace(chr(0x00B4), "_")
+            .replace(chr(0x2026), "_")
+            .replace(chr(0x200B), "_")
+            .replace(chr(0x202F), "_")
+            .replace(chr(0x205F), "_")
+            .replace(chr(0x3000), "_")
+            .replace(chr(0x2000), "_")
+            .replace(chr(0x2001), "_")
+            .replace(chr(0x2002), "_")
+            .replace(chr(0x2003), "_")
+            .replace(chr(0x2004), "_")
+            .replace(chr(0x2005), "_")
+            .replace(chr(0x2006), "_")
+            .replace(chr(0x2007), "_")
+            .replace(chr(0x2008), "_")
+            .replace(chr(0x2009), "_")
+            .replace(chr(0x200A), "_")
+            .replace(chr(0x00A0), "_")
+            .replace(chr(0x0027), "_")
+            .replace(chr(0x2019), "_")
+            .replace(chr(0x2018), "_")
+            .replace(chr(0x201A), "_")
+            .replace(chr(0x00B7), "_")
+        )
+
+        return_string = Utils.replace_acutes_graves_and_circumflexes(amended_input_string).replace("'", "_")
 
         if return_string == "op":
             return_string = "_op"
 
-        return_string = return_string.replace('__', '_').replace('__', '_').replace('__', '_').replace('__', '_').replace('__', '_').replace('__', '_')
-        if return_string.endswith('_'):
-            return_string = return_string[0:len(return_string)-1]
+        return_string = (
+            return_string.replace("__", "_")
+            .replace("__", "_")
+            .replace("__", "_")
+            .replace("__", "_")
+            .replace("__", "_")
+            .replace("__", "_")
+        )
+        if return_string.endswith("_"):
+            return_string = return_string[0 : len(return_string) - 1]
         return return_string
 
     @classmethod
     def replace_acutes_graves_and_circumflexes(cls, the_input_string):
-        '''
+        """
         We replace letters with acutes , graves, and circumflexes, with the basic letter.
         So for example "a acute" is replaced with "a"
-        '''
-        return unicodedata.normalize('NFD', the_input_string).encode('ascii', 'ignore').decode('ascii')
-
+        """
+        return unicodedata.normalize("NFD", the_input_string).encode("ascii", "ignore").decode("ascii")
 
     @classmethod
     def contains_literal(cls, members, adapted_value):
-        '''
+        """
         checks if an enum contains a particular literal
-        '''
+        """
         contains = False
         for e_enum_literal in members:
             if e_enum_literal.name.lower() == adapted_value.lower():
                 contains = True
 
         return contains
 
     @classmethod
     def contains_name(cls, members, adapted_name):
-        '''
+        """
         checks if an enum contains a particular name
-        '''
+        """
         contains = False
         for e_enum_literal in members:
             if e_enum_literal.literal.lower() == adapted_name.lower():
                 contains = True
 
         return contains
 
     @classmethod
     def get_literals_for_enumeration(cls, domain, members_module):
-        '''
+        """
         returns the list of literals for an enumerations
-        '''
+        """
         return_members_list = []
         for member in members_module.members:
             if member.domain_id == domain:
                 return_members_list.append(member)
         return return_members_list
 
     @classmethod
     def get_ecore_datatype_for_datatype(cls, context):
-        '''
+        """
         returns the ecore data type for a data type
-        '''
+        """
         return context.e_string
 
-
-    @classmethod
-    def get_annotation_with_source(cls,element, source):
-        '''
+    @classmethod
+    def get_annotation_with_source(cls, element, source):
+        """
         returns the annotation with the source
-        '''
+        """
         return_annotation = None
         for annotation in element.eAnnotations:
             if annotation.source is not None:
                 if annotation.source.name == source:
                     return_annotation = annotation
@@ -234,32 +396,32 @@
                 print("no source for annotation2" + element.name)
 
         return return_annotation
 
     @classmethod
-    def get_annotation_directive(cls,package, name):
-        '''
+    def get_annotation_directive(cls, package, name):
+        """
         returns the annotation directive with the name
-        '''
+        """
         return_annotation_directive = None
         for annotation_directive in package.annotationDirectives:
             if annotation_directive.name == name:
                 return_annotation_directive = annotation_directive
         return return_annotation_directive
 
     @classmethod
     def number_of_relationships_to_this_class(cls, source_class, target_class):
-        '''
+        """
         Checks how many relationships there are between 2 classes
         It is possible that one class might have 2 different relationships
         to the same class.
-        '''
+        """
         features = source_class.eStructuralFeatures
         counter = 0
         # do this for relationship attributes only.
         for feature in features:
             if isinstance(feature, ELReference):
                 feature_type = feature.eType
                 if feature_type == target_class:
-                    counter = counter+1
+                    counter = counter + 1
 
         return counter
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/utils.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/advanced_migration_generator.py	2025-09-15 13:18:11.408896+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/advanced_migration_generator.py	2025-09-21 17:07:34.989460+00:00
@@ -27,10 +27,11 @@
 
 
 @dataclass
 class FieldInfo:
     """Represents a Django model field."""
+
     name: str
     field_type: str
     verbose_name: Optional[str] = None
     max_length: Optional[int] = None
     blank: bool = False
@@ -52,10 +53,11 @@
 
 
 @dataclass
 class ModelInfo:
     """Represents a Django model."""
+
     name: str
     fields: List[FieldInfo]
     verbose_name: Optional[str] = None
     verbose_name_plural: Optional[str] = None
     db_table: Optional[str] = None
@@ -89,11 +91,11 @@
 
         # Store potential models for two-pass processing
         self.potential_models[node.name] = node
 
         # Check for choice domains (dictionaries with domain suffix)
-        if node.name.endswith('_domain'):
+        if node.name.endswith("_domain"):
             self._parse_choice_domain(node)
 
         self.generic_visit(node)
 
     def process_models(self):
@@ -101,12 +103,16 @@
         # First pass: identify direct models.Model inheritors
         direct_models = set()
 
         for class_name, node in self.potential_models.items():
             for base in node.bases:
-                if (isinstance(base, ast.Attribute) and isinstance(base.value, ast.Name) and
-                    base.value.id == 'models' and base.attr == 'Model'):
+                if (
+                    isinstance(base, ast.Attribute)
+                    and isinstance(base.value, ast.Name)
+                    and base.value.id == "models"
+                    and base.attr == "Model"
+                ):
                     direct_models.add(class_name)
                     self._parse_model(node, parent_model=None)
                     break
 
         # Second pass: identify model inheritors (subclasses of existing models)
@@ -133,30 +139,30 @@
     def _parse_model(self, node: ast.ClassDef, parent_model: Optional[str] = None):
         """Parse a single model class definition."""
         bases = []
         for base in node.bases:
             if isinstance(base, ast.Attribute) and isinstance(base.value, ast.Name):
-                bases.append(f'{base.value.id}.{base.attr}')
+                bases.append(f"{base.value.id}.{base.attr}")
             elif isinstance(base, ast.Name):
                 bases.append(base.id)
 
         self.current_model = ModelInfo(name=node.name, fields=[], parent_model=parent_model, bases=bases)
 
         # Parse class body
         for item in node.body:
             if isinstance(item, ast.Assign):
                 self._parse_field_assignment(item)
-            elif isinstance(item, ast.ClassDef) and item.name == 'Meta':
+            elif isinstance(item, ast.ClassDef) and item.name == "Meta":
                 self._parse_meta_class(item)
 
         self.models.append(self.current_model)
         self.current_model = None
 
     def visit_Assign(self, node: ast.Assign):
         """Visit assignments to find choice domains."""
         for target in node.targets:
-            if isinstance(target, ast.Name) and target.id.endswith('_domain'):
+            if isinstance(target, ast.Name) and target.id.endswith("_domain"):
                 if isinstance(node.value, ast.Dict):
                     choices = []
                     for key, value in zip(node.value.keys, node.value.values):
                         if isinstance(key, ast.Constant) and isinstance(value, ast.Constant):
                             choices.append((str(key.value), str(value.value)))
@@ -177,11 +183,11 @@
         for target in node.targets:
             if isinstance(target, ast.Name):
                 field_name = target.id
 
                 # Skip non-field attributes
-                if field_name.startswith('_') or field_name in ['DoesNotExist', 'MultipleObjectsReturned']:
+                if field_name.startswith("_") or field_name in ["DoesNotExist", "MultipleObjectsReturned"]:
                     continue
 
                 field_info = self._parse_field_call(field_name, node.value)
                 if field_info:
                     # Check for duplicate field names
@@ -192,62 +198,62 @@
     def _parse_field_call(self, field_name: str, node: ast.AST) -> Optional[FieldInfo]:
         """Parse field call to extract field information."""
         if not isinstance(node, ast.Call):
             return None
 
-        field_info = FieldInfo(name=field_name, field_type='CharField')
+        field_info = FieldInfo(name=field_name, field_type="CharField")
 
         # Determine field type
         if isinstance(node.func, ast.Attribute):
-            if isinstance(node.func.value, ast.Name) and node.func.value.id == 'models':
+            if isinstance(node.func.value, ast.Name) and node.func.value.id == "models":
                 field_info.field_type = node.func.attr
 
         # Parse arguments
         for i, arg in enumerate(node.args):
             if i == 0 and isinstance(arg, ast.Constant):
                 # For ForeignKey{os.sep}OneToOneField, first positional arg is 'to'
-                if field_info.field_type in ['ForeignKey', 'OneToOneField']:
+                if field_info.field_type in ["ForeignKey", "OneToOneField"]:
                     field_info.foreign_key_to = str(arg.value)
                 else:
                     field_info.verbose_name = str(arg.value)
 
         # Parse keyword arguments
         for keyword in node.keywords:
-            if keyword.arg == 'max_length' and isinstance(keyword.value, ast.Constant):
+            if keyword.arg == "max_length" and isinstance(keyword.value, ast.Constant):
                 field_info.max_length = int(keyword.value.value)
-            elif keyword.arg == 'blank' and isinstance(keyword.value, ast.Constant):
+            elif keyword.arg == "blank" and isinstance(keyword.value, ast.Constant):
                 field_info.blank = bool(keyword.value.value)
-            elif keyword.arg == 'null' and isinstance(keyword.value, ast.Constant):
+            elif keyword.arg == "null" and isinstance(keyword.value, ast.Constant):
                 field_info.null = bool(keyword.value.value)
-            elif keyword.arg == 'default':
+            elif keyword.arg == "default":
                 if isinstance(keyword.value, ast.Constant):
                     field_info.default = keyword.value.value
-                elif isinstance(keyword.value, ast.Name) and keyword.value.id == 'None':
+                elif isinstance(keyword.value, ast.Name) and keyword.value.id == "None":
                     field_info.default = None
-            elif keyword.arg == 'primary_key' and isinstance(keyword.value, ast.Constant):
+            elif keyword.arg == "primary_key" and isinstance(keyword.value, ast.Constant):
                 field_info.primary_key = bool(keyword.value.value)
-            elif keyword.arg == 'choices':
+            elif keyword.arg == "choices":
                 field_info.choices = self._parse_choices(keyword.value)
-            elif keyword.arg == 'db_comment' and isinstance(keyword.value, ast.Constant):
+            elif keyword.arg == "db_comment" and isinstance(keyword.value, ast.Constant):
                 field_info.db_comment = str(keyword.value.value)
-            elif keyword.arg == 'to' and isinstance(keyword.value, ast.Constant):
+            elif keyword.arg == "to" and isinstance(keyword.value, ast.Constant):
                 field_info.foreign_key_to = str(keyword.value.value)
-            elif keyword.arg == 'on_delete':
+            elif keyword.arg == "on_delete":
                 field_info.on_delete = self._parse_on_delete(keyword.value)
-            elif keyword.arg == 'related_name' and isinstance(keyword.value, ast.Constant):
+            elif keyword.arg == "related_name" and isinstance(keyword.value, ast.Constant):
                 field_info.related_name = str(keyword.value.value)
-            elif keyword.arg == 'auto_created' and isinstance(keyword.value, ast.Constant):
+            elif keyword.arg == "auto_created" and isinstance(keyword.value, ast.Constant):
                 field_info.auto_created = bool(keyword.value.value)
-            elif keyword.arg == 'serialize' and isinstance(keyword.value, ast.Constant):
+            elif keyword.arg == "serialize" and isinstance(keyword.value, ast.Constant):
                 field_info.serialize = bool(keyword.value.value)
-            elif keyword.arg == 'unique' and isinstance(keyword.value, ast.Constant):
+            elif keyword.arg == "unique" and isinstance(keyword.value, ast.Constant):
                 field_info.unique = bool(keyword.value.value)
-            elif keyword.arg == 'db_index' and isinstance(keyword.value, ast.Constant):
+            elif keyword.arg == "db_index" and isinstance(keyword.value, ast.Constant):
                 field_info.db_index = bool(keyword.value.value)
-            elif keyword.arg == 'help_text' and isinstance(keyword.value, ast.Constant):
+            elif keyword.arg == "help_text" and isinstance(keyword.value, ast.Constant):
                 field_info.help_text = str(keyword.value.value)
-            elif keyword.arg == 'parent_link' and isinstance(keyword.value, ast.Constant):
+            elif keyword.arg == "parent_link" and isinstance(keyword.value, ast.Constant):
                 field_info.parent_link = bool(keyword.value.value)
 
         return field_info
 
     def _parse_choices(self, node: ast.AST) -> Optional[List[Tuple[str, str]]]:
@@ -278,18 +284,18 @@
                 parts.append(current.attr)
                 current = current.value
             if isinstance(current, ast.Name):
                 parts.append(current.id)
             # Return the full path to ensure complete on_delete reference
-            full_path = '.'.join(reversed(parts))
+            full_path = ".".join(reversed(parts))
             # Ensure we have the complete django.db.models.deletion.X format
-            if 'django' in full_path and 'deletion' in full_path:
+            if "django" in full_path and "deletion" in full_path:
                 return full_path
-            elif full_path.startswith('models.deletion'):
-                return f'django.db.{full_path}'
-            elif full_path.endswith('CASCADE') or full_path.endswith('SET_NULL') or full_path.endswith('PROTECT'):
-                if not full_path.startswith('django.db.models.deletion'):
+            elif full_path.startswith("models.deletion"):
+                return f"django.db.{full_path}"
+            elif full_path.endswith("CASCADE") or full_path.endswith("SET_NULL") or full_path.endswith("PROTECT"):
+                if not full_path.startswith("django.db.models.deletion"):
                     return f'django.db.models.deletion.{full_path.split(".")[-1]}'
             return full_path
         return None
 
     def _parse_meta_class(self, node: ast.ClassDef):
@@ -299,23 +305,23 @@
 
         for item in node.body:
             if isinstance(item, ast.Assign):
                 for target in item.targets:
                     if isinstance(target, ast.Name):
-                        if target.id == 'verbose_name' and isinstance(item.value, ast.Constant):
+                        if target.id == "verbose_name" and isinstance(item.value, ast.Constant):
                             self.current_model.verbose_name = str(item.value.value)
-                        elif target.id == 'verbose_name_plural' and isinstance(item.value, ast.Constant):
+                        elif target.id == "verbose_name_plural" and isinstance(item.value, ast.Constant):
                             self.current_model.verbose_name_plural = str(item.value.value)
-                        elif target.id == 'db_table' and isinstance(item.value, ast.Constant):
+                        elif target.id == "db_table" and isinstance(item.value, ast.Constant):
                             self.current_model.db_table = str(item.value.value)
-                        elif target.id == 'ordering' and isinstance(item.value, ast.List):
+                        elif target.id == "ordering" and isinstance(item.value, ast.List):
                             ordering = []
                             for elt in item.value.elts:
                                 if isinstance(elt, ast.Constant):
                                     ordering.append(str(elt.value))
                             self.current_model.ordering = ordering
-                        elif target.id == 'abstract' and isinstance(item.value, ast.Constant):
+                        elif target.id == "abstract" and isinstance(item.value, ast.Constant):
                             self.current_model.abstract = bool(item.value.value)
 
 
 class AdvancedMigrationGenerator:
     """Generates Django migration files from parsed model information using only AST."""
@@ -323,11 +329,11 @@
     def __init__(self):
         self.models = []
 
     def parse_file(self, file_path: str) -> List[ModelInfo]:
         """Parse a Python file to extract model definitions."""
-        with open(file_path, 'r', encoding='utf-8') as f:
+        with open(file_path, "r", encoding="utf-8") as f:
             content = f.read()
 
         # Parse the file
         tree = ast.parse(content)
         parser = ModelParser()
@@ -344,11 +350,11 @@
         parser = ModelParser()
 
         # First pass: collect all classes across all files
         for file_path in file_paths:
             try:
-                with open(file_path, 'r', encoding='utf-8') as f:
+                with open(file_path, "r", encoding="utf-8") as f:
                     content = f.read()
                 tree = ast.parse(content)
                 parser.visit(tree)
             except Exception as e:
                 print(f"Error parsing {file_path}: {e}")
@@ -365,16 +371,17 @@
 
     def generate_migration_ast(self, models: List[ModelInfo], migration_name: str = "0001_initial") -> ast.Module:
         """Generate migration AST from model information."""
         # Import statements
         import_nodes = [
-            ast.Import(names=[ast.alias(name='django.db.models.deletion', asname=None)]),
-            ast.Import(names=[ast.alias(name='django.utils.timezone', asname=None)]),
-            ast.ImportFrom(module='django.db', names=[
-                ast.alias(name='migrations', asname=None),
-                ast.alias(name='models', asname=None)
-            ], level=0)
+            ast.Import(names=[ast.alias(name="django.db.models.deletion", asname=None)]),
+            ast.Import(names=[ast.alias(name="django.utils.timezone", asname=None)]),
+            ast.ImportFrom(
+                module="django.db",
+                names=[ast.alias(name="migrations", asname=None), ast.alias(name="models", asname=None)],
+                level=0,
+            ),
         ]
 
         # Create operations list
         operations = []
         add_field_operations = []
@@ -389,35 +396,40 @@
             reverse_fields = []
 
             for field in model.fields:
                 if self._is_reverse_field(field):
                     reverse_fields.append(field)
-                elif field.field_type in ['ForeignKey', 'OneToOneField']:
+                elif field.field_type in ["ForeignKey", "OneToOneField"]:
                     foreign_key_fields.append(field)
                 else:
                     create_model_fields.append(field)
 
             # Build fields list for CreateModel (only non-ForeignKey fields)
             fields_list = []
 
             # Add ID field if no primary key exists and this is not an inherited model
             has_primary_key = any(field.primary_key for field in create_model_fields + foreign_key_fields)
             if not has_primary_key and not model.parent_model:
-                id_field = ast.Tuple(elts=[
-                    ast.Constant(value='id'),
-                    ast.Call(
-                        func=ast.Attribute(value=ast.Name(id='models', ctx=ast.Load()), attr='BigAutoField', ctx=ast.Load()),
-                        args=[],
-                        keywords=[
-                            ast.keyword(arg='auto_created', value=ast.Constant(value=True)),
-                            ast.keyword(arg='default', value=ast.Constant(value=None)),
-                            ast.keyword(arg='primary_key', value=ast.Constant(value=True)),
-                            ast.keyword(arg='serialize', value=ast.Constant(value=False)),
-                            ast.keyword(arg='verbose_name', value=ast.Constant(value='ID'))
-                        ]
-                    )
-                ], ctx=ast.Load())
+                id_field = ast.Tuple(
+                    elts=[
+                        ast.Constant(value="id"),
+                        ast.Call(
+                            func=ast.Attribute(
+                                value=ast.Name(id="models", ctx=ast.Load()), attr="BigAutoField", ctx=ast.Load()
+                            ),
+                            args=[],
+                            keywords=[
+                                ast.keyword(arg="auto_created", value=ast.Constant(value=True)),
+                                ast.keyword(arg="default", value=ast.Constant(value=None)),
+                                ast.keyword(arg="primary_key", value=ast.Constant(value=True)),
+                                ast.keyword(arg="serialize", value=ast.Constant(value=False)),
+                                ast.keyword(arg="verbose_name", value=ast.Constant(value="ID")),
+                            ],
+                        ),
+                    ],
+                    ctx=ast.Load(),
+                )
                 fields_list.append(id_field)
 
             # Add non-ForeignKey model fields (check for duplicates)
             seen_field_names = set()
             for field in create_model_fields:
@@ -426,58 +438,78 @@
                     fields_list.append(field_tuple)
                     seen_field_names.add(field.name)
 
             # Create CreateModel call
             create_model_keywords = [
-                ast.keyword(arg='name', value=ast.Constant(value=model.name)),
-                ast.keyword(arg='fields', value=ast.List(elts=fields_list, ctx=ast.Load()))
+                ast.keyword(arg="name", value=ast.Constant(value=model.name)),
+                ast.keyword(arg="fields", value=ast.List(elts=fields_list, ctx=ast.Load())),
             ]
 
             # Add options if present
             options = self._generate_options_dict(model)
             if options.keys or options.values:
-                create_model_keywords.append(ast.keyword(arg='options', value=options))
+                create_model_keywords.append(ast.keyword(arg="options", value=options))
 
             # Add bases if this is a model inheritance (subclass)
             if model.parent_model:
                 # For inherited models, add parent_link field if not already present
-                has_parent_link = any(field.name.endswith('_ptr') for field in create_model_fields + foreign_key_fields)
+                has_parent_link = any(field.name.endswith("_ptr") for field in create_model_fields + foreign_key_fields)
                 if not has_parent_link:
                     parent_link_name = f"{model.parent_model.lower()}_ptr"
-                    parent_link_field = ast.Tuple(elts=[
-                        ast.Constant(value=parent_link_name),
-                        ast.Call(
-                            func=ast.Attribute(value=ast.Name(id='models', ctx=ast.Load()), attr='OneToOneField', ctx=ast.Load()),
-                            args=[],
-                            keywords=[
-                                ast.keyword(arg='auto_created', value=ast.Constant(value=True)),
-                                ast.keyword(arg='on_delete', value=ast.Attribute(
-                                    value=ast.Attribute(
+                    parent_link_field = ast.Tuple(
+                        elts=[
+                            ast.Constant(value=parent_link_name),
+                            ast.Call(
+                                func=ast.Attribute(
+                                    value=ast.Name(id="models", ctx=ast.Load()), attr="OneToOneField", ctx=ast.Load()
+                                ),
+                                args=[],
+                                keywords=[
+                                    ast.keyword(arg="auto_created", value=ast.Constant(value=True)),
+                                    ast.keyword(
+                                        arg="on_delete",
                                         value=ast.Attribute(
                                             value=ast.Attribute(
-                                                value=ast.Name(id='django', ctx=ast.Load()),
-                                                attr='db', ctx=ast.Load()),
-                                            attr='models', ctx=ast.Load()),
-                                        attr='deletion', ctx=ast.Load()),
-                                    attr='CASCADE', ctx=ast.Load())),
-                                ast.keyword(arg='parent_link', value=ast.Constant(value=True)),
-                                ast.keyword(arg='primary_key', value=ast.Constant(value=True)),
-                                ast.keyword(arg='serialize', value=ast.Constant(value=False)),
-                                ast.keyword(arg='to', value=ast.Constant(value=f'pybirdai.{model.parent_model.lower()}'))
-                            ]
-                        )
-                    ], ctx=ast.Load())
+                                                value=ast.Attribute(
+                                                    value=ast.Attribute(
+                                                        value=ast.Name(id="django", ctx=ast.Load()),
+                                                        attr="db",
+                                                        ctx=ast.Load(),
+                                                    ),
+                                                    attr="models",
+                                                    ctx=ast.Load(),
+                                                ),
+                                                attr="deletion",
+                                                ctx=ast.Load(),
+                                            ),
+                                            attr="CASCADE",
+                                            ctx=ast.Load(),
+                                        ),
+                                    ),
+                                    ast.keyword(arg="parent_link", value=ast.Constant(value=True)),
+                                    ast.keyword(arg="primary_key", value=ast.Constant(value=True)),
+                                    ast.keyword(arg="serialize", value=ast.Constant(value=False)),
+                                    ast.keyword(
+                                        arg="to", value=ast.Constant(value=f"pybirdai.{model.parent_model.lower()}")
+                                    ),
+                                ],
+                            ),
+                        ],
+                        ctx=ast.Load(),
+                    )
                     fields_list.insert(0, parent_link_field)
 
                 # Add bases parameter as tuple (Django convention)
-                bases_tuple = ast.Tuple(elts=[ast.Constant(value=f'pybirdai.{model.parent_model.lower()}')], ctx=ast.Load())
-                create_model_keywords.append(ast.keyword(arg='bases', value=bases_tuple))
+                bases_tuple = ast.Tuple(
+                    elts=[ast.Constant(value=f"pybirdai.{model.parent_model.lower()}")], ctx=ast.Load()
+                )
+                create_model_keywords.append(ast.keyword(arg="bases", value=bases_tuple))
 
             create_model_call = ast.Call(
-                func=ast.Attribute(value=ast.Name(id='migrations', ctx=ast.Load()), attr='CreateModel', ctx=ast.Load()),
+                func=ast.Attribute(value=ast.Name(id="migrations", ctx=ast.Load()), attr="CreateModel", ctx=ast.Load()),
                 args=[],
-                keywords=create_model_keywords
+                keywords=create_model_keywords,
             )
             operations.append(create_model_call)
 
             # Create AddField operations for ForeignKey fields
             for field in foreign_key_fields:
@@ -492,28 +524,24 @@
         # Add all operations (CreateModel first, then AddField)
         all_operations = operations + add_field_operations
 
         # Create Migration class
         migration_class = ast.ClassDef(
-            name='Migration',
-            bases=[ast.Attribute(value=ast.Name(id='migrations', ctx=ast.Load()), attr='Migration', ctx=ast.Load())],
+            name="Migration",
+            bases=[ast.Attribute(value=ast.Name(id="migrations", ctx=ast.Load()), attr="Migration", ctx=ast.Load())],
             keywords=[],
             body=[
+                ast.Assign(targets=[ast.Name(id="initial", ctx=ast.Store())], value=ast.Constant(value=True)),
                 ast.Assign(
-                    targets=[ast.Name(id='initial', ctx=ast.Store())],
-                    value=ast.Constant(value=True)
+                    targets=[ast.Name(id="dependencies", ctx=ast.Store())], value=ast.List(elts=[], ctx=ast.Load())
                 ),
                 ast.Assign(
-                    targets=[ast.Name(id='dependencies', ctx=ast.Store())],
-                    value=ast.List(elts=[], ctx=ast.Load())
+                    targets=[ast.Name(id="operations", ctx=ast.Store())],
+                    value=ast.List(elts=all_operations, ctx=ast.Load()),
                 ),
-                ast.Assign(
-                    targets=[ast.Name(id='operations', ctx=ast.Store())],
-                    value=ast.List(elts=all_operations, ctx=ast.Load())
-                )
             ],
-            decorator_list=[]
+            decorator_list=[],
         )
 
         # Create module
         module = ast.Module(body=import_nodes + [migration_class], type_ignores=[])
         return module
@@ -530,133 +558,139 @@
 
         # Parameters in logical order: auto_created, blank, default, max_length, null, primary_key, serialize, verbose_name, choices, etc.
 
         # Add auto_created first if applicable
         if field.auto_created:
-            keywords.append(ast.keyword(arg='auto_created', value=ast.Constant(value=True)))
+            keywords.append(ast.keyword(arg="auto_created", value=ast.Constant(value=True)))
 
         # Add blank parameter
         if field.blank:
-            keywords.append(ast.keyword(arg='blank', value=ast.Constant(value=True)))
+            keywords.append(ast.keyword(arg="blank", value=ast.Constant(value=True)))
 
         # Only add default for non-ForeignKey fields in CreateModel (avoid clutter)
-        if field.field_type not in ['ForeignKey', 'OneToOneField']:
+        if field.field_type not in ["ForeignKey", "OneToOneField"]:
             default_value = field.default if field.default is not None else None
             if default_value is not None or field.null:
-                keywords.append(ast.keyword(arg='default', value=ast.Constant(value=default_value)))
+                keywords.append(ast.keyword(arg="default", value=ast.Constant(value=default_value)))
 
         # Add field-specific arguments like max_length
-        if field.field_type == 'CharField' and field.max_length:
-            keywords.append(ast.keyword(arg='max_length', value=ast.Constant(value=field.max_length)))
+        if field.field_type == "CharField" and field.max_length:
+            keywords.append(ast.keyword(arg="max_length", value=ast.Constant(value=field.max_length)))
 
         # Add null parameter
         if field.null:
-            keywords.append(ast.keyword(arg='null', value=ast.Constant(value=True)))
+            keywords.append(ast.keyword(arg="null", value=ast.Constant(value=True)))
 
         # For ForeignKey fields, ensure required parameters are present
-        if field.field_type in ['ForeignKey', 'OneToOneField']:
+        if field.field_type in ["ForeignKey", "OneToOneField"]:
             # Add on_delete parameter (required for ForeignKey{os.sep}OneToOneField)
             if field.on_delete:
                 on_delete_node = self._create_on_delete_ast(field.on_delete)
-                keywords.append(ast.keyword(arg='on_delete', value=on_delete_node))
+                keywords.append(ast.keyword(arg="on_delete", value=on_delete_node))
             else:
                 # Choose appropriate default based on field characteristics
                 # Use SET_NULL for nullable fields, CASCADE for required fields
                 if field.null and not field.primary_key:
-                    default_behavior = 'SET_NULL'
+                    default_behavior = "SET_NULL"
                 else:
-                    default_behavior = 'CASCADE'
+                    default_behavior = "CASCADE"
 
                 default_on_delete = ast.Attribute(
                     value=ast.Attribute(
                         value=ast.Attribute(
-                            value=ast.Attribute(
-                                value=ast.Name(id='django', ctx=ast.Load()),
-                                attr='db', ctx=ast.Load()),
-                            attr='models', ctx=ast.Load()),
-                        attr='deletion', ctx=ast.Load()),
-                    attr=default_behavior, ctx=ast.Load())
-                keywords.append(ast.keyword(arg='on_delete', value=default_on_delete))
+                            value=ast.Attribute(value=ast.Name(id="django", ctx=ast.Load()), attr="db", ctx=ast.Load()),
+                            attr="models",
+                            ctx=ast.Load(),
+                        ),
+                        attr="deletion",
+                        ctx=ast.Load(),
+                    ),
+                    attr=default_behavior,
+                    ctx=ast.Load(),
+                )
+                keywords.append(ast.keyword(arg="on_delete", value=default_on_delete))
 
             # Add to parameter (required for ForeignKey{os.sep}OneToOneField)
             if field.foreign_key_to:
-                keywords.append(ast.keyword(arg='to', value=ast.Constant(value="pybirdai."+field.foreign_key_to.lower())))
+                keywords.append(
+                    ast.keyword(arg="to", value=ast.Constant(value="pybirdai." + field.foreign_key_to.lower()))
+                )
             else:
                 # Try to infer from field name or use a placeholder
                 inferred_to = f'pybirdai.{field.name.lower().replace("_", "")}'
-                keywords.append(ast.keyword(arg='to', value=ast.Constant(value="pybirdai."+inferred_to.lower())))
+                keywords.append(ast.keyword(arg="to", value=ast.Constant(value="pybirdai." + inferred_to.lower())))
         else:
             # Add on_delete for other field types if present
             if field.on_delete:
                 on_delete_node = self._create_on_delete_ast(field.on_delete)
-                keywords.append(ast.keyword(arg='on_delete', value=on_delete_node))
+                keywords.append(ast.keyword(arg="on_delete", value=on_delete_node))
 
         # Add primary_key parameter
         if field.primary_key:
-            keywords.append(ast.keyword(arg='primary_key', value=ast.Constant(value=True)))
+            keywords.append(ast.keyword(arg="primary_key", value=ast.Constant(value=True)))
 
         # Always explicitly set serialize=False for primary keys
         if field.primary_key or not field.serialize:
-            keywords.append(ast.keyword(arg='serialize', value=ast.Constant(value=False)))
+            keywords.append(ast.keyword(arg="serialize", value=ast.Constant(value=False)))
 
         # Add to parameter for non-ForeignKey fields if present
-        if field.foreign_key_to and field.field_type not in ['ForeignKey', 'OneToOneField']:
-            keywords.append(ast.keyword(arg='to', value=ast.Constant(value="pybirdai."+field.foreign_key_to.lower())))
+        if field.foreign_key_to and field.field_type not in ["ForeignKey", "OneToOneField"]:
+            keywords.append(ast.keyword(arg="to", value=ast.Constant(value="pybirdai." + field.foreign_key_to.lower())))
 
         # Add other special parameters
         if field.parent_link:
-            keywords.append(ast.keyword(arg='parent_link', value=ast.Constant(value=True)))
+            keywords.append(ast.keyword(arg="parent_link", value=ast.Constant(value=True)))
 
         # Add verbose_name as named parameter
         if field.verbose_name:
-            keywords.append(ast.keyword(arg='verbose_name', value=ast.Constant(value=field.verbose_name)))
+            keywords.append(ast.keyword(arg="verbose_name", value=ast.Constant(value=field.verbose_name)))
 
         # Add other parameters
         if field.choices:
             choices_list = []
             for k, v in field.choices:
                 choice_tuple = ast.Tuple(elts=[ast.Constant(value=k), ast.Constant(value=v)], ctx=ast.Load())
                 choices_list.append(choice_tuple)
-            keywords.append(ast.keyword(arg='choices', value=ast.List(elts=choices_list, ctx=ast.Load())))
+            keywords.append(ast.keyword(arg="choices", value=ast.List(elts=choices_list, ctx=ast.Load())))
 
         if field.db_comment:
-            keywords.append(ast.keyword(arg='db_comment', value=ast.Constant(value=field.db_comment)))
+            keywords.append(ast.keyword(arg="db_comment", value=ast.Constant(value=field.db_comment)))
 
         if field.related_name:
-            keywords.append(ast.keyword(arg='related_name', value=ast.Constant(value=field.related_name)))
+            keywords.append(ast.keyword(arg="related_name", value=ast.Constant(value=field.related_name)))
 
         if field.unique:
-            keywords.append(ast.keyword(arg='unique', value=ast.Constant(value=True)))
+            keywords.append(ast.keyword(arg="unique", value=ast.Constant(value=True)))
 
         if field.db_index:
-            keywords.append(ast.keyword(arg='db_index', value=ast.Constant(value=True)))
+            keywords.append(ast.keyword(arg="db_index", value=ast.Constant(value=True)))
 
         if field.help_text:
-            keywords.append(ast.keyword(arg='help_text', value=ast.Constant(value=field.help_text)))
+            keywords.append(ast.keyword(arg="help_text", value=ast.Constant(value=field.help_text)))
 
         # Create field call with no positional arguments, only named parameters
         field_call = ast.Call(
-            func=ast.Attribute(value=ast.Name(id='models', ctx=ast.Load()), attr=field.field_type, ctx=ast.Load()),
+            func=ast.Attribute(value=ast.Name(id="models", ctx=ast.Load()), attr=field.field_type, ctx=ast.Load()),
             args=[],  # No positional arguments
-            keywords=keywords
+            keywords=keywords,
         )
 
         return ast.Tuple(elts=[ast.Constant(value=field.name), field_call], ctx=ast.Load())
 
     def _generate_add_field_ast(self, model_name: str, field: FieldInfo) -> ast.Call:
         """Generate AddField operation AST node for reverse relationship fields."""
         # Generate field definition
         field_call = self._generate_field_call_ast(field)
 
         add_field_call = ast.Call(
-            func=ast.Attribute(value=ast.Name(id='migrations', ctx=ast.Load()), attr='AddField', ctx=ast.Load()),
+            func=ast.Attribute(value=ast.Name(id="migrations", ctx=ast.Load()), attr="AddField", ctx=ast.Load()),
             args=[],
             keywords=[
-                ast.keyword(arg='model_name', value=ast.Constant(value=model_name.lower())),
-                ast.keyword(arg='name', value=ast.Constant(value=field.name)),
-                ast.keyword(arg='field', value=field_call)
-            ]
+                ast.keyword(arg="model_name", value=ast.Constant(value=model_name.lower())),
+                ast.keyword(arg="name", value=ast.Constant(value=field.name)),
+                ast.keyword(arg="field", value=field_call),
+            ],
         )
 
         return add_field_call
 
     def _generate_field_call_ast(self, field: FieldInfo) -> ast.Call:
@@ -665,153 +699,171 @@
 
         # Parameters in logical order: auto_created, blank, default, max_length, null, primary_key, serialize, verbose_name, choices, etc.
 
         # Add auto_created first if applicable
         if field.auto_created:
-            keywords.append(ast.keyword(arg='auto_created', value=ast.Constant(value=True)))
+            keywords.append(ast.keyword(arg="auto_created", value=ast.Constant(value=True)))
 
         # Add blank parameter
         if field.blank:
-            keywords.append(ast.keyword(arg='blank', value=ast.Constant(value=True)))
+            keywords.append(ast.keyword(arg="blank", value=ast.Constant(value=True)))
 
         # Add field-specific arguments like max_length
-        if field.field_type == 'CharField' and field.max_length:
-            keywords.append(ast.keyword(arg='max_length', value=ast.Constant(value=field.max_length)))
+        if field.field_type == "CharField" and field.max_length:
+            keywords.append(ast.keyword(arg="max_length", value=ast.Constant(value=field.max_length)))
 
         # Add null parameter
         if field.null:
-            keywords.append(ast.keyword(arg='null', value=ast.Constant(value=True)))
+            keywords.append(ast.keyword(arg="null", value=ast.Constant(value=True)))
 
         # For ForeignKey fields, ensure required parameters are present
-        if field.field_type in ['ForeignKey', 'OneToOneField']:
+        if field.field_type in ["ForeignKey", "OneToOneField"]:
             # Add on_delete parameter (required for ForeignKey{os.sep}OneToOneField)
             if field.on_delete:
                 on_delete_node = self._create_on_delete_ast(field.on_delete)
-                keywords.append(ast.keyword(arg='on_delete', value=on_delete_node))
+                keywords.append(ast.keyword(arg="on_delete", value=on_delete_node))
             else:
                 # Choose appropriate default based on field characteristics
                 # Use SET_NULL for nullable fields, CASCADE for required fields
                 if field.null and not field.primary_key:
-                    default_behavior = 'SET_NULL'
+                    default_behavior = "SET_NULL"
                 else:
-                    default_behavior = 'CASCADE'
+                    default_behavior = "CASCADE"
 
                 default_on_delete = ast.Attribute(
                     value=ast.Attribute(
                         value=ast.Attribute(
-                            value=ast.Attribute(
-                                value=ast.Name(id='django', ctx=ast.Load()),
-                                attr='db', ctx=ast.Load()),
-                            attr='models', ctx=ast.Load()),
-                        attr='deletion', ctx=ast.Load()),
-                    attr=default_behavior, ctx=ast.Load())
-                keywords.append(ast.keyword(arg='on_delete', value=default_on_delete))
+                            value=ast.Attribute(value=ast.Name(id="django", ctx=ast.Load()), attr="db", ctx=ast.Load()),
+                            attr="models",
+                            ctx=ast.Load(),
+                        ),
+                        attr="deletion",
+                        ctx=ast.Load(),
+                    ),
+                    attr=default_behavior,
+                    ctx=ast.Load(),
+                )
+                keywords.append(ast.keyword(arg="on_delete", value=default_on_delete))
 
             # Add to parameter (required for ForeignKey{os.sep}OneToOneField)
             if field.foreign_key_to:
-                keywords.append(ast.keyword(arg='to', value=ast.Constant(value="pybirdai."+field.foreign_key_to.lower())))
+                keywords.append(
+                    ast.keyword(arg="to", value=ast.Constant(value="pybirdai." + field.foreign_key_to.lower()))
+                )
             else:
                 # Try to infer from field name or use a placeholder
                 inferred_to = f'pybirdai.{field.name.lower().replace("_", "")}'
-                keywords.append(ast.keyword(arg='to', value=ast.Constant(value="pybirdai."+inferred_to.lower())))
+                keywords.append(ast.keyword(arg="to", value=ast.Constant(value="pybirdai." + inferred_to.lower())))
         else:
             # Add on_delete for other field types if present
             if field.on_delete:
                 on_delete_node = self._create_on_delete_ast(field.on_delete)
-                keywords.append(ast.keyword(arg='on_delete', value=on_delete_node))
+                keywords.append(ast.keyword(arg="on_delete", value=on_delete_node))
 
         # Add primary_key parameter
         if field.primary_key:
-            keywords.append(ast.keyword(arg='primary_key', value=ast.Constant(value=True)))
+            keywords.append(ast.keyword(arg="primary_key", value=ast.Constant(value=True)))
 
         # Always explicitly set serialize=False for primary keys
         if field.primary_key or not field.serialize:
-            keywords.append(ast.keyword(arg='serialize', value=ast.Constant(value=False)))
+            keywords.append(ast.keyword(arg="serialize", value=ast.Constant(value=False)))
 
         # Add to parameter for non-ForeignKey fields if present
-        if field.foreign_key_to and field.field_type not in ['ForeignKey', 'OneToOneField']:
-            keywords.append(ast.keyword(arg='to', value=ast.Constant(value="pybirdai."+field.foreign_key_to.lower())))
+        if field.foreign_key_to and field.field_type not in ["ForeignKey", "OneToOneField"]:
+            keywords.append(ast.keyword(arg="to", value=ast.Constant(value="pybirdai." + field.foreign_key_to.lower())))
 
         # Add verbose_name as named parameter
         if field.verbose_name:
-            keywords.append(ast.keyword(arg='verbose_name', value=ast.Constant(value=field.verbose_name)))
+            keywords.append(ast.keyword(arg="verbose_name", value=ast.Constant(value=field.verbose_name)))
 
         # Add other parameters
         if field.choices:
             choices_list = []
             for k, v in field.choices:
                 choice_tuple = ast.Tuple(elts=[ast.Constant(value=k), ast.Constant(value=v)], ctx=ast.Load())
                 choices_list.append(choice_tuple)
-            keywords.append(ast.keyword(arg='choices', value=ast.List(elts=choices_list, ctx=ast.Load())))
+            keywords.append(ast.keyword(arg="choices", value=ast.List(elts=choices_list, ctx=ast.Load())))
 
         if field.db_comment:
-            keywords.append(ast.keyword(arg='db_comment', value=ast.Constant(value=field.db_comment)))
+            keywords.append(ast.keyword(arg="db_comment", value=ast.Constant(value=field.db_comment)))
 
         if field.related_name:
-            keywords.append(ast.keyword(arg='related_name', value=ast.Constant(value=field.related_name)))
+            keywords.append(ast.keyword(arg="related_name", value=ast.Constant(value=field.related_name)))
 
         if field.unique:
-            keywords.append(ast.keyword(arg='unique', value=ast.Constant(value=True)))
+            keywords.append(ast.keyword(arg="unique", value=ast.Constant(value=True)))
 
         if field.db_index:
-            keywords.append(ast.keyword(arg='db_index', value=ast.Constant(value=True)))
+            keywords.append(ast.keyword(arg="db_index", value=ast.Constant(value=True)))
 
         if field.help_text:
-            keywords.append(ast.keyword(arg='help_text', value=ast.Constant(value=field.help_text)))
+            keywords.append(ast.keyword(arg="help_text", value=ast.Constant(value=field.help_text)))
 
         # Create field call with no positional arguments, only named parameters
         field_call = ast.Call(
-            func=ast.Attribute(value=ast.Name(id='models', ctx=ast.Load()), attr=field.field_type, ctx=ast.Load()),
+            func=ast.Attribute(value=ast.Name(id="models", ctx=ast.Load()), attr=field.field_type, ctx=ast.Load()),
             args=[],  # No positional arguments
-            keywords=keywords
+            keywords=keywords,
         )
 
         return field_call
 
     def _create_on_delete_ast(self, on_delete_str: str) -> ast.AST:
         """Create AST node for on_delete parameter."""
         # Handle complete on_delete paths like 'django.db.models.deletion.CASCADE'
-        if 'django.db.models.deletion' in on_delete_str:
+        if "django.db.models.deletion" in on_delete_str:
             # Extract the specific deletion behavior (CASCADE, SET_NULL, etc.)
-            parts = on_delete_str.split('.')
-            deletion_behavior = parts[-1] if len(parts) > 1 else 'CASCADE'
+            parts = on_delete_str.split(".")
+            deletion_behavior = parts[-1] if len(parts) > 1 else "CASCADE"
 
             # Create the full AST: django.db.models.deletion.CASCADE
             return ast.Attribute(
                 value=ast.Attribute(
                     value=ast.Attribute(
-                        value=ast.Attribute(
-                            value=ast.Name(id='django', ctx=ast.Load()),
-                            attr='db', ctx=ast.Load()),
-                        attr='models', ctx=ast.Load()),
-                    attr='deletion', ctx=ast.Load()),
-                attr=deletion_behavior, ctx=ast.Load())
-        elif on_delete_str in ['CASCADE', 'SET_NULL', 'PROTECT', 'SET_DEFAULT', 'DO_NOTHING']:
+                        value=ast.Attribute(value=ast.Name(id="django", ctx=ast.Load()), attr="db", ctx=ast.Load()),
+                        attr="models",
+                        ctx=ast.Load(),
+                    ),
+                    attr="deletion",
+                    ctx=ast.Load(),
+                ),
+                attr=deletion_behavior,
+                ctx=ast.Load(),
+            )
+        elif on_delete_str in ["CASCADE", "SET_NULL", "PROTECT", "SET_DEFAULT", "DO_NOTHING"]:
             # Handle bare deletion behaviors - add full path
             return ast.Attribute(
                 value=ast.Attribute(
                     value=ast.Attribute(
-                        value=ast.Attribute(
-                            value=ast.Name(id='django', ctx=ast.Load()),
-                            attr='db', ctx=ast.Load()),
-                        attr='models', ctx=ast.Load()),
-                    attr='deletion', ctx=ast.Load()),
-                attr=on_delete_str, ctx=ast.Load())
+                        value=ast.Attribute(value=ast.Name(id="django", ctx=ast.Load()), attr="db", ctx=ast.Load()),
+                        attr="models",
+                        ctx=ast.Load(),
+                    ),
+                    attr="deletion",
+                    ctx=ast.Load(),
+                ),
+                attr=on_delete_str,
+                ctx=ast.Load(),
+            )
         else:
             # Try to parse the existing format
-            parts = on_delete_str.split('.')
+            parts = on_delete_str.split(".")
             if len(parts) == 1:
                 # Single part - assume it's a deletion behavior
                 return ast.Attribute(
                     value=ast.Attribute(
                         value=ast.Attribute(
-                            value=ast.Attribute(
-                                value=ast.Name(id='django', ctx=ast.Load()),
-                                attr='db', ctx=ast.Load()),
-                            attr='models', ctx=ast.Load()),
-                        attr='deletion', ctx=ast.Load()),
-                    attr=parts[0], ctx=ast.Load())
+                            value=ast.Attribute(value=ast.Name(id="django", ctx=ast.Load()), attr="db", ctx=ast.Load()),
+                            attr="models",
+                            ctx=ast.Load(),
+                        ),
+                        attr="deletion",
+                        ctx=ast.Load(),
+                    ),
+                    attr=parts[0],
+                    ctx=ast.Load(),
+                )
             else:
                 # Multiple parts - reconstruct as attribute chain
                 node = ast.Name(id=parts[0], ctx=ast.Load())
                 for part in parts[1:]:
                     node = ast.Attribute(value=node, attr=part, ctx=ast.Load())
@@ -821,23 +873,23 @@
         """Generate model options dictionary as AST."""
         keys = []
         values = []
 
         if model.verbose_name:
-            keys.append(ast.Constant(value='verbose_name'))
+            keys.append(ast.Constant(value="verbose_name"))
             values.append(ast.Constant(value=model.verbose_name))
 
         if model.verbose_name_plural:
-            keys.append(ast.Constant(value='verbose_name_plural'))
+            keys.append(ast.Constant(value="verbose_name_plural"))
             values.append(ast.Constant(value=model.verbose_name_plural))
 
         if model.db_table:
-            keys.append(ast.Constant(value='db_table'))
+            keys.append(ast.Constant(value="db_table"))
             values.append(ast.Constant(value=model.db_table))
 
         if model.ordering:
-            keys.append(ast.Constant(value='ordering'))
+            keys.append(ast.Constant(value="ordering"))
             ordering_list = ast.List(elts=[ast.Constant(value=item) for item in model.ordering], ctx=ast.Load())
             values.append(ordering_list)
 
         return ast.Dict(keys=keys, values=values)
 
@@ -848,11 +900,11 @@
 
     def save_migration_file(self, models: List[ModelInfo], output_path: str, migration_name: str = "0001_initial"):
         """Save migration file to disk."""
         migration_code = self.generate_migration_code(models, migration_name)
 
-        with open(output_path, 'w', encoding='utf-8') as f:
+        with open(output_path, "w", encoding="utf-8") as f:
             f.write(migration_code)
 
         print(f"Migration saved to: {output_path}")
 
 
@@ -894,11 +946,11 @@
     models = generator.parse_directory(source_dir)
     generator.save_migration_file(models, output_file)
 
 
 # Example usage
-if __name__ == '__main__':
+if __name__ == "__main__":
     # Example: Generate migration from the initial_migration_generation_script.py
     import sys
 
     if len(sys.argv) > 1:
         source_path = sys.argv[1]
@@ -926,10 +978,12 @@
     else:
         print("Usage: python advanced_migration_generator.py <source_file_or_directory> [output_file]")
         print("Example: python advanced_migration_generator.py models.py 0001_initial.py")
     generator = AdvancedMigrationGenerator()
 
-    models = generator.parse_files([f"pybirdai{os.sep}models{os.sep}bird_data_model.py", f"pybirdai{os.sep}models{os.sep}bird_meta_data_model.py"])
+    models = generator.parse_files(
+        [f"pybirdai{os.sep}models{os.sep}bird_data_model.py", f"pybirdai{os.sep}models{os.sep}bird_meta_data_model.py"]
+    )
 
     # Generate migration code
     migration_code = generator.generate_migration_code(models)
     generator.save_migration_file(models, f"pybirdai{os.sep}migrations{os.sep}0001_initial.py")
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/advanced_migration_generator.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/speed_improvements_initial_migration/advanced_migration_generator.py	2025-09-15 13:18:11.414871+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/speed_improvements_initial_migration/advanced_migration_generator.py	2025-09-21 17:07:35.157218+00:00
@@ -27,10 +27,11 @@
 
 
 @dataclass
 class FieldInfo:
     """Represents a Django model field."""
+
     name: str
     field_type: str
     verbose_name: Optional[str] = None
     max_length: Optional[int] = None
     blank: bool = False
@@ -52,10 +53,11 @@
 
 
 @dataclass
 class ModelInfo:
     """Represents a Django model."""
+
     name: str
     fields: List[FieldInfo]
     verbose_name: Optional[str] = None
     verbose_name_plural: Optional[str] = None
     db_table: Optional[str] = None
@@ -89,11 +91,11 @@
 
         # Store potential models for two-pass processing
         self.potential_models[node.name] = node
 
         # Check for choice domains (dictionaries with domain suffix)
-        if node.name.endswith('_domain'):
+        if node.name.endswith("_domain"):
             self._parse_choice_domain(node)
 
         self.generic_visit(node)
 
     def process_models(self):
@@ -101,12 +103,16 @@
         # First pass: identify direct models.Model inheritors
         direct_models = set()
 
         for class_name, node in self.potential_models.items():
             for base in node.bases:
-                if (isinstance(base, ast.Attribute) and isinstance(base.value, ast.Name) and
-                    base.value.id == 'models' and base.attr == 'Model'):
+                if (
+                    isinstance(base, ast.Attribute)
+                    and isinstance(base.value, ast.Name)
+                    and base.value.id == "models"
+                    and base.attr == "Model"
+                ):
                     direct_models.add(class_name)
                     self._parse_model(node, parent_model=None)
                     break
 
         # Second pass: identify model inheritors (subclasses of existing models)
@@ -133,30 +139,30 @@
     def _parse_model(self, node: ast.ClassDef, parent_model: Optional[str] = None):
         """Parse a single model class definition."""
         bases = []
         for base in node.bases:
             if isinstance(base, ast.Attribute) and isinstance(base.value, ast.Name):
-                bases.append(f'{base.value.id}.{base.attr}')
+                bases.append(f"{base.value.id}.{base.attr}")
             elif isinstance(base, ast.Name):
                 bases.append(base.id)
 
         self.current_model = ModelInfo(name=node.name, fields=[], parent_model=parent_model, bases=bases)
 
         # Parse class body
         for item in node.body:
             if isinstance(item, ast.Assign):
                 self._parse_field_assignment(item)
-            elif isinstance(item, ast.ClassDef) and item.name == 'Meta':
+            elif isinstance(item, ast.ClassDef) and item.name == "Meta":
                 self._parse_meta_class(item)
 
         self.models.append(self.current_model)
         self.current_model = None
 
     def visit_Assign(self, node: ast.Assign):
         """Visit assignments to find choice domains."""
         for target in node.targets:
-            if isinstance(target, ast.Name) and target.id.endswith('_domain'):
+            if isinstance(target, ast.Name) and target.id.endswith("_domain"):
                 if isinstance(node.value, ast.Dict):
                     choices = []
                     for key, value in zip(node.value.keys, node.value.values):
                         if isinstance(key, ast.Constant) and isinstance(value, ast.Constant):
                             choices.append((str(key.value), str(value.value)))
@@ -177,11 +183,11 @@
         for target in node.targets:
             if isinstance(target, ast.Name):
                 field_name = target.id
 
                 # Skip non-field attributes
-                if field_name.startswith('_') or field_name in ['DoesNotExist', 'MultipleObjectsReturned']:
+                if field_name.startswith("_") or field_name in ["DoesNotExist", "MultipleObjectsReturned"]:
                     continue
 
                 field_info = self._parse_field_call(field_name, node.value)
                 if field_info:
                     # Check for duplicate field names
@@ -192,62 +198,62 @@
     def _parse_field_call(self, field_name: str, node: ast.AST) -> Optional[FieldInfo]:
         """Parse field call to extract field information."""
         if not isinstance(node, ast.Call):
             return None
 
-        field_info = FieldInfo(name=field_name, field_type='CharField')
+        field_info = FieldInfo(name=field_name, field_type="CharField")
 
         # Determine field type
         if isinstance(node.func, ast.Attribute):
-            if isinstance(node.func.value, ast.Name) and node.func.value.id == 'models':
+            if isinstance(node.func.value, ast.Name) and node.func.value.id == "models":
                 field_info.field_type = node.func.attr
 
         # Parse arguments
         for i, arg in enumerate(node.args):
             if i == 0 and isinstance(arg, ast.Constant):
                 # For ForeignKey{os.sep}OneToOneField, first positional arg is 'to'
-                if field_info.field_type in ['ForeignKey', 'OneToOneField']:
+                if field_info.field_type in ["ForeignKey", "OneToOneField"]:
                     field_info.foreign_key_to = str(arg.value)
                 else:
                     field_info.verbose_name = str(arg.value)
 
         # Parse keyword arguments
         for keyword in node.keywords:
-            if keyword.arg == 'max_length' and isinstance(keyword.value, ast.Constant):
+            if keyword.arg == "max_length" and isinstance(keyword.value, ast.Constant):
                 field_info.max_length = int(keyword.value.value)
-            elif keyword.arg == 'blank' and isinstance(keyword.value, ast.Constant):
+            elif keyword.arg == "blank" and isinstance(keyword.value, ast.Constant):
                 field_info.blank = bool(keyword.value.value)
-            elif keyword.arg == 'null' and isinstance(keyword.value, ast.Constant):
+            elif keyword.arg == "null" and isinstance(keyword.value, ast.Constant):
                 field_info.null = bool(keyword.value.value)
-            elif keyword.arg == 'default':
+            elif keyword.arg == "default":
                 if isinstance(keyword.value, ast.Constant):
                     field_info.default = keyword.value.value
-                elif isinstance(keyword.value, ast.Name) and keyword.value.id == 'None':
+                elif isinstance(keyword.value, ast.Name) and keyword.value.id == "None":
                     field_info.default = None
-            elif keyword.arg == 'primary_key' and isinstance(keyword.value, ast.Constant):
+            elif keyword.arg == "primary_key" and isinstance(keyword.value, ast.Constant):
                 field_info.primary_key = bool(keyword.value.value)
-            elif keyword.arg == 'choices':
+            elif keyword.arg == "choices":
                 field_info.choices = self._parse_choices(keyword.value)
-            elif keyword.arg == 'db_comment' and isinstance(keyword.value, ast.Constant):
+            elif keyword.arg == "db_comment" and isinstance(keyword.value, ast.Constant):
                 field_info.db_comment = str(keyword.value.value)
-            elif keyword.arg == 'to' and isinstance(keyword.value, ast.Constant):
+            elif keyword.arg == "to" and isinstance(keyword.value, ast.Constant):
                 field_info.foreign_key_to = str(keyword.value.value)
-            elif keyword.arg == 'on_delete':
+            elif keyword.arg == "on_delete":
                 field_info.on_delete = self._parse_on_delete(keyword.value)
-            elif keyword.arg == 'related_name' and isinstance(keyword.value, ast.Constant):
+            elif keyword.arg == "related_name" and isinstance(keyword.value, ast.Constant):
                 field_info.related_name = str(keyword.value.value)
-            elif keyword.arg == 'auto_created' and isinstance(keyword.value, ast.Constant):
+            elif keyword.arg == "auto_created" and isinstance(keyword.value, ast.Constant):
                 field_info.auto_created = bool(keyword.value.value)
-            elif keyword.arg == 'serialize' and isinstance(keyword.value, ast.Constant):
+            elif keyword.arg == "serialize" and isinstance(keyword.value, ast.Constant):
                 field_info.serialize = bool(keyword.value.value)
-            elif keyword.arg == 'unique' and isinstance(keyword.value, ast.Constant):
+            elif keyword.arg == "unique" and isinstance(keyword.value, ast.Constant):
                 field_info.unique = bool(keyword.value.value)
-            elif keyword.arg == 'db_index' and isinstance(keyword.value, ast.Constant):
+            elif keyword.arg == "db_index" and isinstance(keyword.value, ast.Constant):
                 field_info.db_index = bool(keyword.value.value)
-            elif keyword.arg == 'help_text' and isinstance(keyword.value, ast.Constant):
+            elif keyword.arg == "help_text" and isinstance(keyword.value, ast.Constant):
                 field_info.help_text = str(keyword.value.value)
-            elif keyword.arg == 'parent_link' and isinstance(keyword.value, ast.Constant):
+            elif keyword.arg == "parent_link" and isinstance(keyword.value, ast.Constant):
                 field_info.parent_link = bool(keyword.value.value)
 
         return field_info
 
     def _parse_choices(self, node: ast.AST) -> Optional[List[Tuple[str, str]]]:
@@ -278,18 +284,18 @@
                 parts.append(current.attr)
                 current = current.value
             if isinstance(current, ast.Name):
                 parts.append(current.id)
             # Return the full path to ensure complete on_delete reference
-            full_path = '.'.join(reversed(parts))
+            full_path = ".".join(reversed(parts))
             # Ensure we have the complete django.db.models.deletion.X format
-            if 'django' in full_path and 'deletion' in full_path:
+            if "django" in full_path and "deletion" in full_path:
                 return full_path
-            elif full_path.startswith('models.deletion'):
-                return f'django.db.{full_path}'
-            elif full_path.endswith('CASCADE') or full_path.endswith('SET_NULL') or full_path.endswith('PROTECT'):
-                if not full_path.startswith('django.db.models.deletion'):
+            elif full_path.startswith("models.deletion"):
+                return f"django.db.{full_path}"
+            elif full_path.endswith("CASCADE") or full_path.endswith("SET_NULL") or full_path.endswith("PROTECT"):
+                if not full_path.startswith("django.db.models.deletion"):
                     return f'django.db.models.deletion.{full_path.split(".")[-1]}'
             return full_path
         return None
 
     def _parse_meta_class(self, node: ast.ClassDef):
@@ -299,23 +305,23 @@
 
         for item in node.body:
             if isinstance(item, ast.Assign):
                 for target in item.targets:
                     if isinstance(target, ast.Name):
-                        if target.id == 'verbose_name' and isinstance(item.value, ast.Constant):
+                        if target.id == "verbose_name" and isinstance(item.value, ast.Constant):
                             self.current_model.verbose_name = str(item.value.value)
-                        elif target.id == 'verbose_name_plural' and isinstance(item.value, ast.Constant):
+                        elif target.id == "verbose_name_plural" and isinstance(item.value, ast.Constant):
                             self.current_model.verbose_name_plural = str(item.value.value)
-                        elif target.id == 'db_table' and isinstance(item.value, ast.Constant):
+                        elif target.id == "db_table" and isinstance(item.value, ast.Constant):
                             self.current_model.db_table = str(item.value.value)
-                        elif target.id == 'ordering' and isinstance(item.value, ast.List):
+                        elif target.id == "ordering" and isinstance(item.value, ast.List):
                             ordering = []
                             for elt in item.value.elts:
                                 if isinstance(elt, ast.Constant):
                                     ordering.append(str(elt.value))
                             self.current_model.ordering = ordering
-                        elif target.id == 'abstract' and isinstance(item.value, ast.Constant):
+                        elif target.id == "abstract" and isinstance(item.value, ast.Constant):
                             self.current_model.abstract = bool(item.value.value)
 
 
 class AdvancedMigrationGenerator:
     """Generates Django migration files from parsed model information using only AST."""
@@ -323,11 +329,11 @@
     def __init__(self):
         self.models = []
 
     def parse_file(self, file_path: str) -> List[ModelInfo]:
         """Parse a Python file to extract model definitions."""
-        with open(file_path, 'r', encoding='utf-8') as f:
+        with open(file_path, "r", encoding="utf-8") as f:
             content = f.read()
 
         # Parse the file
         tree = ast.parse(content)
         parser = ModelParser()
@@ -344,11 +350,11 @@
         parser = ModelParser()
 
         # First pass: collect all classes across all files
         for file_path in file_paths:
             try:
-                with open(file_path, 'r', encoding='utf-8') as f:
+                with open(file_path, "r", encoding="utf-8") as f:
                     content = f.read()
                 tree = ast.parse(content)
                 parser.visit(tree)
             except Exception as e:
                 print(f"Error parsing {file_path}: {e}")
@@ -365,16 +371,17 @@
 
     def generate_migration_ast(self, models: List[ModelInfo], migration_name: str = "0001_initial") -> ast.Module:
         """Generate migration AST from model information."""
         # Import statements
         import_nodes = [
-            ast.Import(names=[ast.alias(name='django.db.models.deletion', asname=None)]),
-            ast.Import(names=[ast.alias(name='django.utils.timezone', asname=None)]),
-            ast.ImportFrom(module='django.db', names=[
-                ast.alias(name='migrations', asname=None),
-                ast.alias(name='models', asname=None)
-            ], level=0)
+            ast.Import(names=[ast.alias(name="django.db.models.deletion", asname=None)]),
+            ast.Import(names=[ast.alias(name="django.utils.timezone", asname=None)]),
+            ast.ImportFrom(
+                module="django.db",
+                names=[ast.alias(name="migrations", asname=None), ast.alias(name="models", asname=None)],
+                level=0,
+            ),
         ]
 
         # Create operations list
         operations = []
         add_field_operations = []
@@ -389,35 +396,40 @@
             reverse_fields = []
 
             for field in model.fields:
                 if self._is_reverse_field(field):
                     reverse_fields.append(field)
-                elif field.field_type in ['ForeignKey', 'OneToOneField']:
+                elif field.field_type in ["ForeignKey", "OneToOneField"]:
                     foreign_key_fields.append(field)
                 else:
                     create_model_fields.append(field)
 
             # Build fields list for CreateModel (only non-ForeignKey fields)
             fields_list = []
 
             # Add ID field if no primary key exists and this is not an inherited model
             has_primary_key = any(field.primary_key for field in create_model_fields + foreign_key_fields)
             if not has_primary_key and not model.parent_model:
-                id_field = ast.Tuple(elts=[
-                    ast.Constant(value='id'),
-                    ast.Call(
-                        func=ast.Attribute(value=ast.Name(id='models', ctx=ast.Load()), attr='BigAutoField', ctx=ast.Load()),
-                        args=[],
-                        keywords=[
-                            ast.keyword(arg='auto_created', value=ast.Constant(value=True)),
-                            ast.keyword(arg='default', value=ast.Constant(value=None)),
-                            ast.keyword(arg='primary_key', value=ast.Constant(value=True)),
-                            ast.keyword(arg='serialize', value=ast.Constant(value=False)),
-                            ast.keyword(arg='verbose_name', value=ast.Constant(value='ID'))
-                        ]
-                    )
-                ], ctx=ast.Load())
+                id_field = ast.Tuple(
+                    elts=[
+                        ast.Constant(value="id"),
+                        ast.Call(
+                            func=ast.Attribute(
+                                value=ast.Name(id="models", ctx=ast.Load()), attr="BigAutoField", ctx=ast.Load()
+                            ),
+                            args=[],
+                            keywords=[
+                                ast.keyword(arg="auto_created", value=ast.Constant(value=True)),
+                                ast.keyword(arg="default", value=ast.Constant(value=None)),
+                                ast.keyword(arg="primary_key", value=ast.Constant(value=True)),
+                                ast.keyword(arg="serialize", value=ast.Constant(value=False)),
+                                ast.keyword(arg="verbose_name", value=ast.Constant(value="ID")),
+                            ],
+                        ),
+                    ],
+                    ctx=ast.Load(),
+                )
                 fields_list.append(id_field)
 
             # Add non-ForeignKey model fields (check for duplicates)
             seen_field_names = set()
             for field in create_model_fields:
@@ -426,58 +438,78 @@
                     fields_list.append(field_tuple)
                     seen_field_names.add(field.name)
 
             # Create CreateModel call
             create_model_keywords = [
-                ast.keyword(arg='name', value=ast.Constant(value=model.name)),
-                ast.keyword(arg='fields', value=ast.List(elts=fields_list, ctx=ast.Load()))
+                ast.keyword(arg="name", value=ast.Constant(value=model.name)),
+                ast.keyword(arg="fields", value=ast.List(elts=fields_list, ctx=ast.Load())),
             ]
 
             # Add options if present
             options = self._generate_options_dict(model)
             if options.keys or options.values:
-                create_model_keywords.append(ast.keyword(arg='options', value=options))
+                create_model_keywords.append(ast.keyword(arg="options", value=options))
 
             # Add bases if this is a model inheritance (subclass)
             if model.parent_model:
                 # For inherited models, add parent_link field if not already present
-                has_parent_link = any(field.name.endswith('_ptr') for field in create_model_fields + foreign_key_fields)
+                has_parent_link = any(field.name.endswith("_ptr") for field in create_model_fields + foreign_key_fields)
                 if not has_parent_link:
                     parent_link_name = f"{model.parent_model.lower()}_ptr"
-                    parent_link_field = ast.Tuple(elts=[
-                        ast.Constant(value=parent_link_name),
-                        ast.Call(
-                            func=ast.Attribute(value=ast.Name(id='models', ctx=ast.Load()), attr='OneToOneField', ctx=ast.Load()),
-                            args=[],
-                            keywords=[
-                                ast.keyword(arg='auto_created', value=ast.Constant(value=True)),
-                                ast.keyword(arg='on_delete', value=ast.Attribute(
-                                    value=ast.Attribute(
+                    parent_link_field = ast.Tuple(
+                        elts=[
+                            ast.Constant(value=parent_link_name),
+                            ast.Call(
+                                func=ast.Attribute(
+                                    value=ast.Name(id="models", ctx=ast.Load()), attr="OneToOneField", ctx=ast.Load()
+                                ),
+                                args=[],
+                                keywords=[
+                                    ast.keyword(arg="auto_created", value=ast.Constant(value=True)),
+                                    ast.keyword(
+                                        arg="on_delete",
                                         value=ast.Attribute(
                                             value=ast.Attribute(
-                                                value=ast.Name(id='django', ctx=ast.Load()),
-                                                attr='db', ctx=ast.Load()),
-                                            attr='models', ctx=ast.Load()),
-                                        attr='deletion', ctx=ast.Load()),
-                                    attr='CASCADE', ctx=ast.Load())),
-                                ast.keyword(arg='parent_link', value=ast.Constant(value=True)),
-                                ast.keyword(arg='primary_key', value=ast.Constant(value=True)),
-                                ast.keyword(arg='serialize', value=ast.Constant(value=False)),
-                                ast.keyword(arg='to', value=ast.Constant(value=f'pybirdai.{model.parent_model.lower()}'))
-                            ]
-                        )
-                    ], ctx=ast.Load())
+                                                value=ast.Attribute(
+                                                    value=ast.Attribute(
+                                                        value=ast.Name(id="django", ctx=ast.Load()),
+                                                        attr="db",
+                                                        ctx=ast.Load(),
+                                                    ),
+                                                    attr="models",
+                                                    ctx=ast.Load(),
+                                                ),
+                                                attr="deletion",
+                                                ctx=ast.Load(),
+                                            ),
+                                            attr="CASCADE",
+                                            ctx=ast.Load(),
+                                        ),
+                                    ),
+                                    ast.keyword(arg="parent_link", value=ast.Constant(value=True)),
+                                    ast.keyword(arg="primary_key", value=ast.Constant(value=True)),
+                                    ast.keyword(arg="serialize", value=ast.Constant(value=False)),
+                                    ast.keyword(
+                                        arg="to", value=ast.Constant(value=f"pybirdai.{model.parent_model.lower()}")
+                                    ),
+                                ],
+                            ),
+                        ],
+                        ctx=ast.Load(),
+                    )
                     fields_list.insert(0, parent_link_field)
 
                 # Add bases parameter as tuple (Django convention)
-                bases_tuple = ast.Tuple(elts=[ast.Constant(value=f'pybirdai.{model.parent_model.lower()}')], ctx=ast.Load())
-                create_model_keywords.append(ast.keyword(arg='bases', value=bases_tuple))
+                bases_tuple = ast.Tuple(
+                    elts=[ast.Constant(value=f"pybirdai.{model.parent_model.lower()}")], ctx=ast.Load()
+                )
+                create_model_keywords.append(ast.keyword(arg="bases", value=bases_tuple))
 
             create_model_call = ast.Call(
-                func=ast.Attribute(value=ast.Name(id='migrations', ctx=ast.Load()), attr='CreateModel', ctx=ast.Load()),
+                func=ast.Attribute(value=ast.Name(id="migrations", ctx=ast.Load()), attr="CreateModel", ctx=ast.Load()),
                 args=[],
-                keywords=create_model_keywords
+                keywords=create_model_keywords,
             )
             operations.append(create_model_call)
 
             # Create AddField operations for ForeignKey fields
             for field in foreign_key_fields:
@@ -492,28 +524,24 @@
         # Add all operations (CreateModel first, then AddField)
         all_operations = operations + add_field_operations
 
         # Create Migration class
         migration_class = ast.ClassDef(
-            name='Migration',
-            bases=[ast.Attribute(value=ast.Name(id='migrations', ctx=ast.Load()), attr='Migration', ctx=ast.Load())],
+            name="Migration",
+            bases=[ast.Attribute(value=ast.Name(id="migrations", ctx=ast.Load()), attr="Migration", ctx=ast.Load())],
             keywords=[],
             body=[
+                ast.Assign(targets=[ast.Name(id="initial", ctx=ast.Store())], value=ast.Constant(value=True)),
                 ast.Assign(
-                    targets=[ast.Name(id='initial', ctx=ast.Store())],
-                    value=ast.Constant(value=True)
+                    targets=[ast.Name(id="dependencies", ctx=ast.Store())], value=ast.List(elts=[], ctx=ast.Load())
                 ),
                 ast.Assign(
-                    targets=[ast.Name(id='dependencies', ctx=ast.Store())],
-                    value=ast.List(elts=[], ctx=ast.Load())
+                    targets=[ast.Name(id="operations", ctx=ast.Store())],
+                    value=ast.List(elts=all_operations, ctx=ast.Load()),
                 ),
-                ast.Assign(
-                    targets=[ast.Name(id='operations', ctx=ast.Store())],
-                    value=ast.List(elts=all_operations, ctx=ast.Load())
-                )
             ],
-            decorator_list=[]
+            decorator_list=[],
         )
 
         # Create module
         module = ast.Module(body=import_nodes + [migration_class], type_ignores=[])
         return module
@@ -530,133 +558,139 @@
 
         # Parameters in logical order: auto_created, blank, default, max_length, null, primary_key, serialize, verbose_name, choices, etc.
 
         # Add auto_created first if applicable
         if field.auto_created:
-            keywords.append(ast.keyword(arg='auto_created', value=ast.Constant(value=True)))
+            keywords.append(ast.keyword(arg="auto_created", value=ast.Constant(value=True)))
 
         # Add blank parameter
         if field.blank:
-            keywords.append(ast.keyword(arg='blank', value=ast.Constant(value=True)))
+            keywords.append(ast.keyword(arg="blank", value=ast.Constant(value=True)))
 
         # Only add default for non-ForeignKey fields in CreateModel (avoid clutter)
-        if field.field_type not in ['ForeignKey', 'OneToOneField']:
+        if field.field_type not in ["ForeignKey", "OneToOneField"]:
             default_value = field.default if field.default is not None else None
             if default_value is not None or field.null:
-                keywords.append(ast.keyword(arg='default', value=ast.Constant(value=default_value)))
+                keywords.append(ast.keyword(arg="default", value=ast.Constant(value=default_value)))
 
         # Add field-specific arguments like max_length
-        if field.field_type == 'CharField' and field.max_length:
-            keywords.append(ast.keyword(arg='max_length', value=ast.Constant(value=field.max_length)))
+        if field.field_type == "CharField" and field.max_length:
+            keywords.append(ast.keyword(arg="max_length", value=ast.Constant(value=field.max_length)))
 
         # Add null parameter
         if field.null:
-            keywords.append(ast.keyword(arg='null', value=ast.Constant(value=True)))
+            keywords.append(ast.keyword(arg="null", value=ast.Constant(value=True)))
 
         # For ForeignKey fields, ensure required parameters are present
-        if field.field_type in ['ForeignKey', 'OneToOneField']:
+        if field.field_type in ["ForeignKey", "OneToOneField"]:
             # Add on_delete parameter (required for ForeignKey{os.sep}OneToOneField)
             if field.on_delete:
                 on_delete_node = self._create_on_delete_ast(field.on_delete)
-                keywords.append(ast.keyword(arg='on_delete', value=on_delete_node))
+                keywords.append(ast.keyword(arg="on_delete", value=on_delete_node))
             else:
                 # Choose appropriate default based on field characteristics
                 # Use SET_NULL for nullable fields, CASCADE for required fields
                 if field.null and not field.primary_key:
-                    default_behavior = 'SET_NULL'
+                    default_behavior = "SET_NULL"
                 else:
-                    default_behavior = 'CASCADE'
+                    default_behavior = "CASCADE"
 
                 default_on_delete = ast.Attribute(
                     value=ast.Attribute(
                         value=ast.Attribute(
-                            value=ast.Attribute(
-                                value=ast.Name(id='django', ctx=ast.Load()),
-                                attr='db', ctx=ast.Load()),
-                            attr='models', ctx=ast.Load()),
-                        attr='deletion', ctx=ast.Load()),
-                    attr=default_behavior, ctx=ast.Load())
-                keywords.append(ast.keyword(arg='on_delete', value=default_on_delete))
+                            value=ast.Attribute(value=ast.Name(id="django", ctx=ast.Load()), attr="db", ctx=ast.Load()),
+                            attr="models",
+                            ctx=ast.Load(),
+                        ),
+                        attr="deletion",
+                        ctx=ast.Load(),
+                    ),
+                    attr=default_behavior,
+                    ctx=ast.Load(),
+                )
+                keywords.append(ast.keyword(arg="on_delete", value=default_on_delete))
 
             # Add to parameter (required for ForeignKey{os.sep}OneToOneField)
             if field.foreign_key_to:
-                keywords.append(ast.keyword(arg='to', value=ast.Constant(value="pybirdai."+field.foreign_key_to.lower())))
+                keywords.append(
+                    ast.keyword(arg="to", value=ast.Constant(value="pybirdai." + field.foreign_key_to.lower()))
+                )
             else:
                 # Try to infer from field name or use a placeholder
                 inferred_to = f'pybirdai.{field.name.lower().replace("_", "")}'
-                keywords.append(ast.keyword(arg='to', value=ast.Constant(value="pybirdai."+inferred_to.lower())))
+                keywords.append(ast.keyword(arg="to", value=ast.Constant(value="pybirdai." + inferred_to.lower())))
         else:
             # Add on_delete for other field types if present
             if field.on_delete:
                 on_delete_node = self._create_on_delete_ast(field.on_delete)
-                keywords.append(ast.keyword(arg='on_delete', value=on_delete_node))
+                keywords.append(ast.keyword(arg="on_delete", value=on_delete_node))
 
         # Add primary_key parameter
         if field.primary_key:
-            keywords.append(ast.keyword(arg='primary_key', value=ast.Constant(value=True)))
+            keywords.append(ast.keyword(arg="primary_key", value=ast.Constant(value=True)))
 
         # Always explicitly set serialize=False for primary keys
         if field.primary_key or not field.serialize:
-            keywords.append(ast.keyword(arg='serialize', value=ast.Constant(value=False)))
+            keywords.append(ast.keyword(arg="serialize", value=ast.Constant(value=False)))
 
         # Add to parameter for non-ForeignKey fields if present
-        if field.foreign_key_to and field.field_type not in ['ForeignKey', 'OneToOneField']:
-            keywords.append(ast.keyword(arg='to', value=ast.Constant(value="pybirdai."+field.foreign_key_to.lower())))
+        if field.foreign_key_to and field.field_type not in ["ForeignKey", "OneToOneField"]:
+            keywords.append(ast.keyword(arg="to", value=ast.Constant(value="pybirdai." + field.foreign_key_to.lower())))
 
         # Add other special parameters
         if field.parent_link:
-            keywords.append(ast.keyword(arg='parent_link', value=ast.Constant(value=True)))
+            keywords.append(ast.keyword(arg="parent_link", value=ast.Constant(value=True)))
 
         # Add verbose_name as named parameter
         if field.verbose_name:
-            keywords.append(ast.keyword(arg='verbose_name', value=ast.Constant(value=field.verbose_name)))
+            keywords.append(ast.keyword(arg="verbose_name", value=ast.Constant(value=field.verbose_name)))
 
         # Add other parameters
         if field.choices:
             choices_list = []
             for k, v in field.choices:
                 choice_tuple = ast.Tuple(elts=[ast.Constant(value=k), ast.Constant(value=v)], ctx=ast.Load())
                 choices_list.append(choice_tuple)
-            keywords.append(ast.keyword(arg='choices', value=ast.List(elts=choices_list, ctx=ast.Load())))
+            keywords.append(ast.keyword(arg="choices", value=ast.List(elts=choices_list, ctx=ast.Load())))
 
         if field.db_comment:
-            keywords.append(ast.keyword(arg='db_comment', value=ast.Constant(value=field.db_comment)))
+            keywords.append(ast.keyword(arg="db_comment", value=ast.Constant(value=field.db_comment)))
 
         if field.related_name:
-            keywords.append(ast.keyword(arg='related_name', value=ast.Constant(value=field.related_name)))
+            keywords.append(ast.keyword(arg="related_name", value=ast.Constant(value=field.related_name)))
 
         if field.unique:
-            keywords.append(ast.keyword(arg='unique', value=ast.Constant(value=True)))
+            keywords.append(ast.keyword(arg="unique", value=ast.Constant(value=True)))
 
         if field.db_index:
-            keywords.append(ast.keyword(arg='db_index', value=ast.Constant(value=True)))
+            keywords.append(ast.keyword(arg="db_index", value=ast.Constant(value=True)))
 
         if field.help_text:
-            keywords.append(ast.keyword(arg='help_text', value=ast.Constant(value=field.help_text)))
+            keywords.append(ast.keyword(arg="help_text", value=ast.Constant(value=field.help_text)))
 
         # Create field call with no positional arguments, only named parameters
         field_call = ast.Call(
-            func=ast.Attribute(value=ast.Name(id='models', ctx=ast.Load()), attr=field.field_type, ctx=ast.Load()),
+            func=ast.Attribute(value=ast.Name(id="models", ctx=ast.Load()), attr=field.field_type, ctx=ast.Load()),
             args=[],  # No positional arguments
-            keywords=keywords
+            keywords=keywords,
         )
 
         return ast.Tuple(elts=[ast.Constant(value=field.name), field_call], ctx=ast.Load())
 
     def _generate_add_field_ast(self, model_name: str, field: FieldInfo) -> ast.Call:
         """Generate AddField operation AST node for reverse relationship fields."""
         # Generate field definition
         field_call = self._generate_field_call_ast(field)
 
         add_field_call = ast.Call(
-            func=ast.Attribute(value=ast.Name(id='migrations', ctx=ast.Load()), attr='AddField', ctx=ast.Load()),
+            func=ast.Attribute(value=ast.Name(id="migrations", ctx=ast.Load()), attr="AddField", ctx=ast.Load()),
             args=[],
             keywords=[
-                ast.keyword(arg='model_name', value=ast.Constant(value=model_name.lower())),
-                ast.keyword(arg='name', value=ast.Constant(value=field.name)),
-                ast.keyword(arg='field', value=field_call)
-            ]
+                ast.keyword(arg="model_name", value=ast.Constant(value=model_name.lower())),
+                ast.keyword(arg="name", value=ast.Constant(value=field.name)),
+                ast.keyword(arg="field", value=field_call),
+            ],
         )
 
         return add_field_call
 
     def _generate_field_call_ast(self, field: FieldInfo) -> ast.Call:
@@ -665,153 +699,171 @@
 
         # Parameters in logical order: auto_created, blank, default, max_length, null, primary_key, serialize, verbose_name, choices, etc.
 
         # Add auto_created first if applicable
         if field.auto_created:
-            keywords.append(ast.keyword(arg='auto_created', value=ast.Constant(value=True)))
+            keywords.append(ast.keyword(arg="auto_created", value=ast.Constant(value=True)))
 
         # Add blank parameter
         if field.blank:
-            keywords.append(ast.keyword(arg='blank', value=ast.Constant(value=True)))
+            keywords.append(ast.keyword(arg="blank", value=ast.Constant(value=True)))
 
         # Add field-specific arguments like max_length
-        if field.field_type == 'CharField' and field.max_length:
-            keywords.append(ast.keyword(arg='max_length', value=ast.Constant(value=field.max_length)))
+        if field.field_type == "CharField" and field.max_length:
+            keywords.append(ast.keyword(arg="max_length", value=ast.Constant(value=field.max_length)))
 
         # Add null parameter
         if field.null:
-            keywords.append(ast.keyword(arg='null', value=ast.Constant(value=True)))
+            keywords.append(ast.keyword(arg="null", value=ast.Constant(value=True)))
 
         # For ForeignKey fields, ensure required parameters are present
-        if field.field_type in ['ForeignKey', 'OneToOneField']:
+        if field.field_type in ["ForeignKey", "OneToOneField"]:
             # Add on_delete parameter (required for ForeignKey{os.sep}OneToOneField)
             if field.on_delete:
                 on_delete_node = self._create_on_delete_ast(field.on_delete)
-                keywords.append(ast.keyword(arg='on_delete', value=on_delete_node))
+                keywords.append(ast.keyword(arg="on_delete", value=on_delete_node))
             else:
                 # Choose appropriate default based on field characteristics
                 # Use SET_NULL for nullable fields, CASCADE for required fields
                 if field.null and not field.primary_key:
-                    default_behavior = 'SET_NULL'
+                    default_behavior = "SET_NULL"
                 else:
-                    default_behavior = 'CASCADE'
+                    default_behavior = "CASCADE"
 
                 default_on_delete = ast.Attribute(
                     value=ast.Attribute(
                         value=ast.Attribute(
-                            value=ast.Attribute(
-                                value=ast.Name(id='django', ctx=ast.Load()),
-                                attr='db', ctx=ast.Load()),
-                            attr='models', ctx=ast.Load()),
-                        attr='deletion', ctx=ast.Load()),
-                    attr=default_behavior, ctx=ast.Load())
-                keywords.append(ast.keyword(arg='on_delete', value=default_on_delete))
+                            value=ast.Attribute(value=ast.Name(id="django", ctx=ast.Load()), attr="db", ctx=ast.Load()),
+                            attr="models",
+                            ctx=ast.Load(),
+                        ),
+                        attr="deletion",
+                        ctx=ast.Load(),
+                    ),
+                    attr=default_behavior,
+                    ctx=ast.Load(),
+                )
+                keywords.append(ast.keyword(arg="on_delete", value=default_on_delete))
 
             # Add to parameter (required for ForeignKey{os.sep}OneToOneField)
             if field.foreign_key_to:
-                keywords.append(ast.keyword(arg='to', value=ast.Constant(value="pybirdai."+field.foreign_key_to.lower())))
+                keywords.append(
+                    ast.keyword(arg="to", value=ast.Constant(value="pybirdai." + field.foreign_key_to.lower()))
+                )
             else:
                 # Try to infer from field name or use a placeholder
                 inferred_to = f'pybirdai.{field.name.lower().replace("_", "")}'
-                keywords.append(ast.keyword(arg='to', value=ast.Constant(value="pybirdai."+inferred_to.lower())))
+                keywords.append(ast.keyword(arg="to", value=ast.Constant(value="pybirdai." + inferred_to.lower())))
         else:
             # Add on_delete for other field types if present
             if field.on_delete:
                 on_delete_node = self._create_on_delete_ast(field.on_delete)
-                keywords.append(ast.keyword(arg='on_delete', value=on_delete_node))
+                keywords.append(ast.keyword(arg="on_delete", value=on_delete_node))
 
         # Add primary_key parameter
         if field.primary_key:
-            keywords.append(ast.keyword(arg='primary_key', value=ast.Constant(value=True)))
+            keywords.append(ast.keyword(arg="primary_key", value=ast.Constant(value=True)))
 
         # Always explicitly set serialize=False for primary keys
         if field.primary_key or not field.serialize:
-            keywords.append(ast.keyword(arg='serialize', value=ast.Constant(value=False)))
+            keywords.append(ast.keyword(arg="serialize", value=ast.Constant(value=False)))
 
         # Add to parameter for non-ForeignKey fields if present
-        if field.foreign_key_to and field.field_type not in ['ForeignKey', 'OneToOneField']:
-            keywords.append(ast.keyword(arg='to', value=ast.Constant(value="pybirdai."+field.foreign_key_to.lower())))
+        if field.foreign_key_to and field.field_type not in ["ForeignKey", "OneToOneField"]:
+            keywords.append(ast.keyword(arg="to", value=ast.Constant(value="pybirdai." + field.foreign_key_to.lower())))
 
         # Add verbose_name as named parameter
         if field.verbose_name:
-            keywords.append(ast.keyword(arg='verbose_name', value=ast.Constant(value=field.verbose_name)))
+            keywords.append(ast.keyword(arg="verbose_name", value=ast.Constant(value=field.verbose_name)))
 
         # Add other parameters
         if field.choices:
             choices_list = []
             for k, v in field.choices:
                 choice_tuple = ast.Tuple(elts=[ast.Constant(value=k), ast.Constant(value=v)], ctx=ast.Load())
                 choices_list.append(choice_tuple)
-            keywords.append(ast.keyword(arg='choices', value=ast.List(elts=choices_list, ctx=ast.Load())))
+            keywords.append(ast.keyword(arg="choices", value=ast.List(elts=choices_list, ctx=ast.Load())))
 
         if field.db_comment:
-            keywords.append(ast.keyword(arg='db_comment', value=ast.Constant(value=field.db_comment)))
+            keywords.append(ast.keyword(arg="db_comment", value=ast.Constant(value=field.db_comment)))
 
         if field.related_name:
-            keywords.append(ast.keyword(arg='related_name', value=ast.Constant(value=field.related_name)))
+            keywords.append(ast.keyword(arg="related_name", value=ast.Constant(value=field.related_name)))
 
         if field.unique:
-            keywords.append(ast.keyword(arg='unique', value=ast.Constant(value=True)))
+            keywords.append(ast.keyword(arg="unique", value=ast.Constant(value=True)))
 
         if field.db_index:
-            keywords.append(ast.keyword(arg='db_index', value=ast.Constant(value=True)))
+            keywords.append(ast.keyword(arg="db_index", value=ast.Constant(value=True)))
 
         if field.help_text:
-            keywords.append(ast.keyword(arg='help_text', value=ast.Constant(value=field.help_text)))
+            keywords.append(ast.keyword(arg="help_text", value=ast.Constant(value=field.help_text)))
 
         # Create field call with no positional arguments, only named parameters
         field_call = ast.Call(
-            func=ast.Attribute(value=ast.Name(id='models', ctx=ast.Load()), attr=field.field_type, ctx=ast.Load()),
+            func=ast.Attribute(value=ast.Name(id="models", ctx=ast.Load()), attr=field.field_type, ctx=ast.Load()),
             args=[],  # No positional arguments
-            keywords=keywords
+            keywords=keywords,
         )
 
         return field_call
 
     def _create_on_delete_ast(self, on_delete_str: str) -> ast.AST:
         """Create AST node for on_delete parameter."""
         # Handle complete on_delete paths like 'django.db.models.deletion.CASCADE'
-        if 'django.db.models.deletion' in on_delete_str:
+        if "django.db.models.deletion" in on_delete_str:
             # Extract the specific deletion behavior (CASCADE, SET_NULL, etc.)
-            parts = on_delete_str.split('.')
-            deletion_behavior = parts[-1] if len(parts) > 1 else 'CASCADE'
+            parts = on_delete_str.split(".")
+            deletion_behavior = parts[-1] if len(parts) > 1 else "CASCADE"
 
             # Create the full AST: django.db.models.deletion.CASCADE
             return ast.Attribute(
                 value=ast.Attribute(
                     value=ast.Attribute(
-                        value=ast.Attribute(
-                            value=ast.Name(id='django', ctx=ast.Load()),
-                            attr='db', ctx=ast.Load()),
-                        attr='models', ctx=ast.Load()),
-                    attr='deletion', ctx=ast.Load()),
-                attr=deletion_behavior, ctx=ast.Load())
-        elif on_delete_str in ['CASCADE', 'SET_NULL', 'PROTECT', 'SET_DEFAULT', 'DO_NOTHING']:
+                        value=ast.Attribute(value=ast.Name(id="django", ctx=ast.Load()), attr="db", ctx=ast.Load()),
+                        attr="models",
+                        ctx=ast.Load(),
+                    ),
+                    attr="deletion",
+                    ctx=ast.Load(),
+                ),
+                attr=deletion_behavior,
+                ctx=ast.Load(),
+            )
+        elif on_delete_str in ["CASCADE", "SET_NULL", "PROTECT", "SET_DEFAULT", "DO_NOTHING"]:
             # Handle bare deletion behaviors - add full path
             return ast.Attribute(
                 value=ast.Attribute(
                     value=ast.Attribute(
-                        value=ast.Attribute(
-                            value=ast.Name(id='django', ctx=ast.Load()),
-                            attr='db', ctx=ast.Load()),
-                        attr='models', ctx=ast.Load()),
-                    attr='deletion', ctx=ast.Load()),
-                attr=on_delete_str, ctx=ast.Load())
+                        value=ast.Attribute(value=ast.Name(id="django", ctx=ast.Load()), attr="db", ctx=ast.Load()),
+                        attr="models",
+                        ctx=ast.Load(),
+                    ),
+                    attr="deletion",
+                    ctx=ast.Load(),
+                ),
+                attr=on_delete_str,
+                ctx=ast.Load(),
+            )
         else:
             # Try to parse the existing format
-            parts = on_delete_str.split('.')
+            parts = on_delete_str.split(".")
             if len(parts) == 1:
                 # Single part - assume it's a deletion behavior
                 return ast.Attribute(
                     value=ast.Attribute(
                         value=ast.Attribute(
-                            value=ast.Attribute(
-                                value=ast.Name(id='django', ctx=ast.Load()),
-                                attr='db', ctx=ast.Load()),
-                            attr='models', ctx=ast.Load()),
-                        attr='deletion', ctx=ast.Load()),
-                    attr=parts[0], ctx=ast.Load())
+                            value=ast.Attribute(value=ast.Name(id="django", ctx=ast.Load()), attr="db", ctx=ast.Load()),
+                            attr="models",
+                            ctx=ast.Load(),
+                        ),
+                        attr="deletion",
+                        ctx=ast.Load(),
+                    ),
+                    attr=parts[0],
+                    ctx=ast.Load(),
+                )
             else:
                 # Multiple parts - reconstruct as attribute chain
                 node = ast.Name(id=parts[0], ctx=ast.Load())
                 for part in parts[1:]:
                     node = ast.Attribute(value=node, attr=part, ctx=ast.Load())
@@ -821,23 +873,23 @@
         """Generate model options dictionary as AST."""
         keys = []
         values = []
 
         if model.verbose_name:
-            keys.append(ast.Constant(value='verbose_name'))
+            keys.append(ast.Constant(value="verbose_name"))
             values.append(ast.Constant(value=model.verbose_name))
 
         if model.verbose_name_plural:
-            keys.append(ast.Constant(value='verbose_name_plural'))
+            keys.append(ast.Constant(value="verbose_name_plural"))
             values.append(ast.Constant(value=model.verbose_name_plural))
 
         if model.db_table:
-            keys.append(ast.Constant(value='db_table'))
+            keys.append(ast.Constant(value="db_table"))
             values.append(ast.Constant(value=model.db_table))
 
         if model.ordering:
-            keys.append(ast.Constant(value='ordering'))
+            keys.append(ast.Constant(value="ordering"))
             ordering_list = ast.List(elts=[ast.Constant(value=item) for item in model.ordering], ctx=ast.Load())
             values.append(ordering_list)
 
         return ast.Dict(keys=keys, values=values)
 
@@ -848,11 +900,11 @@
 
     def save_migration_file(self, models: List[ModelInfo], output_path: str, migration_name: str = "0001_initial"):
         """Save migration file to disk."""
         migration_code = self.generate_migration_code(models, migration_name)
 
-        with open(output_path, 'w', encoding='utf-8') as f:
+        with open(output_path, "w", encoding="utf-8") as f:
             f.write(migration_code)
 
         print(f"Migration saved to: {output_path}")
 
 
@@ -894,11 +946,11 @@
     models = generator.parse_directory(source_dir)
     generator.save_migration_file(models, output_file)
 
 
 # Example usage
-if __name__ == '__main__':
+if __name__ == "__main__":
     # Example: Generate migration from the initial_migration_generation_script.py
     import sys
 
     if len(sys.argv) > 1:
         source_path = sys.argv[1]
@@ -926,10 +978,12 @@
     else:
         print("Usage: python advanced_migration_generator.py <source_file_or_directory> [output_file]")
         print("Example: python advanced_migration_generator.py models.py 0001_initial.py")
     generator = AdvancedMigrationGenerator()
 
-    models = generator.parse_files([f"pybirdai{os.sep}models{os.sep}bird_data_model.py", f"pybirdai{os.sep}models{os.sep}bird_meta_data_model.py"])
+    models = generator.parse_files(
+        [f"pybirdai{os.sep}models{os.sep}bird_data_model.py", f"pybirdai{os.sep}models{os.sep}bird_meta_data_model.py"]
+    )
 
     # Generate migration code
     migration_code = generator.generate_migration_code(models)
     generator.save_migration_file(models, f"pybirdai{os.sep}migrations{os.sep}0001_initial.py")
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/speed_improvements_initial_migration/advanced_migration_generator.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/clone_mode/import_from_metadata_export.py	2025-09-15 13:18:11.409925+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/clone_mode/import_from_metadata_export.py	2025-09-21 17:07:35.227794+00:00
@@ -23,18 +23,22 @@
 from datetime import datetime
 from pathlib import Path
 
 # Allowed table name pattern: letters, digits, underscores only
 import re
+
+
 class DjangoSetup:
     @staticmethod
     def setup():
         import django
         from django.conf import settings
+
         if not settings.configured:
-            os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'birds_nest.settings')
+            os.environ.setdefault("DJANGO_SETTINGS_MODULE", "birds_nest.settings")
             django.setup()
+
 
 # Ensure Django is set up
 DjangoSetup.setup()
 
 from django.db import transaction
@@ -50,13 +54,14 @@
 
 # Add console handler if not already present
 if not logger.handlers:
     console_handler = logging.StreamHandler()
     console_handler.setLevel(logging.INFO)
-    formatter = logging.Formatter('%(name)s - %(levelname)s - %(message)s')
+    formatter = logging.Formatter("%(name)s - %(levelname)s - %(message)s")
     console_handler.setFormatter(formatter)
     logger.addHandler(console_handler)
+
 
 class CSVDataImporter:
     def __init__(self, results_dir="import_results"):
         self.model_map = {}
         self.column_mappings = {}
@@ -66,17 +71,17 @@
         self.allowed_table_names = set()
         self._build_model_map()
         self._build_column_mappings()
         self._ensure_results_directory()
         logger.info("CSVDataImporter initialized")
-        
+
     def _is_safe_table_name(self, table_name):
         """Validate table name against whitelist and pattern"""
         if not table_name:
             return False
         # Check if table name matches expected pattern (letters, digits, underscores only)
-        if not re.match(r'^[a-zA-Z][a-zA-Z0-9_]*$', table_name):
+        if not re.match(r"^[a-zA-Z][a-zA-Z0-9_]*$", table_name):
             return False
         # Check against whitelist of allowed table names
         return table_name in self.allowed_table_names
 
     def _ensure_results_directory(self):
@@ -94,21 +99,21 @@
         # Prepare results for JSON serialization (remove non-serializable objects)
         serializable_results = {}
         for key, value in results.items():
             if isinstance(value, dict):
                 serializable_value = {
-                    'success': value.get('success', False),
-                    'imported_count': value.get('imported_count', 0)
+                    "success": value.get("success", False),
+                    "imported_count": value.get("imported_count", 0),
                 }
-                if 'error' in value:
-                    serializable_value['error'] = value['error']
+                if "error" in value:
+                    serializable_value["error"] = value["error"]
                 serializable_results[key] = serializable_value
             else:
                 serializable_results[key] = value
 
         try:
-            with open(filepath, 'w', encoding='utf-8') as f:
+            with open(filepath, "w", encoding="utf-8") as f:
                 json.dump(serializable_results, f, indent=2, ensure_ascii=False)
             logger.info(f"Results saved to: {filepath}")
             return filepath
         except Exception as e:
             logger.error(f"Failed to save results to {filepath}: {e}")
@@ -138,421 +143,421 @@
 
         # Note: Some tables may have an ID column at index 0 if they use Django's auto-generated primary key
         # This is handled dynamically in import_csv_file method
 
         # Maintenance Agency mappings
-        self.column_mappings['pybirdai_maintenance_agency'] = {
-            col_idx.maintenance_agency_id: 'maintenance_agency_id',
-            col_idx.maintenance_agency_code: 'code',
-            col_idx.maintenance_agency_name: 'name',
-            col_idx.maintenance_agency_description: 'description'
+        self.column_mappings["pybirdai_maintenance_agency"] = {
+            col_idx.maintenance_agency_id: "maintenance_agency_id",
+            col_idx.maintenance_agency_code: "code",
+            col_idx.maintenance_agency_name: "name",
+            col_idx.maintenance_agency_description: "description",
         }
 
         # Framework mappings
-        self.column_mappings['pybirdai_framework'] = {
-            col_idx.framework_maintenance_agency_id: 'maintenance_agency_id',
-            col_idx.framework_id: 'framework_id',
-            col_idx.framework_name: 'name',
-            col_idx.framework_code: 'code',
-            col_idx.framework_description: 'description',
-            col_idx.framework_type: 'framework_type',
-            col_idx.framework_reporting_population: 'reporting_population',
-            col_idx.framework_other_links: 'other_links',
-            col_idx.framework_order: 'order',
-            col_idx.framework_status: 'status'
+        self.column_mappings["pybirdai_framework"] = {
+            col_idx.framework_maintenance_agency_id: "maintenance_agency_id",
+            col_idx.framework_id: "framework_id",
+            col_idx.framework_name: "name",
+            col_idx.framework_code: "code",
+            col_idx.framework_description: "description",
+            col_idx.framework_type: "framework_type",
+            col_idx.framework_reporting_population: "reporting_population",
+            col_idx.framework_other_links: "other_links",
+            col_idx.framework_order: "order",
+            col_idx.framework_status: "status",
         }
 
         # Domain mappings
-        self.column_mappings['pybirdai_domain'] = {
-            col_idx.domain_maintenance_agency: 'maintenance_agency_id',
-            col_idx.domain_domain_true_id: 'domain_id',
-            col_idx.domain_domain_name_index: 'name',
-            col_idx.domain_domain_is_enumerated: 'is_enumerated',
-            col_idx.domain_domain_description: 'description',
-            col_idx.domain_domain_data_type: 'data_type',
-            col_idx.domain_code: 'code',
-            col_idx.domain_facet_id: 'facet_id',
-            col_idx.domain_domain_is_reference: 'is_reference'
+        self.column_mappings["pybirdai_domain"] = {
+            col_idx.domain_maintenance_agency: "maintenance_agency_id",
+            col_idx.domain_domain_true_id: "domain_id",
+            col_idx.domain_domain_name_index: "name",
+            col_idx.domain_domain_is_enumerated: "is_enumerated",
+            col_idx.domain_domain_description: "description",
+            col_idx.domain_domain_data_type: "data_type",
+            col_idx.domain_code: "code",
+            col_idx.domain_facet_id: "facet_id",
+            col_idx.domain_domain_is_reference: "is_reference",
         }
 
         # Variable mappings
-        self.column_mappings['pybirdai_variable'] = {
-            col_idx.variable_maintenance_agency: 'maintenance_agency_id',
-            col_idx.variable_variable_true_id: 'variable_id',
-            col_idx.variable_variable_name_index: 'name',
-            col_idx.variable_code_index: 'code',
-            col_idx.variable_domain_index: 'domain_id',
-            col_idx.variable_variable_description: 'description',
-            col_idx.variable_primary_concept: 'primary_concept',
-            col_idx.variable_is_decomposed: 'is_decomposed'
+        self.column_mappings["pybirdai_variable"] = {
+            col_idx.variable_maintenance_agency: "maintenance_agency_id",
+            col_idx.variable_variable_true_id: "variable_id",
+            col_idx.variable_variable_name_index: "name",
+            col_idx.variable_code_index: "code",
+            col_idx.variable_domain_index: "domain_id",
+            col_idx.variable_variable_description: "description",
+            col_idx.variable_primary_concept: "primary_concept",
+            col_idx.variable_is_decomposed: "is_decomposed",
         }
 
         # Member mappings
-        self.column_mappings['pybirdai_member'] = {
-            col_idx.member_maintenance_agency: 'maintenance_agency_id',
-            col_idx.member_member_id_index: 'member_id',
-            col_idx.member_member_code_index: 'code',
-            col_idx.member_member_name_index: 'name',
-            col_idx.member_domain_id_index: 'domain_id',
-            col_idx.member_member_descriptions: 'description'
+        self.column_mappings["pybirdai_member"] = {
+            col_idx.member_maintenance_agency: "maintenance_agency_id",
+            col_idx.member_member_id_index: "member_id",
+            col_idx.member_member_code_index: "code",
+            col_idx.member_member_name_index: "name",
+            col_idx.member_domain_id_index: "domain_id",
+            col_idx.member_member_descriptions: "description",
         }
 
         # Variable Set mappings
-        self.column_mappings['pybirdai_variable_set'] = {
-            col_idx.variable_set_maintenance_agency_id: 'maintenance_agency_id',
-            col_idx.variable_set_variable_set_id: 'variable_set_id',
-            col_idx.variable_set_name: 'name',
-            col_idx.variable_set_code: 'code',
-            col_idx.variable_set_description: 'description'
+        self.column_mappings["pybirdai_variable_set"] = {
+            col_idx.variable_set_maintenance_agency_id: "maintenance_agency_id",
+            col_idx.variable_set_variable_set_id: "variable_set_id",
+            col_idx.variable_set_name: "name",
+            col_idx.variable_set_code: "code",
+            col_idx.variable_set_description: "description",
         }
 
         # Variable Set Enumeration mappings
-        self.column_mappings['pybirdai_variable_set_enumeration'] = {
-            col_idx.variable_set_enumeration_valid_set: 'variable_set_id',
-            col_idx.variable_set_enumeration_variable_id: 'variable_id',
-            col_idx.variable_set_enumeration_valid_from: 'valid_from',
-            col_idx.variable_set_enumeration_valid_to: 'valid_to',
-            col_idx.variable_set_enumeration_subdomain_id: 'subdomain_id',
-            col_idx.variable_set_enumeration_is_flow: 'is_flow',
-            col_idx.variable_set_enumeration_order: 'order'
+        self.column_mappings["pybirdai_variable_set_enumeration"] = {
+            col_idx.variable_set_enumeration_valid_set: "variable_set_id",
+            col_idx.variable_set_enumeration_variable_id: "variable_id",
+            col_idx.variable_set_enumeration_valid_from: "valid_from",
+            col_idx.variable_set_enumeration_valid_to: "valid_to",
+            col_idx.variable_set_enumeration_subdomain_id: "subdomain_id",
+            col_idx.variable_set_enumeration_is_flow: "is_flow",
+            col_idx.variable_set_enumeration_order: "order",
         }
 
         # Subdomain mappings
-        self.column_mappings['pybirdai_subdomain'] = {
-            col_idx.subdomain_maintenance_agency_id: 'maintenance_agency_id',
-            col_idx.subdomain_subdomain_id_index: 'subdomain_id',
-            col_idx.subdomain_subdomain_name: 'name',
-            col_idx.subdomain_domain_id_index: 'domain_id',
-            col_idx.subdomain_is_listed: 'is_listed',
-            col_idx.subdomain_subdomain_code: 'code',
-            col_idx.subdomain_facet_id: 'facet_id',
-            col_idx.subdomain_subdomain_description: 'description',
-            col_idx.subdomain_is_natural: 'is_natural'
+        self.column_mappings["pybirdai_subdomain"] = {
+            col_idx.subdomain_maintenance_agency_id: "maintenance_agency_id",
+            col_idx.subdomain_subdomain_id_index: "subdomain_id",
+            col_idx.subdomain_subdomain_name: "name",
+            col_idx.subdomain_domain_id_index: "domain_id",
+            col_idx.subdomain_is_listed: "is_listed",
+            col_idx.subdomain_subdomain_code: "code",
+            col_idx.subdomain_facet_id: "facet_id",
+            col_idx.subdomain_subdomain_description: "description",
+            col_idx.subdomain_is_natural: "is_natural",
         }
 
         # Subdomain Enumeration mappings
-        self.column_mappings['pybirdai_subdomain_enumeration'] = {
-            col_idx.subdomain_enumeration_member_id_index: 'member_id',
-            col_idx.subdomain_enumeration_subdomain_id_index: 'subdomain_id',
-            col_idx.subdomain_enumeration_valid_from: 'valid_from',
-            col_idx.subdomain_enumeration_valid_to_index: 'valid_to',
-            col_idx.subdomain_enumeration_order: 'order'
+        self.column_mappings["pybirdai_subdomain_enumeration"] = {
+            col_idx.subdomain_enumeration_member_id_index: "member_id",
+            col_idx.subdomain_enumeration_subdomain_id_index: "subdomain_id",
+            col_idx.subdomain_enumeration_valid_from: "valid_from",
+            col_idx.subdomain_enumeration_valid_to_index: "valid_to",
+            col_idx.subdomain_enumeration_order: "order",
         }
 
         # Member Hierarchy mappings
-        self.column_mappings['pybirdai_member_hierarchy'] = {
-            col_idx.member_hierarchy_maintenance_agency: 'maintenance_agency_id',
-            col_idx.member_hierarchy_id: 'member_hierarchy_id',
-            col_idx.member_hierarchy_code: 'code',
-            col_idx.member_hierarchy_domain_id: 'domain_id',
-            col_idx.member_hierarchy_name: 'name',
-            col_idx.member_hierarchy_description: 'description',
-            col_idx.member_hierarchy_is_main_hierarchy: 'is_main_hierarchy'
+        self.column_mappings["pybirdai_member_hierarchy"] = {
+            col_idx.member_hierarchy_maintenance_agency: "maintenance_agency_id",
+            col_idx.member_hierarchy_id: "member_hierarchy_id",
+            col_idx.member_hierarchy_code: "code",
+            col_idx.member_hierarchy_domain_id: "domain_id",
+            col_idx.member_hierarchy_name: "name",
+            col_idx.member_hierarchy_description: "description",
+            col_idx.member_hierarchy_is_main_hierarchy: "is_main_hierarchy",
         }
 
         # Member Hierarchy Node mappings
-        self.column_mappings['pybirdai_member_hierarchy_node'] = {
-            col_idx.member_hierarchy_node_hierarchy_id: 'member_hierarchy_id',
-            col_idx.member_hierarchy_node_member_id: 'member_id',
-            col_idx.member_hierarchy_node_level: 'level',
-            col_idx.member_hierarchy_node_parent_member_id: 'parent_member_id',
-            col_idx.member_hierarchy_node_comparator: 'comparator',
-            col_idx.member_hierarchy_node_operator: 'operator',
-            col_idx.member_hierarchy_node_valid_from: 'valid_from',
-            col_idx.member_hierarchy_node_valid_to: 'valid_to'
+        self.column_mappings["pybirdai_member_hierarchy_node"] = {
+            col_idx.member_hierarchy_node_hierarchy_id: "member_hierarchy_id",
+            col_idx.member_hierarchy_node_member_id: "member_id",
+            col_idx.member_hierarchy_node_level: "level",
+            col_idx.member_hierarchy_node_parent_member_id: "parent_member_id",
+            col_idx.member_hierarchy_node_comparator: "comparator",
+            col_idx.member_hierarchy_node_operator: "operator",
+            col_idx.member_hierarchy_node_valid_from: "valid_from",
+            col_idx.member_hierarchy_node_valid_to: "valid_to",
         }
 
         # Table mappings
-        self.column_mappings['pybirdai_table'] = {
-            col_idx.table_table_id: 'table_id',
-            col_idx.table_table_name: 'name',
-            col_idx.table_code: 'code',
-            col_idx.table_description: 'description',
-            col_idx.table_maintenance_agency_id: 'maintenance_agency_id',
-            col_idx.table_version: 'version',
-            col_idx.table_valid_from: 'valid_from',
-            col_idx.table_valid_to: 'valid_to'
+        self.column_mappings["pybirdai_table"] = {
+            col_idx.table_table_id: "table_id",
+            col_idx.table_table_name: "name",
+            col_idx.table_code: "code",
+            col_idx.table_description: "description",
+            col_idx.table_maintenance_agency_id: "maintenance_agency_id",
+            col_idx.table_version: "version",
+            col_idx.table_valid_from: "valid_from",
+            col_idx.table_valid_to: "valid_to",
         }
 
         # Axis mappings
-        self.column_mappings['pybirdai_axis'] = {
-            col_idx.axis_id: 'axis_id',
-            col_idx.axis_code: 'code',
-            col_idx.axis_orientation: 'orientation',
-            col_idx.axis_order: 'order',
-            col_idx.axis_name: 'name',
-            col_idx.axis_description: 'description',
-            col_idx.axis_table_id: 'table_id',
-            col_idx.axis_is_open_axis: 'is_open_axis'
+        self.column_mappings["pybirdai_axis"] = {
+            col_idx.axis_id: "axis_id",
+            col_idx.axis_code: "code",
+            col_idx.axis_orientation: "orientation",
+            col_idx.axis_order: "order",
+            col_idx.axis_name: "name",
+            col_idx.axis_description: "description",
+            col_idx.axis_table_id: "table_id",
+            col_idx.axis_is_open_axis: "is_open_axis",
         }
 
         # Axis Ordinate mappings
-        self.column_mappings['pybirdai_axis_ordinate'] = {
-            col_idx.axis_ordinate_axis_ordinate_id: 'axis_ordinate_id',
-            col_idx.axis_ordinate_is_abstract_header: 'is_abstract_header',
-            col_idx.axis_ordinate_code: 'code',
-            col_idx.axis_ordinate_order: 'order',
-            col_idx.axis_ordinate_level: 'level',
-            col_idx.axis_ordinate_path: 'path',
-            col_idx.axis_ordinate_axis_id: 'axis_id',
-            col_idx.axis_ordinate_parent_axis_ordinate_id: 'parent_axis_ordinate_id',
-            col_idx.axis_ordinate_name: 'name',
-            col_idx.axis_ordinate_description: 'description'
+        self.column_mappings["pybirdai_axis_ordinate"] = {
+            col_idx.axis_ordinate_axis_ordinate_id: "axis_ordinate_id",
+            col_idx.axis_ordinate_is_abstract_header: "is_abstract_header",
+            col_idx.axis_ordinate_code: "code",
+            col_idx.axis_ordinate_order: "order",
+            col_idx.axis_ordinate_level: "level",
+            col_idx.axis_ordinate_path: "path",
+            col_idx.axis_ordinate_axis_id: "axis_id",
+            col_idx.axis_ordinate_parent_axis_ordinate_id: "parent_axis_ordinate_id",
+            col_idx.axis_ordinate_name: "name",
+            col_idx.axis_ordinate_description: "description",
         }
 
         # Ordinate Item mappings
-        self.column_mappings['pybirdai_ordinate_item'] = {
-            col_idx.ordinate_item_axis_ordinate_id: 'axis_ordinate_id',
-            col_idx.ordinate_item_variable_id: 'variable_id',
-            col_idx.ordinate_item_member_id: 'member_id',
-            col_idx.ordinate_item_member_hierarchy_id: 'member_hierarchy_id',
-            col_idx.ordinate_item_member_hierarchy_valid_from: 'member_hierarchy_valid_from',
-            col_idx.ordinate_item_starting_member_id: 'starting_member_id',
-            col_idx.ordinate_item_is_starting_member_included: 'is_starting_member_included'
+        self.column_mappings["pybirdai_ordinate_item"] = {
+            col_idx.ordinate_item_axis_ordinate_id: "axis_ordinate_id",
+            col_idx.ordinate_item_variable_id: "variable_id",
+            col_idx.ordinate_item_member_id: "member_id",
+            col_idx.ordinate_item_member_hierarchy_id: "member_hierarchy_id",
+            col_idx.ordinate_item_member_hierarchy_valid_from: "member_hierarchy_valid_from",
+            col_idx.ordinate_item_starting_member_id: "starting_member_id",
+            col_idx.ordinate_item_is_starting_member_included: "is_starting_member_included",
         }
 
         # Table Cell mappings
-        self.column_mappings['pybirdai_table_cell'] = {
-            col_idx.table_cell_cell_id: 'cell_id',
-            col_idx.table_cell_is_shaded: 'is_shaded',
-            col_idx.table_cell_combination_id: 'table_cell_combination_id',
-            col_idx.table_cell_table_id: 'table_id',
-            col_idx.table_cell_system_data_code: 'system_data_code',
-            col_idx.table_cell_name: 'name'
+        self.column_mappings["pybirdai_table_cell"] = {
+            col_idx.table_cell_cell_id: "cell_id",
+            col_idx.table_cell_is_shaded: "is_shaded",
+            col_idx.table_cell_combination_id: "table_cell_combination_id",
+            col_idx.table_cell_table_id: "table_id",
+            col_idx.table_cell_system_data_code: "system_data_code",
+            col_idx.table_cell_name: "name",
         }
 
         # Cell Position mappings
-        self.column_mappings['pybirdai_cell_position'] = {
-            col_idx.cell_positions_cell_id: 'cell_id',
-            col_idx.cell_positions_axis_ordinate_id: 'axis_ordinate_id'
+        self.column_mappings["pybirdai_cell_position"] = {
+            col_idx.cell_positions_cell_id: "cell_id",
+            col_idx.cell_positions_axis_ordinate_id: "axis_ordinate_id",
         }
 
         # Member Mapping mappings
-        self.column_mappings['pybirdai_member_mapping'] = {
-            col_idx.member_mapping_maintenance_agency_id: 'maintenance_agency_id',
-            col_idx.member_mapping_member_mapping_id: 'member_mapping_id',
-            col_idx.member_mapping_name: 'name',
-            col_idx.member_mapping_code: 'code'
+        self.column_mappings["pybirdai_member_mapping"] = {
+            col_idx.member_mapping_maintenance_agency_id: "maintenance_agency_id",
+            col_idx.member_mapping_member_mapping_id: "member_mapping_id",
+            col_idx.member_mapping_name: "name",
+            col_idx.member_mapping_code: "code",
         }
 
         # Member Mapping Item mappings
-        self.column_mappings['pybirdai_member_mapping_item'] = {
-            col_idx.member_mapping_item_member_mapping_id: 'member_mapping_id',
-            col_idx.member_mapping_row: 'member_mapping_row',
-            col_idx.member_mapping_variable_id: 'variable_id',
-            col_idx.member_mapping_is_source: 'is_source',
-            col_idx.member_mapping_member_id: 'member_id',
-            col_idx.member_mapping_item_valid_from: 'valid_from',
-            col_idx.member_mapping_item_valid_to: 'valid_to',
-            col_idx.member_mapping_item_member_hierarchy: 'member_hierarchy'
+        self.column_mappings["pybirdai_member_mapping_item"] = {
+            col_idx.member_mapping_item_member_mapping_id: "member_mapping_id",
+            col_idx.member_mapping_row: "member_mapping_row",
+            col_idx.member_mapping_variable_id: "variable_id",
+            col_idx.member_mapping_is_source: "is_source",
+            col_idx.member_mapping_member_id: "member_id",
+            col_idx.member_mapping_item_valid_from: "valid_from",
+            col_idx.member_mapping_item_valid_to: "valid_to",
+            col_idx.member_mapping_item_member_hierarchy: "member_hierarchy",
         }
 
         # Combination mappings
-        self.column_mappings['pybirdai_combination'] = {
-            col_idx.combination_combination_id: 'combination_id',
-            col_idx.combination_combination_code: 'code',
-            col_idx.combination_combination_name: 'name',
-            col_idx.combination_maintenance_agency: 'maintenance_agency_id',
-            col_idx.combination_version: 'version',
-            col_idx.combination_valid_from: 'valid_from',
-            col_idx.combination_combination_valid_to: 'valid_to',
-            col_idx.combination_metric: 'metric'
+        self.column_mappings["pybirdai_combination"] = {
+            col_idx.combination_combination_id: "combination_id",
+            col_idx.combination_combination_code: "code",
+            col_idx.combination_combination_name: "name",
+            col_idx.combination_maintenance_agency: "maintenance_agency_id",
+            col_idx.combination_version: "version",
+            col_idx.combination_valid_from: "valid_from",
+            col_idx.combination_combination_valid_to: "valid_to",
+            col_idx.combination_metric: "metric",
         }
 
         # Combination Item mappings
-        self.column_mappings['pybirdai_combination_item'] = {
-            col_idx.combination_item_combination_id: 'combination_id',
-            col_idx.combination_item_variable_id: 'variable_id',
-            col_idx.combination_item_subdomain_id: 'subdomain_id',
-            col_idx.combination_variable_set: 'variable_set_id',
-            col_idx.combination_member_id: 'member_id',
-            col_idx.combination_item_member_hierarchy: 'member_hierarchy'
+        self.column_mappings["pybirdai_combination_item"] = {
+            col_idx.combination_item_combination_id: "combination_id",
+            col_idx.combination_item_variable_id: "variable_id",
+            col_idx.combination_item_subdomain_id: "subdomain_id",
+            col_idx.combination_variable_set: "variable_set_id",
+            col_idx.combination_member_id: "member_id",
+            col_idx.combination_item_member_hierarchy: "member_hierarchy",
         }
 
         # Mapping Definition mappings
-        self.column_mappings['pybirdai_mapping_definition'] = {
-            col_idx.mapping_definition_maintenance_agency_id: 'maintenance_agency_id',
-            col_idx.mapping_definition_mapping_id: 'mapping_id',
-            col_idx.mapping_definition_name: 'name',
-            col_idx.mapping_definition_mapping_type: 'mapping_type',
-            col_idx.mapping_definition_code: 'code',
-            col_idx.mapping_definition_algorithm: 'algorithm',
-            col_idx.mapping_definition_member_mapping_id: 'member_mapping_id',
-            col_idx.mapping_definition_variable_mapping_id: 'variable_mapping_id'
+        self.column_mappings["pybirdai_mapping_definition"] = {
+            col_idx.mapping_definition_maintenance_agency_id: "maintenance_agency_id",
+            col_idx.mapping_definition_mapping_id: "mapping_id",
+            col_idx.mapping_definition_name: "name",
+            col_idx.mapping_definition_mapping_type: "mapping_type",
+            col_idx.mapping_definition_code: "code",
+            col_idx.mapping_definition_algorithm: "algorithm",
+            col_idx.mapping_definition_member_mapping_id: "member_mapping_id",
+            col_idx.mapping_definition_variable_mapping_id: "variable_mapping_id",
         }
 
         # Mapping To Cube mappings
-        self.column_mappings['pybirdai_mapping_to_cube'] = {
-            col_idx.mapping_to_cube_cube_mapping_id: 'cube_mapping_id',
-            col_idx.mapping_to_cube_mapping_id: 'mapping_id',
-            col_idx.mapping_to_cube_valid_from: 'valid_from',
-            col_idx.mapping_to_cube_valid_to: 'valid_to'
+        self.column_mappings["pybirdai_mapping_to_cube"] = {
+            col_idx.mapping_to_cube_cube_mapping_id: "cube_mapping_id",
+            col_idx.mapping_to_cube_mapping_id: "mapping_id",
+            col_idx.mapping_to_cube_valid_from: "valid_from",
+            col_idx.mapping_to_cube_valid_to: "valid_to",
         }
 
         # Variable Mapping mappings
-        self.column_mappings['pybirdai_variable_mapping'] = {
-            col_idx.variable_mapping_variable_mapping_id: 'variable_mapping_id',
-            col_idx.variable_mapping_maintenance_agency_id: 'maintenance_agency_id',
-            col_idx.variable_mapping_code: 'code',
-            col_idx.variable_mapping_name: 'name'
+        self.column_mappings["pybirdai_variable_mapping"] = {
+            col_idx.variable_mapping_variable_mapping_id: "variable_mapping_id",
+            col_idx.variable_mapping_maintenance_agency_id: "maintenance_agency_id",
+            col_idx.variable_mapping_code: "code",
+            col_idx.variable_mapping_name: "name",
         }
 
         # Variable Mapping Item mappings
-        self.column_mappings['pybirdai_variable_mapping_item'] = {
-            col_idx.variable_mapping_item_variable_mapping_id: 'variable_mapping_id',
-            col_idx.variable_mapping_item_variable_id: 'variable_id',
-            col_idx.variable_mapping_item_is_source: 'is_source',
-            col_idx.variable_mapping_item_valid_from: 'valid_from',
-            col_idx.variable_mapping_item_valid_to: 'valid_to'
+        self.column_mappings["pybirdai_variable_mapping_item"] = {
+            col_idx.variable_mapping_item_variable_mapping_id: "variable_mapping_id",
+            col_idx.variable_mapping_item_variable_id: "variable_id",
+            col_idx.variable_mapping_item_is_source: "is_source",
+            col_idx.variable_mapping_item_valid_from: "valid_from",
+            col_idx.variable_mapping_item_valid_to: "valid_to",
         }
 
         # Cube Structure mappings
-        self.column_mappings['pybirdai_cube_structure'] = {
-            col_idx.cube_structure_maintenance_agency: 'maintenance_agency_id',
-            col_idx.cube_structure_id_index: 'cube_structure_id',
-            col_idx.cube_structure_name_index: 'name',
-            col_idx.cube_structure_code_index: 'code',
-            col_idx.cube_structure_description_index: 'description',
-            col_idx.cube_structure_valid_from: 'valid_from',
-            col_idx.cube_structure_valid_to_index: 'valid_to',
-            col_idx.cube_structure_version: 'version'
+        self.column_mappings["pybirdai_cube_structure"] = {
+            col_idx.cube_structure_maintenance_agency: "maintenance_agency_id",
+            col_idx.cube_structure_id_index: "cube_structure_id",
+            col_idx.cube_structure_name_index: "name",
+            col_idx.cube_structure_code_index: "code",
+            col_idx.cube_structure_description_index: "description",
+            col_idx.cube_structure_valid_from: "valid_from",
+            col_idx.cube_structure_valid_to_index: "valid_to",
+            col_idx.cube_structure_version: "version",
         }
 
         # Cube Structure Item mappings
-        self.column_mappings['pybirdai_cube_structure_item'] = {
-            col_idx.cube_structure_item_cube_structure_id: 'cube_structure_id',
-            col_idx.cube_structure_item_variable_index: 'cube_variable_code',
-            col_idx.cube_structure_item_variable_id: 'variable_id',
-            col_idx.cube_structure_item_role_index: 'role',
-            col_idx.cube_structure_item_order: 'order',
-            col_idx.cube_structure_item_subdomain_index: 'subdomain_id',
-            col_idx.cube_structure_item_variable_set: 'variable_set_id',
-            col_idx.cube_structure_item_specific_member: 'member_id',
-            col_idx.cube_structure_item_dimension_type: 'dimension_type',
-            col_idx.cube_structure_item_attribute_associated_variable: 'attribute_associated_variable',
-            col_idx.cube_structure_item_is_flow: 'is_flow',
-            col_idx.cube_structure_item_is_mandatory: 'is_mandatory',
-            col_idx.cube_structure_item_description: 'description',
-            col_idx.cube_structure_item_is_implemented: 'is_implemented',
-            col_idx.cube_structure_item_is_identifier: 'is_identifier'
+        self.column_mappings["pybirdai_cube_structure_item"] = {
+            col_idx.cube_structure_item_cube_structure_id: "cube_structure_id",
+            col_idx.cube_structure_item_variable_index: "cube_variable_code",
+            col_idx.cube_structure_item_variable_id: "variable_id",
+            col_idx.cube_structure_item_role_index: "role",
+            col_idx.cube_structure_item_order: "order",
+            col_idx.cube_structure_item_subdomain_index: "subdomain_id",
+            col_idx.cube_structure_item_variable_set: "variable_set_id",
+            col_idx.cube_structure_item_specific_member: "member_id",
+            col_idx.cube_structure_item_dimension_type: "dimension_type",
+            col_idx.cube_structure_item_attribute_associated_variable: "attribute_associated_variable",
+            col_idx.cube_structure_item_is_flow: "is_flow",
+            col_idx.cube_structure_item_is_mandatory: "is_mandatory",
+            col_idx.cube_structure_item_description: "description",
+            col_idx.cube_structure_item_is_implemented: "is_implemented",
+            col_idx.cube_structure_item_is_identifier: "is_identifier",
         }
 
         # Cube mappings
-        self.column_mappings['pybirdai_cube'] = {
-            col_idx.cube_maintenance_agency_id: 'maintenance_agency_id',
-            col_idx.cube_object_id_index: 'cube_id',
-            col_idx.cube_class_name_index: 'name',
-            col_idx.cube_class_code_index: 'code',
-            col_idx.cube_framework_index: 'framework_id',
-            col_idx.cube_cube_structure_id_index: 'cube_structure_id',
-            col_idx.cube_cube_type_index: 'cube_type',
-            col_idx.cube_is_allowed: 'is_allowed',
-            col_idx.cube_valid_from: 'valid_from',
-            col_idx.cube_valid_to_index: 'valid_to',
-            col_idx.cube_version: 'version',
-            col_idx.cube_description: 'description',
-            col_idx.cube_published: 'published',
-            col_idx.cube_dataset_url: 'dataset_url',
-            col_idx.cube_filters: 'filters',
-            col_idx.cube_di_export: 'di_export'
+        self.column_mappings["pybirdai_cube"] = {
+            col_idx.cube_maintenance_agency_id: "maintenance_agency_id",
+            col_idx.cube_object_id_index: "cube_id",
+            col_idx.cube_class_name_index: "name",
+            col_idx.cube_class_code_index: "code",
+            col_idx.cube_framework_index: "framework_id",
+            col_idx.cube_cube_structure_id_index: "cube_structure_id",
+            col_idx.cube_cube_type_index: "cube_type",
+            col_idx.cube_is_allowed: "is_allowed",
+            col_idx.cube_valid_from: "valid_from",
+            col_idx.cube_valid_to_index: "valid_to",
+            col_idx.cube_version: "version",
+            col_idx.cube_description: "description",
+            col_idx.cube_published: "published",
+            col_idx.cube_dataset_url: "dataset_url",
+            col_idx.cube_filters: "filters",
+            col_idx.cube_di_export: "di_export",
         }
 
         # Cube to Combination mappings
-        self.column_mappings['pybirdai_cube_to_combination'] = {
-            col_idx.cube_to_combination_cube_id: 'cube_id',
-            col_idx.cube_to_combination_combination_id: 'combination_id'
+        self.column_mappings["pybirdai_cube_to_combination"] = {
+            col_idx.cube_to_combination_cube_id: "cube_id",
+            col_idx.cube_to_combination_combination_id: "combination_id",
         }
 
         # Cube Link mappings
-        self.column_mappings['pybirdai_cube_link'] = {
-            col_idx.cube_link_maintenance_agency_id: 'maintenance_agency_id',
-            col_idx.cube_link_id: 'cube_link_id',
-            col_idx.cube_link_code: 'code',
-            col_idx.cube_link_name: 'name',
-            col_idx.cube_link_description: 'description',
-            col_idx.cube_link_valid_from: 'valid_from',
-            col_idx.cube_link_valid_to: 'valid_to',
-            col_idx.cube_link_version: 'version',
-            col_idx.cube_link_order_relevance: 'order_relevance',
-            col_idx.cube_link_primary_cube_id: 'primary_cube_id',
-            col_idx.cube_link_foreign_cube_id: 'foreign_cube_id',
-            col_idx.cube_link_type: 'link_type',
-            col_idx.cube_link_join_identifier: 'join_identifier'
+        self.column_mappings["pybirdai_cube_link"] = {
+            col_idx.cube_link_maintenance_agency_id: "maintenance_agency_id",
+            col_idx.cube_link_id: "cube_link_id",
+            col_idx.cube_link_code: "code",
+            col_idx.cube_link_name: "name",
+            col_idx.cube_link_description: "description",
+            col_idx.cube_link_valid_from: "valid_from",
+            col_idx.cube_link_valid_to: "valid_to",
+            col_idx.cube_link_version: "version",
+            col_idx.cube_link_order_relevance: "order_relevance",
+            col_idx.cube_link_primary_cube_id: "primary_cube_id",
+            col_idx.cube_link_foreign_cube_id: "foreign_cube_id",
+            col_idx.cube_link_type: "link_type",
+            col_idx.cube_link_join_identifier: "join_identifier",
         }
 
         # Cube Structure Item Link mappings
-        self.column_mappings['pybirdai_cube_structure_item_link'] = {
-            col_idx.cube_structure_item_link_id: 'cube_structure_item_link_id',
-            col_idx.cube_structure_item_link_cube_link_id: 'cube_link_id',
-            col_idx.cube_structure_item_link_foreign_cube_variable_code: 'foreign_cube_variable_code',
-            col_idx.cube_structure_item_link_primary_cube_variable_code: 'primary_cube_variable_code'
+        self.column_mappings["pybirdai_cube_structure_item_link"] = {
+            col_idx.cube_structure_item_link_id: "cube_structure_item_link_id",
+            col_idx.cube_structure_item_link_cube_link_id: "cube_link_id",
+            col_idx.cube_structure_item_link_foreign_cube_variable_code: "foreign_cube_variable_code",
+            col_idx.cube_structure_item_link_primary_cube_variable_code: "primary_cube_variable_code",
         }
 
         # Member Link mappings
-        self.column_mappings['pybirdai_member_link'] = {
-            col_idx.member_link_cube_structure_item_link_id: 'cube_structure_item_link_id',
-            col_idx.member_link_primary_member_id: 'primary_member_id',
-            col_idx.member_link_foreign_member_id: 'foreign_member_id',
-            col_idx.member_link_is_linked: 'is_linked',
-            col_idx.member_link_valid_from: 'valid_from',
-            col_idx.member_link_valid_to: 'valid_to'
+        self.column_mappings["pybirdai_member_link"] = {
+            col_idx.member_link_cube_structure_item_link_id: "cube_structure_item_link_id",
+            col_idx.member_link_primary_member_id: "primary_member_id",
+            col_idx.member_link_foreign_member_id: "foreign_member_id",
+            col_idx.member_link_is_linked: "is_linked",
+            col_idx.member_link_valid_from: "valid_from",
+            col_idx.member_link_valid_to: "valid_to",
         }
 
         # Facet Collection mappings
-        self.column_mappings['pybirdai_facet_collection'] = {
-            col_idx.facet_collection_code: 'code',
-            col_idx.facet_collection_facet_id: 'facet_id',
-            col_idx.facet_collection_facet_value_type: 'facet_value_type',
-            col_idx.facet_collection_maintenance_agency_id: 'maintenance_agency_id',
-            col_idx.facet_collection_name: 'name'
+        self.column_mappings["pybirdai_facet_collection"] = {
+            col_idx.facet_collection_code: "code",
+            col_idx.facet_collection_facet_id: "facet_id",
+            col_idx.facet_collection_facet_value_type: "facet_value_type",
+            col_idx.facet_collection_maintenance_agency_id: "maintenance_agency_id",
+            col_idx.facet_collection_name: "name",
         }
 
         logger.info(f"Built column mappings for {len(self.column_mappings)} model types")
 
     def _get_import_order(self):
         """Define the order in which tables should be imported to respect foreign key dependencies"""
         return [
-            'pybirdai_maintenance_agency',  # No dependencies
-            'pybirdai_facet_collection',    # No dependencies
-            'pybirdai_domain',              # Depends on maintenance_agency
-            'pybirdai_framework',           # Depends on maintenance_agency
-            'pybirdai_variable',            # Depends on maintenance_agency, domain
-            'pybirdai_member',              # Depends on maintenance_agency, domain
-            'pybirdai_subdomain',           # Depends on domain
-            'pybirdai_subdomain_enumeration', # Depends on member, subdomain
-            'pybirdai_variable_set',        # Depends on maintenance_agency
-            'pybirdai_variable_set_enumeration', # Depends on variable_set, variable, subdomain
-            'pybirdai_member_hierarchy',    # Depends on maintenance_agency, domain
-            'pybirdai_member_hierarchy_node', # Depends on member_hierarchy, member
-            'pybirdai_cube_structure',      # Depends on maintenance_agency
-            'pybirdai_cube_structure_item', # Depends on cube_structure, variable, subdomain, variable_set, member
-            'pybirdai_cube',                # Depends on maintenance_agency, framework, cube_structure
-            'pybirdai_combination',         # Depends on maintenance_agency
-            'pybirdai_combination_item',    # Depends on combination, variable, subdomain, variable_set, member
-            'pybirdai_cube_to_combination', # Depends on cube, combination
-            'pybirdai_cube_link',           # Depends on maintenance_agency, cube
-            'pybirdai_cube_structure_item_link', # Depends on cube_link, cube_structure_item
-            'pybirdai_table',               # Depends on maintenance_agency
-            'pybirdai_axis',                # Depends on table
-            'pybirdai_axis_ordinate',       # Depends on axis
-            'pybirdai_ordinate_item',       # Depends on axis_ordinate, variable, member, member_hierarchy
-            'pybirdai_table_cell',          # Depends on table
-            'pybirdai_cell_position',       # Depends on table_cell, axis_ordinate
-            'pybirdai_member_mapping',      # Depends on maintenance_agency
-            'pybirdai_member_mapping_item', # Depends on member_mapping, variable, member
-            'pybirdai_variable_mapping',    # Depends on maintenance_agency
-            'pybirdai_variable_mapping_item', # Depends on variable_mapping, variable
-            'pybirdai_mapping_definition',  # Depends on maintenance_agency, member_mapping, variable_mapping
-            'pybirdai_mapping_to_cube',     # Depends on mapping_definition
+            "pybirdai_maintenance_agency",  # No dependencies
+            "pybirdai_facet_collection",  # No dependencies
+            "pybirdai_domain",  # Depends on maintenance_agency
+            "pybirdai_framework",  # Depends on maintenance_agency
+            "pybirdai_variable",  # Depends on maintenance_agency, domain
+            "pybirdai_member",  # Depends on maintenance_agency, domain
+            "pybirdai_subdomain",  # Depends on domain
+            "pybirdai_subdomain_enumeration",  # Depends on member, subdomain
+            "pybirdai_variable_set",  # Depends on maintenance_agency
+            "pybirdai_variable_set_enumeration",  # Depends on variable_set, variable, subdomain
+            "pybirdai_member_hierarchy",  # Depends on maintenance_agency, domain
+            "pybirdai_member_hierarchy_node",  # Depends on member_hierarchy, member
+            "pybirdai_cube_structure",  # Depends on maintenance_agency
+            "pybirdai_cube_structure_item",  # Depends on cube_structure, variable, subdomain, variable_set, member
+            "pybirdai_cube",  # Depends on maintenance_agency, framework, cube_structure
+            "pybirdai_combination",  # Depends on maintenance_agency
+            "pybirdai_combination_item",  # Depends on combination, variable, subdomain, variable_set, member
+            "pybirdai_cube_to_combination",  # Depends on cube, combination
+            "pybirdai_cube_link",  # Depends on maintenance_agency, cube
+            "pybirdai_cube_structure_item_link",  # Depends on cube_link, cube_structure_item
+            "pybirdai_table",  # Depends on maintenance_agency
+            "pybirdai_axis",  # Depends on table
+            "pybirdai_axis_ordinate",  # Depends on axis
+            "pybirdai_ordinate_item",  # Depends on axis_ordinate, variable, member, member_hierarchy
+            "pybirdai_table_cell",  # Depends on table
+            "pybirdai_cell_position",  # Depends on table_cell, axis_ordinate
+            "pybirdai_member_mapping",  # Depends on maintenance_agency
+            "pybirdai_member_mapping_item",  # Depends on member_mapping, variable, member
+            "pybirdai_variable_mapping",  # Depends on maintenance_agency
+            "pybirdai_variable_mapping_item",  # Depends on variable_mapping, variable
+            "pybirdai_mapping_definition",  # Depends on maintenance_agency, member_mapping, variable_mapping
+            "pybirdai_mapping_to_cube",  # Depends on mapping_definition
         ]
 
     def _get_table_name_from_csv_filename(self, filename):
         """Convert CSV filename back to table name"""
-        base_name = filename.replace('.csv', '')
-        if base_name.startswith('bird_'):
+        base_name = filename.replace(".csv", "")
+        if base_name.startswith("bird_"):
             table_name = f"pybirdai_{base_name.replace('bird_', '')}"
-        elif base_name.startswith('auth_') or base_name.startswith('django_'):
+        elif base_name.startswith("auth_") or base_name.startswith("django_"):
             table_name = base_name
         else:
             table_name = f"pybirdai_{base_name}"
 
         logger.debug(f"Converted filename '{filename}' to table name '{table_name}'")
@@ -568,39 +573,40 @@
         return headers, rows
 
     def _convert_value(self, field, value, defer_foreign_keys=False):
         """Convert CSV string value to appropriate Python type for the field"""
 
-
-        if not value or value == '' or value == 'None':
+        if not value or value == "" or value == "None":
             return None
 
         if isinstance(field, models.CharField):
             return value
         if isinstance(field, models.IntegerField):
             return int(float(value))  # Handle cases where int comes as float string
         elif isinstance(field, models.FloatField):
             return float(value)
         elif isinstance(field, models.DecimalField):
             from decimal import Decimal
+
             return Decimal(str(value))
         elif isinstance(field, models.BooleanField):
-            return value.lower() in ('true', '1', 'yes', 't')
+            return value.lower() in ("true", "1", "yes", "t")
         elif isinstance(field, models.DateField) or isinstance(field, models.DateTimeField):
             if value and value.strip():
                 # Try various date formats
                 from django.utils.dateparse import parse_date, parse_datetime
+
                 parsed_date = parse_date(value) or parse_datetime(value)
                 return parsed_date
             return None
         elif isinstance(field, models.ForeignKey):
             if defer_foreign_keys:
                 # Return the raw ID value for later processing
-                return value if value and str(value).strip() not in ('', 'None', 'NULL') else None
+                return value if value and str(value).strip() not in ("", "None", "NULL") else None
             else:
                 # For foreign keys, we need to return the related object, not just the ID
-                if value and str(value).strip() not in ('', 'None', 'NULL'):
+                if value and str(value).strip() not in ("", "None", "NULL"):
                     try:
                         fk_id = value
                         # Get the related model
                         related_model = field.related_model
                         # Try to get the object by its primary key
@@ -613,31 +619,33 @@
             return str(value) if value else None
 
     def _get_model_fields(self, model_class):
         """Get model fields as a dictionary"""
         return {field.name: field for field in model_class._meta.fields}
-    
+
     def _calculate_optimal_batch_size(self, model_class, base_batch_size=250):
         """Calculate optimal batch size based on model field count and database constraints"""
         field_count = len(model_class._meta.fields)
-        
+
         # SQLite has a limit of 999 variables per statement
         # Leave some margin for safety
-        max_variables = 900 if connection.vendor == 'sqlite' else 10000
-        
+        max_variables = 900 if connection.vendor == "sqlite" else 10000
+
         # Calculate max records per batch
         max_records_per_batch = max_variables // max(field_count, 1)
-        
+
         # Use the smaller of base batch size or calculated maximum
         optimal_batch_size = min(base_batch_size, max_records_per_batch)
-        
+
         # Ensure minimum batch size of 10
         optimal_batch_size = max(10, optimal_batch_size)
-        
-        logger.debug(f"Model {model_class.__name__}: {field_count} fields, "
-                    f"optimal batch size: {optimal_batch_size} (max variables: {max_variables})")
-        
+
+        logger.debug(
+            f"Model {model_class.__name__}: {field_count} fields, "
+            f"optimal batch size: {optimal_batch_size} (max variables: {max_variables})"
+        )
+
         return optimal_batch_size
 
     def _bulk_sqlite_import_with_index(self, csv_content, model_class, table_name):
         """
         High-performance bulk import for large tables using SQLite3 directly.
@@ -648,34 +656,36 @@
         if not self._is_safe_table_name(table_name) or table_name not in self.model_map:
             logger.error(f"Unsafe or unknown table name detected: {table_name}")
             raise Exception(f"Unsafe or unknown table name detected: {table_name}")
         # Parse CSV content
         headers, rows = self._parse_csv_content(csv_content)
-        
+
         if not rows:
             logger.warning(f"No data rows found in CSV for {table_name}")
             return []
-        
+
         # Check if model uses auto-generated primary key
         pk_fields = [field for field in model_class._meta.fields if field.primary_key]
-        has_auto_pk = len(pk_fields) == 1 and pk_fields[0].name == 'id'
-        
+        has_auto_pk = len(pk_fields) == 1 and pk_fields[0].name == "id"
+
         if not has_auto_pk:
-            raise ValueError(f"Bulk import with auto-generated index only supports models with auto-generated 'id' primary key. {model_class.__name__} doesn't qualify.")
-        
+            raise ValueError(
+                f"Bulk import with auto-generated index only supports models with auto-generated 'id' primary key. {model_class.__name__} doesn't qualify."
+            )
+
         # Get database file path
-        db_file = Path(connection.settings_dict['NAME']).absolute()
+        db_file = Path(connection.settings_dict["NAME"]).absolute()
         if not db_file.exists():
             raise FileNotFoundError(f"Database file not found: {db_file}")
-        
+
         # Create temporary CSV file with auto-generated indices
-        with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False, encoding='utf-8') as temp_file:
+        with tempfile.NamedTemporaryFile(mode="w", suffix=".csv", delete=False, encoding="utf-8") as temp_file:
             csv_writer = csv.writer(temp_file)
-            
+
             # Write headers with 'id' column first
             # For foreign key fields, we need to use the Django field names (with _id suffix)
-            django_headers = ['id']
+            django_headers = ["id"]
             if table_name in self.column_mappings:
                 column_mapping = self.column_mappings[table_name]
                 for i, header in enumerate(headers):
                     if i in column_mapping:
                         field_name = column_mapping[i]
@@ -687,86 +697,78 @@
                             django_headers.append(field_name)
                     else:
                         django_headers.append(header)
             else:
                 django_headers.extend(headers)
-            
+
             csv_writer.writerow(django_headers)
-            
+
             # Write rows with auto-generated sequential IDs
             id_generator = itertools.count(1)  # Start from 1
             for row in rows:
                 # Generate sequential ID
                 row_id = next(id_generator)
                 modified_row = [row_id] + list(row)
                 csv_writer.writerow(modified_row)
-            
+
             temp_csv_path = temp_file.name
-        
+
         try:
             # Clear existing data first
             # Validate table name to prevent SQL injection
             if not self._is_safe_table_name(table_name):
                 raise ValueError(f"Unsafe table name detected: {table_name}")
-                
+
             with connection.cursor() as cursor:
-                if connection.vendor == 'sqlite':
+                if connection.vendor == "sqlite":
                     cursor.execute("PRAGMA foreign_keys = 0;")
-                
+
                 # Table name validated above
                 cursor.execute(f"DELETE FROM {table_name};")
-                
-                if connection.vendor == 'sqlite':
+
+                if connection.vendor == "sqlite":
                     # Table name validated above
                     cursor.execute(f"DELETE FROM sqlite_sequence WHERE name='{table_name}';")
                     cursor.execute("PRAGMA foreign_keys = 1;")
-            
+
             logger.info(f"Cleared existing data from {table_name}")
-            
+
             # Prepare SQLite import commands
-            sqlite_commands = [
-                ".mode csv",
-                f".separator ','",
-                f".import --skip 1 '{temp_csv_path}' {table_name}"
-            ]
-            
-            sqlite_script = '\n'.join(sqlite_commands)
-            
+            sqlite_commands = [".mode csv", f".separator ','", f".import --skip 1 '{temp_csv_path}' {table_name}"]
+
+            sqlite_script = "\n".join(sqlite_commands)
+
             # Execute SQLite import
             logger.info(f"Executing bulk SQLite import for {table_name}")
             result = subprocess.run(
-                ['sqlite3', str(db_file)],
-                input=sqlite_script,
-                text=True,
-                capture_output=True,
-                check=False
+                ["sqlite3", str(db_file)], input=sqlite_script, text=True, capture_output=True, check=False
             )
-            
+
             if result.returncode != 0:
                 error_msg = f"SQLite bulk import failed: {result.stderr}"
                 logger.error(error_msg)
                 raise Exception(error_msg)
-            
+
             # Verify import success
             with connection.cursor() as cursor:
                 # Table name already validated above, but double-check for safety
                 if not self._is_safe_table_name(table_name) or table_name not in self.model_map:
                     logger.error(f"Unsafe or unknown table name detected (count): {table_name}")
                     raise Exception(f"Unsafe or unknown table name detected (count): {table_name}")
                 # Table name validated above
                 cursor.execute(f"SELECT COUNT(*) FROM {table_name};")
                 imported_count = cursor.fetchone()[0]
-            
+
             logger.info(f"Bulk SQLite import completed: {imported_count} records imported to {table_name}")
-            
+
             # Note: FK resolution is no longer needed since we now store values in correct _id columns during bulk import
             logger.info(f"Foreign keys stored directly in correct columns during bulk import")
-            
-            # Return mock objects list (limited for memory efficiency)  
-            imported_objects = list(model_class.objects.all()[:min(100, imported_count)])
+
+            # Return mock objects list (limited for memory efficiency)
+            imported_objects = list(model_class.objects.all()[: min(100, imported_count)])
             return imported_objects
-            
+
         except Exception as e:
             logger.error(f"Bulk SQLite import failed for {table_name}: {e}")
             raise
         finally:
             # Clean up temporary file
@@ -779,163 +781,169 @@
         """
         Determine if a table should use bulk SQLite import based on volume and table characteristics.
         """
         # Known high-volume tables that benefit from bulk import
         high_volume_tables = {
-            'pybirdai_cell_position',
-            'pybirdai_table_cell', 
-            'pybirdai_axis_ordinate',
-            'pybirdai_ordinate_item'
-        }
-        
+            "pybirdai_cell_position",
+            "pybirdai_table_cell",
+            "pybirdai_axis_ordinate",
+            "pybirdai_ordinate_item",
+        }
+
         # Use bulk import for known high-volume tables or tables with >50,000 rows
         return table_name in high_volume_tables or row_count > 50000
 
     def _resolve_foreign_keys_post_bulk_import(self, model_class, table_name, csv_headers):
         """
         Resolve foreign key relationships after bulk SQLite import.
         The bulk import stores string values, we need to convert them to proper FK references.
         """
         logger.info(f"Resolving foreign keys for {table_name}")
-        
+
         model_fields = self._get_model_fields(model_class)
         fk_fields = {name: field for name, field in model_fields.items() if isinstance(field, models.ForeignKey)}
-        
+
         if not fk_fields:
             logger.info(f"No foreign keys to resolve for {table_name}")
             return
-        
+
         # Get column mappings for this table
         if table_name not in self.column_mappings:
             logger.warning(f"No column mappings found for {table_name}, skipping FK resolution")
             return
-            
+
         column_mapping = self.column_mappings[table_name]
-        
+
         # Build mapping of CSV column index to FK field name
         csv_to_fk_mapping = {}
         for col_idx, field_name in column_mapping.items():
             if field_name in fk_fields and col_idx < len(csv_headers):
                 csv_to_fk_mapping[col_idx] = field_name
-        
+
         if not csv_to_fk_mapping:
             logger.info(f"No FK mappings found for {table_name}")
             return
-        
+
         logger.info(f"Resolving {len(csv_to_fk_mapping)} foreign key fields: {list(csv_to_fk_mapping.values())}")
-        
+
         # Process FK resolution in batches to avoid memory issues
         batch_size = 1000
-        
+
         # Validate table name to prevent SQL injection
         if not self._is_safe_table_name(table_name):
             raise ValueError(f"Unsafe table name detected in FK resolution: {table_name}")
-            
+
         with connection.cursor() as cursor:
             # Table name validated above
             cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
             total_records = cursor.fetchone()[0]
-            
+
             for offset in range(0, total_records, batch_size):
                 # Get batch of records with string FK values
                 # Table name validated above, column names come from model field definitions
-                cursor.execute(f"SELECT id, {', '.join(csv_to_fk_mapping.values())} FROM {table_name} LIMIT {batch_size} OFFSET {offset}")
+                cursor.execute(
+                    f"SELECT id, {', '.join(csv_to_fk_mapping.values())} FROM {table_name} LIMIT {batch_size} OFFSET {offset}"
+                )
                 records = cursor.fetchall()
-                
+
                 if not records:
                     break
-                
+
                 # Process each record in the batch
                 updates = []
                 for record in records:
                     record_id = record[0]
                     fk_updates = {}
-                    
+
                     # Process each FK field
                     for i, (col_idx, field_name) in enumerate(csv_to_fk_mapping.items(), 1):
                         fk_string_value = record[i]
-                        
+
                         if fk_string_value and fk_string_value.strip():
                             # Get related model and try to find the object
                             fk_field = fk_fields[field_name]
                             related_model = fk_field.related_model
-                            
+
                             try:
                                 # Try to find the related object by primary key
                                 related_obj = related_model.objects.get(pk=fk_string_value.strip())
                                 fk_updates[f"{field_name}_id"] = related_obj.pk
                             except related_model.DoesNotExist:
-                                logger.warning(f"Foreign key object not found: {related_model.__name__} with pk '{fk_string_value}'")
+                                logger.warning(
+                                    f"Foreign key object not found: {related_model.__name__} with pk '{fk_string_value}'"
+                                )
                                 # Create a minimal object if it doesn't exist
                                 try:
                                     related_obj = related_model.objects.create(pk=fk_string_value.strip())
                                     fk_updates[f"{field_name}_id"] = related_obj.pk
-                                    logger.info(f"Created missing {related_model.__name__} object with pk '{fk_string_value}'")
+                                    logger.info(
+                                        f"Created missing {related_model.__name__} object with pk '{fk_string_value}'"
+                                    )
                                 except Exception as create_error:
                                     logger.error(f"Failed to create missing FK object: {create_error}")
                             except Exception as lookup_error:
                                 logger.error(f"Error looking up FK object: {lookup_error}")
-                    
+
                     if fk_updates:
                         updates.append((record_id, fk_updates))
-                
+
                 # Execute batch updates
                 for record_id, fk_updates in updates:
                     if fk_updates:
                         # Field names come from model field definitions, so they're safe
-                        set_clause = ', '.join([f"{field} = ?" for field in fk_updates.keys()])
+                        set_clause = ", ".join([f"{field} = ?" for field in fk_updates.keys()])
                         values = list(fk_updates.values()) + [record_id]
                         # Table name validated above
                         cursor.execute(f"UPDATE {table_name} SET {set_clause} WHERE id = ?", values)
-                
+
                 logger.debug(f"Processed FK resolution for batch {offset}-{offset + len(records)} of {total_records}")
-        
+
         logger.info(f"Completed foreign key resolution for {table_name}")
 
     def _fallback_csv_import(self, csv_file, table_name, delimiter):
         """Fallback CSV import for databases that don't support native CSV import"""
         logger.info(f"Using fallback CSV import for {table_name}")
-        
+
         # Read CSV and import row by row (existing bulk_create approach)
-        with open(csv_file, 'r', encoding='utf-8') as f:
+        with open(csv_file, "r", encoding="utf-8") as f:
             csv_reader = csv.reader(f, delimiter=delimiter)
             headers = next(csv_reader)  # Skip header
-            
+
             # Get model class
             if table_name not in self.model_map:
                 raise ValueError(f"No model found for table: {table_name}")
-            
+
             model_class = self.model_map[table_name]
             model_fields = self._get_model_fields(model_class)
-            
+
             # Prepare objects for bulk create
             objects_to_create = []
             # Calculate optimal batch size based on model field count
             batch_size = self._calculate_optimal_batch_size(model_class)
-            
+
             for row in csv_reader:
                 if not any(row):  # Skip empty rows
                     continue
-                
+
                 obj_data = {}
                 # Map CSV columns to model fields based on position
                 for i, value in enumerate(row):
                     if i < len(headers) and headers[i] in model_fields:
                         field = model_fields[headers[i]]
                         converted_value = self._convert_value(field, value, defer_foreign_keys=True)
                         if converted_value is not None:
                             obj_data[headers[i]] = converted_value
-                
+
                 if obj_data:
                     obj = model_class(**obj_data)
                     objects_to_create.append(obj)
-                    
+
                     if len(objects_to_create) >= batch_size:
                         model_class.objects.bulk_create(objects_to_create, batch_size=batch_size)
                         objects_to_create = []
                         logger.debug(f"Bulk created {batch_size} objects for {table_name}")
-            
+
             # Create remaining objects
             if objects_to_create:
                 model_class.objects.bulk_create(objects_to_create, batch_size=batch_size)
                 logger.debug(f"Bulk created final {len(objects_to_create)} objects for {table_name}")
 
@@ -952,113 +960,109 @@
         if not csv_file.exists():
             raise FileNotFoundError(f"CSV file not found: {csv_file}")
 
         # Validate CSV file has content and proper headers
         try:
-            with open(csv_file, 'r', encoding='utf-8') as f:
+            with open(csv_file, "r", encoding="utf-8") as f:
                 first_line = f.readline().strip()
                 if not first_line:
                     raise ValueError(f"CSV file {csv_file} is empty")
-                
+
                 # Check if file has reasonable number of columns (basic validation)
                 headers = first_line.split(delimiter)
                 if len(headers) < 1:
                     raise ValueError(f"CSV file {csv_file} has no columns")
-                
+
                 logger.info(f"CSV validation passed: {len(headers)} columns found in {csv_file}")
-                
+
         except Exception as e:
             logger.error(f"CSV validation failed for {csv_file}: {e}")
             raise
 
         logger.info(f"Starting fast CSV import for {table_name} from {csv_file}")
 
         try:
             # Clear the table first with proper foreign key handling
             with connection.cursor() as cursor:
                 # Disable foreign key constraints for SQLite
-                if connection.vendor == 'sqlite':
+                if connection.vendor == "sqlite":
                     cursor.execute("PRAGMA foreign_keys = 0;")
-                
+
                 # Delete all records from the table
                 cursor.execute(f"DELETE FROM {table_name};")
-                
+
                 # For SQLite, also reset the auto-increment counter if it exists
-                if connection.vendor == 'sqlite':
+                if connection.vendor == "sqlite":
                     cursor.execute(f"DELETE FROM sqlite_sequence WHERE name='{table_name}';")
                     cursor.execute("PRAGMA foreign_keys = 1;")
-                
+
                 # Verify table is empty
                 cursor.execute(f"SELECT COUNT(*) FROM {table_name};")
                 count = cursor.fetchone()[0]
                 if count > 0:
                     raise Exception(f"Failed to clear table {table_name}. Still has {count} records.")
-                
+
                 logger.info(f"Successfully cleared table {table_name}")
 
             # Get database file path (assumes SQLite database)
-            db_file = Path(connection.settings_dict['NAME']).absolute()
-            
+            db_file = Path(connection.settings_dict["NAME"]).absolute()
+
             # Validate that we have a SQLite database file
             if not db_file.exists():
                 raise FileNotFoundError(f"Database file not found: {db_file}")
 
             # Create the SQLite commands
-            commands = [
-                ".mode csv",
-                f".separator '{delimiter}'",
-                f".import --skip 1 '{csv_file}' {table_name}"
-            ]
+            commands = [".mode csv", f".separator '{delimiter}'", f".import --skip 1 '{csv_file}' {table_name}"]
 
             # Join commands with newlines
-            sqlite_script = '\n'.join(commands)
+            sqlite_script = "\n".join(commands)
 
             # Execute the SQLite import
             logger.info(f"Executing SQLite import: sqlite3 {db_file}")
             result = subprocess.run(
-                ['sqlite3', str(db_file)],
+                ["sqlite3", str(db_file)],
                 input=sqlite_script,
                 text=True,
                 capture_output=True,
-                check=False  # Don't raise exception immediately, handle errors manually
+                check=False,  # Don't raise exception immediately, handle errors manually
             )
-            
+
             # Check for errors
             if result.returncode != 0:
                 error_msg = f"SQLite import failed with return code {result.returncode}"
                 if result.stderr:
                     error_msg += f": {result.stderr}"
                 if result.stdout:
                     error_msg += f" (stdout: {result.stdout})"
                 raise Exception(error_msg)
-            
+
             if result.stderr and result.stderr.strip():
                 logger.warning(f"SQLite import warnings: {result.stderr}")
-                
+
             # Verify import success by checking record count
             with connection.cursor() as cursor:
                 cursor.execute(f"SELECT COUNT(*) FROM {table_name};")
                 imported_count = cursor.fetchone()[0]
                 logger.info(f"SQLite import completed successfully for {table_name}: {imported_count} records imported")
-                
+
                 if imported_count == 0:
                     logger.warning(f"No records were imported into {table_name}. Check CSV file format.")
-            
+
             return result
 
         except Exception as e:
             logger.error(f"Error importing CSV for {table_name}: {str(e)}")
             # Provide more detailed error information
-            if hasattr(e, 'stderr') and e.stderr:
+            if hasattr(e, "stderr") and e.stderr:
                 logger.error(f"SQLite stderr: {e.stderr}")
-            if hasattr(e, 'stdout') and e.stdout:
+            if hasattr(e, "stdout") and e.stdout:
                 logger.error(f"SQLite stdout: {e.stdout}")
             raise
 
     def import_csv_file(self, csv_filename, csv_content, use_fast_import=False):
         """Import a single CSV file using column index mappings
-        
+
         Args:
             csv_filename: Name of the CSV file
             csv_content: Content of the CSV file as string
             use_fast_import: If True, use fast SQL-based import method
         """
@@ -1072,11 +1076,11 @@
         # Verify that the debug_path is within the results_dir (no path traversal allowed)
         results_dir_norm = os.path.normpath(self.results_dir)
         if not debug_path.startswith(results_dir_norm):
             logger.error(f"Attempted debug file write outside results directory: {debug_path}")
             raise Exception("Unsafe debug file path detected!")
-        
+
         table_name = self._get_table_name_from_csv_filename(csv_filename)
         logger.info(f"Mapped CSV file '{csv_filename}' to table '{table_name}'")
 
         if table_name not in self.model_map:
             logger.warning(f"No model found for table: {table_name}. Skipping file {csv_filename}")
@@ -1087,108 +1091,112 @@
             return []
 
         model_class = self.model_map[table_name]
         column_mapping = self.column_mappings[table_name]
         model_fields = self._get_model_fields(model_class)
-        
+
         # Calculate optimal batch size for this model
         optimal_batch_size = self._calculate_optimal_batch_size(model_class)
 
         logger.info(f"Using model {model_class.__name__} for table {table_name}")
         logger.info(f"Optimal batch size for {model_class.__name__}: {optimal_batch_size}")
-        
+
         # Parse CSV to get row count for high-volume detection
         headers, rows = self._parse_csv_content(csv_content)
         row_count = len(rows)
-        
+
         # Check if this should use bulk SQLite import for high-volume data
         if self._is_high_volume_table(table_name, row_count):
             logger.info(f"High-volume table detected ({row_count} rows). Using bulk SQLite3 import for {table_name}")
-            
+
             # Check if model is compatible with bulk import (has auto-generated PK)
             pk_fields = [field for field in model_fields.values() if field.primary_key]
-            has_auto_pk = len(pk_fields) == 1 and pk_fields[0].name == 'id'
-            
+            has_auto_pk = len(pk_fields) == 1 and pk_fields[0].name == "id"
+
             if has_auto_pk:
                 try:
                     return self._bulk_sqlite_import_with_index(csv_content, model_class, table_name)
                 except Exception as bulk_error:
                     logger.warning(f"Bulk SQLite import failed for {table_name}: {bulk_error}")
                     logger.info(f"Falling back to Django ORM import for {table_name}")
-                    
+
                     # Log the specific error type for debugging
                     error_str = str(bulk_error).lower()
                     if "no such table" in error_str:
                         logger.error(f"Database table {table_name} does not exist. Please run migrations first.")
                     elif "database is locked" in error_str:
                         logger.error(f"Database is locked. Ensure no other processes are using the database.")
                     elif "permission denied" in error_str:
                         logger.error(f"Permission denied accessing database file.")
-                    
+
                     # Continue with normal Django ORM import below
             else:
                 logger.info(f"Table {table_name} not compatible with bulk import (no auto PK). Using Django ORM.")
-        
+
         # Fast import path: Use SQL-based import for simple cases
         if use_fast_import:
             logger.info(f"Using fast SQL-based import for {csv_filename}")
-            
+
             # Check if this table has complex foreign key relationships that require custom handling
             # For now, we'll use fast import for tables without complex FK dependencies
             has_complex_fks = any(
-                isinstance(field, models.ForeignKey) and 
-                field.related_model._meta.db_table in self.id_mappings
+                isinstance(field, models.ForeignKey) and field.related_model._meta.db_table in self.id_mappings
                 for field in model_fields.values()
             )
-            
+
             if not has_complex_fks:
                 try:
                     # Write CSV content to a temporary file
                     import tempfile
-                    with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False, encoding='utf-8') as temp_file:
+
+                    with tempfile.NamedTemporaryFile(
+                        mode="w", suffix=".csv", delete=False, encoding="utf-8"
+                    ) as temp_file:
                         temp_file.write(csv_content)
                         temp_csv_path = temp_file.name
-                    
+
                     try:
                         # Use fast SQL import
                         self._create_instances_from_csv_copy(temp_csv_path, model_class)
-                        
+
                         # Get count of imported records
                         imported_count = model_class.objects.count()
                         logger.info(f"Fast import completed: {imported_count} records imported for {table_name}")
-                        
+
                         # Create mock objects list for return value compatibility
-                        imported_objects = list(model_class.objects.all()[:min(100, imported_count)])  # Limit to avoid memory issues
+                        imported_objects = list(
+                            model_class.objects.all()[: min(100, imported_count)]
+                        )  # Limit to avoid memory issues
                         return imported_objects
-                        
+
                     finally:
                         # Clean up temporary file
                         os.unlink(temp_csv_path)
-                        
+
                 except Exception as e:
                     logger.warning(f"Fast import failed for {csv_filename}: {e}. Falling back to standard import.")
                     # Continue with standard import below
             else:
                 logger.info(f"Complex foreign keys detected for {table_name}, using standard import method")
-        
+
         # Show CSV information for debugging (already parsed above)
         logger.info(f"CSV headers: {headers}")
         if rows:
             logger.info(f"First CSV row: {rows[0]}")
             logger.info(f"Column mapping: {column_mapping}")
-        
+
         # Show which fields will be populated
         mapped_fields = []
         for col_idx, field_name in column_mapping.items():
             if col_idx < len(headers):
                 mapped_fields.append(f"{headers[col_idx]} -> {field_name}")
         logger.info(f"Field mappings: {mapped_fields}")
-        
+
         # Write detailed debug info to file
         try:
             # 'debug_path' is now validated to stay within 'self.results_dir'
-            with open(debug_path, 'w') as f:
+            with open(debug_path, "w") as f:
                 f.write(f"=== DEBUG: Import of {csv_filename} ===\n")
                 f.write(f"CSV filename: {csv_filename}\n")
                 f.write(f"Table name: {table_name}\n")
                 f.write(f"Model class: {model_class.__name__}\n")
                 f.write(f"CSV headers: {headers}\n")
@@ -1201,11 +1209,11 @@
                 for mapping in mapped_fields:
                     f.write(f"  {mapping}\n")
                 f.write(f"Total rows to import: {len(rows)}\n")
         except Exception as e:
             logger.warning(f"Could not write debug file: {e}")
-        
+
         # Debug: Show available ID mappings
         if self.id_mappings:
             logger.info(f"Available ID mappings: {list(self.id_mappings.keys())}")
             for table, mappings in self.id_mappings.items():
                 logger.info(f"  {table}: {len(mappings)} mappings")
@@ -1213,32 +1221,32 @@
                 sample_mappings = list(mappings.items())[:3]
                 for old_id, obj in sample_mappings:
                     logger.info(f"    {old_id} -> object id {obj.id}")
         else:
             logger.info("No ID mappings available yet")
-        
+
         # Check if the model has an explicit primary key
-        pk_fields = [field for field in model_class._meta.fields if field.primary_key and field.name != 'id']
+        pk_fields = [field for field in model_class._meta.fields if field.primary_key and field.name != "id"]
         has_explicit_pk = len(pk_fields) > 0
-        
+
         # Check if ID column is present in CSV (for models using Django's auto ID)
-        has_id_column = headers and headers[0].upper() == 'ID'
+        has_id_column = headers and headers[0].upper() == "ID"
         column_offset = 1 if has_id_column else 0
-        
+
         # Debug: Show ID column and PK detection
         logger.info(f"ID column detection: has_id_column={has_id_column}, has_explicit_pk={has_explicit_pk}")
         logger.info(f"Primary key fields: {[f.name for f in pk_fields]}")
         if headers:
             logger.info(f"First header: '{headers[0]}'")
         logger.info(f"Original column_offset: {column_offset}")
-        
+
         # If we have an ID column and the model uses auto ID, we'll need to handle it specially
         should_store_id_mappings = has_id_column and not has_explicit_pk
         id_to_object_map = {} if should_store_id_mappings else None
-        
+
         logger.info(f"Will store ID mappings: {should_store_id_mappings}")
-        
+
         # Adjust column mapping if ID column is present
         adjusted_column_mapping = {}
         logger.info(f"Original column mapping: {column_mapping}")
         for col_idx, field_name in column_mapping.items():
             # For models with auto-generated IDs, the CSV has an ID column at index 0
@@ -1264,112 +1272,126 @@
                 raise Exception(f"Blocked potentially unsafe table_name: {table_name}")
             if not self._is_safe_table_name(table_name):
                 raise Exception(f"Unsafe table name (violates allowed character rules): {table_name}")
             with connection.cursor() as cursor:
                 # Disable foreign key constraints for SQLite during clearing
-                if connection.vendor == 'sqlite':
+                if connection.vendor == "sqlite":
                     cursor.execute("PRAGMA foreign_keys = 0;")
-                
+
                 # Delete all records from the table
                 cursor.execute(f"DELETE FROM {table_name};")
-                
+
                 # For SQLite, also reset the auto-increment counter if it exists
-                if connection.vendor == 'sqlite':
+                if connection.vendor == "sqlite":
                     cursor.execute("DELETE FROM sqlite_sequence WHERE name=?;", [table_name])
                     cursor.execute("PRAGMA foreign_keys = 1;")
-                
+
             logger.info(f"Cleared {existing_count} existing records from {table_name}")
-            
+
             # Verify clearing worked
             remaining_count = model_class.objects.count()
             logger.info(f"After clearing, table {table_name} has {remaining_count} records")
-            
+
             if remaining_count > 0:
-                raise Exception(f"Failed to clear table {table_name}. Still has {remaining_count} records after clearing.")
+                raise Exception(
+                    f"Failed to clear table {table_name}. Still has {remaining_count} records after clearing."
+                )
 
         # First pass: collect all foreign key references
         foreign_key_refs = {}
         for field_name, field in model_fields.items():
             if isinstance(field, models.ForeignKey):
                 related_model = field.related_model
                 # Check if the related model has an explicit primary key or uses Django's auto id
-                related_has_explicit_pk = any(f.primary_key for f in related_model._meta.fields if f.name != 'id')
+                related_has_explicit_pk = any(f.primary_key for f in related_model._meta.fields if f.name != "id")
                 related_table = related_model._meta.db_table
                 foreign_key_refs[field_name] = {
-                    'model': related_model,
-                    'table': related_table,
-                    'ids': set(),
-                    'uses_auto_id': not related_has_explicit_pk
+                    "model": related_model,
+                    "table": related_table,
+                    "ids": set(),
+                    "uses_auto_id": not related_has_explicit_pk,
                 }
-                logger.debug(f"FK {field_name} -> {related_model.__name__} (table: {related_table}, uses_auto_id: {not related_has_explicit_pk})")
+                logger.debug(
+                    f"FK {field_name} -> {related_model.__name__} (table: {related_table}, uses_auto_id: {not related_has_explicit_pk})"
+                )
 
         # Scan rows to collect all foreign key IDs
         for row_num, row in enumerate(rows, 1):
             if not any(row):  # Skip empty rows
                 continue
-            
+
             for column_index, field_name in adjusted_column_mapping.items():
                 if column_index < len(row) and field_name in foreign_key_refs:
                     value = row[column_index].strip() if isinstance(row[column_index], str) else row[column_index]
-                    if value and value not in ('', 'None', 'NULL'):
+                    if value and value not in ("", "None", "NULL"):
                         try:
                             # For models using auto ID, the value should be an integer
-                            if foreign_key_refs[field_name]['uses_auto_id']:
+                            if foreign_key_refs[field_name]["uses_auto_id"]:
                                 value = int(float(value))  # Handle cases where int comes as float string
-                            foreign_key_refs[field_name]['ids'].add(value)
+                            foreign_key_refs[field_name]["ids"].add(value)
                         except (ValueError, TypeError):
                             logger.warning(f"Invalid foreign key value for {field_name}: {value}")
 
         # Pre-fetch all foreign key objects and handle missing ones
         foreign_key_cache = {}
-        
+
         for field_name, ref_info in foreign_key_refs.items():
-            logger.info(f"Processing FK {field_name}: {len(ref_info['ids'])} unique values, uses_auto_id={ref_info['uses_auto_id']}")
-            if ref_info['ids']:
-                related_model = ref_info['model']
-                related_table = ref_info['table']
-                
-                if ref_info['uses_auto_id']:
+            logger.info(
+                f"Processing FK {field_name}: {len(ref_info['ids'])} unique values, uses_auto_id={ref_info['uses_auto_id']}"
+            )
+            if ref_info["ids"]:
+                related_model = ref_info["model"]
+                related_table = ref_info["table"]
+
+                if ref_info["uses_auto_id"]:
                     # For models using auto ID, we rely on the global ID mappings
                     # Don't create missing objects here since we can't set their IDs
-                    logger.info(f"Foreign key {field_name} references model {related_model.__name__} "
-                              f"which uses auto-generated IDs. Will use ID mappings for resolution.")
+                    logger.info(
+                        f"Foreign key {field_name} references model {related_model.__name__} "
+                        f"which uses auto-generated IDs. Will use ID mappings for resolution."
+                    )
                     foreign_key_cache[field_name] = {}
                 else:
                     # For models with explicit primary keys, we can look them up normally
-                    existing_ids = set(related_model.objects.filter(pk__in=ref_info['ids']).values_list('pk', flat=True))
-                    missing_ids = ref_info['ids'] - existing_ids
-                    
+                    existing_ids = set(
+                        related_model.objects.filter(pk__in=ref_info["ids"]).values_list("pk", flat=True)
+                    )
+                    missing_ids = ref_info["ids"] - existing_ids
+
                     logger.info(f"FK {field_name}: {len(existing_ids)} exist, {len(missing_ids)} missing")
                     if missing_ids and len(missing_ids) <= 10:
                         logger.info(f"Missing IDs: {list(missing_ids)}")
-                    
+
                     # Create missing foreign key objects
                     if missing_ids:
                         missing_objects = [related_model(pk=pk_id) for pk_id in missing_ids]
-                        related_model.objects.bulk_create(missing_objects, batch_size=optimal_batch_size, ignore_conflicts=True)
+                        related_model.objects.bulk_create(
+                            missing_objects, batch_size=optimal_batch_size, ignore_conflicts=True
+                        )
                         logger.info(f"Created {len(missing_objects)} missing {related_model.__name__} objects")
-                    
+
                     # Cache all foreign key objects
                     foreign_key_cache[field_name] = {
-                        str(obj.pk): obj for obj in related_model.objects.filter(pk__in=ref_info['ids'])
+                        str(obj.pk): obj for obj in related_model.objects.filter(pk__in=ref_info["ids"])
                     }
-                    logger.info(f"Cached {len(foreign_key_cache[field_name])} {related_model.__name__} objects for FK {field_name}")
+                    logger.info(
+                        f"Cached {len(foreign_key_cache[field_name])} {related_model.__name__} objects for FK {field_name}"
+                    )
 
         # Second pass: prepare objects for bulk creation
         objects_to_create = []
         errors = []
         old_id_to_row_data = {}  # Map old IDs to row data for models using auto ID
-        
+
         for row_num, row in enumerate(rows, 1):
             if not any(row):  # Skip empty rows
                 logger.debug(f"Skipping empty row {row_num}")
                 continue
 
             obj_data = {}
             old_id = None
-            
+
             # Extract the old ID if present
             if has_id_column and not has_explicit_pk:
                 old_id = row[0].strip() if row[0] else None
                 if old_id:
                     try:
@@ -1381,294 +1403,296 @@
             # Use column index mapping to extract values
             for column_index, field_name in adjusted_column_mapping.items():
                 if column_index < len(row) and field_name in model_fields:
                     value = row[column_index].strip() if isinstance(row[column_index], str) else row[column_index]
                     field = model_fields[field_name]
-                    
-                    if isinstance(field, models.ForeignKey) and value and value not in ('', 'None', 'NULL'):
+
+                    if isinstance(field, models.ForeignKey) and value and value not in ("", "None", "NULL"):
                         # Handle foreign keys differently based on whether they use auto ID
-                        if field_name in foreign_key_refs and foreign_key_refs[field_name]['uses_auto_id']:
+                        if field_name in foreign_key_refs and foreign_key_refs[field_name]["uses_auto_id"]:
                             # Check if we have a mapping for this foreign key from a previous import
-                            related_table = foreign_key_refs[field_name]['table']
-                            
+                            related_table = foreign_key_refs[field_name]["table"]
+
                             try:
                                 fk_old_id = int(float(value))
-                                if (related_table in self.id_mappings and 
-                                    fk_old_id in self.id_mappings[related_table]):
+                                if related_table in self.id_mappings and fk_old_id in self.id_mappings[related_table]:
                                     # We have a mapping from a previous import
                                     obj_data[field_name] = self.id_mappings[related_table][fk_old_id]
-                                    logger.info(f"Resolved foreign key {field_name} using existing mapping: {fk_old_id} -> {self.id_mappings[related_table][fk_old_id].id}")
+                                    logger.info(
+                                        f"Resolved foreign key {field_name} using existing mapping: {fk_old_id} -> {self.id_mappings[related_table][fk_old_id].id}"
+                                    )
                                 else:
                                     # Store the foreign key reference value for later resolution
-                                    obj_data[f'_fk_{field_name}'] = value
+                                    obj_data[f"_fk_{field_name}"] = value
                                     # Don't set the actual foreign key yet
                             except (ValueError, TypeError):
                                 logger.warning(f"Invalid foreign key value for {field_name}: {value}")
                         else:
                             # Use cached foreign key object for models with explicit PKs
                             value_str = str(value)
                             if field_name in foreign_key_cache and value_str in foreign_key_cache[field_name]:
                                 obj_data[field_name] = foreign_key_cache[field_name][value_str]
-                                logger.info(f"Row {row_num}: Set FK {field_name} = {value_str} -> {foreign_key_cache[field_name][value_str]}")
+                                logger.info(
+                                    f"Row {row_num}: Set FK {field_name} = {value_str} -> {foreign_key_cache[field_name][value_str]}"
+                                )
                             else:
-                                logger.warning(f"Row {row_num}: Foreign key object not found for {field_name} with value '{value_str}'. Available keys: {list(foreign_key_cache.get(field_name, {}).keys())[:5]}")
+                                logger.warning(
+                                    f"Row {row_num}: Foreign key object not found for {field_name} with value '{value_str}'. Available keys: {list(foreign_key_cache.get(field_name, {}).keys())[:5]}"
+                                )
                     else:
                         # Convert non-foreign key values
                         converted_value = self._convert_value(field, value, defer_foreign_keys=True)
                         if converted_value is not None:
                             obj_data[field_name] = converted_value
 
             if obj_data:
                 try:
                     # Remove deferred foreign key fields from obj_data
-                    deferred_fks = {k: v for k, v in obj_data.items() if k.startswith('_fk_')}
-                    clean_obj_data = {k: v for k, v in obj_data.items() if not k.startswith('_fk_')}
-                    
+                    deferred_fks = {k: v for k, v in obj_data.items() if k.startswith("_fk_")}
+                    clean_obj_data = {k: v for k, v in obj_data.items() if not k.startswith("_fk_")}
+
                     # Create model instance (without saving)
                     obj = model_class(**clean_obj_data)
                     objects_to_create.append(obj)
-                    
+
                     # Store mapping of old ID to object data for later FK resolution
                     if old_id and should_store_id_mappings:
-                        old_id_to_row_data[old_id] = {
-                            'obj': obj,
-                            'deferred_fks': deferred_fks,
-                            'row_num': row_num
-                        }
-                    
+                        old_id_to_row_data[old_id] = {"obj": obj, "deferred_fks": deferred_fks, "row_num": row_num}
+
                     if row_num % 1000 == 0:  # Log progress every 1000 rows
                         logger.debug(f"Processed {row_num} rows, prepared {len(objects_to_create)} objects")
                 except Exception as e:
-                    error_info = {
-                        'row_num': row_num,
-                        'error': str(e),
-                        'row_data': row,
-                        'obj_data': obj_data
-                    }
+                    error_info = {"row_num": row_num, "error": str(e), "row_data": row, "obj_data": obj_data}
                     errors.append(error_info)
                     logger.warning(f"Skipping row {row_num} due to error: {e}")
                     continue
 
         # Validate and deduplicate objects if model has explicit primary key
         if has_explicit_pk and objects_to_create:
             logger.info(f"Validating primary keys for {len(objects_to_create)} objects")
             pk_field_name = next((f.name for f in model_class._meta.fields if f.primary_key), None)
-            
+
             if pk_field_name:
                 seen_pks = set()
                 unique_objects = []
                 duplicate_count = 0
-                
+
                 for obj in objects_to_create:
                     pk_value = getattr(obj, pk_field_name, None)
                     if pk_value is not None:
                         if pk_value not in seen_pks:
                             seen_pks.add(pk_value)
                             unique_objects.append(obj)
                         else:
                             duplicate_count += 1
-                
+
                 if duplicate_count > 0:
-                    logger.warning(f"Found {duplicate_count} duplicate primary keys in {table_name}. "
-                                 f"Keeping {len(unique_objects)} unique objects.")
+                    logger.warning(
+                        f"Found {duplicate_count} duplicate primary keys in {table_name}. "
+                        f"Keeping {len(unique_objects)} unique objects."
+                    )
                     objects_to_create = unique_objects
 
         # Bulk create all objects
         imported_objects = []
         if objects_to_create:
             try:
                 logger.info(f"About to bulk create {len(objects_to_create)} objects to {table_name}")
                 logger.info(f"should_store_id_mappings: {should_store_id_mappings}")
-                
+
                 # Debug: Show first object to be created
                 if objects_to_create:
                     first_obj = objects_to_create[0]
                     logger.info(f"First object to create: {first_obj}")
                     logger.info(f"First object fields: {first_obj.__dict__}")
-                
+
                 # For models that need ID mappings, don't ignore conflicts so we get proper IDs
                 # For models with explicit primary keys, ignore conflicts to handle duplicates
                 if should_store_id_mappings:
-                    imported_objects = model_class.objects.bulk_create(
-                        objects_to_create, 
-                        batch_size=optimal_batch_size
-                    )
+                    imported_objects = model_class.objects.bulk_create(objects_to_create, batch_size=optimal_batch_size)
                 elif has_explicit_pk:
                     # Models with explicit primary keys may have duplicates, ignore conflicts
-                    logger.info(f"Using ignore_conflicts=True for model {model_class.__name__} with explicit primary key")
+                    logger.info(
+                        f"Using ignore_conflicts=True for model {model_class.__name__} with explicit primary key"
+                    )
                     imported_objects = model_class.objects.bulk_create(
-                        objects_to_create, 
-                        batch_size=optimal_batch_size,
-                        ignore_conflicts=True
+                        objects_to_create, batch_size=optimal_batch_size, ignore_conflicts=True
                     )
                 else:
                     imported_objects = model_class.objects.bulk_create(
-                        objects_to_create, 
+                        objects_to_create,
                         batch_size=optimal_batch_size,
-                        ignore_conflicts=False  # Changed to False to see actual errors
+                        ignore_conflicts=False,  # Changed to False to see actual errors
                     )
                 logger.info(f"Successfully bulk created {len(imported_objects)} objects to {table_name}")
-                
+
                 # Verify final count in database
                 final_count = model_class.objects.count()
                 logger.info(f"Table {table_name} now has {final_count} total records in database")
-                
+
                 # Debug: Check if count is unexpected
                 expected_new_count = len(imported_objects)
                 if final_count != expected_new_count:
-                    logger.warning(f"UNEXPECTED COUNT: Expected {expected_new_count} but table has {final_count} records!")
+                    logger.warning(
+                        f"UNEXPECTED COUNT: Expected {expected_new_count} but table has {final_count} records!"
+                    )
                     logger.warning(f"This suggests data from other CSV files may have been imported to this table!")
-                
+
                 # Handle ID mappings and deferred foreign keys
                 if old_id_to_row_data and id_to_object_map is not None:
                     logger.info(f"Building ID mapping for {len(imported_objects)} objects")
-                    
+
                     # Build mapping of old IDs to new objects
                     # Create reverse lookup for faster mapping: object -> old_id
-                    obj_to_old_id = {data['obj']: old_id for old_id, data in old_id_to_row_data.items()}
-                    
+                    obj_to_old_id = {data["obj"]: old_id for old_id, data in old_id_to_row_data.items()}
+
                     # Initialize global ID mappings for this model if needed
                     if table_name not in self.id_mappings:
                         self.id_mappings[table_name] = {}
-                    
+
                     # Map objects to their old IDs efficiently
                     for i, obj in enumerate(imported_objects):
                         if i < len(objects_to_create):
                             created_obj = objects_to_create[i]
                             if created_obj in obj_to_old_id:
                                 old_id = obj_to_old_id[created_obj]
                                 id_to_object_map[old_id] = obj
                                 self.id_mappings[table_name][old_id] = obj
                                 # Only log a few examples to avoid log spam
                                 if i < 5 or i % 1000 == 0:
-                                    logger.info(f"Stored ID mapping: {table_name}[{old_id}] -> object with new id {obj.id}")
-                    
+                                    logger.info(
+                                        f"Stored ID mapping: {table_name}[{old_id}] -> object with new id {obj.id}"
+                                    )
+
                     # Now update objects with deferred foreign keys
                     objects_to_update = []
                     for old_id, data in old_id_to_row_data.items():
                         if old_id in id_to_object_map:
                             obj = id_to_object_map[old_id]
                             needs_update = False
-                            
+
                             # Resolve deferred foreign keys
-                            for fk_field, fk_value in data['deferred_fks'].items():
+                            for fk_field, fk_value in data["deferred_fks"].items():
                                 field_name = fk_field[4:]  # Remove '_fk_' prefix
                                 try:
                                     fk_old_id = int(float(fk_value))
                                     resolved = False
-                                    
+
                                     # First try local mapping from current import
                                     if fk_old_id in id_to_object_map:
                                         setattr(obj, field_name, id_to_object_map[fk_old_id])
                                         needs_update = True
                                         resolved = True
-                                        logger.debug(f"Resolved FK {field_name} for object {old_id} -> {fk_old_id} (local)")
+                                        logger.debug(
+                                            f"Resolved FK {field_name} for object {old_id} -> {fk_old_id} (local)"
+                                        )
                                     else:
                                         # Try global mappings from previous imports
                                         if field_name in foreign_key_refs:
-                                            related_table = foreign_key_refs[field_name]['table']
-                                            
-                                            if (related_table in self.id_mappings and 
-                                                fk_old_id in self.id_mappings[related_table]):
+                                            related_table = foreign_key_refs[field_name]["table"]
+
+                                            if (
+                                                related_table in self.id_mappings
+                                                and fk_old_id in self.id_mappings[related_table]
+                                            ):
                                                 setattr(obj, field_name, self.id_mappings[related_table][fk_old_id])
                                                 needs_update = True
                                                 resolved = True
-                                                logger.info(f"Resolved FK {field_name} for object {old_id} -> {fk_old_id} (global mapping)")
-                                    
+                                                logger.info(
+                                                    f"Resolved FK {field_name} for object {old_id} -> {fk_old_id} (global mapping)"
+                                                )
+
                                     if not resolved:
-                                        logger.warning(f"Could not resolve FK {field_name} with value {fk_value} for object {old_id}")
-                                        
+                                        logger.warning(
+                                            f"Could not resolve FK {field_name} with value {fk_value} for object {old_id}"
+                                        )
+
                                 except (ValueError, TypeError):
                                     logger.warning(f"Invalid FK value {fk_value} for field {field_name}")
-                            
+
                             if needs_update:
                                 objects_to_update.append(obj)
-                    
+
                     # Bulk update objects with resolved foreign keys
                     if objects_to_update:
                         # Get field names from last processed object's deferred FKs
                         field_names = []
                         for old_id, data in old_id_to_row_data.items():
-                            if data['deferred_fks']:
-                                field_names = [fk_field[4:] for fk_field in data['deferred_fks'].keys()]
+                            if data["deferred_fks"]:
+                                field_names = [fk_field[4:] for fk_field in data["deferred_fks"].keys()]
                                 break
-                        
+
                         if field_names:
                             model_class.objects.bulk_update(
-                                objects_to_update, 
-                                fields=field_names,
-                                batch_size=optimal_batch_size
+                                objects_to_update, fields=field_names, batch_size=optimal_batch_size
                             )
                             logger.info(f"Updated {len(objects_to_update)} objects with resolved foreign keys")
-                
+
                 elif should_store_id_mappings and not old_id_to_row_data:
                     logger.warning(f"Expected to store ID mappings but no old_id_to_row_data found!")
                 elif not should_store_id_mappings:
                     logger.info(f"Not storing ID mappings (should_store_id_mappings={should_store_id_mappings})")
-                
+
             except Exception as e:
                 error_str = str(e).lower()
-                
+
                 # Check if it's a SQLite "too many variables" error
                 if "too many sql variables" in error_str or "too many variables" in error_str:
-                    logger.warning(f"SQLite variable limit exceeded for {table_name}. Attempting recovery with smaller batch size.")
-                    
+                    logger.warning(
+                        f"SQLite variable limit exceeded for {table_name}. Attempting recovery with smaller batch size."
+                    )
+
                     # Try with progressively smaller batch sizes
                     recovery_batch_sizes = [optimal_batch_size // 2, optimal_batch_size // 4, 10]
-                    
+
                     for recovery_batch_size in recovery_batch_sizes:
                         if recovery_batch_size <= 0:
                             continue
-                            
+
                         logger.info(f"Retrying {table_name} with batch size {recovery_batch_size}")
                         try:
                             # Split objects into smaller batches
                             imported_objects = []
                             for i in range(0, len(objects_to_create), recovery_batch_size):
-                                batch = objects_to_create[i:i + recovery_batch_size]
+                                batch = objects_to_create[i : i + recovery_batch_size]
                                 if should_store_id_mappings:
                                     batch_result = model_class.objects.bulk_create(
-                                        batch, 
-                                        batch_size=recovery_batch_size
+                                        batch, batch_size=recovery_batch_size
                                     )
                                 elif has_explicit_pk:
                                     batch_result = model_class.objects.bulk_create(
-                                        batch, 
-                                        batch_size=recovery_batch_size,
-                                        ignore_conflicts=True
+                                        batch, batch_size=recovery_batch_size, ignore_conflicts=True
                                     )
                                 else:
                                     batch_result = model_class.objects.bulk_create(
-                                        batch, 
-                                        batch_size=recovery_batch_size,
-                                        ignore_conflicts=False
+                                        batch, batch_size=recovery_batch_size, ignore_conflicts=False
                                     )
                                 imported_objects.extend(batch_result)
-                                
+
                             logger.info(f"Recovery successful for {table_name} with batch size {recovery_batch_size}")
                             break
-                            
+
                         except Exception as recovery_error:
-                            logger.warning(f"Recovery attempt with batch size {recovery_batch_size} failed: {recovery_error}")
+                            logger.warning(
+                                f"Recovery attempt with batch size {recovery_batch_size} failed: {recovery_error}"
+                            )
                             continue
                     else:
                         # All recovery attempts failed
                         logger.error(f"All recovery attempts failed for {table_name}. Original error: {e}")
                         raise
-                        
+
                 elif "unique constraint failed" in error_str or "duplicate key" in error_str:
                     logger.error(f"UNIQUE constraint violation in {table_name}: {e}")
                     logger.error(f"This suggests duplicate primary keys in the CSV data.")
-                    
+
                     # For explicit primary key models, try again with ignore_conflicts=True
                     if has_explicit_pk and not should_store_id_mappings:
                         logger.info(f"Retrying {table_name} with ignore_conflicts=True to handle duplicates")
                         try:
                             imported_objects = model_class.objects.bulk_create(
-                                objects_to_create, 
-                                batch_size=optimal_batch_size,
-                                ignore_conflicts=True
+                                objects_to_create, batch_size=optimal_batch_size, ignore_conflicts=True
                             )
                             logger.info(f"Recovery successful for {table_name} by ignoring duplicate conflicts")
                         except Exception as recovery_error:
                             logger.error(f"Recovery attempt failed: {recovery_error}")
                             raise
@@ -1692,56 +1716,41 @@
     def import_from_csv_string(self, csv_string, filename="data.csv", use_fast_import=False):
         """Import CSV data from a string"""
         logger.info(f"Importing CSV data from string (filename: {filename}, fast_import={use_fast_import})")
         try:
             imported_objects = self.import_csv_file(filename, csv_string, use_fast_import=use_fast_import)
-            result = {
-                filename: {
-                    'success': True,
-                    'imported_count': len(imported_objects),
-                    'objects': imported_objects
-                }
-            }
+            result = {filename: {"success": True, "imported_count": len(imported_objects), "objects": imported_objects}}
             logger.info(f"Successfully imported {len(imported_objects)} objects from CSV string")
             # Save results
             self._save_results(result, "csv_string_import")
             return result
         except Exception as e:
             logger.error(f"Failed to import CSV string for {filename}: {e}")
-            result = {
-                filename: {
-                    'success': False,
-                    'error': str(e)
-                }
-            }
+            result = {filename: {"success": False, "error": str(e)}}
             # Save results even on failure
             self._save_results(result, "csv_string_import")
             return result
 
     def import_from_path(self, path):
         """Import CSV files from either a zip file or a directory"""
         logger.info(f"Starting import from path: {path}")
 
         if os.path.isfile(path):
-            if path.endswith('.zip'):
+            if path.endswith(".zip"):
                 logger.info(f"Processing zip file: {path}")
                 result = self.import_zip_file(path)
                 self._save_results(result, "zip_import")
                 return result
-            elif path.endswith('.csv'):
+            elif path.endswith(".csv"):
                 logger.info(f"Processing single CSV file: {path}")
                 # Single CSV file
-                with open(path, 'r', encoding='utf-8') as f:
+                with open(path, "r", encoding="utf-8") as f:
                     csv_content = f.read()
                 filename = os.path.basename(path)
                 imported_objects = self.import_csv_file(filename, csv_content)
                 result = {
-                    filename: {
-                        'success': True,
-                        'imported_count': len(imported_objects),
-                        'objects': imported_objects
-                    }
+                    filename: {"success": True, "imported_count": len(imported_objects), "objects": imported_objects}
                 }
                 self._save_results(result, "single_csv_import")
                 return result
             else:
                 error_msg = f"Unsupported file type: {path}"
@@ -1768,66 +1777,60 @@
 
         for csv_file_path in csv_files:
             filename = os.path.basename(csv_file_path)
             logger.info(f"Processing file: {filename}")
             try:
-                with open(csv_file_path, 'r', encoding='utf-8') as f:
+                with open(csv_file_path, "r", encoding="utf-8") as f:
                     csv_content = f.read()
                 imported_objects = self.import_csv_file(filename, csv_content)
                 results[filename] = {
-                    'success': True,
-                    'imported_count': len(imported_objects),
-                    'objects': imported_objects
+                    "success": True,
+                    "imported_count": len(imported_objects),
+                    "objects": imported_objects,
                 }
                 logger.info(f"Successfully processed {filename}: {len(imported_objects)} objects imported")
             except Exception as e:
                 logger.error(f"Failed to process {filename}: {e}")
-                results[filename] = {
-                    'success': False,
-                    'error': str(e)
-                }
+                results[filename] = {"success": False, "error": str(e)}
 
         logger.info(f"Completed folder import. Processed {len(csv_files)} files")
         return results
 
     def import_zip_file(self, zip_file_path_or_content):
         logger.info("Starting zip file import")
         if isinstance(zip_file_path_or_content, str):
             # It's a file path
             logger.info(f"Processing zip file from path: {zip_file_path_or_content}")
-            with zipfile.ZipFile(zip_file_path_or_content, 'r') as zip_file:
+            with zipfile.ZipFile(zip_file_path_or_content, "r") as zip_file:
                 return self._process_zip_contents(zip_file)
         else:
             # It's file content (bytes)
             logger.info("Processing zip file from bytes content")
-            with zipfile.ZipFile(io.BytesIO(zip_file_path_or_content), 'r') as zip_file:
+            with zipfile.ZipFile(io.BytesIO(zip_file_path_or_content), "r") as zip_file:
                 return self._process_zip_contents(zip_file)
 
     def _process_zip_contents(self, zip_file):
         """Process contents of an opened zip file"""
         logger.debug("Processing zip file contents")
         results = {}
-        csv_files = [f for f in zip_file.namelist() if f.endswith('.csv')]
+        csv_files = [f for f in zip_file.namelist() if f.endswith(".csv")]
         logger.info(f"Found {len(csv_files)} CSV files in zip archive")
 
         for csv_filename in csv_files:
             logger.info(f"Processing CSV file from zip: {csv_filename}")
             try:
-                csv_content = zip_file.read(csv_filename).decode('utf-8')
+                csv_content = zip_file.read(csv_filename).decode("utf-8")
                 imported_objects = self.import_csv_file(csv_filename, csv_content)
                 results[csv_filename] = {
-                    'success': True,
-                    'imported_count': len(imported_objects),
-                    'objects': imported_objects
+                    "success": True,
+                    "imported_count": len(imported_objects),
+                    "objects": imported_objects,
                 }
                 logger.info(f"Successfully processed {csv_filename}: {len(imported_objects)} objects imported")
             except Exception as e:
                 logger.error(f"Failed to process {csv_filename} from zip: {e}")
-                results[csv_filename] = {
-                    'success': False,
-                    'error': str(e)
-                }
+                results[csv_filename] = {"success": False, "error": str(e)}
 
         logger.info(f"Completed zip file processing. Processed {len(csv_files)} CSV files")
         return results
 
     def import_from_csv_strings(self, csv_strings_list):
@@ -1838,104 +1841,95 @@
         for filename, csv_string in csv_strings_list.items():
             logger.info(f"Processing CSV string for filename: {filename}")
             try:
                 imported_objects = self.import_csv_file(filename, csv_string)
                 results[filename] = {
-                    'success': True,
-                    'imported_count': len(imported_objects),
-                    'objects': imported_objects
+                    "success": True,
+                    "imported_count": len(imported_objects),
+                    "objects": imported_objects,
                 }
                 logger.info(f"Successfully processed {filename}: {len(imported_objects)} objects imported")
             except Exception as e:
                 logger.error(f"Failed to process CSV string for {filename}: {e}")
-                results[filename] = {
-                    'success': False,
-                    'error': str(e)
-                }
+                results[filename] = {"success": False, "error": str(e)}
 
         logger.info(f"Completed CSV strings import. Processed {len(csv_strings_list)} files")
         # Save results
         self._save_results(results, "csv_strings_import")
         return results
 
     def import_from_csv_strings_ordered(self, csv_strings_list, use_fast_import=False):
         """Import CSV data from a list of CSV strings in dependency order"""
         logger.info(f"Starting ordered import from {len(csv_strings_list)} CSV strings (fast_import={use_fast_import})")
-        
+
         # Debug: Show all available CSV files
         logger.info(f"Available CSV files: {list(csv_strings_list.keys())}")
-        
+
         # Get the import order
         import_order = self._get_import_order()
         results = {}
-        
+
         # Debug: Show import order
         logger.info(f"Import order: {import_order}")
-        
+
         # Import files in dependency order
         for table_name in import_order:
             # Find CSV file for this table
             csv_filename = None
-            
+
             # Debug: Show matching attempt
             logger.info(f"Looking for CSV file for table: {table_name}")
             for filename in csv_strings_list.keys():
                 converted_table_name = self._get_table_name_from_csv_filename(filename)
                 logger.info(f"  File '{filename}' converts to table '{converted_table_name}'")
                 if converted_table_name == table_name:
                     csv_filename = filename
                     break
-            
+
             if csv_filename and csv_filename in csv_strings_list:
                 logger.info(f"Importing {csv_filename} for table {table_name}")
                 try:
                     csv_content = csv_strings_list[csv_filename]
                     imported_objects = self.import_csv_file(csv_filename, csv_content, use_fast_import=use_fast_import)
                     results[csv_filename] = {
-                        'success': True,
-                        'imported_count': len(imported_objects),
-                        'objects': imported_objects
+                        "success": True,
+                        "imported_count": len(imported_objects),
+                        "objects": imported_objects,
                     }
                     logger.info(f"Successfully imported {csv_filename}: {len(imported_objects)} objects")
                 except Exception as e:
                     logger.error(f"Failed to import {csv_filename}: {e}")
-                    results[csv_filename] = {
-                        'success': False,
-                        'error': str(e)
-                    }
+                    results[csv_filename] = {"success": False, "error": str(e)}
             else:
                 logger.info(f"No CSV file found for table {table_name} (expected filename pattern)")
-        
+
         # Import any remaining CSV files that weren't in the ordered list
         for filename, csv_content in csv_strings_list.items():
             if filename not in results:
                 logger.info(f"Importing remaining file: {filename}")
                 try:
                     imported_objects = self.import_csv_file(filename, csv_content, use_fast_import=use_fast_import)
                     results[filename] = {
-                        'success': True,
-                        'imported_count': len(imported_objects),
-                        'objects': imported_objects
+                        "success": True,
+                        "imported_count": len(imported_objects),
+                        "objects": imported_objects,
                     }
                     logger.info(f"Successfully imported {filename}: {len(imported_objects)} objects")
                 except Exception as e:
                     logger.error(f"Failed to import {filename}: {e}")
-                    results[filename] = {
-                        'success': False,
-                        'error': str(e)
-                    }
-        
+                    results[filename] = {"success": False, "error": str(e)}
+
         # Save results
         self._save_results(results, "csv_strings_ordered_import")
         logger.info(f"Completed ordered CSV strings import. Processed {len(results)} files")
         return results
 
-
     def _is_safe_table_name(self, table_name):
         """Return True if table_name contains only allowed characters and matches expected pattern."""
         # Only letters, digits, and underscores permitted
-        return bool(re.fullmatch(r'[A-Za-z0-9_]+', table_name))
+        return bool(re.fullmatch(r"[A-Za-z0-9_]+", table_name))
+
 
 def import_bird_data_from_csv_export(path_or_content, use_fast_import=False):
     """
     Convenience function to import bird data from a CSV export.
 
@@ -1968,26 +1962,26 @@
 
         # First, collect all available CSV files
         csv_files_data = {}
 
         if os.path.isfile(path):
-            if path.endswith('.zip'):
+            if path.endswith(".zip"):
                 logger.info(f"Processing zip file: {path}")
-                with zipfile.ZipFile(path, 'r') as zip_file:
-                    csv_files = [f for f in zip_file.namelist() if f.endswith('.csv')]
+                with zipfile.ZipFile(path, "r") as zip_file:
+                    csv_files = [f for f in zip_file.namelist() if f.endswith(".csv")]
                     for csv_filename in csv_files:
-                        csv_content = zip_file.read(csv_filename).decode('utf-8')
+                        csv_content = zip_file.read(csv_filename).decode("utf-8")
                         csv_files_data[csv_filename] = csv_content
-            elif path.endswith('.csv'):
+            elif path.endswith(".csv"):
                 filename = os.path.basename(path)
-                with open(path, 'r', encoding='utf-8') as f:
+                with open(path, "r", encoding="utf-8") as f:
                     csv_files_data[filename] = f.read()
         elif os.path.isdir(path):
             csv_files = glob.glob(os.path.join(path, "*.csv"))
             for csv_file_path in csv_files:
                 filename = os.path.basename(csv_file_path)
-                with open(csv_file_path, 'r', encoding='utf-8') as f:
+                with open(csv_file_path, "r", encoding="utf-8") as f:
                     csv_files_data[filename] = f.read()
 
         # Now import in dependency order
         results = {}
         import_order = self._get_import_order()
@@ -2001,47 +1995,44 @@
                     break
 
             if csv_filename and csv_filename in csv_files_data:
                 logger.info(f"Importing {csv_filename} for table {table_name}")
                 try:
-                    imported_objects = self.import_csv_file(csv_filename, csv_files_data[csv_filename], use_fast_import=use_fast_import)
+                    imported_objects = self.import_csv_file(
+                        csv_filename, csv_files_data[csv_filename], use_fast_import=use_fast_import
+                    )
                     results[csv_filename] = {
-                        'success': True,
-                        'imported_count': len(imported_objects),
-                        'objects': imported_objects
+                        "success": True,
+                        "imported_count": len(imported_objects),
+                        "objects": imported_objects,
                     }
                     logger.info(f"Successfully imported {csv_filename}: {len(imported_objects)} objects")
                 except Exception as e:
                     logger.error(f"Failed to import {csv_filename}: {e}")
-                    results[csv_filename] = {
-                        'success': False,
-                        'error': str(e)
-                    }
+                    results[csv_filename] = {"success": False, "error": str(e)}
             else:
                 logger.debug(f"No CSV file found for table {table_name}")
 
         # Import any remaining CSV files that weren't in the ordered list
         for filename, csv_content in csv_files_data.items():
             if filename not in results:
                 logger.info(f"Importing remaining file: {filename}")
                 try:
                     imported_objects = self.import_csv_file(filename, csv_content, use_fast_import=use_fast_import)
                     results[filename] = {
-                        'success': True,
-                        'imported_count': len(imported_objects),
-                        'objects': imported_objects
+                        "success": True,
+                        "imported_count": len(imported_objects),
+                        "objects": imported_objects,
                     }
                 except Exception as e:
                     logger.error(f"Failed to import {filename}: {e}")
-                    results[filename] = {
-                        'success': False,
-                        'error': str(e)
-                    }
+                    results[filename] = {"success": False, "error": str(e)}
 
         self._save_results(results, "ordered_import")
         logger.info(f"Completed ordered import. Processed {len(results)} files")
         return results
+
 
 def import_bird_data_from_csv_export_ordered(path_or_content, use_fast_import=False):
     """
     Convenience function to import bird data from a CSV export in dependency order.
     This ensures foreign key relationships are respected during import.
@@ -2058,11 +2049,12 @@
 
     # If it's bytes, treat as zip content - we need to save it temporarily for ordered import
     if isinstance(path_or_content, bytes):
         logger.info("Processing as zip file content (bytes) - saving temporarily for ordered import")
         import tempfile
-        with tempfile.NamedTemporaryFile(suffix='.zip', delete=False) as temp_file:
+
+        with tempfile.NamedTemporaryFile(suffix=".zip", delete=False) as temp_file:
             temp_file.write(path_or_content)
             temp_file_path = temp_file.name
 
         try:
             result = importer.import_from_path_ordered(temp_file_path, use_fast_import=use_fast_import)
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/utils/clone_mode/import_from_metadata_export.py

Oh no! 💥 💔 💥
33 files would be reformatted, 6 files would be left unchanged.

Checking formatting with Black: pybirdai/entry_points/
-----------------------------------
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/convert_ldm_to_sdd_hierarchies.py	2025-08-02 18:37:08.441302+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/convert_ldm_to_sdd_hierarchies.py	2025-09-21 17:07:35.736238+00:00
@@ -14,36 +14,37 @@
 import os
 from django.apps import AppConfig
 from pybirdai.context.sdd_context_django import SDDContext
 from django.conf import settings
 
+
 class RunConvertLDMToSDDHierarchies(AppConfig):
     """
     Django AppConfig for converting LDM hierarchies to SDD hierarchies.
     """
 
-    path = os.path.join(settings.BASE_DIR, 'birds_nest')
+    path = os.path.join(settings.BASE_DIR, "birds_nest")
 
     @staticmethod
     def run_convert_hierarchies():
         """
         Execute the process of converting LDM hierarchies to SDD hierarchies.
         """
         from pybirdai.process_steps.hierarchy_conversion.convert_ldm_to_sdd_hierarchies import (
-            ConvertLDMToSDDHierarchies
+            ConvertLDMToSDDHierarchies,
         )
         from pybirdai.context.context import Context
 
         base_dir = settings.BASE_DIR
         sdd_context = SDDContext()
-        sdd_context.file_directory = os.path.join(base_dir, 'resources')
-        sdd_context.output_directory = os.path.join(base_dir, 'results')
-        
+        sdd_context.file_directory = os.path.join(base_dir, "resources")
+        sdd_context.output_directory = os.path.join(base_dir, "results")
+
         context = Context()
         context.file_directory = sdd_context.file_directory
         context.output_directory = sdd_context.output_directory
 
         ConvertLDMToSDDHierarchies().convert_hierarchies(context, sdd_context)
 
     def ready(self):
         # This method is still needed for Django's AppConfig
-        pass 
\ No newline at end of file
+        pass
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/convert_ldm_to_sdd_hierarchies.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/create_filters.py	2025-09-15 13:18:11.372236+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/create_filters.py	2025-09-21 17:07:35.747960+00:00
@@ -14,20 +14,21 @@
 import os
 import django
 from django.apps import AppConfig
 from django.conf import settings
 
+
 class RunCreateFilters(AppConfig):
     """
     AppConfig class for creating reports in the pybirdai application.
 
     This class sets up the necessary context and runs a series of import
     and creation processes for SDD (Structured Data Definition) models,
     output layers, and report filters.
     """
 
-    path = os.path.join(settings.BASE_DIR, 'birds_nest')
+    path = os.path.join(settings.BASE_DIR, "birds_nest")
 
     @staticmethod
     def run_create_filters():
         """
         Executes the report creation process when the application is ready.
@@ -40,44 +41,32 @@
         5. Creates report filters
 
         The specific processes and their parameters are defined within the method.
         """
         from pybirdai.models.bird_meta_data_model import MAINTENANCE_AGENCY
-        from pybirdai.process_steps.input_model.import_database_to_sdd_model import (
-            ImportDatabaseToSDDModel
-        )
+        from pybirdai.process_steps.input_model.import_database_to_sdd_model import ImportDatabaseToSDDModel
         from pybirdai.context.sdd_context_django import SDDContext
         from pybirdai.context.context import Context
 
-        from pybirdai.process_steps.report_filters.create_output_layers import (
-            CreateOutputLayers
-        )
-        from pybirdai.process_steps.report_filters.create_report_filters import (
-            CreateReportFilters
-        )
-        from pybirdai.process_steps.input_model.import_database_to_sdd_model import (
-            ImportDatabaseToSDDModel
-        )
+        from pybirdai.process_steps.report_filters.create_output_layers import CreateOutputLayers
+        from pybirdai.process_steps.report_filters.create_report_filters import CreateReportFilters
+        from pybirdai.process_steps.input_model.import_database_to_sdd_model import ImportDatabaseToSDDModel
 
         base_dir = settings.BASE_DIR
 
         sdd_context = SDDContext()
-        sdd_context.file_directory = os.path.join(base_dir, 'resources')
-        sdd_context.output_directory = os.path.join(base_dir, 'results')
+        sdd_context.file_directory = os.path.join(base_dir, "resources")
+        sdd_context.output_directory = os.path.join(base_dir, "results")
 
         context = Context()
         context.file_directory = sdd_context.file_directory
         context.output_directory = sdd_context.output_directory
 
-        #ImportDatabaseToSDDModel().import_sdd(sdd_context)
+        # ImportDatabaseToSDDModel().import_sdd(sdd_context)
+
+        CreateOutputLayers().create_filters(context, sdd_context, "FINREP_REF", "3.0")
+        CreateReportFilters().create_report_filters(context, sdd_context, "FINREP_REF", "3.0")
 
 
-        CreateOutputLayers().create_filters(
-            context, sdd_context, "FINREP_REF", "3.0"
-        )
-        CreateReportFilters().create_report_filters(
-            context, sdd_context, "FINREP_REF", "3.0"
-        )
-
 def ready(self):
-        # This method is still needed for Django's AppConfig
-        pass
+    # This method is still needed for Django's AppConfig
+    pass
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/delete_bird_metadata_database.py	2025-08-02 18:37:08.442443+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/delete_bird_metadata_database.py	2025-09-21 17:07:35.748388+00:00
@@ -17,41 +17,35 @@
 
 import django
 from django.apps import AppConfig
 from django.conf import settings
 
+
 class RunDeleteBirdMetadataDatabase(AppConfig):
     """Django AppConfig for running the creation of generation rules."""
 
-    path = os.path.join(settings.BASE_DIR, 'birds_nest')
+    path = os.path.join(settings.BASE_DIR, "birds_nest")
 
     @staticmethod
     def run_delete_bird_metadata_database():
         """Execute the process of creating generation rules when the app is ready."""
 
         from pybirdai.context.sdd_context_django import SDDContext
         from pybirdai.context.context import Context
 
-        from pybirdai.process_steps.joins_meta_data.delete_joins_meta_data import (
-            TransformationMetaDataDestroyer
-        )
+        from pybirdai.process_steps.joins_meta_data.delete_joins_meta_data import TransformationMetaDataDestroyer
 
+        base_dir = settings.BASE_DIR
+        sdd_context = SDDContext()
+        sdd_context.file_directory = os.path.join(base_dir, "resources")
+        sdd_context.output_directory = os.path.join(base_dir, "results")
 
-        base_dir = settings.BASE_DIR 
-        sdd_context = SDDContext()
-        sdd_context.file_directory = os.path.join(base_dir, 'resources')
-        sdd_context.output_directory = os.path.join(base_dir, 'results')
-        
         context = Context()
         context.file_directory = sdd_context.file_directory
         context.output_directory = sdd_context.output_directory
 
+        TransformationMetaDataDestroyer().delete_bird_metadata_database(context, sdd_context, "FINREP_REF")
 
-        TransformationMetaDataDestroyer().delete_bird_metadata_database(
-            context,
-            sdd_context,
-            "FINREP_REF"
-        )
 
 def ready(self):
-        # This method is still needed for Django's AppConfig
-        pass
\ No newline at end of file
+    # This method is still needed for Django's AppConfig
+    pass
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/create_filters.py
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/delete_bird_metadata_database.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/create_joins_metadata.py	2025-08-02 18:37:08.442279+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/create_joins_metadata.py	2025-09-21 17:07:35.749176+00:00
@@ -17,56 +17,45 @@
 
 import django
 from django.apps import AppConfig
 from django.conf import settings
 
+
 class RunCreateJoinsMetadata(AppConfig):
     """Django AppConfig for running the creation of generation rules."""
 
-    path = os.path.join(settings.BASE_DIR, 'birds_nest')
+    path = os.path.join(settings.BASE_DIR, "birds_nest")
 
     @staticmethod
     def run_create_joins_meta_data():
         """Execute the process of creating generation rules when the app is ready."""
         print("Running create transformation metadata")
-        from pybirdai.process_steps.input_model.import_database_to_sdd_model import (
-            ImportDatabaseToSDDModel
-        )
+        from pybirdai.process_steps.input_model.import_database_to_sdd_model import ImportDatabaseToSDDModel
         from pybirdai.context.sdd_context_django import SDDContext
         from pybirdai.context.context import Context
 
         # from pybirdai.process_steps.joins_meta_data.create_joins_meta_data import (
         #     JoinsMetaDataCreator
         # )
-        from pybirdai.process_steps.joins_meta_data.create_joins_meta_data_combinations import (
-            JoinsMetaDataCreator
-        )
-        from pybirdai.process_steps.joins_meta_data.main_category_finder import (
-            MainCategoryFinder
-        )
+        from pybirdai.process_steps.joins_meta_data.create_joins_meta_data_combinations import JoinsMetaDataCreator
+        from pybirdai.process_steps.joins_meta_data.main_category_finder import MainCategoryFinder
 
         base_dir = settings.BASE_DIR
         sdd_context = SDDContext()
-        sdd_context.file_directory = os.path.join(base_dir, 'resources')
-        sdd_context.output_directory = os.path.join(base_dir, 'results')
+        sdd_context.file_directory = os.path.join(base_dir, "resources")
+        sdd_context.output_directory = os.path.join(base_dir, "results")
 
         context = Context()
         context.file_directory = sdd_context.file_directory
         context.output_directory = sdd_context.output_directory
 
-        #ImportDatabaseToSDDModel().import_sdd(sdd_context)
+        # ImportDatabaseToSDDModel().import_sdd(sdd_context)
 
         MainCategoryFinder().create_report_to_main_category_maps(
-            context,
-            sdd_context,
-            "FINREP_REF",
-            ["3", "3.0-Ind", "FINREP 3.0-Ind"]
+            context, sdd_context, "FINREP_REF", ["3", "3.0-Ind", "FINREP 3.0-Ind"]
         )
-        JoinsMetaDataCreator().generate_joins_meta_data(
-            context,
-            sdd_context,
-            "FINREP_REF"
-        )
+        JoinsMetaDataCreator().generate_joins_meta_data(context, sdd_context, "FINREP_REF")
+
 
 def ready(self):
-        # This method is still needed for Django's AppConfig
-        pass
+    # This method is still needed for Django's AppConfig
+    pass
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/create_joins_metadata.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/create_django_models.py	2025-08-02 18:37:08.441524+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/create_django_models.py	2025-09-21 17:07:35.751745+00:00
@@ -28,28 +28,28 @@
 )
 from pybirdai.process_steps.sqldeveloper_import.import_sqldev_ldm_to_django import (
     RegDNAToDJango,
 )
 
+
 class RunCreateDjangoModels(AppConfig):
     """AppConfig for creating Django models from SQL Developer Logical Data Model."""
 
-    path = os.path.join(settings.BASE_DIR, 'birds_nest')
-
+    path = os.path.join(settings.BASE_DIR, "birds_nest")
 
     def ready(self):
         """Prepare the context and run the import and conversion processes."""
         base_dir = settings.BASE_DIR
-        
+
         sdd_context = SDDContext()
-        sdd_context.file_directory = os.path.join(base_dir, 'resources')
-        sdd_context.output_directory = os.path.join(base_dir, 'results')
-        
+        sdd_context.file_directory = os.path.join(base_dir, "resources")
+        sdd_context.output_directory = os.path.join(base_dir, "results")
+
         context = Context()
         context.file_directory = sdd_context.file_directory
         context.output_directory = sdd_context.output_directory
-        if context.ldm_or_il == 'ldm':
+        if context.ldm_or_il == "ldm":
             SQLDevLDMImport.do_import(self, context)
         else:
             SQLDeveloperILImport.do_import(self, context)
 
         output_directory = "results" + os.sep + "xcore_output"
@@ -62,9 +62,9 @@
 
         RegDNAToDJango.convert(self, context)
 
         print("Django models created successfully.")
 
-if __name__ == '__main__':
+
+if __name__ == "__main__":
     django.setup()
-    RunCreateDjangoModels('pybirdai', 'birds_nest').ready()
-
+    RunCreateDjangoModels("pybirdai", "birds_nest").ready()
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/create_django_models.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/delete_joins.py	2025-08-02 18:37:08.442581+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/delete_joins.py	2025-09-21 17:07:35.759488+00:00
@@ -17,41 +17,35 @@
 
 import django
 from django.apps import AppConfig
 from django.conf import settings
 
+
 class RunDeleteBirdMetadataDatabase(AppConfig):
     """Django AppConfig for running the creation of generation rules."""
 
-    path = os.path.join(settings.BASE_DIR, 'birds_nest')
+    path = os.path.join(settings.BASE_DIR, "birds_nest")
 
     @staticmethod
     def run_joins():
         """Execute the process of creating generation rules when the app is ready."""
 
         from pybirdai.context.sdd_context_django import SDDContext
         from pybirdai.context.context import Context
 
-        from pybirdai.process_steps.joins_meta_data.delete_joins_meta_data import (
-            TransformationMetaDataDestroyer
-        )
+        from pybirdai.process_steps.joins_meta_data.delete_joins_meta_data import TransformationMetaDataDestroyer
 
+        base_dir = settings.BASE_DIR
+        sdd_context = SDDContext()
+        sdd_context.file_directory = os.path.join(base_dir, "resources")
+        sdd_context.output_directory = os.path.join(base_dir, "results")
 
-        base_dir = settings.BASE_DIR 
-        sdd_context = SDDContext()
-        sdd_context.file_directory = os.path.join(base_dir, 'resources')
-        sdd_context.output_directory = os.path.join(base_dir, 'results')
-        
         context = Context()
         context.file_directory = sdd_context.file_directory
         context.output_directory = sdd_context.output_directory
 
+        TransformationMetaDataDestroyer().delete_joins_meta_data(context, sdd_context, "FINREP_REF")
 
-        TransformationMetaDataDestroyer().delete_joins_meta_data(
-            context,
-            sdd_context,
-            "FINREP_REF"
-        )
 
 def ready(self):
-        # This method is still needed for Django's AppConfig
-        pass
\ No newline at end of file
+    # This method is still needed for Django's AppConfig
+    pass
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/delete_joins.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/delete_joins_metadata.py	2025-08-02 18:37:08.442757+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/delete_joins_metadata.py	2025-09-21 17:07:35.762100+00:00
@@ -17,41 +17,35 @@
 
 import django
 from django.apps import AppConfig
 from django.conf import settings
 
+
 class RunDeleteJoinsMetadata(AppConfig):
     """Django AppConfig for running the creation of generation rules."""
 
-    path = os.path.join(settings.BASE_DIR, 'birds_nest')
+    path = os.path.join(settings.BASE_DIR, "birds_nest")
 
     @staticmethod
     def run_delete_joins_meta_data():
         """Execute the process of creating generation rules when the app is ready."""
 
         from pybirdai.context.sdd_context_django import SDDContext
         from pybirdai.context.context import Context
 
-        from pybirdai.process_steps.joins_meta_data.delete_joins_meta_data import (
-            TransformationMetaDataDestroyer
-        )
+        from pybirdai.process_steps.joins_meta_data.delete_joins_meta_data import TransformationMetaDataDestroyer
 
+        base_dir = settings.BASE_DIR
+        sdd_context = SDDContext()
+        sdd_context.file_directory = os.path.join(base_dir, "resources")
+        sdd_context.output_directory = os.path.join(base_dir, "results")
 
-        base_dir = settings.BASE_DIR 
-        sdd_context = SDDContext()
-        sdd_context.file_directory = os.path.join(base_dir, 'resources')
-        sdd_context.output_directory = os.path.join(base_dir, 'results')
-        
         context = Context()
         context.file_directory = sdd_context.file_directory
         context.output_directory = sdd_context.output_directory
 
+        TransformationMetaDataDestroyer().delete_joins_meta_data(context, sdd_context, "FINREP_REF")
 
-        TransformationMetaDataDestroyer().delete_joins_meta_data(
-            context,
-            sdd_context,
-            "FINREP_REF"
-        )
 
 def ready(self):
-        # This method is still needed for Django's AppConfig
-        pass
\ No newline at end of file
+    # This method is still needed for Django's AppConfig
+    pass
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/delete_output_concepts.py	2025-08-02 18:37:08.442894+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/delete_output_concepts.py	2025-09-21 17:07:35.763651+00:00
@@ -17,41 +17,35 @@
 
 import django
 from django.apps import AppConfig
 from django.conf import settings
 
+
 class RunDeleteOutputConcepts(AppConfig):
     """Django AppConfig for running the creation of generation rules."""
 
-    path = os.path.join(settings.BASE_DIR, 'birds_nest')
+    path = os.path.join(settings.BASE_DIR, "birds_nest")
 
     @staticmethod
     def run_delete_output_concepts():
         """Execute the process of creating generation rules when the app is ready."""
 
         from pybirdai.context.sdd_context_django import SDDContext
         from pybirdai.context.context import Context
 
-        from pybirdai.process_steps.joins_meta_data.delete_joins_meta_data import (
-            TransformationMetaDataDestroyer
-        )
+        from pybirdai.process_steps.joins_meta_data.delete_joins_meta_data import TransformationMetaDataDestroyer
 
+        base_dir = settings.BASE_DIR
+        sdd_context = SDDContext()
+        sdd_context.file_directory = os.path.join(base_dir, "resources")
+        sdd_context.output_directory = os.path.join(base_dir, "results")
 
-        base_dir = settings.BASE_DIR 
-        sdd_context = SDDContext()
-        sdd_context.file_directory = os.path.join(base_dir, 'resources')
-        sdd_context.output_directory = os.path.join(base_dir, 'results')
-        
         context = Context()
         context.file_directory = sdd_context.file_directory
         context.output_directory = sdd_context.output_directory
 
+        TransformationMetaDataDestroyer().delete_output_concepts(context, sdd_context, "FINREP_REF")
 
-        TransformationMetaDataDestroyer().delete_output_concepts(
-            context,
-            sdd_context,
-            "FINREP_REF"
-        )
 
 def ready(self):
-        # This method is still needed for Django's AppConfig
-        pass
\ No newline at end of file
+    # This method is still needed for Django's AppConfig
+    pass
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/delete_joins_metadata.py
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/delete_output_concepts.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/execute_datapoint.py	2025-08-02 18:37:08.443330+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/execute_datapoint.py	2025-09-21 17:07:35.764367+00:00
@@ -16,20 +16,21 @@
 import os
 from django.apps import AppConfig
 from pybirdai.context.sdd_context_django import SDDContext
 from django.conf import settings
 
+
 class RunExecuteDataPoint(AppConfig):
     """
     Django AppConfig for running the website to SDD model conversion process.
 
     This class sets up the necessary context and runs the import process
     to convert website data into an SDD  model.
     """
 
-    path = os.path.join(settings.BASE_DIR, 'birds_nest')
-    #path = os.path.join(settings.BASE_DIR, 'birds_nest')
+    path = os.path.join(settings.BASE_DIR, "birds_nest")
+    # path = os.path.join(settings.BASE_DIR, 'birds_nest')
 
     @staticmethod
     def run_execute_data_point(data_point_id):
         """
         Prepare and execute the website to SDD model conversion process.
@@ -37,34 +38,23 @@
         This method sets up the necessary contexts, creates reference domains
         and variables, and imports the website data into the SDD model.
         """
         from pybirdai.models.bird_meta_data_model import MAINTENANCE_AGENCY
 
-        from pybirdai.process_steps.pybird.execute_datapoint import (
-            ExecuteDataPoint
-        )
+        from pybirdai.process_steps.pybird.execute_datapoint import ExecuteDataPoint
         from pybirdai.context.context import Context
 
         base_dir = settings.BASE_DIR
         sdd_context = SDDContext()
-        sdd_context.file_directory = os.path.join(base_dir, 'resources')
-        sdd_context.output_directory = os.path.join(base_dir, 'results')
-        
+        sdd_context.file_directory = os.path.join(base_dir, "resources")
+        sdd_context.output_directory = os.path.join(base_dir, "results")
+
         context = Context()
         context.file_directory = sdd_context.file_directory
         context.output_directory = sdd_context.output_directory
 
         # Create reference domains, variables, and cubes
-        return ExecuteDataPoint.execute_data_point(
-             data_point_id
-        )
+        return ExecuteDataPoint.execute_data_point(data_point_id)
 
     def ready(self):
         # This method is still needed for Django's AppConfig
         pass
-
-       
-
-
-
-      
-    
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/execute_datapoint.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/delete_semantic_integrations.py	2025-08-02 18:37:08.443029+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/delete_semantic_integrations.py	2025-09-21 17:07:35.765714+00:00
@@ -17,41 +17,35 @@
 
 import django
 from django.apps import AppConfig
 from django.conf import settings
 
+
 class RunDeleteSemanticIntegrations(AppConfig):
     """Django AppConfig for running the creation of generation rules."""
 
-    path = os.path.join(settings.BASE_DIR, 'birds_nest')
+    path = os.path.join(settings.BASE_DIR, "birds_nest")
 
     @staticmethod
     def run_delete_semantic_integrations():
         """Execute the process of creating generation rules when the app is ready."""
 
         from pybirdai.context.sdd_context_django import SDDContext
         from pybirdai.context.context import Context
 
-        from pybirdai.process_steps.joins_meta_data.delete_joins_meta_data import (
-            TransformationMetaDataDestroyer
-        )
+        from pybirdai.process_steps.joins_meta_data.delete_joins_meta_data import TransformationMetaDataDestroyer
 
+        base_dir = settings.BASE_DIR
+        sdd_context = SDDContext()
+        sdd_context.file_directory = os.path.join(base_dir, "resources")
+        sdd_context.output_directory = os.path.join(base_dir, "results")
 
-        base_dir = settings.BASE_DIR 
-        sdd_context = SDDContext()
-        sdd_context.file_directory = os.path.join(base_dir, 'resources')
-        sdd_context.output_directory = os.path.join(base_dir, 'results')
-        
         context = Context()
         context.file_directory = sdd_context.file_directory
         context.output_directory = sdd_context.output_directory
 
+        TransformationMetaDataDestroyer().delete_semantic_integration_meta_data(context, sdd_context, "FINREP_REF")
 
-        TransformationMetaDataDestroyer().delete_semantic_integration_meta_data(
-            context,
-            sdd_context,
-            "FINREP_REF"
-        )
 
 def ready(self):
-        # This method is still needed for Django's AppConfig
-        pass
\ No newline at end of file
+    # This method is still needed for Django's AppConfig
+    pass
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/delete_semantic_integrations.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/dpm_output_layer_creation.py	2025-08-02 18:37:08.443186+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/dpm_output_layer_creation.py	2025-09-21 17:07:35.771811+00:00
@@ -18,25 +18,23 @@
 from django.apps import AppConfig
 from pybirdai.context.sdd_context_django import SDDContext
 from django.conf import settings
 import logging
 
+
 class RunDPMOutputLayerCreation(AppConfig):
     """
     Django AppConfig for running the DPM output layer creation process.
 
     This entry point creates output layers (CUBE, CUBE_STRUCTURE, COMBINATION, etc.)
     from DPM table data, supporting any framework (FINREP, COREP, AE, etc.).
     """
 
-    path = os.path.join(settings.BASE_DIR, 'birds_nest')
+    path = os.path.join(settings.BASE_DIR, "birds_nest")
 
     @staticmethod
-    def run_creation(
-        framework: str = "",
-        version: str = "",
-        table_code: str = ""):
+    def run_creation(framework: str = "", version: str = "", table_code: str = ""):
         """
         Run the output layer creation process.
 
         This is the entry point that delegates to the business logic layer.
         Supports four different processing modes:
@@ -57,12 +55,12 @@
         from pybirdai.context.context import Context
 
         # Set up context
         base_dir = settings.BASE_DIR
         sdd_context = SDDContext()
-        sdd_context.file_directory = os.path.join(base_dir, 'results')
-        sdd_context.output_directory = os.path.join(base_dir, 'results')
+        sdd_context.file_directory = os.path.join(base_dir, "results")
+        sdd_context.output_directory = os.path.join(base_dir, "results")
 
         context = Context()
         context.file_directory = sdd_context.output_directory
         context.output_directory = sdd_context.output_directory
 
@@ -73,40 +71,37 @@
         try:
             if framework and version:
                 # Mode 1: Framework + Version
                 logging.info(f"Processing framework '{framework}' version '{version}'")
                 return creator.process_by_framework_version(framework, version, save_to_db=True)
-                
+
             elif framework and not version:
                 # Mode 2: Framework only (all versions)
                 logging.info(f"Processing all versions of framework '{framework}'")
                 return creator.process_by_framework(framework, save_to_db=True)
-                
+
             elif not framework and table_code and version:
                 # Mode 3: Table code + Version
                 logging.info(f"Processing table '{table_code}' version '{version}'")
                 return creator.process_by_table_code_version(table_code, version, save_to_db=True)
-                
+
             elif not framework and table_code and not version:
                 # Mode 4: Table code only (all versions)
                 logging.info(f"Processing all versions of table '{table_code}'")
                 return creator.process_by_table_code(table_code, save_to_db=True)
-                
+
             else:
                 # Invalid parameter combination
                 return {
-                    'status': 'error',
-                    'message': 'Invalid parameters. Please provide either: '
-                               '1) framework and optionally version, or '
-                               '2) table_code and optionally version'
+                    "status": "error",
+                    "message": "Invalid parameters. Please provide either: "
+                    "1) framework and optionally version, or "
+                    "2) table_code and optionally version",
                 }
-                
+
         except Exception as e:
             logging.error(f"Error during output layer creation: {str(e)}")
-            return {
-                'status': 'error',
-                'message': f'Unexpected error: {str(e)}'
-            }
+            return {"status": "error", "message": f"Unexpected error: {str(e)}"}
 
     def ready(self):
         # This method is still needed for Django's AppConfig
-        pass
\ No newline at end of file
+        pass
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/dpm_output_layer_creation.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/import_input_model.py	2025-08-02 18:37:08.444224+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/import_input_model.py	2025-09-21 17:07:35.774228+00:00
@@ -16,51 +16,47 @@
 import os
 from django.apps import AppConfig
 from pybirdai.context.sdd_context_django import SDDContext
 from django.conf import settings
 
+
 class RunImportInputModelFromSQLDev(AppConfig):
     """
     Django AppConfig for running the website to SDD model conversion process.
 
     This class sets up the necessary context and runs the import process
     to convert website data into an SDD  model.
     """
 
-    path = os.path.join(settings.BASE_DIR, 'birds_nest')
-    #path = os.path.join(settings.BASE_DIR, 'birds_nest')
-
+    path = os.path.join(settings.BASE_DIR, "birds_nest")
+    # path = os.path.join(settings.BASE_DIR, 'birds_nest')
 
     def ready(self):
         """
         Prepare and execute the website to SDD model conversion process.
 
         This method sets up the necessary contexts, creates reference domains
         and variables, and imports the website data into the SDD model.
         """
         from pybirdai.models.bird_meta_data_model import MAINTENANCE_AGENCY
 
-        from pybirdai.process_steps.input_model.import_input_model import (
-            ImportInputModel
-        )
+        from pybirdai.process_steps.input_model.import_input_model import ImportInputModel
         from pybirdai.context.context import Context
+
         # from pybirdai.context.context_ancrdt import Context
 
         base_dir = settings.BASE_DIR
         sdd_context = SDDContext()
-        sdd_context.file_directory = os.path.join(base_dir, 'resources')
-        sdd_context.output_directory = os.path.join(base_dir, 'results')
+        sdd_context.file_directory = os.path.join(base_dir, "resources")
+        sdd_context.output_directory = os.path.join(base_dir, "results")
 
         context = Context()
         context.file_directory = sdd_context.file_directory
         context.output_directory = sdd_context.output_directory
 
         # Create reference domains, variables, and cubes
-        ImportInputModel.import_input_model(
-             sdd_context, context
-        )
+        ImportInputModel.import_input_model(sdd_context, context)
 
 
-
-if __name__ == '__main__':
+if __name__ == "__main__":
     django.setup()
-    RunImportInputModelFromSQLDev('pybirdai', 'birds_nest').ready()
+    RunImportInputModelFromSQLDev("pybirdai", "birds_nest").ready()
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/import_input_model.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/import_semantic_integrations_from_website.py	2025-08-02 18:37:08.444519+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/import_semantic_integrations_from_website.py	2025-09-21 17:07:35.777232+00:00
@@ -16,19 +16,20 @@
 import os
 from django.apps import AppConfig
 from pybirdai.context.sdd_context_django import SDDContext
 from django.conf import settings
 
+
 class RunImportSemanticIntegrationsFromWebsite(AppConfig):
     """
     Django AppConfig for running the website to SDD model conversion process.
 
     This class sets up the necessary context and runs the import process
     to convert website data into an SDD  model.
     """
 
-    path = os.path.join(settings.BASE_DIR, 'birds_nest')
+    path = os.path.join(settings.BASE_DIR, "birds_nest")
 
     @staticmethod
     def import_mappings_from_website():
         """
         Prepare and execute the website to SDD model conversion process.
@@ -36,31 +37,26 @@
         This method sets up the necessary contexts, creates reference domains
         and variables, and imports the website data into the SDD model.
         """
         from pybirdai.models.bird_meta_data_model import MAINTENANCE_AGENCY
         from pybirdai.process_steps.website_to_sddmodel.import_website_to_sdd_model_django import (
-            ImportWebsiteToSDDModel
+            ImportWebsiteToSDDModel,
         )
-        from pybirdai.process_steps.input_model.import_database_to_sdd_model import (
-            ImportDatabaseToSDDModel
-        )
+        from pybirdai.process_steps.input_model.import_database_to_sdd_model import ImportDatabaseToSDDModel
         from pybirdai.context.context import Context
 
         base_dir = settings.BASE_DIR
         sdd_context = SDDContext()
-        sdd_context.file_directory = os.path.join(base_dir, 'resources')
-        sdd_context.output_directory = os.path.join(base_dir, 'results')
-        
+        sdd_context.file_directory = os.path.join(base_dir, "resources")
+        sdd_context.output_directory = os.path.join(base_dir, "results")
+
         context = Context()
         context.file_directory = sdd_context.file_directory
         context.output_directory = sdd_context.output_directory
 
         # ImportDatabaseToSDDModel().import_sdd(sdd_context)
         ImportWebsiteToSDDModel().import_semantic_integrations_from_sdd(sdd_context)
 
+
 def ready(self):
-        # This method is still needed for Django's AppConfig
-        pass
-
-
-      
-    
+    # This method is still needed for Django's AppConfig
+    pass
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/import_report_templates_from_website.py	2025-08-02 18:37:08.444378+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/import_report_templates_from_website.py	2025-09-21 17:07:35.777554+00:00
@@ -19,43 +19,45 @@
 from pybirdai.context.sdd_context_django import SDDContext
 from django.conf import settings
 
 logger = logging.getLogger(__name__)
 
+
 class RunImportReportTemplatesFromWebsite(AppConfig):
     """
     Django AppConfig for running the website to SDD model conversion process.
 
     This class sets up the necessary context and runs the import process
     to convert website data into an SDD  model.
     """
 
-    path = os.path.join(settings.BASE_DIR, 'birds_nest')
+    path = os.path.join(settings.BASE_DIR, "birds_nest")
 
     @staticmethod
     def run_import():
         # Move the content of the ready() method here
         from pybirdai.process_steps.website_to_sddmodel.import_website_to_sdd_model_django import (
-            ImportWebsiteToSDDModel
+            ImportWebsiteToSDDModel,
         )
         from pybirdai.context.context import Context
         from django.conf import settings
 
         base_dir = settings.BASE_DIR
         sdd_context = SDDContext()
-        sdd_context.file_directory = os.path.join(base_dir, 'resources')
-        sdd_context.output_directory = os.path.join(base_dir, 'results')
-        
+        sdd_context.file_directory = os.path.join(base_dir, "resources")
+        sdd_context.output_directory = os.path.join(base_dir, "results")
+
         context = Context()
         context.file_directory = sdd_context.file_directory
         context.output_directory = sdd_context.output_directory
 
         if not sdd_context.exclude_reference_info_from_website:
             ImportWebsiteToSDDModel().import_report_templates_from_sdd(sdd_context)
-            
+
             # Download REF_FINREP report template HTML files from GitHub
             from pybirdai.utils.github_file_fetcher import GitHubFileFetcher
+
             logger.info("Downloading REF_FINREP report template HTML files from GitHub")
             fetcher = GitHubFileFetcher("https://github.com/regcommunity/FreeBIRD")
             fetcher.fetch_report_template_htmls()
 
     def ready(self):
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/import_semantic_integrations_from_website.py
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/import_report_templates_from_website.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/create_executable_joins.py	2025-08-02 18:37:08.441725+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/create_executable_joins.py	2025-09-21 17:07:35.777725+00:00
@@ -17,73 +17,69 @@
 
 import django
 from django.apps import AppConfig
 from django.conf import settings
 
+
 class RunCreateExecutableJoins(AppConfig):
     """Django AppConfig for running the creation of generation rules."""
 
-    path = os.path.join(settings.BASE_DIR, 'birds_nest')
+    path = os.path.join(settings.BASE_DIR, "birds_nest")
 
     @staticmethod
     def create_python_joins():
         """Execute the process of creating generation rules when the app is ready."""
-        from pybirdai.process_steps.input_model.import_database_to_sdd_model import (
-            ImportDatabaseToSDDModel
-        )
+        from pybirdai.process_steps.input_model.import_database_to_sdd_model import ImportDatabaseToSDDModel
         from pybirdai.context.sdd_context_django import SDDContext
         from pybirdai.context.context import Context
-        from pybirdai.process_steps.pybird.create_python_django_transformations import (
-            CreatePythonTransformations
-        )
+        from pybirdai.process_steps.pybird.create_python_django_transformations import CreatePythonTransformations
 
-        base_dir = settings.BASE_DIR 
+        base_dir = settings.BASE_DIR
         sdd_context = SDDContext()
-        sdd_context.file_directory = os.path.join(base_dir, 'resources')
-        sdd_context.output_directory = os.path.join(base_dir, 'results')
-        
+        sdd_context.file_directory = os.path.join(base_dir, "resources")
+        sdd_context.output_directory = os.path.join(base_dir, "results")
+
         context = Context()
         context.file_directory = sdd_context.file_directory
         context.output_directory = sdd_context.output_directory
 
-        #ImportDatabaseToSDDModel().import_sdd(sdd_context)
+        # ImportDatabaseToSDDModel().import_sdd(sdd_context)
         CreatePythonTransformations().create_python_joins(context, sdd_context)
 
     @staticmethod
     def create_python_joins_from_db():
         """Execute the process of creating generation rules from the database when the app is ready."""
-        from pybirdai.process_steps.input_model.import_database_to_sdd_model import (
-            ImportDatabaseToSDDModel
-        )
+        from pybirdai.process_steps.input_model.import_database_to_sdd_model import ImportDatabaseToSDDModel
         from pybirdai.context.sdd_context_django import SDDContext
         from pybirdai.context.context import Context
-        from pybirdai.process_steps.pybird.create_python_django_transformations import (
-            CreatePythonTransformations
-        )
+        from pybirdai.process_steps.pybird.create_python_django_transformations import CreatePythonTransformations
 
-        base_dir = settings.BASE_DIR 
+        base_dir = settings.BASE_DIR
         sdd_context = SDDContext()
-        sdd_context.file_directory = os.path.join(base_dir, 'resources')
-        sdd_context.output_directory = os.path.join(base_dir, 'results')
-        
+        sdd_context.file_directory = os.path.join(base_dir, "resources")
+        sdd_context.output_directory = os.path.join(base_dir, "results")
+
         context = Context()
         context.file_directory = sdd_context.file_directory
         context.output_directory = sdd_context.output_directory
 
         # Only import the necessary tables for joins
         importer = ImportDatabaseToSDDModel()
-        
-        importer.import_sdd_for_joins(sdd_context, [
-            'MAINTENANCE_AGENCY',
-            'DOMAIN',
-            'VARIABLE',
-            'CUBE',
-            'CUBE_STRUCTURE',
-            'CUBE_STRUCTURE_ITEM',
-            'CUBE_LINK',
-            'CUBE_STRUCTURE_ITEM_LINK'
-        ])
+
+        importer.import_sdd_for_joins(
+            sdd_context,
+            [
+                "MAINTENANCE_AGENCY",
+                "DOMAIN",
+                "VARIABLE",
+                "CUBE",
+                "CUBE_STRUCTURE",
+                "CUBE_STRUCTURE_ITEM",
+                "CUBE_LINK",
+                "CUBE_STRUCTURE_ITEM_LINK",
+            ],
+        )
         CreatePythonTransformations().create_python_joins(context, sdd_context)
 
     def ready(self):
         # This method is still needed for Django's AppConfig
-        pass
\ No newline at end of file
+        pass
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/import_export_mapping_join_metadata.py	2025-09-15 13:18:11.372483+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/import_export_mapping_join_metadata.py	2025-09-21 17:07:35.778714+00:00
@@ -19,31 +19,34 @@
 from django.conf import settings
 from pybirdai.process_steps.import_export_join_metadata.export_join_metadata import ExporterJoins
 from pybirdai.process_steps.import_export_join_metadata.import_join_metadata import ImporterJoins
 from pybirdai.process_steps.mapping_join_metadata_eil_ldm.mapping_join_eil_ldm import LinkProcessor
 
+
 class RunExporterJoins(AppConfig):
     """Django AppConfig for running the creation of generation rules."""
 
-    path = os.path.join(settings.BASE_DIR, 'birds_nest')
+    path = os.path.join(settings.BASE_DIR, "birds_nest")
 
     @staticmethod
     def run_export_joins_meta_data():
         ExporterJoins.handle()
 
+
 class RunImporterJoins(AppConfig):
     """Django AppConfig for running the creation of generation rules."""
 
-    path = os.path.join(settings.BASE_DIR, 'birds_nest')
+    path = os.path.join(settings.BASE_DIR, "birds_nest")
 
     @staticmethod
     def run_import_joins_meta_data():
         ImporterJoins.handle()
 
+
 class RunMappingJoinsEIL_LDM(AppConfig):
     """Django AppConfig for running the creation of generation rules."""
 
-    path = os.path.join(settings.BASE_DIR, 'birds_nest')
+    path = os.path.join(settings.BASE_DIR, "birds_nest")
 
     @staticmethod
     def run_mapping_joins_meta_data():
         LinkProcessor.handle()
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/create_executable_joins.py
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/import_export_mapping_join_metadata.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/import_dpm_data.py	2025-09-18 09:16:40.852694+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/import_dpm_data.py	2025-09-21 17:07:35.779468+00:00
@@ -17,35 +17,35 @@
 from django.apps import AppConfig
 from pybirdai.context.sdd_context_django import SDDContext
 from django.conf import settings
 import logging
 
+
 class RunImportDPMData(AppConfig):
     """
     Django AppConfig for running the DPM data import process.
 
     This class sets up the necessary context and runs the import process
     to download and convert DPM data from the EBA website into SDD format.
     """
 
-    path = os.path.join(settings.BASE_DIR, 'birds_nest')
+    path = os.path.join(settings.BASE_DIR, "birds_nest")
 
     @staticmethod
-    def run_import(import_:bool):
+    def run_import(import_: bool):
         # Import and run the DPM integration service
         from pybirdai.process_steps.dpm_integration.dpm_integration_service import DPMImporterService
         from pybirdai.process_steps.website_to_sddmodel.import_website_to_sdd_model_django import (
-            ImportWebsiteToSDDModel
+            ImportWebsiteToSDDModel,
         )
         from pybirdai.context.context import Context
         from django.conf import settings
 
         base_dir = settings.BASE_DIR
         sdd_context = SDDContext()
-        sdd_context.file_directory = os.path.join(base_dir, 'results')
-        sdd_context.output_directory = os.path.join(base_dir, 'results')
-
+        sdd_context.file_directory = os.path.join(base_dir, "results")
+        sdd_context.output_directory = os.path.join(base_dir, "results")
 
         context = Context()
         context.file_directory = sdd_context.output_directory
         context.output_directory = sdd_context.output_directory
 
@@ -56,10 +56,10 @@
             dpm_service.run_application()
 
         # After DPM import, run the report templates import
         if import_:
             logging.info("Running Import on the DPM Metadata")
-            ImportWebsiteToSDDModel().import_report_templates_from_sdd(sdd_context,dpm=True)
+            ImportWebsiteToSDDModel().import_report_templates_from_sdd(sdd_context, dpm=True)
 
     def ready(self):
         # This method is still needed for Django's AppConfig
         pass
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/import_dpm_data.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/import_hierarchy_analysis_from_website.py	2025-08-02 18:37:08.443814+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/import_hierarchy_analysis_from_website.py	2025-09-21 17:07:35.781321+00:00
@@ -16,19 +16,20 @@
 import os
 from django.apps import AppConfig
 from pybirdai.context.sdd_context_django import SDDContext
 from django.conf import settings
 
+
 class RunImportHierarchiesFromWebsite(AppConfig):
     """
     Django AppConfig for running the website to SDD model conversion process.
 
     This class sets up the necessary context and runs the import process
     to convert website data into an SDD  model.
     """
 
-    path = os.path.join(settings.BASE_DIR, 'birds_nest')
+    path = os.path.join(settings.BASE_DIR, "birds_nest")
 
     @staticmethod
     def import_hierarchies():
         """
         Prepare and execute the website to SDD model conversion process.
@@ -36,35 +37,30 @@
         This method sets up the necessary contexts, creates reference domains
         and variables, and imports the website data into the SDD model.
         """
         from pybirdai.models.bird_meta_data_model import MAINTENANCE_AGENCY
         from pybirdai.process_steps.website_to_sddmodel.import_website_to_sdd_model_django import (
-            ImportWebsiteToSDDModel
+            ImportWebsiteToSDDModel,
         )
-        from pybirdai.process_steps.input_model.import_database_to_sdd_model import (
-            ImportDatabaseToSDDModel
-        )
+        from pybirdai.process_steps.input_model.import_database_to_sdd_model import ImportDatabaseToSDDModel
         from pybirdai.context.context import Context
 
         base_dir = settings.BASE_DIR
         sdd_context = SDDContext()
-        sdd_context.file_directory = os.path.join(base_dir, 'resources')
-        sdd_context.output_directory = os.path.join(base_dir, 'results')
-        
+        sdd_context.file_directory = os.path.join(base_dir, "resources")
+        sdd_context.output_directory = os.path.join(base_dir, "results")
+
         context = Context()
         context.file_directory = sdd_context.file_directory
         context.output_directory = sdd_context.output_directory
 
-        #ImportDatabaseToSDDModel().import_sdd(sdd_context)
+        # ImportDatabaseToSDDModel().import_sdd(sdd_context)
         ImportWebsiteToSDDModel().import_hierarchies_from_sdd(sdd_context)
 
     def ready(self):
         # This method is still needed for Django's AppConfig
         pass
 
-if __name__ == '__main__':
+
+if __name__ == "__main__":
     django.setup()
-    RunImportHierarchiesFromWebsite('pybirdai', 'birds_nest').ready()
-
-
-      
-    
+    RunImportHierarchiesFromWebsite("pybirdai", "birds_nest").ready()
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/import_hierarchy_analysis_from_website.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/ancrdt_transformation.py	2025-09-15 13:18:11.371988+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/ancrdt_transformation.py	2025-09-21 17:07:35.780916+00:00
@@ -20,15 +20,12 @@
 import logging
 
 # Configure logging
 logging.basicConfig(
     level=logging.INFO,
-    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
-    handlers=[
-        logging.FileHandler("log.log"),
-        logging.StreamHandler()
-    ]
+    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
+    handlers=[logging.FileHandler("log.log"), logging.StreamHandler()],
 )
 
 logger = logging.getLogger(__name__)
 
 
@@ -40,11 +37,11 @@
     1. Import ANCRDT data
     2. Create joins metadata
     3. Create executable joins
     """
 
-    path = os.path.join(settings.BASE_DIR, 'birds_nest')
+    path = os.path.join(settings.BASE_DIR, "birds_nest")
 
     @staticmethod
     def run_step_0_fetch_ancrdt_csv():
         """Step 0: Fetch ANCRDT CSV data from ECB website"""
         logger.info("Starting ANCRDT Step 0: Fetch CSV data from ECB website")
@@ -59,16 +56,17 @@
                 output_dir="results/ancrdt_csv",
                 format_type="csv",
                 include_mapping_content=False,
                 include_rendering_content=False,
                 include_transformation_content=False,
-                only_currently_valid_metadata=False
+                only_currently_valid_metadata=False,
             )
             logger.info(f"ANCRDT Step 0 completed successfully. Data saved to: {output_dir}")
             return True
         except Exception as e:
             import traceback
+
             error_detail = traceback.format_exc()
             logger.error(f"ANCRDT Step 0 failed: {str(e)}\nDetails: {error_detail}")
             raise Exception(f"ANCRDT Step 0 failed: {str(e) or 'Unknown error occurred'}")
 
     @staticmethod
@@ -82,49 +80,50 @@
             RunANCRDTImport.run_import()
             logger.info("ANCRDT Step 1 completed successfully")
             return True
         except Exception as e:
             import traceback
+
             error_detail = traceback.format_exc()
             logger.error(f"ANCRDT Step 1 failed: {str(e)}\nDetails: {error_detail}")
             raise Exception(f"ANCRDT Step 1 failed: {str(e) or 'Unknown error occurred'}")
 
     @staticmethod
     def run_step_2_joins_metadata():
         """Step 2: Create joins metadata"""
         logger.info("Starting ANCRDT Step 2: Create joins metadata")
 
         from pybirdai.process_steps.ancrdt_transformation.create_joins_meta_data_ancrdt import (
-            JoinsMetaDataCreatorANCRDT
+            JoinsMetaDataCreatorANCRDT,
         )
 
         try:
             creator = JoinsMetaDataCreatorANCRDT()
             creator.generate_joins_meta_data()
             logger.info("ANCRDT Step 2 completed successfully")
             return True
         except Exception as e:
             import traceback
+
             error_detail = traceback.format_exc()
             logger.error(f"ANCRDT Step 2 failed: {str(e)}\nDetails: {error_detail}")
             raise Exception(f"ANCRDT Step 2 failed: {str(e) or 'Unknown error occurred'}")
 
     @staticmethod
     def run_step_3_executable_joins():
         """Step 3: Create executable joins"""
         logger.info("Starting ANCRDT Step 3: Create executable joins")
 
-        from pybirdai.process_steps.ancrdt_transformation.create_executable_joins_ancrdt import (
-            RunCreateExecutableJoins
-        )
+        from pybirdai.process_steps.ancrdt_transformation.create_executable_joins_ancrdt import RunCreateExecutableJoins
 
         try:
             RunCreateExecutableJoins.create_python_joins_from_db(logger=logger)
             logger.info("ANCRDT Step 3 completed successfully")
             return True
         except Exception as e:
             import traceback
+
             error_detail = traceback.format_exc()
             logger.error(f"ANCRDT Step 3 failed: {str(e)}\nDetails: {error_detail}")
             raise Exception(f"ANCRDT Step 3 failed: {str(e) or 'Unknown error occurred'}")
 
     @staticmethod
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/ancrdt_transformation.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/upload_sqldev_eldm_files.py	2025-08-02 18:37:08.445686+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/upload_sqldev_eldm_files.py	2025-09-21 17:07:35.789717+00:00
@@ -16,51 +16,43 @@
 import os
 from django.apps import AppConfig
 from pybirdai.context.sdd_context_django import SDDContext
 from django.conf import settings
 
+
 class UploadSQLDevELDMFiles(AppConfig):
     """
     Django AppConfig for running the website to SDD model conversion process.
 
     This class sets up the necessary context and runs the import process
     to convert website data into an SDD  model.
     """
 
-    path = os.path.join(settings.BASE_DIR, 'birds_nest')
+    path = os.path.join(settings.BASE_DIR, "birds_nest")
 
     @staticmethod
     def upload_sqldev_eldm_files(request):
         """
         Prepare and execute the website to SDD model conversion process.
 
         This method sets up the necessary contexts, creates reference domains
         and variables, and imports the website data into the SDD model.
         """
         from pybirdai.models.bird_meta_data_model import MAINTENANCE_AGENCY
-        from pybirdai.process_steps.upload_files.file_uploader import (
-            FileUploader
-        )
-       
+        from pybirdai.process_steps.upload_files.file_uploader import FileUploader
+
         from pybirdai.context.context import Context
 
         base_dir = settings.BASE_DIR
         sdd_context = SDDContext()
-        sdd_context.file_directory = os.path.join(base_dir, 'resources')
-        sdd_context.output_directory = os.path.join(base_dir, 'results')
-        
+        sdd_context.file_directory = os.path.join(base_dir, "resources")
+        sdd_context.output_directory = os.path.join(base_dir, "results")
+
         context = Context()
         context.file_directory = sdd_context.file_directory
         context.output_directory = sdd_context.output_directory
 
         FileUploader().upload_sqldev_eldm_files(sdd_context, request)
-        
 
     def ready(self):
         # This method is still needed for Django's AppConfig
         pass
-
-
-
-
-      
-    
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/upload_sqldev_eldm_files.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/upload_technical_export_files.py	2025-08-02 18:37:08.445835+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/upload_technical_export_files.py	2025-09-21 17:07:35.790495+00:00
@@ -16,51 +16,43 @@
 import os
 from django.apps import AppConfig
 from pybirdai.context.sdd_context_django import SDDContext
 from django.conf import settings
 
+
 class UploadTechnicalExportFiles(AppConfig):
     """
     Django AppConfig for running the website to SDD model conversion process.
 
     This class sets up the necessary context and runs the import process
     to convert website data into an SDD  model.
     """
 
-    path = os.path.join(settings.BASE_DIR, 'birds_nest')
+    path = os.path.join(settings.BASE_DIR, "birds_nest")
 
     @staticmethod
     def upload_technical_export_files(request):
         """
         Prepare and execute the website to SDD model conversion process.
 
         This method sets up the necessary contexts, creates reference domains
         and variables, and imports the website data into the SDD model.
         """
         from pybirdai.models.bird_meta_data_model import MAINTENANCE_AGENCY
-        from pybirdai.process_steps.upload_files.file_uploader import (
-            FileUploader
-        )
-       
+        from pybirdai.process_steps.upload_files.file_uploader import FileUploader
+
         from pybirdai.context.context import Context
 
         base_dir = settings.BASE_DIR
         sdd_context = SDDContext()
-        sdd_context.file_directory = os.path.join(base_dir, 'resources')
-        sdd_context.output_directory = os.path.join(base_dir, 'results')
-        
+        sdd_context.file_directory = os.path.join(base_dir, "resources")
+        sdd_context.output_directory = os.path.join(base_dir, "results")
+
         context = Context()
         context.file_directory = sdd_context.file_directory
         context.output_directory = sdd_context.output_directory
 
         FileUploader().upload_technical_export_files(sdd_context, request)
-        
 
     def ready(self):
         # This method is still needed for Django's AppConfig
         pass
-
-
-
-
-      
-    
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/upload_technical_export_files.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/metadata_lineage_processor.py	2025-09-02 15:09:38.712162+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/metadata_lineage_processor.py	2025-09-21 17:07:35.792740+00:00
@@ -14,61 +14,62 @@
 import django
 import os
 from django.apps import AppConfig
 from django.conf import settings
 
+
 class RunMetadataLineageProcessor(AppConfig):
     """
     Django AppConfig for processing metadata lineage.
-    
+
     This class sets up the necessary context and runs the metadata
     lineage processing workflow to create DataItem, Process, and
     Relationship instances from BIRD metadata.
     """
-    
-    path = os.path.join(settings.BASE_DIR, 'birds_nest')
-    
+
+    path = os.path.join(settings.BASE_DIR, "birds_nest")
+
     def ready(self):
         """
         Prepare and execute the metadata lineage processing workflow.
         """
         from pybirdai.context.context import Context
         from pybirdai.context.sdd_context_django import SDDContext
         from pybirdai.process_steps.metadata_lineage.metadata_lineage_processor import MetadataLineageProcessor
-        
+
         print("Starting metadata lineage processing...")
-        
+
         base_dir = settings.BASE_DIR
         sdd_context = SDDContext()
-        sdd_context.file_directory = os.path.join(base_dir, 'resources')
-        
+        sdd_context.file_directory = os.path.join(base_dir, "resources")
+
         # Initialize the metadata lineage processor
         processor = MetadataLineageProcessor(sdd_context)
-        
+
         # Process different types of metadata lineage
         try:
             # Process input tables
             processor.process_input_tables()
             print("✓ Input tables processed")
-            
+
             # Process output tables
             processor.process_output_tables()
             print("✓ Output tables processed")
-            
+
             # Process product-specific joins
             processor.process_product_specific_joins()
             print("✓ Product-specific joins processed")
-            
+
             # Process datapoints
             processor.process_datapoints()
             print("✓ Datapoints processed")
-            
+
             # Export lineage graph to JSON
-            output_path = os.path.join(base_dir, 'results', 'metadata_lineage.json')
+            output_path = os.path.join(base_dir, "results", "metadata_lineage.json")
             processor.export_lineage_to_json(output_path)
             print(f"✓ Metadata lineage exported to {output_path}")
-            
+
             print("Metadata lineage processing completed successfully!")
-            
+
         except Exception as e:
             print(f"Error during metadata lineage processing: {e}")
-            raise
\ No newline at end of file
+            raise
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/metadata_lineage_processor.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/upload_sqldev_eil_files.py	2025-08-02 18:37:08.445544+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/upload_sqldev_eil_files.py	2025-09-21 17:07:35.793446+00:00
@@ -16,51 +16,43 @@
 import os
 from django.apps import AppConfig
 from pybirdai.context.sdd_context_django import SDDContext
 from django.conf import settings
 
+
 class UploadSQLDevEILFiles(AppConfig):
     """
     Django AppConfig for running the website to SDD model conversion process.
 
     This class sets up the necessary context and runs the import process
     to convert website data into an SDD  model.
     """
 
-    path = os.path.join(settings.BASE_DIR, 'birds_nest')
+    path = os.path.join(settings.BASE_DIR, "birds_nest")
 
     @staticmethod
     def upload_sqldev_eil_files(request):
         """
         Prepare and execute the website to SDD model conversion process.
 
         This method sets up the necessary contexts, creates reference domains
         and variables, and imports the website data into the SDD model.
         """
         from pybirdai.models.bird_meta_data_model import MAINTENANCE_AGENCY
-        from pybirdai.process_steps.upload_files.file_uploader import (
-            FileUploader
-        )
-       
+        from pybirdai.process_steps.upload_files.file_uploader import FileUploader
+
         from pybirdai.context.context import Context
 
         base_dir = settings.BASE_DIR
         sdd_context = SDDContext()
-        sdd_context.file_directory = os.path.join(base_dir, 'resources')
-        sdd_context.output_directory = os.path.join(base_dir, 'results')
-        
+        sdd_context.file_directory = os.path.join(base_dir, "resources")
+        sdd_context.output_directory = os.path.join(base_dir, "results")
+
         context = Context()
         context.file_directory = sdd_context.file_directory
         context.output_directory = sdd_context.output_directory
 
         FileUploader().upload_sqldev_eil_files(sdd_context, request)
-        
 
     def ready(self):
         # This method is still needed for Django's AppConfig
         pass
-
-
-
-
-      
-    
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/upload_joins_configuration.py	2025-08-02 18:37:08.445375+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/upload_joins_configuration.py	2025-09-21 17:07:35.793581+00:00
@@ -16,51 +16,43 @@
 import os
 from django.apps import AppConfig
 from pybirdai.context.sdd_context_django import SDDContext
 from django.conf import settings
 
+
 class UploadJoinsConfiguration(AppConfig):
     """
     Django AppConfig for running the website to SDD model conversion process.
 
     This class sets up the necessary context and runs the import process
     to convert website data into an SDD  model.
     """
 
-    path = os.path.join(settings.BASE_DIR, 'birds_nest')
+    path = os.path.join(settings.BASE_DIR, "birds_nest")
 
     @staticmethod
     def upload_joins_configuration(request):
         """
         Prepare and execute the website to SDD model conversion process.
 
         This method sets up the necessary contexts, creates reference domains
         and variables, and imports the website data into the SDD model.
         """
         from pybirdai.models.bird_meta_data_model import MAINTENANCE_AGENCY
-        from pybirdai.process_steps.upload_files.file_uploader import (
-            FileUploader
-        )
-       
+        from pybirdai.process_steps.upload_files.file_uploader import FileUploader
+
         from pybirdai.context.context import Context
 
         base_dir = settings.BASE_DIR
         sdd_context = SDDContext()
-        sdd_context.file_directory = os.path.join(base_dir, 'resources')
-        sdd_context.output_directory = os.path.join(base_dir, 'results')
-        
+        sdd_context.file_directory = os.path.join(base_dir, "resources")
+        sdd_context.output_directory = os.path.join(base_dir, "results")
+
         context = Context()
         context.file_directory = sdd_context.file_directory
         context.output_directory = sdd_context.output_directory
 
         FileUploader().upload_joins_configuration(sdd_context, request)
-        
 
     def ready(self):
         # This method is still needed for Django's AppConfig
         pass
-
-
-
-
-      
-    
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/upload_sqldev_eil_files.py
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/upload_joins_configuration.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/run_create_executable_filters.py	2025-08-02 18:37:08.445201+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/run_create_executable_filters.py	2025-09-21 17:07:35.801016+00:00
@@ -16,78 +16,74 @@
 import os
 from django.apps import AppConfig
 from pybirdai.context.sdd_context_django import SDDContext
 from django.conf import settings
 
+
 class RunCreateExecutableFilters(AppConfig):
     """
     Django AppConfig for running the website to SDD model conversion process.
 
     This class sets up the necessary context and runs the import process
     to convert website data into an SDD  model.
     """
 
-    path = os.path.join(settings.BASE_DIR, 'birds_nest')
+    path = os.path.join(settings.BASE_DIR, "birds_nest")
 
     @staticmethod
     def run_create_executable_filters():
         from pybirdai.models.bird_meta_data_model import MAINTENANCE_AGENCY
 
-        from pybirdai.process_steps.input_model.import_database_to_sdd_model import (
-            ImportDatabaseToSDDModel
-        )
-        from pybirdai.process_steps.pybird.create_executable_filters import (
-            CreateExecutableFilters
-        )
+        from pybirdai.process_steps.input_model.import_database_to_sdd_model import ImportDatabaseToSDDModel
+        from pybirdai.process_steps.pybird.create_executable_filters import CreateExecutableFilters
         from pybirdai.context.context import Context
 
         base_dir = settings.BASE_DIR
         sdd_context = SDDContext()
-        sdd_context.file_directory = os.path.join(base_dir, 'resources')
-        sdd_context.output_directory = os.path.join(base_dir, 'results')
-        
+        sdd_context.file_directory = os.path.join(base_dir, "resources")
+        sdd_context.output_directory = os.path.join(base_dir, "results")
+
         context = Context()
         context.file_directory = sdd_context.file_directory
         context.output_directory = sdd_context.output_directory
 
-        #ImportDatabaseToSDDModel().import_sdd(sdd_context)
+        # ImportDatabaseToSDDModel().import_sdd(sdd_context)
         CreateExecutableFilters().create_executable_filters(context, sdd_context)
 
     @staticmethod
     def run_create_executable_filters_from_db():
         from pybirdai.models.bird_meta_data_model import MAINTENANCE_AGENCY
 
-        from pybirdai.process_steps.input_model.import_database_to_sdd_model import (
-            ImportDatabaseToSDDModel
-        )
-        from pybirdai.process_steps.pybird.create_executable_filters import (
-            CreateExecutableFilters
-        )
+        from pybirdai.process_steps.input_model.import_database_to_sdd_model import ImportDatabaseToSDDModel
+        from pybirdai.process_steps.pybird.create_executable_filters import CreateExecutableFilters
         from pybirdai.context.context import Context
 
         base_dir = settings.BASE_DIR
         sdd_context = SDDContext()
-        sdd_context.file_directory = os.path.join(base_dir, 'resources')
-        sdd_context.output_directory = os.path.join(base_dir, 'results')
-        
+        sdd_context.file_directory = os.path.join(base_dir, "resources")
+        sdd_context.output_directory = os.path.join(base_dir, "results")
+
         context = Context()
         context.file_directory = sdd_context.file_directory
         context.output_directory = sdd_context.output_directory
 
         # Only import the necessary tables for filters
         importer = ImportDatabaseToSDDModel()
-        importer.import_sdd_for_filters(sdd_context, [
-            'MAINTENANCE_AGENCY',
-            'DOMAIN',
-            'MEMBER',
-            'VARIABLE',
-            'MEMBER_HIERARCHY',
-            'MEMBER_HIERARCHY_NODE',
-            'COMBINATION',
-            'COMBINATION_ITEM',
-            'CUBE_TO_COMBINATION'
-        ])
+        importer.import_sdd_for_filters(
+            sdd_context,
+            [
+                "MAINTENANCE_AGENCY",
+                "DOMAIN",
+                "MEMBER",
+                "VARIABLE",
+                "MEMBER_HIERARCHY",
+                "MEMBER_HIERARCHY_NODE",
+                "COMBINATION",
+                "COMBINATION_ITEM",
+                "CUBE_TO_COMBINATION",
+            ],
+        )
         CreateExecutableFilters().create_executable_filters(context, sdd_context)
 
     def ready(self):
         # This method is still needed for Django's AppConfig
         pass
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/run_create_executable_filters.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/mapping_assistant_entry.py	2025-08-02 18:37:08.445000+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/mapping_assistant_entry.py	2025-09-21 17:07:35.871980+00:00
@@ -20,25 +20,30 @@
 import django
 from django.apps import AppConfig
 from django.conf import settings
 
 from pybirdai.models.bird_meta_data_model import (
-    COMBINATION, COMBINATION_ITEM, MEMBER_MAPPING, MEMBER_MAPPING_ITEM,
-    VARIABLE_MAPPING, VARIABLE_MAPPING_ITEM, FRAMEWORK, CUBE, CUBE_TO_COMBINATION
+    COMBINATION,
+    COMBINATION_ITEM,
+    MEMBER_MAPPING,
+    MEMBER_MAPPING_ITEM,
+    VARIABLE_MAPPING,
+    VARIABLE_MAPPING_ITEM,
+    FRAMEWORK,
+    CUBE,
+    CUBE_TO_COMBINATION,
 )
 
 
 class RunMappingAssistant(AppConfig):
     """Django AppConfig for running the Mapping Assistant."""
 
-    path = os.path.join(settings.BASE_DIR, 'birds_nest')
+    path = os.path.join(settings.BASE_DIR, "birds_nest")
 
     @staticmethod
     def generate_mapping_proposals(
-        combination_ids: List[str],
-        variable_mapping_ids: Optional[List[str]] = None,
-        confidence_threshold: float = 0.7
+        combination_ids: List[str], variable_mapping_ids: Optional[List[str]] = None, confidence_threshold: float = 0.7
     ) -> Dict:
         """
         Generate mapping proposals for given combinations.
 
         Args:
@@ -47,13 +52,11 @@
             confidence_threshold: Minimum confidence score for proposals
 
         Returns:
             Dictionary containing proposals and metadata
         """
-        from pybirdai.process_steps.mapping_assistant import (
-            MappingAssistant, CombinationData, MappingProposal
-        )
+        from pybirdai.process_steps.mapping_assistant import MappingAssistant, CombinationData, MappingProposal
 
         assistant = MappingAssistant()
 
         # Fetch combinations and their items
         combinations_data = []
@@ -64,100 +67,101 @@
 
                 # Convert to CombinationData format
                 items_data = []
                 for item in items:
                     item_dict = {
-                        'variable_id': item.variable_id.variable_id if item.variable_id else None,
-                        'member_id': item.member_id.member_id if item.member_id else None,
-                        'subdomain_id': item.subdomain_id.subdomain_id if item.subdomain_id else None
+                        "variable_id": item.variable_id.variable_id if item.variable_id else None,
+                        "member_id": item.member_id.member_id if item.member_id else None,
+                        "subdomain_id": item.subdomain_id.subdomain_id if item.subdomain_id else None,
                     }
                     items_data.append(item_dict)
 
-                combo_data = CombinationData(
-                    combination_id=combination_id,
-                    items=items_data
-                )
+                combo_data = CombinationData(combination_id=combination_id, items=items_data)
                 combinations_data.append(combo_data)
 
             except COMBINATION.DoesNotExist:
                 print(f"Warning: Combination {combination_id} not found")
                 continue
 
         # Fetch all member mapping items
         member_mapping_items = []
-        for item in MEMBER_MAPPING_ITEM.objects.all().select_related('member_mapping_id', 'variable_id', 'member_id'):
+        for item in MEMBER_MAPPING_ITEM.objects.all().select_related("member_mapping_id", "variable_id", "member_id"):
             item_dict = {
-                'member_mapping_id': item.member_mapping_id.member_mapping_id if item.member_mapping_id else None,
-                'member_mapping_row': item.member_mapping_row,
-                'variable_id': item.variable_id.variable_id if item.variable_id else None,
-                'member_id': item.member_id.member_id if item.member_id else None,
-                'is_source': item.is_source
+                "member_mapping_id": item.member_mapping_id.member_mapping_id if item.member_mapping_id else None,
+                "member_mapping_row": item.member_mapping_row,
+                "variable_id": item.variable_id.variable_id if item.variable_id else None,
+                "member_id": item.member_id.member_id if item.member_id else None,
+                "is_source": item.is_source,
             }
             member_mapping_items.append(item_dict)
 
         # Fetch variable mapping items if specified
         variable_mapping_items = None
         if variable_mapping_ids:
             variable_mapping_items = []
             for vm_id in variable_mapping_ids:
                 items = VARIABLE_MAPPING_ITEM.objects.filter(
                     variable_mapping_id__variable_mapping_id=vm_id
-                ).select_related('variable_mapping_id', 'variable_id')
+                ).select_related("variable_mapping_id", "variable_id")
 
                 for item in items:
                     item_dict = {
-                        'variable_mapping_id': item.variable_mapping_id.variable_mapping_id if item.variable_mapping_id else None,
-                        'variable_id': item.variable_id.variable_id if item.variable_id else None,
-                        'is_source': item.is_source
+                        "variable_mapping_id": (
+                            item.variable_mapping_id.variable_mapping_id if item.variable_mapping_id else None
+                        ),
+                        "variable_id": item.variable_id.variable_id if item.variable_id else None,
+                        "is_source": item.is_source,
                     }
                     variable_mapping_items.append(item_dict)
 
         # Generate proposals
         proposals = assistant.generate_mapping_proposals(
             combinations=combinations_data,
             member_mapping_items=member_mapping_items,
             variable_mapping_items=variable_mapping_items,
-            confidence_threshold=confidence_threshold
+            confidence_threshold=confidence_threshold,
         )
 
         # Convert proposals to serializable format
         result = {
-            'proposals': {},
-            'summary': {
-                'total_combinations': len(combinations_data),
-                'combinations_with_proposals': 0,
-                'total_proposals': 0
-            }
+            "proposals": {},
+            "summary": {
+                "total_combinations": len(combinations_data),
+                "combinations_with_proposals": 0,
+                "total_proposals": 0,
+            },
         }
 
         for combination_id, proposal_list in proposals.items():
             if proposal_list:
-                result['summary']['combinations_with_proposals'] += 1
-                result['summary']['total_proposals'] += len(proposal_list)
-
-            result['proposals'][combination_id] = []
+                result["summary"]["combinations_with_proposals"] += 1
+                result["summary"]["total_proposals"] += len(proposal_list)
+
+            result["proposals"][combination_id] = []
             for proposal in proposal_list:
-                result['proposals'][combination_id].append({
-                    'source_variable_id': proposal.source_variable_id,
-                    'source_member_id': proposal.source_member_id,
-                    'target_variable_id': proposal.target_variable_id,
-                    'target_member_id': proposal.target_member_id,
-                    'confidence_score': proposal.confidence_score,
-                    'member_mapping_id': proposal.member_mapping_id,
-                    'member_mapping_row': proposal.member_mapping_row,
-                    'reasoning': proposal.reasoning
-                })
+                result["proposals"][combination_id].append(
+                    {
+                        "source_variable_id": proposal.source_variable_id,
+                        "source_member_id": proposal.source_member_id,
+                        "target_variable_id": proposal.target_variable_id,
+                        "target_member_id": proposal.target_member_id,
+                        "confidence_score": proposal.confidence_score,
+                        "member_mapping_id": proposal.member_mapping_id,
+                        "member_mapping_row": proposal.member_mapping_row,
+                        "reasoning": proposal.reasoning,
+                    }
+                )
 
         return result
 
     @staticmethod
     def save_proposals_to_file(proposals: Dict, output_file: str):
         """Save proposals to a JSON file."""
-        output_path = Path(settings.BASE_DIR) / 'results' / 'mapping_assistant' / output_file
+        output_path = Path(settings.BASE_DIR) / "results" / "mapping_assistant" / output_file
         output_path.parent.mkdir(parents=True, exist_ok=True)
 
-        with open(output_path, 'w') as f:
+        with open(output_path, "w") as f:
             json.dump(proposals, f, indent=2)
 
         print(f"Proposals saved to: {output_path}")
         return str(output_path)
 
@@ -177,63 +181,59 @@
 
         for proposal in accepted_proposal_ids:
             try:
                 # Check if mapping already exists
                 existing = MEMBER_MAPPING_ITEM.objects.filter(
-                    member_mapping_id__member_mapping_id=proposal['member_mapping_id'],
-                    member_mapping_row=proposal['member_mapping_row'],
-                    variable_id__variable_id=proposal['source_variable_id'],
-                    member_id__member_id=proposal['source_member_id'],
-                    is_source='true'
+                    member_mapping_id__member_mapping_id=proposal["member_mapping_id"],
+                    member_mapping_row=proposal["member_mapping_row"],
+                    variable_id__variable_id=proposal["source_variable_id"],
+                    member_id__member_id=proposal["source_member_id"],
+                    is_source="true",
                 ).exists()
 
                 if not existing:
                     # Create source mapping item
                     source_item = MEMBER_MAPPING_ITEM.objects.create(
-                        member_mapping_id_id=proposal['member_mapping_id'],
-                        member_mapping_row=proposal['member_mapping_row'],
-                        variable_id_id=proposal['source_variable_id'],
-                        member_id_id=proposal['source_member_id'],
-                        is_source='true'
+                        member_mapping_id_id=proposal["member_mapping_id"],
+                        member_mapping_row=proposal["member_mapping_row"],
+                        variable_id_id=proposal["source_variable_id"],
+                        member_id_id=proposal["source_member_id"],
+                        is_source="true",
                     )
                     created_mappings.append(f"Source: {proposal['source_variable_id']}/{proposal['source_member_id']}")
 
                 # Check if target mapping already exists
                 existing_target = MEMBER_MAPPING_ITEM.objects.filter(
-                    member_mapping_id__member_mapping_id=proposal['member_mapping_id'],
-                    member_mapping_row=proposal['member_mapping_row'],
-                    variable_id__variable_id=proposal['target_variable_id'],
-                    member_id__member_id=proposal['target_member_id'],
-                    is_source='false'
+                    member_mapping_id__member_mapping_id=proposal["member_mapping_id"],
+                    member_mapping_row=proposal["member_mapping_row"],
+                    variable_id__variable_id=proposal["target_variable_id"],
+                    member_id__member_id=proposal["target_member_id"],
+                    is_source="false",
                 ).exists()
 
                 if not existing_target:
                     # Create target mapping item
                     target_item = MEMBER_MAPPING_ITEM.objects.create(
-                        member_mapping_id_id=proposal['member_mapping_id'],
-                        member_mapping_row=proposal['member_mapping_row'],
-                        variable_id_id=proposal['target_variable_id'],
-                        member_id_id=proposal['target_member_id'],
-                        is_source='false'
+                        member_mapping_id_id=proposal["member_mapping_id"],
+                        member_mapping_row=proposal["member_mapping_row"],
+                        variable_id_id=proposal["target_variable_id"],
+                        member_id_id=proposal["target_member_id"],
+                        is_source="false",
                     )
                     created_mappings.append(f"Target: {proposal['target_variable_id']}/{proposal['target_member_id']}")
 
             except Exception as e:
                 errors.append(f"Error processing proposal: {str(e)}")
 
-        return {
-            'success': len(errors) == 0,
-            'created_mappings': created_mappings,
-            'errors': errors
-        }
+        return {"success": len(errors) == 0, "created_mappings": created_mappings, "errors": errors}
 
     @staticmethod
     def get_combinations_for_template(cube_id: str) -> List[str]:
         """Get all combination IDs for a specific template/cube."""
-        cube_combinations = CUBE_TO_COMBINATION.objects.filter(
-            cube_id__cube_id=cube_id
-        ).select_related('combination_id')
+        cube_combinations = CUBE_TO_COMBINATION.objects.filter(cube_id__cube_id=cube_id).select_related(
+            "combination_id"
+        )
 
         return [cc.combination_id.combination_id for cc in cube_combinations if cc.combination_id]
 
     @staticmethod
     def get_combinations_for_framework(framework_id: str) -> List[str]:
@@ -241,13 +241,11 @@
         # Get all cubes for the framework
         cubes = CUBE.objects.filter(framework_id__framework_id=framework_id)
 
         all_combinations = []
         for cube in cubes:
-            cube_combinations = CUBE_TO_COMBINATION.objects.filter(
-                cube_id=cube
-            ).select_related('combination_id')
+            cube_combinations = CUBE_TO_COMBINATION.objects.filter(cube_id=cube).select_related("combination_id")
 
             for cc in cube_combinations:
                 if cc.combination_id:
                     all_combinations.append(cc.combination_id.combination_id)
 
@@ -261,13 +259,11 @@
 
         return unique_combinations
 
     @staticmethod
     def generate_proposals_for_template(
-        cube_id: str,
-        variable_mapping_ids: Optional[List[str]] = None,
-        confidence_threshold: float = 0.7
+        cube_id: str, variable_mapping_ids: Optional[List[str]] = None, confidence_threshold: float = 0.7
     ) -> Dict:
         """
         Generate mapping proposals for all combinations in a specific template/cube.
 
         Args:
@@ -281,37 +277,37 @@
         # Get all combinations for this template
         combination_ids = RunMappingAssistant.get_combinations_for_template(cube_id)
 
         if not combination_ids:
             return {
-                'proposals': {},
-                'summary': {
-                    'template_id': cube_id,
-                    'total_combinations': 0,
-                    'combinations_with_proposals': 0,
-                    'total_proposals': 0
-                }
+                "proposals": {},
+                "summary": {
+                    "template_id": cube_id,
+                    "total_combinations": 0,
+                    "combinations_with_proposals": 0,
+                    "total_proposals": 0,
+                },
             }
 
         # Generate proposals using existing method
         result = RunMappingAssistant.generate_mapping_proposals(
             combination_ids=combination_ids,
             variable_mapping_ids=variable_mapping_ids,
-            confidence_threshold=confidence_threshold
+            confidence_threshold=confidence_threshold,
         )
 
         # Add template-specific metadata
-        result['summary']['template_id'] = cube_id
+        result["summary"]["template_id"] = cube_id
 
         return result
 
     @staticmethod
     def generate_proposals_for_framework(
         framework_id: str,
         variable_mapping_ids: Optional[List[str]] = None,
         confidence_threshold: float = 0.7,
-        ensure_consistency: bool = True
+        ensure_consistency: bool = True,
     ) -> Dict:
         """
         Generate mapping proposals for all combinations across all templates in a framework.
 
         Args:
@@ -326,31 +322,31 @@
         # Get all combinations for this framework
         combination_ids = RunMappingAssistant.get_combinations_for_framework(framework_id)
 
         if not combination_ids:
             return {
-                'proposals': {},
-                'summary': {
-                    'framework_id': framework_id,
-                    'total_combinations': 0,
-                    'combinations_with_proposals': 0,
-                    'total_proposals': 0,
-                    'templates_processed': 0
-                }
+                "proposals": {},
+                "summary": {
+                    "framework_id": framework_id,
+                    "total_combinations": 0,
+                    "combinations_with_proposals": 0,
+                    "total_proposals": 0,
+                    "templates_processed": 0,
+                },
             }
 
         # Generate proposals using existing method
         result = RunMappingAssistant.generate_mapping_proposals(
             combination_ids=combination_ids,
             variable_mapping_ids=variable_mapping_ids,
-            confidence_threshold=confidence_threshold
+            confidence_threshold=confidence_threshold,
         )
 
         # Add framework-specific metadata
         templates = CUBE.objects.filter(framework_id__framework_id=framework_id)
-        result['summary']['framework_id'] = framework_id
-        result['summary']['templates_processed'] = templates.count()
+        result["summary"]["framework_id"] = framework_id
+        result["summary"]["templates_processed"] = templates.count()
 
         # If consistency is required, apply framework-wide conflict resolution
         if ensure_consistency:
             result = RunMappingAssistant._apply_framework_consistency(result, framework_id)
 
@@ -367,51 +363,50 @@
         from collections import defaultdict
 
         # Group proposals by source (variable_id, member_id) across all combinations
         source_to_targets = defaultdict(list)
 
-        for combination_id, proposal_list in proposals['proposals'].items():
+        for combination_id, proposal_list in proposals["proposals"].items():
             for proposal in proposal_list:
-                source_key = (proposal['source_variable_id'], proposal['source_member_id'])
-                source_to_targets[source_key].append({
-                    'combination_id': combination_id,
-                    'proposal': proposal
-                })
+                source_key = (proposal["source_variable_id"], proposal["source_member_id"])
+                source_to_targets[source_key].append({"combination_id": combination_id, "proposal": proposal})
 
         # Resolve conflicts by selecting highest confidence proposal for each source
         resolved_mappings = {}
         conflicts_resolved = 0
 
         for source_key, target_list in source_to_targets.items():
             if len(target_list) > 1:
                 # Multiple targets for same source - resolve by highest confidence
-                best_proposal = max(target_list, key=lambda x: x['proposal']['confidence_score'])
-                resolved_mappings[source_key] = best_proposal['proposal']
+                best_proposal = max(target_list, key=lambda x: x["proposal"]["confidence_score"])
+                resolved_mappings[source_key] = best_proposal["proposal"]
                 conflicts_resolved += 1
             else:
                 # Single target - keep as is
-                resolved_mappings[source_key] = target_list[0]['proposal']
+                resolved_mappings[source_key] = target_list[0]["proposal"]
 
         # Rebuild proposals with consistent mappings
         consistent_proposals = {}
-        for combination_id, proposal_list in proposals['proposals'].items():
+        for combination_id, proposal_list in proposals["proposals"].items():
             consistent_list = []
             for proposal in proposal_list:
-                source_key = (proposal['source_variable_id'], proposal['source_member_id'])
+                source_key = (proposal["source_variable_id"], proposal["source_member_id"])
                 resolved = resolved_mappings[source_key]
 
                 # Only include if this proposal matches the resolved mapping
-                if (resolved['target_variable_id'] == proposal['target_variable_id'] and
-                    resolved['target_member_id'] == proposal['target_member_id']):
+                if (
+                    resolved["target_variable_id"] == proposal["target_variable_id"]
+                    and resolved["target_member_id"] == proposal["target_member_id"]
+                ):
                     consistent_list.append(proposal)
 
             consistent_proposals[combination_id] = consistent_list
 
         # Update result
-        proposals['proposals'] = consistent_proposals
-        proposals['summary']['conflicts_resolved'] = conflicts_resolved
-        proposals['summary']['consistency_applied'] = True
+        proposals["proposals"] = consistent_proposals
+        proposals["summary"]["conflicts_resolved"] = conflicts_resolved
+        proposals["summary"]["consistency_applied"] = True
 
         return proposals
 
     @staticmethod
     def get_framework_summary(framework_id: str) -> Dict:
@@ -425,27 +420,24 @@
 
             for template in templates:
                 combo_count = CUBE_TO_COMBINATION.objects.filter(cube_id=template).count()
                 total_combinations += combo_count
 
-                template_info.append({
-                    'cube_id': template.cube_id,
-                    'name': template.name,
-                    'combination_count': combo_count
-                })
+                template_info.append(
+                    {"cube_id": template.cube_id, "name": template.name, "combination_count": combo_count}
+                )
 
             return {
-                'framework_id': framework_id,
-                'framework_name': framework.name,
-                'total_templates': templates.count(),
-                'total_combinations': total_combinations,
-                'templates': template_info
+                "framework_id": framework_id,
+                "framework_name": framework.name,
+                "total_templates": templates.count(),
+                "total_combinations": total_combinations,
+                "templates": template_info,
             }
 
         except FRAMEWORK.DoesNotExist:
-            return {
-                'error': f'Framework {framework_id} not found'
-            }
+            return {"error": f"Framework {framework_id} not found"}
+
 
 def ready(self):
     # This method is still needed for Django's AppConfig
     pass
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/mapping_assistant_entry.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/automode_database_setup.py	2025-09-18 09:16:40.852287+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/automode_database_setup.py	2025-09-21 17:07:35.927736+00:00
@@ -31,21 +31,22 @@
 from importlib import metadata
 
 # Create a logger
 logger = logging.getLogger(__name__)
 
+
 class RunAutomodeDatabaseSetup(AppConfig):
     """
     Entry point for automode database setup that performs all necessary
     steps to create and configure the BIRD database automatically.
     """
 
     def __init__(self, app_name, app_module, *args, **kwargs):
         self.app_name = app_name
         self.app_module = app_module
         self.token = ""
-        for k,v in kwargs.items():
+        for k, v in kwargs.items():
             if k == "token":
                 self.token = v
                 break
 
     def run_automode_database_setup(self):
@@ -55,40 +56,27 @@
         """
         try:
             logger.info("Starting automode database setup...")
 
             base_dir = settings.BASE_DIR
-            migration_file_path = os.path.join(
-                base_dir,
-                "pybirdai",
-                "migrations"
-            )
+            migration_file_path = os.path.join(base_dir, "pybirdai", "migrations")
             for file in os.listdir(migration_file_path):
                 if file.endswith(".py") and not file.startswith("__"):
                     logger.info(f"Processing file: {file}")
                     os.remove(os.path.join(migration_file_path, file))
 
-            admin_file_path = os.path.join(
-                base_dir,
-                "pybirdai",
-                "admin.py"
-            )
-
-            bird_data_model_path = os.path.join(
-                base_dir,
-                "pybirdai",
-                "models",
-                "bird_data_model.py"
-            )
+            admin_file_path = os.path.join(base_dir, "pybirdai", "admin.py")
+
+            bird_data_model_path = os.path.join(base_dir, "pybirdai", "models", "bird_data_model.py")
 
             if os.path.exists(admin_file_path):
                 with open(admin_file_path) as rf:
-                    with open(admin_file_path,"w") as wf:
+                    with open(admin_file_path, "w") as wf:
                         wf.write(rf.read().split("\n\n")[0])
 
             if os.path.exists(bird_data_model_path):
-                with open(bird_data_model_path,"w") as wf:
+                with open(bird_data_model_path, "w") as wf:
                     wf.write("")
 
             # Step 1: Create Django models (this generates files but doesn't modify existing ones)
             logger.info("Step 1: Creating Django models...")
             try:
@@ -99,40 +87,25 @@
                 logger.error(f"Failed to create Django models: {str(e)}")
                 raise RuntimeError(f"Django model creation failed: {str(e)}") from e
 
             # Step 2: Check if generated files exist
 
-
             results_admin_path = os.path.join(
                 base_dir,
-                "results"
-                + os.sep
-                + "database_configuration_files"
-                + os.sep
-                + "admin.py",
+                "results" + os.sep + "database_configuration_files" + os.sep + "admin.py",
             )
             results_models_path = os.path.join(
                 base_dir,
-                "results"
-                + os.sep
-                + "database_configuration_files"
-                + os.sep
-                + "models.py",
+                "results" + os.sep + "database_configuration_files" + os.sep + "models.py",
             )
 
             logger.info(f"Base directory: {base_dir}")
-            logger.info(
-                f"Results models path exists: {os.path.exists(results_models_path)}"
-            )
-            logger.info(
-                f"Results admin path exists: {os.path.exists(results_admin_path)}"
-            )
+            logger.info(f"Results models path exists: {os.path.exists(results_models_path)}")
+            logger.info(f"Results admin path exists: {os.path.exists(results_admin_path)}")
 
             if not os.path.exists(results_models_path):
-                raise RuntimeError(
-                    f"Generated models file not found: {results_models_path}"
-                )
+                raise RuntimeError(f"Generated models file not found: {results_models_path}")
 
             if not os.path.exists(results_admin_path):
                 logger.warning(f"Generated admin file not found: {results_admin_path}")
 
             # Step 3: Run the complete automode setup operations directly
@@ -145,37 +118,29 @@
                 # Step 3a: Load temporary configuration to check when_to_stop setting
                 config = self._load_temp_config()
 
                 # Step 3b: Handle generated Python files if needed
                 if config and config.get("when_to_stop") == "FULL_EXECUTION":
-                    logger.info(
-                        "Transferring generated Python files for full execution..."
-                    )
+                    logger.info("Transferring generated Python files for full execution...")
                     self._transfer_generated_python_files()
 
                 # Step 3c: Prepare for post-setup operations but don't execute them yet
                 # The file modifications that trigger restart should be done by the workflow views
                 # after the status has been properly communicated to the frontend
                 logger.info("Database setup preparation completed.")
-                logger.info(
-                    "Post-setup operations (file updates) will be triggered by workflow after status update."
-                )
+                logger.info("Post-setup operations (file updates) will be triggered by workflow after status update.")
 
                 logger.info("Database models and configuration completed successfully!")
 
             except Exception as e:
                 logger.error(f"Failed to run post-setup operations: {str(e)}")
                 raise RuntimeError(f"Automatic database setup failed: {str(e)}") from e
 
             # Return success with server restart warning
             logger.info("Automode database setup completed successfully!")
-            logger.warning(
-                "IMPORTANT: Django will restart automatically due to file changes."
-            )
-            logger.warning(
-                "The restart process has been initiated. Please wait for the server to come back online."
-            )
+            logger.warning("IMPORTANT: Django will restart automatically due to file changes.")
+            logger.warning("The restart process has been initiated. Please wait for the server to come back online.")
 
             return {
                 "success": True,
                 "message": "Database setup completed successfully",
                 "server_restart_required": True,
@@ -214,18 +179,14 @@
             logger.info(f"Looking for config file at: {temp_config_path}")
 
             if os.path.exists(temp_config_path):
                 with open(temp_config_path, "r") as f:
                     config = json.load(f)
-                logger.info(
-                    f"Loaded automode configuration: when_to_stop = {config.get('when_to_stop')}"
-                )
+                logger.info(f"Loaded automode configuration: when_to_stop = {config.get('when_to_stop')}")
                 return config
             else:
-                logger.warning(
-                    f"No temporary configuration file found at {temp_config_path}"
-                )
+                logger.warning(f"No temporary configuration file found at {temp_config_path}")
 
                 # Try fallback location for backwards compatibility
                 fallback_path = os.path.join(".", "automode_config.json")
                 if os.path.exists(fallback_path):
                     logger.info(f"Found config at fallback location: {fallback_path}")
@@ -245,13 +206,11 @@
         source_dir = os.path.join(".", "resources", "generated_python")
         target_dir = os.path.join(".", "pybirdai", "process_steps", "filter_code")
 
         try:
             if not os.path.exists(source_dir):
-                logger.warning(
-                    f"Source directory {source_dir} does not exist - no Python files to transfer"
-                )
+                logger.warning(f"Source directory {source_dir} does not exist - no Python files to transfer")
                 return
 
             # Ensure target directory exists
             os.makedirs(target_dir, exist_ok=True)
 
@@ -274,13 +233,11 @@
                     transferred_count += 1
                     logger.info(f"Transferred {file_name} to filter_code directory")
                 except Exception as e:
                     logger.error(f"Error transferring {file_name}: {e}")
 
-            logger.info(
-                f"Successfully transferred {transferred_count} generated Python files"
-            )
+            logger.info(f"Successfully transferred {transferred_count} generated Python files")
 
         except Exception as e:
             logger.error(f"Error during Python file transfer: {e}")
             raise
 
@@ -296,74 +253,53 @@
             initial_migration_file = os.path.join(
                 base_dir,
                 "pybirdai" + os.sep + "migrations" + os.sep + "0001_initial.py",
             )
             db_file = os.path.join(base_dir, "db.sqlite3")
-            pybirdai_admin_path = os.path.join(
-                base_dir, "pybirdai" + os.sep + "admin.py"
-            )
+            pybirdai_admin_path = os.path.join(base_dir, "pybirdai" + os.sep + "admin.py")
             pybirdai_meta_data_model_path = os.path.join(
                 base_dir, "pybirdai" + os.sep + "models" + os.sep + "bird_meta_data_model.py"
             )
             results_admin_path = os.path.join(
                 base_dir,
-                "results"
-                + os.sep
-                + "database_configuration_files"
-                + os.sep
-                + "admin.py",
+                "results" + os.sep + "database_configuration_files" + os.sep + "admin.py",
             )
             pybirdai_models_path = os.path.join(
                 base_dir, "pybirdai" + os.sep + "models" + os.sep + "bird_data_model.py"
             )
             results_models_path = os.path.join(
                 base_dir,
-                "results"
-                + os.sep
-                + "database_configuration_files"
-                + os.sep
-                + "models.py",
+                "results" + os.sep + "database_configuration_files" + os.sep + "models.py",
             )
 
             # Cleanup existing files
             logger.info("Cleaning up existing files...")
             # self._cleanup_files(initial_migration_file, db_file)
 
-
             # Update models file (this is safe, won't trigger restart)
             logger.info("Updating bird_data_model.py...")
             self._update_models_file(pybirdai_models_path, results_models_path)
 
             # Merge derived fields after pybirdai{os.sep}bird_data_model.py has been generated
 
             derived_fields_file_path = os.path.join(
                 base_dir,
-                "resources"
-                + os.sep
-                + "derivation_files"
-                + os.sep
-                + "derived_field_configuration.py",
+                "resources" + os.sep + "derivation_files" + os.sep + "derived_field_configuration.py",
             )
 
             os.makedirs(os.path.dirname(derived_fields_file_path), exist_ok=True)
 
-            merge_derived_fields_into_original_model(
-                pybirdai_models_path, derived_fields_file_path
-            )
-
-            self._update_admin_file(
-                pybirdai_admin_path, pybirdai_meta_data_model_path, pybirdai_models_path
-            )
+            merge_derived_fields_into_original_model(pybirdai_models_path, derived_fields_file_path)
+
+            self._update_admin_file(pybirdai_admin_path, pybirdai_meta_data_model_path, pybirdai_models_path)
 
             # Create a marker file to indicate we're ready for step 2
             self._create_migration_ready_marker(base_dir)
 
             logger.info("STEP 1 completed successfully!")
 
-            logger.info(
-                "Django will restart now. After restart, user needs to setup database."
-            )
+            logger.info("Django will restart now. After restart, user needs to setup database.")
             return {
                 "success": True,
                 "step": 1,
                 "message": "Admin files updated successfully. Server will restart.",
                 "next_action": "run_migrations_after_restart",
@@ -378,31 +314,22 @@
         """
         STEP 2: Run migrations after Django has restarted with updated admin.py
         """
         try:
             logger.info("Starting STEP 2: Running migrations after restart...")
-            logger.info(
-                "IMPORTANT: This step should ONLY run Django migrations - no file downloads or deletions"
-            )
-
+            logger.info("IMPORTANT: This step should ONLY run Django migrations - no file downloads or deletions")
 
             base_dir = settings.BASE_DIR
 
             # Check if we have the ready marker
             if not self._check_migration_ready_marker(base_dir):
-                raise RuntimeError(
-                    "Migration ready marker not found. Please run STEP 1 first."
-                )
+                raise RuntimeError("Migration ready marker not found. Please run STEP 1 first.")
 
             # Run migrations in subprocess (admin.py is already updated)
-            logger.info(
-                "Running Django migrations in subprocess - NO file operations should happen"
-            )
+            logger.info("Running Django migrations in subprocess - NO file operations should happen")
             self._run_migrations_in_subprocess(base_dir)
-            logger.info(
-                "Django migrations completed - confirming no files were downloaded or deleted"
-            )
+            logger.info("Django migrations completed - confirming no files were downloaded or deleted")
 
             # Clean up the marker file
             self._remove_migration_ready_marker(base_dir)
 
             logger.info("STEP 2 completed successfully!")
@@ -426,31 +353,25 @@
             try:
                 # os.remove(initial_migration_file)
                 logger.info(f"Successfully removed {initial_migration_file}")
             except OSError as e:
                 logger.error(f"Error removing file {initial_migration_file}: {e}")
-                raise RuntimeError(
-                    f"Failed to remove file {initial_migration_file}"
-                ) from e
+                raise RuntimeError(f"Failed to remove file {initial_migration_file}") from e
 
         # Remove database file with enhanced error handling
         if os.path.exists(db_file):
             try:
                 # Check file properties
                 file_size = os.path.getsize(db_file)
-                logger.info(
-                    f"Database file {db_file} exists with size {file_size} bytes"
-                )
+                logger.info(f"Database file {db_file} exists with size {file_size} bytes")
 
                 # Try to make it writable first
                 try:
                     os.chmod(db_file, 0o666)
                     logger.info(f"Made database file writable: {db_file}")
                 except OSError as chmod_error:
-                    logger.warning(
-                        f"Could not change permissions for {db_file}: {chmod_error}"
-                    )
+                    logger.warning(f"Could not change permissions for {db_file}: {chmod_error}")
 
                 # Remove the file
                 os.remove(db_file)
                 logger.info(f"Successfully removed database file {db_file}")
 
@@ -468,20 +389,16 @@
                     logger.warning(f"Failed to force-remove database file: {e2}")
                     # Last resort: rename the file so Django can create a new one
                     try:
                         backup_name = f"{db_file}.backup.{int(time.time())}"
                         os.rename(db_file, backup_name)
-                        logger.info(
-                            f"Renamed problematic database file to {backup_name}"
-                        )
+                        logger.info(f"Renamed problematic database file to {backup_name}")
                     except OSError as e3:
                         logger.error(f"Could not even rename database file: {e3}")
                         # Continue anyway - let Django handle it
 
-    def _update_admin_file(
-        self, pybirdai_admin_path, pybirdai_meta_data_model_path, pybirdai_data_model_path
-    ):
+    def _update_admin_file(self, pybirdai_admin_path, pybirdai_meta_data_model_path, pybirdai_data_model_path):
         """Update the admin.py file with model registrations, avoiding duplicates."""
         import glob
 
         registered_models = set()
         models_dir = os.path.join(os.path.dirname(pybirdai_admin_path), "models")
@@ -503,62 +420,49 @@
                         file_content = f_read.read()
                         tree = ast.parse(file_content)
 
                         # Find all class definitions in the file
                         for node in ast.walk(tree):
-                            if isinstance(node, ast.ClassDef) and node.name not in [
-                                "Meta", "Admin"
-                            ]:
+                            if isinstance(node, ast.ClassDef) and node.name not in ["Meta", "Admin"]:
                                 # Check if this is an abstract model
                                 is_abstract = self._is_abstract_model(node, file_content)
 
                                 if not is_abstract and node.name not in registered_models:
-                                    f_write.write(
-                                        f"from .models.{model_module_name} import {node.name}\n"
-                                    )
+                                    f_write.write(f"from .models.{model_module_name} import {node.name}\n")
                                     f_write.write(f"admin.site.register({node.name})\n")
                                     registered_models.add(node.name)
 
                 except Exception as e:
                     logger.warning(f"Error processing model file {model_file_path}: {e}")
                     continue
 
     def _is_abstract_model(self, class_node, file_content):
         """Check if a Django model class is abstract by examining its Meta class."""
         for node in class_node.body:
-            if (isinstance(node, ast.ClassDef) and
-                node.name == "Meta"):
+            if isinstance(node, ast.ClassDef) and node.name == "Meta":
                 # Check if Meta class contains abstract = True
                 for meta_node in node.body:
-                    if (isinstance(meta_node, ast.Assign) and
-                        any(isinstance(target, ast.Name) and target.id == "abstract"
-                            for target in meta_node.targets)):
+                    if isinstance(meta_node, ast.Assign) and any(
+                        isinstance(target, ast.Name) and target.id == "abstract" for target in meta_node.targets
+                    ):
                         # Check if the value is True
-                        if (isinstance(meta_node.value, ast.Constant) and
-                            meta_node.value.value is True):
+                        if isinstance(meta_node.value, ast.Constant) and meta_node.value.value is True:
                             return True
-                        elif (isinstance(meta_node.value, ast.NameConstant) and
-                              meta_node.value.value is True):  # Python < 3.8 compatibility
+                        elif (
+                            isinstance(meta_node.value, ast.NameConstant) and meta_node.value.value is True
+                        ):  # Python < 3.8 compatibility
                             return True
         return False
-
-
-
-
-
-
 
     def _cleanup_results_admin_file(self, results_admin_path):
         """Clean up the results admin.py file after successful use to prevent duplicate content."""
         try:
             if os.path.exists(results_admin_path):
                 os.remove(results_admin_path)
                 logger.info(f"Cleaned up results admin file: {results_admin_path}")
         except (OSError, PermissionError) as e:
-            logger.warning(
-                f"Could not clean up results admin file {results_admin_path}: {e}"
-            )
+            logger.warning(f"Could not clean up results admin file {results_admin_path}: {e}")
             # Don't raise an exception here as this is cleanup, not critical functionality
 
     def _update_models_file(self, pybirdai_models_path, results_models_path):
         """Update the bird_data_model.py file with generated models."""
         if not os.path.exists(results_models_path):
@@ -574,11 +478,11 @@
             f_write.write("\n")
             f_write.write(results_models_str)
 
         logger.info(f"{pybirdai_models_path} updated successfully.")
 
-    def _fetch_preconfigured_database(self,python_executable):
+    def _fetch_preconfigured_database(self, python_executable):
         fetcher = PreconfiguredDatabaseFetcher(self.token)
         db_content = fetcher.fetch()
         db_file = "db.sqlite3"
         if os.path.exists(db_file):
             os.chmod(db_file, 0o666)
@@ -599,21 +503,27 @@
         for file in os.listdir(f"pybirdai{os.sep}migrations"):
             if file.endswith(".py") and file != "__init__.py":
                 logger.info(f"Found migration file: {file}")
                 found_migration_files.append(file)
 
-        found_migration_files = sorted(found_migration_files,
-            key=lambda x: int(x.split("_")[0]))
+        found_migration_files = sorted(found_migration_files, key=lambda x: int(x.split("_")[0]))
 
         for file in found_migration_files:
-            fake_migrate_cmd = [python_executable, "manage.py", "migrate", "--fake", "pybirdai", file.replace(".py", "")]
+            fake_migrate_cmd = [
+                python_executable,
+                "manage.py",
+                "migrate",
+                "--fake",
+                "pybirdai",
+                file.replace(".py", ""),
+            ]
             migrate_result = subprocess.run(
                 fake_migrate_cmd,
                 capture_output=True,
                 text=True,
                 timeout=600,
-                shell=(os.name == 'nt'),  # Use shell=True on Windows
+                shell=(os.name == "nt"),  # Use shell=True on Windows
             )
 
         return 0, migrate_result
 
     def _get_python_exc(self):
@@ -622,11 +532,11 @@
 
         # Get the virtual environment path if we're in one
         venv_path = os.environ.get("VIRTUAL_ENV")
         if venv_path:
             # Handle Windows vs Unix virtual environment structure
-            if os.name == 'nt':  # Windows
+            if os.name == "nt":  # Windows
                 python_executable = os.path.join(venv_path, "Scripts", "python.exe")
             else:  # Unix/Linux/macOS
                 python_executable = os.path.join(venv_path, "bin", "python")
 
         # Change to project directory for subprocess
@@ -650,11 +560,16 @@
                 os.remove(db_file)
 
             venv_path, _, python_executable = self._get_python_exc()
 
             generator = AdvancedMigrationGenerator()
-            models = generator.parse_files([f"pybirdai{os.sep}models{os.sep}bird_data_model.py", f"pybirdai{os.sep}models{os.sep}bird_meta_data_model.py"])
+            models = generator.parse_files(
+                [
+                    f"pybirdai{os.sep}models{os.sep}bird_data_model.py",
+                    f"pybirdai{os.sep}models{os.sep}bird_meta_data_model.py",
+                ]
+            )
             _ = generator.generate_migration_code(models)
             generator.save_migration_file(models, f"pybirdai{os.sep}migrations{os.sep}0001_initial.py")
 
             logger.info("Running makemigrations in subprocess...")
             logger.info(f"Using Python executable: {python_executable}")
@@ -668,19 +583,17 @@
             makemig_result = subprocess.run(
                 makemig_cmd,
                 capture_output=True,
                 text=True,
                 timeout=900,
-                shell=(os.name == 'nt'),  # Use shell=True on Windows
+                shell=(os.name == "nt"),  # Use shell=True on Windows
             )  # 15 minute timeout
 
             makemig_time = time.time() - makemig_start
 
             if makemig_result.returncode != 0:
-                logger.error(
-                    f"Makemigrations failed with return code {makemig_result.returncode}"
-                )
+                logger.error(f"Makemigrations failed with return code {makemig_result.returncode}")
                 logger.error(f"Python executable used: {python_executable}")
                 logger.error(f"Python executable exists: {os.path.exists(python_executable)}")
                 logger.error(f"manage.py exists: {os.path.exists('manage.py')}")
                 logger.error(f"Current directory: {os.getcwd()}")
                 logger.error(f"Stdout: {makemig_result.stdout}")
@@ -700,34 +613,35 @@
 
                 db_file = "db.sqlite3"
                 if os.path.exists(db_file):
                     try:
                         if is_file_locked(os.path.abspath(db_file)):
-                            logger.warning(f"Database file {db_file} is locked by another process. Please stop all Django/related processes and try again.")
-                            raise RuntimeError(f"Database file {db_file} is locked. Please stop all Django/related processes (such as the development server) and try again.")
+                            logger.warning(
+                                f"Database file {db_file} is locked by another process. Please stop all Django/related processes and try again."
+                            )
+                            raise RuntimeError(
+                                f"Database file {db_file} is locked. Please stop all Django/related processes (such as the development server) and try again."
+                            )
                         os.chmod(db_file, 0o666)
                         os.remove(db_file)
                     except Exception as e:
                         logger.warning(f"Error removing database file {db_file}: {e}")
-
 
                 # Run migrate
                 migrate_cmd = [python_executable, "manage.py", "migrate"]
                 logger.info(f"Running migrate command: {' '.join(migrate_cmd)}")
 
                 migrate_result = subprocess.run(
                     migrate_cmd,
                     capture_output=True,
                     text=True,
                     timeout=600,
-                    shell=(os.name == 'nt'),  # Use shell=True on Windows
+                    shell=(os.name == "nt"),  # Use shell=True on Windows
                 )  # 10 minute timeout
 
                 if migrate_result.returncode != 0:
-                    logger.error(
-                        f"Migrate failed with return code {migrate_result.returncode}"
-                    )
+                    logger.error(f"Migrate failed with return code {migrate_result.returncode}")
                     logger.error(f"Python executable used: {python_executable}")
                     logger.error(f"Python executable exists: {os.path.exists(python_executable)}")
                     logger.error(f"manage.py exists: {os.path.exists('manage.py')}")
                     logger.error(f"Current directory: {os.getcwd()}")
                     logger.error(f"Stdout: {migrate_result.stdout}")
@@ -737,21 +651,17 @@
             migrate_time = time.time() - migrate_start
             logger.info(f"Migrate completed in {migrate_time:.2f}s")
             logger.info(f"Migrate output: {migrate_result.stdout.strip()}")
 
             total_time = time.time() - start_time
-            logger.info(
-                f"All Django migrations completed in {total_time:.2f}s total via subprocess"
-            )
+            logger.info(f"All Django migrations completed in {total_time:.2f}s total via subprocess")
 
             self._create_setup_ready_marker(base_dir)
 
         except subprocess.TimeoutExpired as e:
             logger.error(f"Migration subprocess timed out: {e}")
-            raise RuntimeError(
-                f"Migration subprocess timed out after {e.timeout} seconds"
-            )
+            raise RuntimeError(f"Migration subprocess timed out after {e.timeout} seconds")
         except Exception as e:
             logger.error(f"Migration subprocess failed: {e}")
             raise
         finally:
             os.chdir(original_dir)
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/entry_points/automode_database_setup.py

Oh no! 💥 💔 💥
27 files would be reformatted.

Checking formatting with Black: pybirdai/process_steps/
-----------------------------------
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/ancrdt_transformation/ancrdt_importer.py	2025-09-15 13:18:11.375057+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/ancrdt_transformation/ancrdt_importer.py	2025-09-21 17:07:36.225982+00:00
@@ -19,10 +19,11 @@
 import logging
 
 # Create a logger
 logger = logging.getLogger(__name__)
 
+
 class DjangoSetup:
     _initialized = False
 
     @classmethod
     def configure_django(cls):
@@ -30,23 +31,23 @@
         if cls._initialized:
             return
 
         try:
             # Set up Django settings module for birds_nest in parent directory
-            project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '../../..'))
+            project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../.."))
             sys.path.insert(0, project_root)
-            os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'birds_nest.settings')
+            os.environ.setdefault("DJANGO_SETTINGS_MODULE", "birds_nest.settings")
 
             # This allows us to use Django models without running the server
             django.setup()
 
-            logger.info("Django configured successfully with settings module: %s",
-                       os.environ['DJANGO_SETTINGS_MODULE'])
+            logger.info("Django configured successfully with settings module: %s", os.environ["DJANGO_SETTINGS_MODULE"])
             cls._initialized = True
         except Exception as e:
             logger.error(f"Django configuration failed: {str(e)}")
             raise
+
 
 class RunANCRDTImport(AppConfig):
     """
     Django AppConfig for running the website to SDD model conversion process.
 
@@ -60,35 +61,38 @@
     def run_import():
         DjangoSetup.configure_django()
         from pybirdai.process_steps.ancrdt_transformation.context_ancrdt import Context
         from pybirdai.process_steps.ancrdt_transformation.sdd_context_django_ancrdt import SDDContext
         from pybirdai.process_steps.ancrdt_transformation.import_website_to_sdd_model_django_ancrdt import (
-            ImportWebsiteToSDDModel
+            ImportWebsiteToSDDModel,
         )
+
         # Move the content of the ready() method here
-        path = os.path.join(settings.BASE_DIR, 'birds_nest')
+        path = os.path.join(settings.BASE_DIR, "birds_nest")
 
         base_dir = settings.BASE_DIR
         sdd_context = SDDContext()
-        sdd_context.file_directory = os.path.join(base_dir, 'resources')
-        sdd_context.output_directory = os.path.join(base_dir, 'results')
+        sdd_context.file_directory = os.path.join(base_dir, "resources")
+        sdd_context.output_directory = os.path.join(base_dir, "results")
         sdd_context.save_sdd_to_db = True
 
         context = Context()
         context.file_directory = sdd_context.file_directory
         context.output_directory = sdd_context.output_directory
 
         import_anacrdt_path = f"..{os.sep}results{os.sep}ancrdt_csv"
         ancrdt_include = True
         if not sdd_context.exclude_reference_info_from_website:
-            ImportWebsiteToSDDModel().import_report_templates_from_sdd(sdd_context,import_anacrdt_path,ancrdt_include)
+            ImportWebsiteToSDDModel().import_report_templates_from_sdd(sdd_context, import_anacrdt_path, ancrdt_include)
 
     def ready(self):
         # This method is still needed for Django's AppConfig
         pass
 
+
 def main():
     DjangoSetup.configure_django()
     RunANCRDTImport.run_import()
 
+
 if __name__ == "__main__":
     main()
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/ancrdt_transformation/ancrdt_importer.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/ancrdt_transformation/create_executable_joins_ancrdt.py	2025-09-15 13:18:11.375712+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/ancrdt_transformation/create_executable_joins_ancrdt.py	2025-09-21 17:07:36.259109+00:00
@@ -22,18 +22,16 @@
 import logging
 
 # Configure logging
 logging.basicConfig(
     level=logging.INFO,
-    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
-    handlers=[
-        logging.FileHandler("log.log"),
-        logging.StreamHandler()
-    ]
+    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
+    handlers=[logging.FileHandler("log.log"), logging.StreamHandler()],
 )
 
 logger = logging.getLogger(__name__)
+
 
 class DjangoSetup:
     _initialized = False
 
     @classmethod
@@ -42,72 +40,75 @@
         if cls._initialized:
             return
 
         try:
             # Set up Django settings module for birds_nest in parent directory
-            project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '../../..'))
+            project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../.."))
             sys.path.insert(0, project_root)
-            os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'birds_nest.settings')
+            os.environ.setdefault("DJANGO_SETTINGS_MODULE", "birds_nest.settings")
 
             # This allows us to use Django models without running the server
             django.setup()
 
-            logger.info("Django configured successfully with settings module: %s",
-                       os.environ['DJANGO_SETTINGS_MODULE'])
+            logger.info("Django configured successfully with settings module: %s", os.environ["DJANGO_SETTINGS_MODULE"])
             cls._initialized = True
         except Exception as e:
             logger.error(f"Django configuration failed: {str(e)}")
             raise
 
+
 class RunCreateExecutableJoins(AppConfig):
     """Django AppConfig for running the creation of generation rules."""
 
     DjangoSetup.configure_django()
-    path = os.path.join(settings.BASE_DIR, 'birds_nest')
+    path = os.path.join(settings.BASE_DIR, "birds_nest")
 
     @staticmethod
     def create_python_joins_from_db(logger=logger):
         """Execute the process of creating generation rules from the database when the app is ready."""
 
-        from pybirdai.process_steps.input_model.import_database_to_sdd_model import (
-            ImportDatabaseToSDDModel
-        )
+        from pybirdai.process_steps.input_model.import_database_to_sdd_model import ImportDatabaseToSDDModel
         from pybirdai.context.sdd_context_django import SDDContext
         from pybirdai.context.context import Context
         from pybirdai.process_steps.ancrdt_transformation.create_python_django_transformations_ancrdt import (
-            CreatePythonTransformations
+            CreatePythonTransformations,
         )
 
         base_dir = settings.BASE_DIR
         sdd_context = SDDContext()
-        sdd_context.file_directory = os.path.join(base_dir, 'resources')
-        sdd_context.output_directory = os.path.join(base_dir, 'results')
+        sdd_context.file_directory = os.path.join(base_dir, "resources")
+        sdd_context.output_directory = os.path.join(base_dir, "results")
 
         context = Context()
         context.file_directory = sdd_context.file_directory
         context.output_directory = sdd_context.output_directory
 
         # Only import the necessary tables for joins
         importer = ImportDatabaseToSDDModel()
 
-        importer.import_sdd_for_joins(sdd_context, [
-            'MAINTENANCE_AGENCY',
-            'DOMAIN',
-            'VARIABLE',
-            'CUBE',
-            'CUBE_STRUCTURE',
-            'CUBE_STRUCTURE_ITEM',
-            'CUBE_LINK',
-            'CUBE_STRUCTURE_ITEM_LINK'
-        ])
-        CreatePythonTransformations().create_python_joins(context, sdd_context,logger)
+        importer.import_sdd_for_joins(
+            sdd_context,
+            [
+                "MAINTENANCE_AGENCY",
+                "DOMAIN",
+                "VARIABLE",
+                "CUBE",
+                "CUBE_STRUCTURE",
+                "CUBE_STRUCTURE_ITEM",
+                "CUBE_LINK",
+                "CUBE_STRUCTURE_ITEM_LINK",
+            ],
+        )
+        CreatePythonTransformations().create_python_joins(context, sdd_context, logger)
 
     def ready(self):
         # This method is still needed for Django's AppConfig
         pass
 
+
 def main():
     DjangoSetup.configure_django()
     RunCreateExecutableJoins.create_python_joins_from_db()
 
+
 if __name__ == "__main__":
     main()
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/ancrdt_transformation/create_executable_joins_ancrdt.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/ancrdt_transformation/sdd_context_django_ancrdt.py	2025-09-15 13:18:11.377902+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/ancrdt_transformation/sdd_context_django_ancrdt.py	2025-09-21 17:07:36.260705+00:00
@@ -10,15 +10,15 @@
 # Contributors:
 #    Neil Mackenzie - initial API and implementation
 #
 
 
+class SDDContext(object):
+    """
+    Documentation for Context
+    """
 
-class SDDContext(object):
-    '''
-    Documentation for Context
-    '''
     # variables to configure the behaviour
 
     use_codes = True
 
     # the directory where we get our input files
@@ -32,28 +32,25 @@
     member_plus_hierarchy_to_child_literals = {}
     domain_to_hierarchy_dictionary = {}
     combinations_dictionary = {}
     member_dictionary = {}
     domain_dictionary = {}
-    variable_dictionary= {}
+    variable_dictionary = {}
     member_hierarchy_dictionary = {}
     member_hierarchy_node_dictionary = {}
     bird_cube_structure_dictionary = {}
     bird_cube_dictionary = {}
     bird_cube_structure_item_dictionary = {}
     bird_cube_structure_dictionary = {}
 
-
-
     combination_dictionary = {}
     combination_item_dictionary = {}
     combination_to_rol_cube_map = {}
 
-
-    axis_ordinate_dictionary= {}
-    table_cell_dictionary= {}
-    table_to_table_cell_dictionary= {}
+    axis_ordinate_dictionary = {}
+    table_cell_dictionary = {}
+    table_to_table_cell_dictionary = {}
     member_mapping_dictionary = {}
     member_mapping_items_dictionary = {}
     cell_positions_dictionary = {}
     variable_set_enumeration_dictionary = {}
     report_tables_dictionary = {}
@@ -75,13 +72,11 @@
     variable_to_primary_concept_map = {}
 
     combination_to_typ_instrmnt_map = {}
     table_to_combination_dictionary = {}
 
-
-
-     # For the reference output layers we record a map between members ids
+    # For the reference output layers we record a map between members ids
     # andtheir containing domains
     member_id_to_domain_map = {}
 
     # For the reference output layers we record a map between members ids
     # and their codes
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/ancrdt_transformation/sdd_context_django_ancrdt.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/ancrdt_transformation/context_ancrdt.py	2025-09-15 13:18:11.375411+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/ancrdt_transformation/context_ancrdt.py	2025-09-21 17:07:36.264803+00:00
@@ -10,21 +10,23 @@
 # Contributors:
 #    Neil Mackenzie - initial API and implementation
 #
 
 
-from pybirdai.regdna import  ELPackage, ModuleList, GenerationRulesModule, ReportModule, ELAnnotationDirective
+from pybirdai.regdna import ELPackage, ModuleList, GenerationRulesModule, ReportModule, ELAnnotationDirective
 from pybirdai.context.ecore_lite_types import EcoreLiteTypes
 
+
 class Context(object):
-    '''
+    """
     Documentation for Context
-    '''
+    """
+
     # variables to configure the behaviour
 
-    ldm_or_il = 'il'
-    alternative_folder_for_subdomains = 'sqldev_subdomains'
+    ldm_or_il = "il"
+    alternative_folder_for_subdomains = "sqldev_subdomains"
 
     enrich_ldm_relationships = False
     use_codes = True
 
     reference_data_class_list = []
@@ -37,31 +39,26 @@
     types = EcoreLiteTypes()
     # create the moduleList to hold all the modules
     module_list = ModuleList()
 
     # create  regdna  packages
-    types_package = ELPackage(name='types')
+    types_package = ELPackage(name="types")
     ldm_domains_package = ELPackage(
-        name='ldm_domains',
-        nsURI='http://www.eclipse.org/bird/ldm_domains',
-        nsPrefix='ldm_domains')
+        name="ldm_domains", nsURI="http://www.eclipse.org/bird/ldm_domains", nsPrefix="ldm_domains"
+    )
 
     ldm_entities_package = ELPackage(
-        name='ldm_entities',
-        nsURI='http://www.eclipse.org/bird/ldm_entities',
-        nsPrefix='ldm_entities')
+        name="ldm_entities", nsURI="http://www.eclipse.org/bird/ldm_entities", nsPrefix="ldm_entities"
+    )
 
     il_domains_package = ELPackage(
-        name='il_domains',
-        nsURI='http://www.eclipse.org/bird/il_domains',
-        nsPrefix='ldm_domains')
+        name="il_domains", nsURI="http://www.eclipse.org/bird/il_domains", nsPrefix="ldm_domains"
+    )
 
     il_tables_package = ELPackage(
-        name='il_entities',
-        nsURI='http://www.eclipse.org/bird/il_entities',
-        nsPrefix='il_entities')
-
+        name="il_entities", nsURI="http://www.eclipse.org/bird/il_entities", nsPrefix="il_entities"
+    )
 
     skip_reference_data_in_ldm = True
     reports_dictionary = {}
 
     classification_types = {}
@@ -113,30 +110,37 @@
     enums_used = []
 
     main_categories_in_scope_finrep = []
     main_categories_in_scope_ae = []
 
-    load_sdd_from_website =False
+    load_sdd_from_website = False
 
     save_derived_sdd_items = True
 
     def __init__(self):
 
-        ldm_key_annotation_directive = ELAnnotationDirective(name='key', sourceURI='key')
-        ldm_dependency_annotation_directive = ELAnnotationDirective(name='dep', sourceURI='dep')
-        ldm_entity_hierarchy_annotation_directive = ELAnnotationDirective(name='entity_hierarchy', sourceURI='entity_hierarchy')
-        ldm_relationship_type_annotation_directive = ELAnnotationDirective(name='relationship_type', sourceURI='relationship_type')
-        code_annotation_directive = ELAnnotationDirective(name='code', sourceURI='code')
-        long_name_directive_ldm_entities = ELAnnotationDirective(name='long_name', sourceURI='long_name')
+        ldm_key_annotation_directive = ELAnnotationDirective(name="key", sourceURI="key")
+        ldm_dependency_annotation_directive = ELAnnotationDirective(name="dep", sourceURI="dep")
+        ldm_entity_hierarchy_annotation_directive = ELAnnotationDirective(
+            name="entity_hierarchy", sourceURI="entity_hierarchy"
+        )
+        ldm_relationship_type_annotation_directive = ELAnnotationDirective(
+            name="relationship_type", sourceURI="relationship_type"
+        )
+        code_annotation_directive = ELAnnotationDirective(name="code", sourceURI="code")
+        long_name_directive_ldm_entities = ELAnnotationDirective(name="long_name", sourceURI="long_name")
 
-        il_key_annotation_directive = ELAnnotationDirective(name='key', sourceURI='key')
-        il_dependency_annotation_directive = ELAnnotationDirective(name='dep', sourceURI='dep')
-        il_entity_hierarchy_annotation_directive = ELAnnotationDirective(name='entity_hierarchy', sourceURI='entity_hierarchy')
-        il_relationship_type_annotation_directive = ELAnnotationDirective(name='relationship_type', sourceURI='relationship_type')
-        il_code_annotation_directive = ELAnnotationDirective(name='code', sourceURI='code')
-        long_name_directive_il_entities = ELAnnotationDirective(name='long_name', sourceURI='long_name')
-
+        il_key_annotation_directive = ELAnnotationDirective(name="key", sourceURI="key")
+        il_dependency_annotation_directive = ELAnnotationDirective(name="dep", sourceURI="dep")
+        il_entity_hierarchy_annotation_directive = ELAnnotationDirective(
+            name="entity_hierarchy", sourceURI="entity_hierarchy"
+        )
+        il_relationship_type_annotation_directive = ELAnnotationDirective(
+            name="relationship_type", sourceURI="relationship_type"
+        )
+        il_code_annotation_directive = ELAnnotationDirective(name="code", sourceURI="code")
+        long_name_directive_il_entities = ELAnnotationDirective(name="long_name", sourceURI="long_name")
 
         self.ldm_entities_package.annotationDirectives.append(ldm_key_annotation_directive)
         self.ldm_entities_package.annotationDirectives.append(ldm_dependency_annotation_directive)
         self.ldm_entities_package.annotationDirectives.append(ldm_entity_hierarchy_annotation_directive)
         self.ldm_entities_package.annotationDirectives.append(ldm_relationship_type_annotation_directive)
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/ancrdt_transformation/context_ancrdt.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/dpm_integration/mapping_functions/__init__.py	2025-09-21 16:25:09.555303+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/dpm_integration/mapping_functions/__init__.py	2025-09-21 17:07:36.276280+00:00
@@ -33,66 +33,57 @@
     drop_fields,
     select_fields,
     add_field,
     merge_arrays,
     array_to_dict,
-    clean_spaces
+    clean_spaces,
 )
 
 # Import all mapping functions
 from .frameworks import map_frameworks
 from .domains import map_domains
 from .members import map_members
 from .dimensions import map_dimensions
-from .tables import (
-    map_tables,
-    load_template_to_framework_mapping,
-    load_taxonomy_version_to_table_mapping
-)
+from .tables import map_tables, load_template_to_framework_mapping, load_taxonomy_version_to_table_mapping
 from .axis import map_axis
 from .axis_ordinate import map_axis_ordinate
 from .table_cell import map_table_cell
 from .cell_position import map_cell_position
 from .datapoint_version import map_datapoint_version
 from .context_definition import map_context_definition
 from .hierarchy import map_hierarchy
 from .hierarchy_node import map_hierarchy_node
-from .ordinate_categorisation import (
-    map_ordinate_categorisation,
-    traceback_restrictions
-)
+from .ordinate_categorisation import map_ordinate_categorisation, traceback_restrictions
 
 # Define what gets exported when using "from mapping_functions import *"
 __all__ = [
     # Utility functions
-    'pascal_to_upper_snake',
-    'read_csv_to_dict',
-    'dict_list_to_structured_array',
-    'rename_fields',
-    'drop_fields',
-    'select_fields',
-    'add_field',
-    'merge_arrays',
-    'array_to_dict',
-    'clean_spaces',
-
+    "pascal_to_upper_snake",
+    "read_csv_to_dict",
+    "dict_list_to_structured_array",
+    "rename_fields",
+    "drop_fields",
+    "select_fields",
+    "add_field",
+    "merge_arrays",
+    "array_to_dict",
+    "clean_spaces",
     # Mapping functions
-    'map_frameworks',
-    'map_domains',
-    'map_members',
-    'map_dimensions',
-    'map_tables',
-    'map_axis',
-    'map_axis_ordinate',
-    'map_table_cell',
-    'map_cell_position',
-    'map_datapoint_version',
-    'map_context_definition',
-    'map_hierarchy',
-    'map_hierarchy_node',
-    'map_ordinate_categorisation',
-
+    "map_frameworks",
+    "map_domains",
+    "map_members",
+    "map_dimensions",
+    "map_tables",
+    "map_axis",
+    "map_axis_ordinate",
+    "map_table_cell",
+    "map_cell_position",
+    "map_datapoint_version",
+    "map_context_definition",
+    "map_hierarchy",
+    "map_hierarchy_node",
+    "map_ordinate_categorisation",
     # Helper functions
-    'load_template_to_framework_mapping',
-    'load_taxonomy_version_to_table_mapping',
-    'traceback_restrictions'
-]
\ No newline at end of file
+    "load_template_to_framework_mapping",
+    "load_taxonomy_version_to_table_mapping",
+    "traceback_restrictions",
+]
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/dpm_integration/mapping_functions/__init__.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/ancrdt_transformation/filter_buildr.py	2025-09-15 13:18:11.376942+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/ancrdt_transformation/filter_buildr.py	2025-09-21 17:07:36.278792+00:00
@@ -38,13 +38,11 @@
     @staticmethod
     def configure_django():
         """Configure Django settings without starting the application"""
         if not settings.configured:
             # Set up Django settings module for birds_nest in parent directory
-            project_root = os.path.abspath(
-                os.path.join(os.path.dirname(__file__), "../..")
-            )
+            project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "../.."))
             sys.path.insert(0, project_root)
             os.environ["DJANGO_SETTINGS_MODULE"] = "birds_nest.settings"
             logger.info(
                 "Configuring Django with settings module: %s",
                 os.environ["DJANGO_SETTINGS_MODULE"],
@@ -57,20 +55,18 @@
     @staticmethod
     def define_filter_from_structure_link(
         cube_structure_item_link_id: str,
     ):
         DjangoSetup.configure_django()
-        from pybirdai.models.bird_meta_data_model import CUBE_LINK, MEMBER_LINK,CUBE_STRUCTURE_ITEM_LINK
+        from pybirdai.models.bird_meta_data_model import CUBE_LINK, MEMBER_LINK, CUBE_STRUCTURE_ITEM_LINK
 
         conditions = []
         # Query related MEMBER_LINK objects for the given cube_structure_item_link
         cube_structure_item_link = CUBE_STRUCTURE_ITEM_LINK.objects.get(
-            cube_structure_item_link_id = cube_structure_item_link_id
+            cube_structure_item_link_id=cube_structure_item_link_id
         )
-        member_links = MEMBER_LINK.objects.filter(
-            cube_structure_item_link_id=cube_structure_item_link
-        )
+        member_links = MEMBER_LINK.objects.filter(cube_structure_item_link_id=cube_structure_item_link)
 
         # Group member links by their associated foreign variables
         variable_links = {}
         for member_link in member_links:
             foreign_var_code = member_link.cube_structure_item_link_id.foreign_cube_variable_code.cube_variable_code
@@ -78,18 +74,16 @@
                 variable_links[foreign_var_code] = []
             variable_links[foreign_var_code].append(member_link)
 
         # Build conditions for each foreign variable
         for foreign_var_code, links in variable_links.items():
-            foreign_var_name = ast.Name(id="item."+foreign_var_code, ctx=ast.Load())
+            foreign_var_name = ast.Name(id="item." + foreign_var_code, ctx=ast.Load())
             member_comparisons = []
 
             for member_link in links:
                 member_code = ast.Constant(value=member_link.foreign_member_id.code)
-                comparison = ast.Compare(
-                    left=foreign_var_name, ops=[ast.Eq()], comparators=[member_code]
-                )
+                comparison = ast.Compare(left=foreign_var_name, ops=[ast.Eq()], comparators=[member_code])
                 member_comparisons.append(comparison)
 
             if len(member_comparisons) > 1:
                 conditions.append(ast.BoolOp(op=ast.Or(), values=member_comparisons))
             elif len(member_comparisons) == 1:
@@ -100,37 +94,35 @@
         elif len(conditions) == 1:
             filter_rule = conditions[0]
         else:
             filter_rule = ast.Constant(value=True)
 
-        return "("+ast.unparse(filter_rule)+")"
+        return "(" + ast.unparse(filter_rule) + ")"
 
     @staticmethod
     def reverse_apply_member_links(
         cube_structure_item_link_id: str,
     ):
         DjangoSetup.configure_django()
-        from pybirdai.models.bird_meta_data_model import CUBE_LINK, MEMBER_LINK,CUBE_STRUCTURE_ITEM_LINK
+        from pybirdai.models.bird_meta_data_model import CUBE_LINK, MEMBER_LINK, CUBE_STRUCTURE_ITEM_LINK
+
         conditions = []
         # Query related MEMBER_LINK objects for the given cube_structure_item_link
         cube_structure_item_link = CUBE_STRUCTURE_ITEM_LINK.objects.get(
-            cube_structure_item_link_id = cube_structure_item_link_id
+            cube_structure_item_link_id=cube_structure_item_link_id
         )
-        member_links = MEMBER_LINK.objects.filter(
-            cube_structure_item_link_id=cube_structure_item_link
-        )
+        member_links = MEMBER_LINK.objects.filter(cube_structure_item_link_id=cube_structure_item_link)
 
         # Group member links by their associated foreign variables
         variable_links = {}
         for member_link in member_links:
             foreign_var_code = member_link.cube_structure_item_link_id.foreign_cube_variable_code.cube_variable_code
             if foreign_var_code not in variable_links:
                 variable_links[foreign_var_code] = []
-            variable_links[foreign_var_code].append({
-                "source":member_link.primary_member_id.code,
-                "target":member_link.foreign_member_id.code
-            })
+            variable_links[foreign_var_code].append(
+                {"source": member_link.primary_member_id.code, "target": member_link.foreign_member_id.code}
+            )
 
         return variable_links
 
 
 if __name__ == "__main__":
@@ -144,9 +136,9 @@
     # TransformationBuildr.print_cube_variables_comparison(
     #     "ANCRDT_INSTRMNT_C_1:INSTRMNT_RL:Loans and advances"
     # )
     # TransformationBuildr.find_similarities()
     print(
-    TransformationBuildr.define_filter_from_structure_link(
-        "ANCRDT_INSTRMNT_C_1:INSTRMNT_RL:Loans and advances:TYP_INSTRMNT:TYP_INSTRMNT_RL"
+        TransformationBuildr.define_filter_from_structure_link(
+            "ANCRDT_INSTRMNT_C_1:INSTRMNT_RL:Loans and advances:TYP_INSTRMNT:TYP_INSTRMNT_RL"
+        )
     )
-    )
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/ancrdt_transformation/filter_buildr.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/dpm_integration/mapping_functions/context_definition.py	2025-09-21 16:23:45.743154+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/dpm_integration/mapping_functions/context_definition.py	2025-09-21 17:07:36.293628+00:00
@@ -11,22 +11,21 @@
 #    Benjamin Arfa - initial API and implementation
 #
 
 import os
 from collections import defaultdict
-from .utils import (
-    read_csv_to_dict, dict_list_to_structured_array, add_field,
-    rename_fields, pascal_to_upper_snake
-)
+from .utils import read_csv_to_dict, dict_list_to_structured_array, add_field, rename_fields, pascal_to_upper_snake
 
 
-def map_context_definition(path=os.path.join("target", "ContextDefinition.csv"), dimension_map: dict = {}, member_map: dict = {}):
+def map_context_definition(
+    path=os.path.join("target", "ContextDefinition.csv"), dimension_map: dict = {}, member_map: dict = {}
+):
     """Map context definitions from ContextDefinition.csv to the target format"""
     types = defaultdict(lambda: str, ContextID="str")
     data_list = read_csv_to_dict(path)
     # Force ID fields to be strings since they will be mapped to string values
-    data = dict_list_to_structured_array(data_list, force_str_columns={'DimensionID', 'MemberID'})
+    data = dict_list_to_structured_array(data_list, force_str_columns={"DimensionID", "MemberID"})
 
     column_mapping = {col: pascal_to_upper_snake(col) for col in data.dtype.names}
     data = rename_fields(data, column_mapping)
     data = add_field(data, "MAINTENANCE_AGENCY_ID", "EBA")
 
@@ -46,6 +45,6 @@
         member_ids.append(member_map.get(str(mem_id), str(mem_id)))
 
     for i, row in enumerate(data):
         data[i]["MEMBER_ID"] = member_ids[i]
 
-    return data, {}
\ No newline at end of file
+    return data, {}
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/dpm_integration/mapping_functions/context_definition.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/dpm_integration/mapping_functions/axis.py	2025-09-21 16:22:12.502803+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/dpm_integration/mapping_functions/axis.py	2025-09-21 17:07:36.299026+00:00
@@ -11,21 +11,27 @@
 #    Benjamin Arfa - initial API and implementation
 #
 
 import os
 from .utils import (
-    read_csv_to_dict, dict_list_to_structured_array, add_field, drop_fields,
-    select_fields, rename_fields, pascal_to_upper_snake, clean_spaces
+    read_csv_to_dict,
+    dict_list_to_structured_array,
+    add_field,
+    drop_fields,
+    select_fields,
+    rename_fields,
+    pascal_to_upper_snake,
+    clean_spaces,
 )
 
 
 def map_axis(path=os.path.join("target", "Axis.csv"), table_map: dict = {}):
     """Map axis from Axis.csv to the target format"""
     orientation_id_map = {"X": "1", "Y": "2", "Z": "3", "0": "0"}
     data = read_csv_to_dict(path)
     # Force ID fields to be strings since they will be mapped to string values
-    axes = dict_list_to_structured_array(data, force_str_columns={'AxisID', 'TableVID'})
+    axes = dict_list_to_structured_array(data, force_str_columns={"AxisID", "TableVID"})
 
     column_mapping = {col: pascal_to_upper_snake(col) for col in axes.dtype.names}
     axes = rename_fields(axes, column_mapping)
     axes = add_field(axes, "MAINTENANCE_AGENCY_ID", "EBA")
 
@@ -41,15 +47,11 @@
     for row in axes:
         id_mapping[str(row["AXIS_ID"])] = str(row["NEW_AXIS_ID"])
 
     axes = drop_fields(axes, "AXIS_ID")
 
-    axes = rename_fields(axes, {
-        "NEW_AXIS_ID": "AXIS_ID",
-        "AXIS_LABEL": "NAME",
-        "AXIS_ORIENTATION": "ORIENTATION"
-    })
+    axes = rename_fields(axes, {"NEW_AXIS_ID": "AXIS_ID", "AXIS_LABEL": "NAME", "AXIS_ORIENTATION": "ORIENTATION"})
 
     # Add CODE field
     codes = []
     for row in axes:
         parts = str(row["AXIS_ID"]).rsplit("_", 4)
@@ -82,20 +84,20 @@
     for row in axes:
         if "IS_OPEN_AXIS" in axes.dtype.names:
             val = str(row["IS_OPEN_AXIS"])
         else:
             val = "False"
-        is_open.append(val.lower() in ['true', '1', 'yes'])
+        is_open.append(val.lower() in ["true", "1", "yes"])
 
-    axes = add_field(axes, "IS_OPEN_AXIS_BOOL", is_open, dtype='bool')
+    axes = add_field(axes, "IS_OPEN_AXIS_BOOL", is_open, dtype="bool")
     if "IS_OPEN_AXIS" in axes.dtype.names:
         axes = drop_fields(axes, "IS_OPEN_AXIS")
     axes = rename_fields(axes, {"IS_OPEN_AXIS_BOOL": "IS_OPEN_AXIS"})
 
-    axes = select_fields(axes, [
-        "AXIS_ID", "CODE", "ORIENTATION", "ORDER", "NAME", "DESCRIPTION", "TABLE_ID", "IS_OPEN_AXIS"
-    ])
+    axes = select_fields(
+        axes, ["AXIS_ID", "CODE", "ORIENTATION", "ORDER", "NAME", "DESCRIPTION", "TABLE_ID", "IS_OPEN_AXIS"]
+    )
 
     # Clean text fields
     axes = clean_spaces(axes)
 
-    return axes, id_mapping
\ No newline at end of file
+    return axes, id_mapping
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/dpm_integration/mapping_functions/axis.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/ancrdt_transformation/csv_column_index_context_ancrdt.py	2025-08-02 18:37:08.448820+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/ancrdt_transformation/csv_column_index_context_ancrdt.py	2025-09-21 17:07:36.301353+00:00
@@ -8,10 +8,11 @@
 # SPDX-License-Identifier: EPL-2.0
 #
 # Contributors:
 #    Neil Mackenzie - initial API and implementation
 
+
 class ColumnIndexes(object):
 
     maintenance_agency_id = 0
     maintenance_agency_code = 1
     maintenance_agency_name = 2
@@ -58,29 +59,25 @@
     domain_domain_description = 4
     domain_domain_true_id = 1
     domain_domain_is_enumerated = 3
     domain_domain_is_reference = 8
 
-
     member_member_maintenence_agency = 0
     member_member_id_index = 1
     member_member_code_index = 2
     member_member_name_index = 3
     member_domain_id_index = 4
 
-
-
     subdomain_domain_id_index = 3
     subdomain_subdomain_id_index = 1
     subdomain_subdomain_code = 0
     subdomain_subdomain_description = 1
     subdomain_subdomain_name = 7
 
     subdomain_enumeration_member_id_index = 0
     subdomain_enumeration_subdomain_id_index = 1
     subdomain_enumeration_valid_to_index = 3
-
 
     cube_structure_code_index = 3
     cube_structure_id_index = 1
     cube_structure_name_index = 2
     cube_structure_valid_to_index = 6
@@ -99,24 +96,21 @@
 
     combination_combination_code = 1
     combination_combination_id = 0
     combination_combination_name = 2
 
-
     combination_item_combination_id = 0
     combination_item_variable_id = 1
-    combination_variable_set=3
+    combination_variable_set = 3
     combination_member_id = 4
-
 
     variable_set_enumeration_valid_to = 3
     variable_set_enumeration_variable_id = 1
     variable_set_enumeration_valid_set = 0
     variable_set_enumeration_subdomain_id = 4
 
     variable_set_variable_set_id = 1
-
 
     cube_structure_item_variable_index = 1
     cube_structure_item_class_id_index = 0
     cube_structure_item_subdomain_index = 5
     cube_structure_item_specific_member = 7
@@ -131,11 +125,11 @@
     combination_combination_maintenance_agency = 3
 
     combination_item_combination_id = 0
     combination_item_variable_id = 1
     combination_member_id = 4
-    combination_variable_set=3
+    combination_variable_set = 3
 
     member_mapping_id = 0
     member_mapping_row = 1
     member_mapping_variable_id = 2
     member_mapping_is_source = 3
@@ -160,11 +154,11 @@
     axis_code = 1
     axis_orientation = 2
     axis_order = 3
     axis_name = 4
     axis_description = 5
-    axis_table_id =  6
+    axis_table_id = 6
     axis_is_open_axis = 7
 
     axis_ordinate_axis_ordinate_id = 0
     axis_ordinate_is_abstract_header = 1
     axis_ordinate_code = 2
@@ -181,11 +175,10 @@
     ordinate_item_member_id = 2
     ordinate_item_member_hierarchy_id = 3
     ordinate_item_member_hierarchy_valid_from = 4
     ordinate_item_starting_member_id = 5
     ordinate_item_is_starting_member_included = 6
-
 
     cell_positions_cell_id = 0
     cell_positions_axis_ordinate_id = 1
 
     member_hierarchy_maintenance_agency = 0
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/ancrdt_transformation/csv_column_index_context_ancrdt.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/dpm_integration/mapping_functions/cell_position.py	2025-09-21 16:23:08.719388+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/dpm_integration/mapping_functions/cell_position.py	2025-09-21 17:07:36.303897+00:00
@@ -11,20 +11,29 @@
 #    Benjamin Arfa - initial API and implementation
 #
 
 import os
 from .utils import (
-    read_csv_to_dict, dict_list_to_structured_array, add_field, drop_fields,
-    rename_fields, pascal_to_upper_snake
+    read_csv_to_dict,
+    dict_list_to_structured_array,
+    add_field,
+    drop_fields,
+    rename_fields,
+    pascal_to_upper_snake,
 )
 
 
-def map_cell_position(path=os.path.join("target", "CellPosition.csv"), cell_map: dict = {}, ordinate_map: dict = {}, start_index_after_last: bool = False):
+def map_cell_position(
+    path=os.path.join("target", "CellPosition.csv"),
+    cell_map: dict = {},
+    ordinate_map: dict = {},
+    start_index_after_last: bool = False,
+):
     """Map cell positions from CellPosition.csv to the target format"""
     data_list = read_csv_to_dict(path)
     # Force ID fields to be strings since they will be mapped to string values
-    data = dict_list_to_structured_array(data_list, force_str_columns={'CellID', 'OrdinateID'})
+    data = dict_list_to_structured_array(data_list, force_str_columns={"CellID", "OrdinateID"})
 
     column_mapping = {col: pascal_to_upper_snake(col) for col in data.dtype.names}
     data = rename_fields(data, column_mapping)
 
     # Update CELL_ID
@@ -42,17 +51,17 @@
 
     for i, row in enumerate(data):
         data[i]["ORDINATE_ID"] = ordinate_ids[i]
 
     if start_index_after_last and "ID" in data.dtype.names and len(data) > 0:
-        max_id = max(int(float(row["ID"])) for row in data if str(row["ID"]) != 'nan')
+        max_id = max(int(float(row["ID"])) for row in data if str(row["ID"]) != "nan")
         start_idx = max_id + 1 if max_id else 0
         ids = list(range(start_idx, start_idx + len(data)))
         for i, row in enumerate(data):
             data[i]["ID"] = ids[i]
     else:
         if "ID" in data.dtype.names:
             data = drop_fields(data, "ID")
         ids = list(range(len(data)))
-        data = add_field(data, "ID", ids, dtype='i8')
+        data = add_field(data, "ID", ids, dtype="i8")
 
-    return data, {}
\ No newline at end of file
+    return data, {}
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/dpm_integration/mapping_functions/cell_position.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/dpm_integration/mapping_functions/axis_ordinate.py	2025-09-21 16:22:32.791370+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/dpm_integration/mapping_functions/axis_ordinate.py	2025-09-21 17:07:36.318233+00:00
@@ -12,21 +12,29 @@
 #
 
 import os
 from collections import defaultdict
 from .utils import (
-    read_csv_to_dict, dict_list_to_structured_array, add_field, drop_fields,
-    select_fields, rename_fields, pascal_to_upper_snake, clean_spaces
+    read_csv_to_dict,
+    dict_list_to_structured_array,
+    add_field,
+    drop_fields,
+    select_fields,
+    rename_fields,
+    pascal_to_upper_snake,
+    clean_spaces,
 )
 
 
 def map_axis_ordinate(path=os.path.join("target", "AxisOrdinate.csv"), axis_map: dict = {}):
     """Map axis ordinates from AxisOrdinate.csv to the target format"""
     types = defaultdict(lambda: str, OrdinateID="int", OrdinateCode="str", AxisID="int")
     data = read_csv_to_dict(path)
     # Force ID and PATH fields to be strings since they will be mapped to string values
-    ordinates = dict_list_to_structured_array(data, force_str_columns={'OrdinateID', 'AxisID', 'ParentOrdinateID', 'Path'})
+    ordinates = dict_list_to_structured_array(
+        data, force_str_columns={"OrdinateID", "AxisID", "ParentOrdinateID", "Path"}
+    )
 
     column_mapping = {col: pascal_to_upper_snake(col) for col in ordinates.dtype.names}
     ordinates = rename_fields(ordinates, column_mapping)
     ordinates = add_field(ordinates, "MAINTENANCE_AGENCY_ID", "EBA")
 
@@ -42,16 +50,19 @@
     for row in ordinates:
         id_mapping[str(row["ORDINATE_ID"])] = str(row["NEW_ORDINATE_ID"])
 
     ordinates = drop_fields(ordinates, "ORDINATE_ID")
 
-    ordinates = rename_fields(ordinates, {
-        "NEW_ORDINATE_ID": "AXIS_ORDINATE_ID",
-        "ORDINATE_CODE": "CODE",
-        "PARENT_ORDINATE_ID": "PARENT_AXIS_ORDINATE_ID",
-        "ORDINATE_LABEL": "NAME"
-    })
+    ordinates = rename_fields(
+        ordinates,
+        {
+            "NEW_ORDINATE_ID": "AXIS_ORDINATE_ID",
+            "ORDINATE_CODE": "CODE",
+            "PARENT_ORDINATE_ID": "PARENT_AXIS_ORDINATE_ID",
+            "ORDINATE_LABEL": "NAME",
+        },
+    )
 
     # Update PARENT_AXIS_ORDINATE_ID
     parent_ids = []
     for row in ordinates:
         parent_ids.append(id_mapping.get(str(row["PARENT_AXIS_ORDINATE_ID"]), str(row["PARENT_AXIS_ORDINATE_ID"])))
@@ -86,13 +97,25 @@
         paths.append(".".join(new_parts))
 
     for i, row in enumerate(ordinates):
         ordinates[i]["PATH"] = paths[i]
 
-    ordinates = select_fields(ordinates, [
-        "AXIS_ORDINATE_ID", "IS_ABSTRACT_HEADER", "CODE", "ORDER", "LEVEL", "PATH", "AXIS_ID", "PARENT_AXIS_ORDINATE_ID", "NAME", "DESCRIPTION"
-    ])
+    ordinates = select_fields(
+        ordinates,
+        [
+            "AXIS_ORDINATE_ID",
+            "IS_ABSTRACT_HEADER",
+            "CODE",
+            "ORDER",
+            "LEVEL",
+            "PATH",
+            "AXIS_ID",
+            "PARENT_AXIS_ORDINATE_ID",
+            "NAME",
+            "DESCRIPTION",
+        ],
+    )
 
     # Clean text fields
     ordinates = clean_spaces(ordinates)
 
-    return ordinates, id_mapping
\ No newline at end of file
+    return ordinates, id_mapping
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/dpm_integration/mapping_functions/axis_ordinate.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/dpm_integration/mapping_functions/domains.py	2025-09-21 16:20:37.441730+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/dpm_integration/mapping_functions/domains.py	2025-09-21 17:07:36.319882+00:00
@@ -11,12 +11,18 @@
 #    Benjamin Arfa - initial API and implementation
 #
 
 import os
 from .utils import (
-    read_csv_to_dict, dict_list_to_structured_array, add_field, drop_fields,
-    select_fields, rename_fields, pascal_to_upper_snake, clean_spaces
+    read_csv_to_dict,
+    dict_list_to_structured_array,
+    add_field,
+    drop_fields,
+    select_fields,
+    rename_fields,
+    pascal_to_upper_snake,
+    clean_spaces,
 )
 
 
 def map_domains(path=os.path.join("target", "Domain.csv")):
     """Map domains from Domain.csv to the target format"""
@@ -40,24 +46,38 @@
     for row in domains:
         id_mapping[str(row["DOMAIN_ID"])] = str(row["NEW_DOMAIN_ID"])
 
     domains = drop_fields(domains, "DOMAIN_ID")
 
-    domains = rename_fields(domains, {
-        "NEW_DOMAIN_ID": "DOMAIN_ID",
-        "DOMAIN_CODE": "CODE",
-        "DOMAIN_LABEL": "NAME",
-        "DOMAIN_DESCRIPTION": "DESCRIPTION",
-        "DATA_TYPE_ID": "DATA_TYPE",
-    })
+    domains = rename_fields(
+        domains,
+        {
+            "NEW_DOMAIN_ID": "DOMAIN_ID",
+            "DOMAIN_CODE": "CODE",
+            "DOMAIN_LABEL": "NAME",
+            "DOMAIN_DESCRIPTION": "DESCRIPTION",
+            "DATA_TYPE_ID": "DATA_TYPE",
+        },
+    )
 
     domains = clean_spaces(domains)
 
-    domains = add_field(domains, "FACET_ID", False, dtype='bool')
-    domains = add_field(domains, "IS_REFERENCE", False, dtype='bool')
-    domains = add_field(domains, "IS_ENUMERATED", False, dtype='bool')
+    domains = add_field(domains, "FACET_ID", False, dtype="bool")
+    domains = add_field(domains, "IS_REFERENCE", False, dtype="bool")
+    domains = add_field(domains, "IS_ENUMERATED", False, dtype="bool")
 
-    domains = select_fields(domains, [
-        "MAINTENANCE_AGENCY_ID", "DOMAIN_ID", "NAME", "IS_ENUMERATED", "DESCRIPTION", "DATA_TYPE", "CODE", "FACET_ID", "IS_REFERENCE"
-    ])
+    domains = select_fields(
+        domains,
+        [
+            "MAINTENANCE_AGENCY_ID",
+            "DOMAIN_ID",
+            "NAME",
+            "IS_ENUMERATED",
+            "DESCRIPTION",
+            "DATA_TYPE",
+            "CODE",
+            "FACET_ID",
+            "IS_REFERENCE",
+        ],
+    )
 
-    return domains, id_mapping
\ No newline at end of file
+    return domains, id_mapping
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/dpm_integration/mapping_functions/domains.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/dpm_integration/mapping_functions/frameworks.py	2025-09-21 16:20:25.904440+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/dpm_integration/mapping_functions/frameworks.py	2025-09-21 17:07:36.322603+00:00
@@ -10,24 +10,29 @@
 # Contributors:
 #    Benjamin Arfa - initial API and implementation
 #
 
 import os
-from .utils import (
-    read_csv_to_dict, dict_list_to_structured_array, add_field, drop_fields,
-    select_fields, clean_spaces
-)
+from .utils import read_csv_to_dict, dict_list_to_structured_array, add_field, drop_fields, select_fields, clean_spaces
 
 
 def map_frameworks(path=os.path.join("target", "ReportingFramework.csv")):
     """Map frameworks from ReportingFramework.csv to the target format"""
     data = read_csv_to_dict(path)
     frameworks = dict_list_to_structured_array(data)
 
     framework_columns = [
-        "MAINTENANCE_AGENCY_ID", "FRAMEWORK_ID", "NAME", "CODE", "DESCRIPTION",
-        "FRAMEWORK_TYPE", "REPORTING_POPULATION", "OTHER_LINKS", "ORDER", "FRAMEWORK_STATUS"
+        "MAINTENANCE_AGENCY_ID",
+        "FRAMEWORK_ID",
+        "NAME",
+        "CODE",
+        "DESCRIPTION",
+        "FRAMEWORK_TYPE",
+        "REPORTING_POPULATION",
+        "OTHER_LINKS",
+        "ORDER",
+        "FRAMEWORK_STATUS",
     ]
 
     # Add new fields
     frameworks = add_field(frameworks, "MAINTENANCE_AGENCY_ID", "EBA")
 
@@ -62,6 +67,6 @@
     frameworks = select_fields(frameworks, framework_columns)
 
     # Clean text fields
     frameworks = clean_spaces(frameworks)
 
-    return frameworks, framework_id_mapping
\ No newline at end of file
+    return frameworks, framework_id_mapping
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/dpm_integration/mapping_functions/frameworks.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/dpm_integration/mapping_functions/dimensions.py	2025-09-21 16:21:17.165688+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/dpm_integration/mapping_functions/dimensions.py	2025-09-21 17:07:36.328985+00:00
@@ -11,20 +11,26 @@
 #    Benjamin Arfa - initial API and implementation
 #
 
 import os
 from .utils import (
-    read_csv_to_dict, dict_list_to_structured_array, add_field, drop_fields,
-    select_fields, rename_fields, pascal_to_upper_snake, clean_spaces
+    read_csv_to_dict,
+    dict_list_to_structured_array,
+    add_field,
+    drop_fields,
+    select_fields,
+    rename_fields,
+    pascal_to_upper_snake,
+    clean_spaces,
 )
 
 
 def map_dimensions(path=os.path.join("target", "Dimension.csv"), domain_id_map: dict = {}):
     """Map dimensions from Dimension.csv to the target format"""
     data = read_csv_to_dict(path)
     # Force DomainID to be string since it will be mapped to string values
-    dimensions = dict_list_to_structured_array(data, force_str_columns={'DomainID'})
+    dimensions = dict_list_to_structured_array(data, force_str_columns={"DomainID"})
 
     # Transform column names to UPPER_SNAKE_CASE
     column_mapping = {col: pascal_to_upper_snake(col) for col in dimensions.dtype.names}
     dimensions = rename_fields(dimensions, column_mapping)
 
@@ -46,36 +52,50 @@
     # Create ID mapping
     id_mapping = {}
     for row in dimensions:
         id_mapping[str(row["DIMENSION_ID"])] = str(row["NEW_DIMENSION_ID"])
 
-    dimensions = rename_fields(dimensions, {
-        "MAINTENANCE_AGENCY_ID": "MAINTENANCE_AGENCY_ID",
-        "NEW_DIMENSION_ID": "VARIABLE_ID",
-        "DIMENSION_CODE": "CODE",
-        "DIMENSION_LABEL": "NAME",
-        "DIMENSION_DESCRIPTION": "DESCRIPTION"
-    })
+    dimensions = rename_fields(
+        dimensions,
+        {
+            "MAINTENANCE_AGENCY_ID": "MAINTENANCE_AGENCY_ID",
+            "NEW_DIMENSION_ID": "VARIABLE_ID",
+            "DIMENSION_CODE": "CODE",
+            "DIMENSION_LABEL": "NAME",
+            "DIMENSION_DESCRIPTION": "DESCRIPTION",
+        },
+    )
 
     dimensions = add_field(dimensions, "PRIMARY_CONCEPT", "")
-    dimensions = add_field(dimensions, "IS_DECOMPOSED", False, dtype='bool')
+    dimensions = add_field(dimensions, "IS_DECOMPOSED", False, dtype="bool")
 
     # Convert IS_IMPLIED_IF_NOT_EXPLICITLY_MODELLED to bool
     is_implied = []
     for row in dimensions:
         if "IS_IMPLIED_IF_NOT_EXPLICITLY_MODELLED" in dimensions.dtype.names:
             val = str(row["IS_IMPLIED_IF_NOT_EXPLICITLY_MODELLED"])
         else:
             val = "False"
-        is_implied.append(val.lower() in ['true', '1', 'yes'])
+        is_implied.append(val.lower() in ["true", "1", "yes"])
 
-    dimensions = add_field(dimensions, "IS_IMPLIED_BOOL", is_implied, dtype='bool')
+    dimensions = add_field(dimensions, "IS_IMPLIED_BOOL", is_implied, dtype="bool")
     dimensions = drop_fields(dimensions, "IS_IMPLIED_IF_NOT_EXPLICITLY_MODELLED")
     dimensions = rename_fields(dimensions, {"IS_IMPLIED_BOOL": "IS_IMPLIED_IF_NOT_EXPLICITLY_MODELLED"})
 
-    dimensions = select_fields(dimensions, [
-        "MAINTENANCE_AGENCY_ID", "VARIABLE_ID", "CODE", "NAME", "DOMAIN_ID", "DESCRIPTION", "PRIMARY_CONCEPT", "IS_DECOMPOSED", "IS_IMPLIED_IF_NOT_EXPLICITLY_MODELLED"
-    ])
+    dimensions = select_fields(
+        dimensions,
+        [
+            "MAINTENANCE_AGENCY_ID",
+            "VARIABLE_ID",
+            "CODE",
+            "NAME",
+            "DOMAIN_ID",
+            "DESCRIPTION",
+            "PRIMARY_CONCEPT",
+            "IS_DECOMPOSED",
+            "IS_IMPLIED_IF_NOT_EXPLICITLY_MODELLED",
+        ],
+    )
 
     dimensions = clean_spaces(dimensions)
 
-    return dimensions, id_mapping
\ No newline at end of file
+    return dimensions, id_mapping
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/ancrdt_transformation/create_joins_meta_data_ancrdt.py	2025-09-15 13:18:11.376137+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/ancrdt_transformation/create_joins_meta_data_ancrdt.py	2025-09-21 17:07:36.328384+00:00
@@ -17,31 +17,22 @@
 
 import sys
 import django
 from django.db.models import Q
 
-IGNORED_DOMAINS = [
-    "String",
-    "Integer",
-    "Date",
-    "Float",
-    "Boolean",
-    "FRQNCY"
-]
+IGNORED_DOMAINS = ["String", "Integer", "Date", "Float", "Boolean", "FRQNCY"]
 import logging
 
 # Configure logging
 logging.basicConfig(
     level=logging.INFO,
-    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
-    handlers=[
-        logging.FileHandler("log.log"),
-        logging.StreamHandler()
-    ]
+    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
+    handlers=[logging.FileHandler("log.log"), logging.StreamHandler()],
 )
 
 logger = logging.getLogger(__name__)
+
 
 class DjangoSetup:
     _initialized = False
 
     @classmethod
@@ -50,19 +41,18 @@
         if cls._initialized:
             return
 
         try:
             # Set up Django settings module for birds_nest in parent directory
-            project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '../../..'))
+            project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../.."))
             sys.path.insert(0, project_root)
-            os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'birds_nest.settings')
+            os.environ.setdefault("DJANGO_SETTINGS_MODULE", "birds_nest.settings")
 
             # This allows us to use Django models without running the server
             django.setup()
 
-            logger.info("Django configured successfully with settings module: %s",
-                       os.environ['DJANGO_SETTINGS_MODULE'])
+            logger.info("Django configured successfully with settings module: %s", os.environ["DJANGO_SETTINGS_MODULE"])
             cls._initialized = True
         except Exception as e:
             logger.error(f"Django configuration failed: {str(e)}")
             raise
 
@@ -73,38 +63,41 @@
     """
 
     def __init__(self):
         DjangoSetup.configure_django()
         self.join_map = {}
-        join_config_file1 = os.path.join(os.getcwd(),"resources/joins_configuration", "join_for_product_to_reference_category_ANCRDT_REF.csv")
-        join_config_file2 = os.path.join(os.getcwd(),"resources/joins_configuration", "join_for_product_il_definitions_ANCRDT_REF.csv")
+        join_config_file1 = os.path.join(
+            os.getcwd(), "resources/joins_configuration", "join_for_product_to_reference_category_ANCRDT_REF.csv"
+        )
+        join_config_file2 = os.path.join(
+            os.getcwd(), "resources/joins_configuration", "join_for_product_il_definitions_ANCRDT_REF.csv"
+        )
         try:
             # First, read all data from file2 into a dictionary for efficient lookup
             file2_data = {}
-            with open(join_config_file2, encoding='utf-8') as f2:
+            with open(join_config_file2, encoding="utf-8") as f2:
                 reader2 = csv.DictReader(f2)
                 for row2 in reader2:
-                    file2_data[row2['Name']] = row2
+                    file2_data[row2["Name"]] = row2
 
             # Now read file1 and match with file2 data
-            with open(join_config_file1, encoding='utf-8') as f1:
+            with open(join_config_file1, encoding="utf-8") as f1:
                 reader1 = csv.DictReader(f1)
                 for row1 in reader1:
-                    key = (row1['rolc'], row1['join_identifier'])
+                    key = (row1["rolc"], row1["join_identifier"])
                     # Look for matching entry in file2 data
-                    if row1['join_identifier'] in file2_data:
-                        row2 = file2_data[row1['join_identifier']]
+                    if row1["join_identifier"] in file2_data:
+                        row2 = file2_data[row1["join_identifier"]]
                         self.join_map[key] = {
-                            "rolc": row1['rolc'],
-                            "join_identifier": row1['join_identifier'],
-                            "ilc": [_
-                                for _ in [row2["Main Table"]] + row2["Related Tables"].split(":")
-                                if _
-                            ]
+                            "rolc": row1["rolc"],
+                            "join_identifier": row1["join_identifier"],
+                            "ilc": [_ for _ in [row2["Main Table"]] + row2["Related Tables"].split(":") if _],
                         }
                     else:
-                        logger.warning(f"No matching entry found in file2 for join_identifier: {row1['join_identifier']}")
+                        logger.warning(
+                            f"No matching entry found in file2 for join_identifier: {row1['join_identifier']}"
+                        )
 
             logger.info(f"Successfully loaded {len(self.join_map)} join configurations")
         except FileNotFoundError as e:
             logger.error(f"Join configuration file not found: {e.filename}")
             raise
@@ -115,11 +108,21 @@
     def generate_joins_meta_data(self) -> dict:
         """
         Generate generation rules for the given context and framework.
         """
         # Import here to ensure Django is fully configured first
-        from pybirdai.models.bird_meta_data_model import CUBE, CUBE_STRUCTURE_ITEM, CUBE_STRUCTURE, DOMAIN, VARIABLE, CUBE_LINK, CUBE_STRUCTURE_ITEM_LINK,MAINTENANCE_AGENCY,MEMBER_LINK
+        from pybirdai.models.bird_meta_data_model import (
+            CUBE,
+            CUBE_STRUCTURE_ITEM,
+            CUBE_STRUCTURE,
+            DOMAIN,
+            VARIABLE,
+            CUBE_LINK,
+            CUBE_STRUCTURE_ITEM_LINK,
+            MAINTENANCE_AGENCY,
+            MEMBER_LINK,
+        )
 
         ignored_domains = [DOMAIN.objects.get(domain_id=domain_id) for domain_id in IGNORED_DOMAINS]
 
         mock_join_identifier = "mock_join_identifier"
 
@@ -134,19 +137,14 @@
                 try:
                     ilc_cubes += [CUBE.objects.get(cube_id=cube)]
                 except:
                     logger.warning(f"{cube} not found")
 
-            rolc_items_to_match = {
-                rolc_cube.cube_id: self.fetch_cube_structure_items_dict(rolc_cube)
-            }
-
-            ilc_items_to_match = {
-                cube.cube_id: self.fetch_cube_structure_items_dict(cube)
-                for cube in ilc_cubes
-            }
-            comparison_results = self.compare(rolc_items_to_match, ilc_items_to_match,ignored_domains)
+            rolc_items_to_match = {rolc_cube.cube_id: self.fetch_cube_structure_items_dict(rolc_cube)}
+
+            ilc_items_to_match = {cube.cube_id: self.fetch_cube_structure_items_dict(cube) for cube in ilc_cubes}
+            comparison_results = self.compare(rolc_items_to_match, ilc_items_to_match, ignored_domains)
 
             for (rolc, ilc), matches in comparison_results.items():
                 rolc_cube = CUBE.objects.get(cube_id=rolc)
                 ilc_cube = CUBE.objects.get(cube_id=ilc)
                 rolc_cube_structure = CUBE_STRUCTURE.objects.get(cube=rolc_cube)
@@ -161,24 +159,24 @@
                     code=name_code_description,
                     name=name_code_description,
                     description=name_code_description,
                     valid_from=None,
                     valid_to=None,
-                    version=1
+                    version=1,
                 )
 
                 for (variable_rolc, variable_ilc), domain in matches.items():
                     rolc_cube_structure_item = CUBE_STRUCTURE_ITEM.objects.all().get(
-                        cube_structure_id = rolc_cube_structure,variable_id=variable_rolc
+                        cube_structure_id=rolc_cube_structure, variable_id=variable_rolc
                     )
                     ilc_cube_structure_item = CUBE_STRUCTURE_ITEM.objects.all().get(
-                        cube_structure_id = ilc_cube_structure,variable_id=variable_ilc
+                        cube_structure_id=ilc_cube_structure, variable_id=variable_ilc
                     )
 
                     csilink, exists = CUBE_STRUCTURE_ITEM_LINK.objects.get_or_create(
-                        cube_structure_item_link_id = f"{ilc}:{variable_ilc}:{mock_join_identifier}:{rolc}:{variable_rolc}",
-                        cube_link_id = cube_link,
+                        cube_structure_item_link_id=f"{ilc}:{variable_ilc}:{mock_join_identifier}:{rolc}:{variable_rolc}",
+                        cube_link_id=cube_link,
                         primary_cube_variable_code=ilc_cube_structure_item,
                         foreign_cube_variable_code=rolc_cube_structure_item,
                     )
 
                     for member in domain:
@@ -186,21 +184,20 @@
                             cube_structure_item_link_id=csilink,
                             primary_member_id=member,
                             foreign_member_id=member,
                             is_linked=True,
                             valid_from=None,
-                            valid_to=None
+                            valid_to=None,
                         )
 
                     if not MEMBER_LINK.objects.all().filter(cube_structure_item_link_id=csilink):
                         csilink.delete()
 
-
         return comparison_results
 
-    def compare(self, cube_items_1: dict, cube_items_2: dict,ignored_domains:list,flag_log:bool=False):
-        from pybirdai.models.bird_meta_data_model import CUBE_LINK, CUBE_STRUCTURE_ITEM_LINK,MAINTENANCE_AGENCY,MEMBER
+    def compare(self, cube_items_1: dict, cube_items_2: dict, ignored_domains: list, flag_log: bool = False):
+        from pybirdai.models.bird_meta_data_model import CUBE_LINK, CUBE_STRUCTURE_ITEM_LINK, MAINTENANCE_AGENCY, MEMBER
 
         matched_variables = dict()
         cube_iter = itertools.product(cube_items_1.items(), cube_items_2.items())
         for (key_rolc, value_rolc), (key_ilc, value_ilc) in cube_iter:
             cube_items_iter = itertools.product(value_rolc.items(), value_ilc.items())
@@ -213,42 +210,43 @@
                     self.fetch_members(infos_ilc["subdomain"])
                 )
                 if members:
                     if (key_rolc, key_ilc) not in matched_variables:
                         matched_variables[(key_rolc, key_ilc)] = {}
-                    matched_variables[(key_rolc, key_ilc)][(variable_rolc.variable_id, variable_ilc.variable_id)] = members
+                    matched_variables[(key_rolc, key_ilc)][
+                        (variable_rolc.variable_id, variable_ilc.variable_id)
+                    ] = members
         return matched_variables
 
-    def fetch_members(self,subdomain):
+    def fetch_members(self, subdomain):
         from pybirdai.models.bird_meta_data_model import MEMBER
-        members = MEMBER.objects.all().filter(
-            subdomain_enumeration__subdomain_id = subdomain
-        )
+
+        members = MEMBER.objects.all().filter(subdomain_enumeration__subdomain_id=subdomain)
         if not members:
             return set()
         return set(members)
 
     def fetch_cube_structure_items_dict(self, cube):
         # Import here to ensure Django is fully configured first
         from pybirdai.models.bird_meta_data_model import CUBE, CUBE_STRUCTURE_ITEM, CUBE_STRUCTURE
 
         cube_structure = CUBE_STRUCTURE.objects.get(cube=cube)
-        cube_structure_items = CUBE_STRUCTURE_ITEM.objects.all().filter(
-            cube_structure_id=cube_structure
-        )
+        cube_structure_items = CUBE_STRUCTURE_ITEM.objects.all().filter(cube_structure_id=cube_structure)
 
         return {
             csi.variable_id: {
                 "variable_or_cvc": csi.cube_variable_code or csi.variable_id,
                 "domain": csi.variable_id.domain_id,
                 "subdomain": csi.subdomain_id,
             }
             for csi in cube_structure_items
         }
 
+
 def main():
     DjangoSetup.configure_django()
     creator = JoinsMetaDataCreatorANCRDT()
     result = creator.generate_joins_meta_data()
 
+
 if __name__ == "__main__":
     main()
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/dpm_integration/mapping_functions/dimensions.py
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/ancrdt_transformation/create_joins_meta_data_ancrdt.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/automode/database_setup_first_use.py	2025-09-15 13:18:11.378405+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/automode/database_setup_first_use.py	2025-09-21 17:07:36.340889+00:00
@@ -21,10 +21,11 @@
 import ast
 
 # Create a logger
 logger = logging.getLogger(__name__)
 
+
 class DjangoSetup:
     _initialized = False
 
     @classmethod
     def configure_django(cls):
@@ -32,38 +33,39 @@
         if cls._initialized:
             return
 
         try:
             # Set up Django settings module for birds_nest in parent directory
-            project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '../..'))
+            project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "../.."))
             sys.path.insert(0, project_root)
-            os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'birds_nest.settings')
+            os.environ.setdefault("DJANGO_SETTINGS_MODULE", "birds_nest.settings")
 
             # This allows us to use Django models without running the server
             django.setup()
 
-            logger.info("Django configured successfully with settings module: %s",
-                       os.environ['DJANGO_SETTINGS_MODULE'])
+            logger.info("Django configured successfully with settings module: %s", os.environ["DJANGO_SETTINGS_MODULE"])
             cls._initialized = True
         except Exception as e:
             logger.error(f"Django configuration failed: {str(e)}")
             raise
 
+
 class RunDatabaseSetup(AppConfig):
     DjangoSetup.configure_django()
     from pybirdai.entry_points.create_django_models import RunCreateDjangoModels
-    app_config = RunCreateDjangoModels('pybirdai', 'birds_nest')
+
+    app_config = RunCreateDjangoModels("pybirdai", "birds_nest")
     app_config.ready()
 
     # File paths - Define paths relative to where the script is executed
     initial_migration_file = "pybirdai/migrations/0001_initial.py"
     db_file = "db.sqlite3"
     pybirdai_admin_path = "pybirdai/admin.py"
     pybirdai_meta_data_model_path = "pybirdai/models/bird_meta_data_model.py"
     results_admin_path = "results/database_configuration_files/admin.py"
-    pybirdai_models_path = "pybirdai/models/bird_data_model.py" # Target file
-    results_models_path = "results/database_configuration_files/models.py" # Source file
+    pybirdai_models_path = "pybirdai/models/bird_data_model.py"  # Target file
+    results_models_path = "results/database_configuration_files/models.py"  # Source file
 
     # --- Cleanup steps ---
 
     # Remove initial migration file - Strict interpretation: must exist to remove
     logger.info(f"Attempting to remove initial migration file: {initial_migration_file}")
@@ -73,11 +75,10 @@
             logger.info(f"Successfully removed {initial_migration_file}")
         except OSError as e:
             logger.error(f"Error removing file {initial_migration_file}: {e}")
             # Raising specific error for removal failure
             raise RuntimeError(f"Failed to remove file {initial_migration_file}") from e
-
 
     # Remove database file - Strict interpretation: must exist to remove
     logger.info(f"Attempting to remove database file: {db_file}")
     if os.path.exists(db_file):
         try:
@@ -124,18 +125,19 @@
     except IOError as e:
         logger.error(f"Error reading or writing files for {pybirdai_admin_path} update: {e}")
         # Raising specific error for file operation failure
         raise RuntimeError(f"Failed to update {pybirdai_admin_path}") from e
 
-
     # --- Update bird_data_model.py (models.py) ---
 
     logger.info(f"Updating {pybirdai_models_path}...")
     # Check if results file for reading (results/models.py) exists
     if not os.path.exists(results_models_path):
         logger.error(f"Results models file not found: {results_models_path}")
-        raise FileNotFoundError(f"Source file '{results_models_path}' not found. Cannot read results models.py content.")
+        raise FileNotFoundError(
+            f"Source file '{results_models_path}' not found. Cannot read results models.py content."
+        )
 
     try:
         # Read content from results file
         with open(results_models_path, "r") as f_read_results:
             results_models_str = f_read_results.read()
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/automode/database_setup_first_use.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/dpm_integration/mapping_functions/hierarchy.py	2025-09-21 16:23:58.352026+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/dpm_integration/mapping_functions/hierarchy.py	2025-09-21 17:07:36.344247+00:00
@@ -11,20 +11,26 @@
 #    Benjamin Arfa - initial API and implementation
 #
 
 import os
 from .utils import (
-    read_csv_to_dict, dict_list_to_structured_array, add_field, drop_fields,
-    select_fields, rename_fields, pascal_to_upper_snake, clean_spaces
+    read_csv_to_dict,
+    dict_list_to_structured_array,
+    add_field,
+    drop_fields,
+    select_fields,
+    rename_fields,
+    pascal_to_upper_snake,
+    clean_spaces,
 )
 
 
 def map_hierarchy(path=os.path.join("target", "Hierarchy.csv"), domain_id_map: dict = {}):
     """Map hierarchies from Hierarchy.csv to the target format"""
     data_list = read_csv_to_dict(path)
     # Force DomainID to be string since it will be mapped to string values
-    hierarchies = dict_list_to_structured_array(data_list, force_str_columns={'DomainID'})
+    hierarchies = dict_list_to_structured_array(data_list, force_str_columns={"DomainID"})
 
     column_mapping = {col: pascal_to_upper_snake(col) for col in hierarchies.dtype.names}
     hierarchies = rename_fields(hierarchies, column_mapping)
     hierarchies = add_field(hierarchies, "MAINTENANCE_AGENCY_ID", "EBA")
 
@@ -45,22 +51,34 @@
     # Generate id mapping
     id_mapping = {}
     for row in hierarchies:
         id_mapping[str(row["HIERARCHY_ID"])] = str(row["NEW_HIERARCHY_ID"])
 
-    hierarchies = rename_fields(hierarchies, {
-        "NEW_HIERARCHY_ID": "MEMBER_HIERARCHY_ID",
-        "HIERARCHY_CODE": "CODE",
-        "HIERARCHY_LABEL": "NAME",
-        "HIERARCHY_DESCRIPTION": "DESCRIPTION"
-    })
+    hierarchies = rename_fields(
+        hierarchies,
+        {
+            "NEW_HIERARCHY_ID": "MEMBER_HIERARCHY_ID",
+            "HIERARCHY_CODE": "CODE",
+            "HIERARCHY_LABEL": "NAME",
+            "HIERARCHY_DESCRIPTION": "DESCRIPTION",
+        },
+    )
 
-    hierarchies = add_field(hierarchies, "IS_MAIN_HIERARCHY", False, dtype='bool')
+    hierarchies = add_field(hierarchies, "IS_MAIN_HIERARCHY", False, dtype="bool")
 
-    hierarchies = select_fields(hierarchies, [
-        "MAINTENANCE_AGENCY_ID", "MEMBER_HIERARCHY_ID", "CODE", "DOMAIN_ID", "NAME", "DESCRIPTION", "IS_MAIN_HIERARCHY"
-    ])
+    hierarchies = select_fields(
+        hierarchies,
+        [
+            "MAINTENANCE_AGENCY_ID",
+            "MEMBER_HIERARCHY_ID",
+            "CODE",
+            "DOMAIN_ID",
+            "NAME",
+            "DESCRIPTION",
+            "IS_MAIN_HIERARCHY",
+        ],
+    )
 
     # Clean text fields
     hierarchies = clean_spaces(hierarchies)
 
-    return hierarchies, id_mapping
\ No newline at end of file
+    return hierarchies, id_mapping
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/dpm_integration/mapping_functions/hierarchy.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/dpm_integration/mapping_functions/members.py	2025-09-21 16:20:51.667290+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/dpm_integration/mapping_functions/members.py	2025-09-21 17:07:36.349523+00:00
@@ -12,20 +12,26 @@
 #
 
 import os
 import numpy as np
 from .utils import (
-    read_csv_to_dict, dict_list_to_structured_array, add_field, drop_fields,
-    select_fields, rename_fields, pascal_to_upper_snake, clean_spaces
+    read_csv_to_dict,
+    dict_list_to_structured_array,
+    add_field,
+    drop_fields,
+    select_fields,
+    rename_fields,
+    pascal_to_upper_snake,
+    clean_spaces,
 )
 
 
 def map_members(path=os.path.join("target", "Member.csv"), domain_id_map: dict = {}):
     """Map members from Member.csv to the target format"""
     data = read_csv_to_dict(path)
     # Force DomainID to be string since it will be mapped to string values
-    members = dict_list_to_structured_array(data, force_str_columns={'DomainID'})
+    members = dict_list_to_structured_array(data, force_str_columns={"DomainID"})
 
     # Transform column names to UPPER_SNAKE_CASE
     column_mapping = {col: pascal_to_upper_snake(col) for col in members.dtype.names}
     members = rename_fields(members, column_mapping)
 
@@ -58,23 +64,24 @@
     id_mapping = {}
     for row in members:
         id_mapping[str(row["MEMBER_ID"])] = str(row["NEW_MEMBER_ID"])
 
     members = drop_fields(members, "MEMBER_ID")
-    members = rename_fields(members, {
-        "NEW_MEMBER_ID": "MEMBER_ID",
-        "MEMBER_CODE": "CODE",
-        "MEMBER_LABEL": "NAME",
-        "MEMBER_DESCRIPTION": "DESCRIPTION",
-    })
+    members = rename_fields(
+        members,
+        {
+            "NEW_MEMBER_ID": "MEMBER_ID",
+            "MEMBER_CODE": "CODE",
+            "MEMBER_LABEL": "NAME",
+            "MEMBER_DESCRIPTION": "DESCRIPTION",
+        },
+    )
 
     # Filter out rows with empty MEMBER_ID
-    mask = np.array([row["MEMBER_ID"] != '' and row["MEMBER_ID"] != 'nan' for row in members])
+    mask = np.array([row["MEMBER_ID"] != "" and row["MEMBER_ID"] != "nan" for row in members])
     members = members[mask]
 
-    members = select_fields(members, [
-        "MAINTENANCE_AGENCY_ID", "MEMBER_ID", "CODE", "NAME", "DOMAIN_ID", "DESCRIPTION"
-    ])
+    members = select_fields(members, ["MAINTENANCE_AGENCY_ID", "MEMBER_ID", "CODE", "NAME", "DOMAIN_ID", "DESCRIPTION"])
 
     members = clean_spaces(members)
 
-    return members, id_mapping
\ No newline at end of file
+    return members, id_mapping
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/dpm_integration/mapping_functions/members.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/dpm_integration/mapping_functions/datapoint_version.py	2025-09-21 16:23:32.627225+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/dpm_integration/mapping_functions/datapoint_version.py	2025-09-21 17:07:36.350773+00:00
@@ -13,21 +13,34 @@
 
 import os
 import numpy as np
 from collections import defaultdict
 from .utils import (
-    read_csv_to_dict, dict_list_to_structured_array, add_field, drop_fields,
-    select_fields, rename_fields, pascal_to_upper_snake, merge_arrays, clean_spaces
+    read_csv_to_dict,
+    dict_list_to_structured_array,
+    add_field,
+    drop_fields,
+    select_fields,
+    rename_fields,
+    pascal_to_upper_snake,
+    merge_arrays,
+    clean_spaces,
 )
 
 
-def map_datapoint_version(path=os.path.join("target", "DataPointVersion.csv"), context_map: dict = {}, context_data=None, dimension_map: dict = {}, member_map: dict = {}):
+def map_datapoint_version(
+    path=os.path.join("target", "DataPointVersion.csv"),
+    context_map: dict = {},
+    context_data=None,
+    dimension_map: dict = {},
+    member_map: dict = {},
+):
     """Map datapoint versions from DataPointVersion.csv to the target format"""
     types = defaultdict(lambda: str, ContextID="str")
     data_list = read_csv_to_dict(path)
     # Force ID fields to be strings since they will be mapped to string values
-    dpv = dict_list_to_structured_array(data_list, force_str_columns={'DataPointVID', 'ContextID'})
+    dpv = dict_list_to_structured_array(data_list, force_str_columns={"DataPointVID", "ContextID"})
 
     column_mapping = {col: pascal_to_upper_snake(col) for col in dpv.dtype.names}
     dpv = rename_fields(dpv, column_mapping)
     dpv = add_field(dpv, "MAINTENANCE_AGENCY_ID", "EBA")
 
@@ -80,17 +93,20 @@
 
             previous_char = char
 
         return "|".join(f"{key}({member_map.get(int(float(value)), value)})" for key, value in key_value.items())
 
-    dpv = rename_fields(dpv, {
-        "NEW_DATA_POINT_VID": "COMBINATION_ID",
-        "DATA_POINT_VID": "CODE",
-        "FROM_DATE": "VALID_FROM",
-        "TO_DATE": "VALID_TO",
-        "CATEGORISATION_KEY": "NAME"
-    })
+    dpv = rename_fields(
+        dpv,
+        {
+            "NEW_DATA_POINT_VID": "COMBINATION_ID",
+            "DATA_POINT_VID": "CODE",
+            "FROM_DATE": "VALID_FROM",
+            "TO_DATE": "VALID_TO",
+            "CATEGORISATION_KEY": "NAME",
+        },
+    )
 
     # Update NAME field
     names = []
     for row in dpv:
         names.append(compute_code(str(row["NAME"])))
@@ -99,26 +115,24 @@
         dpv[i]["NAME"] = names[i]
 
     dpv = add_field(dpv, "VERSION", "")
 
     if len(dp_items) > 0:
-        dp_items = rename_fields(dp_items, {
-            "NEW_DATA_POINT_VID": "COMBINATION_ID",
-            "DIMENSION_ID": "VARIABLE_ID",
-            "MEMBER_ID": "MEMBER_ID"
-        })
+        dp_items = rename_fields(
+            dp_items, {"NEW_DATA_POINT_VID": "COMBINATION_ID", "DIMENSION_ID": "VARIABLE_ID", "MEMBER_ID": "MEMBER_ID"}
+        )
 
         dp_items = add_field(dp_items, "VARIABLE_SET", "")
         dp_items = add_field(dp_items, "SUBDOMAIN_ID", "")
 
-        dp_items = select_fields(dp_items, [
-            "COMBINATION_ID", "VARIABLE_ID", "MEMBER_ID", "VARIABLE_SET", "SUBDOMAIN_ID"
-        ])
+        dp_items = select_fields(
+            dp_items, ["COMBINATION_ID", "VARIABLE_ID", "MEMBER_ID", "VARIABLE_SET", "SUBDOMAIN_ID"]
+        )
 
-    dpv = select_fields(dpv, [
-        "COMBINATION_ID", "CODE", "NAME", "MAINTENANCE_AGENCY_ID", "VERSION", "VALID_FROM", "VALID_TO"
-    ])
+    dpv = select_fields(
+        dpv, ["COMBINATION_ID", "CODE", "NAME", "MAINTENANCE_AGENCY_ID", "VERSION", "VALID_FROM", "VALID_TO"]
+    )
 
     # Clean text fields
     dpv = clean_spaces(dpv)
 
-    return (dpv, dp_items), id_mapping
\ No newline at end of file
+    return (dpv, dp_items), id_mapping
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/dpm_integration/mapping_functions/datapoint_version.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/dpm_integration/dpm_integration_service.py	2025-09-21 16:35:50.225175+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/dpm_integration/dpm_integration_service.py	2025-09-21 17:07:36.366938+00:00
@@ -25,21 +25,22 @@
 SCRIPT_RUNNER = "" if platform.system() == "Windows" else "bash "
 PROCESS_FILE_END = ".bat" if platform.system() == "Windows" else ".sh"
 SCRIPT_PATH = f"pybirdai{os.sep}process_steps{os.sep}dpm_integration{os.sep}process{PROCESS_FILE_END}"
 EXTRACTED_DB_PATH = f"dpm_database{os.sep}dpm_database.accdb"
 
+
 def save_numpy_array_to_csv(array, filepath, index=False):
     """Save numpy structured array to CSV file"""
     if len(array) == 0:
         # Write empty file with headers only
-        with open(filepath, 'w', newline='', encoding='utf-8') as f:
-            if hasattr(array, 'dtype') and array.dtype.names:
+        with open(filepath, "w", newline="", encoding="utf-8") as f:
+            if hasattr(array, "dtype") and array.dtype.names:
                 writer = csv.writer(f)
                 writer.writerow(array.dtype.names)
         return
 
-    with open(filepath, 'w', newline='', encoding='utf-8') as f:
+    with open(filepath, "w", newline="", encoding="utf-8") as f:
         writer = csv.writer(f)
 
         # Write header
         writer.writerow(array.dtype.names)
 
@@ -54,56 +55,60 @@
                 elif isinstance(val, (np.integer, int)):
                     row_data.append(str(val))
                 elif isinstance(val, (np.floating, float)):
                     # Check for NaN
                     if np.isnan(val):
-                        row_data.append('')
+                        row_data.append("")
                     else:
                         row_data.append(str(val))
                 else:
                     # String or other types
                     row_data.append(str(val))
             writer.writerow(row_data)
 
+
 class DPMImporterService:
 
-    def __init__(self, output_directory:str = f"export_debug{os.sep}"):
+    def __init__(self, output_directory: str = f"export_debug{os.sep}"):
         self.link_db = None
         self.output_directory = f"{output_directory}{os.sep}technical_export{os.sep}"
         shutil.rmtree(self.output_directory, ignore_errors=True)
         os.makedirs(self.output_directory, exist_ok=True)
 
     def fetch_link_for_database_download(self):
         try:
             from bs4 import BeautifulSoup
+
             main_page = "https://www.eba.europa.eu/risk-and-data-analysis/reporting-frameworks/dpm-data-dictionary"
             soup = BeautifulSoup(requests.get(main_page).text)
             a_links = soup.find_all("a")
             print(a_links[0]["href"])
 
             def is_right_link(a_el: any) -> bool:
-                return "https://www.eba.europa.eu/sites/default/files" in a_el.get("href","") \
-                and "/dpm_database_v" in a_el.get("href","") \
-                and ".zip" in a_el.get("href","")
+                return (
+                    "https://www.eba.europa.eu/sites/default/files" in a_el.get("href", "")
+                    and "/dpm_database_v" in a_el.get("href", "")
+                    and ".zip" in a_el.get("href", "")
+                )
 
             for a_el in a_links:
                 if a_el and is_right_link(a_el):
-                    self.link_db = a_el.get("href",DEFAULT_DB_VERSION)
+                    self.link_db = a_el.get("href", DEFAULT_DB_VERSION)
                     break
         except Exception as e:
             pass
         finally:
             if not self.link_db:
                 self.link_db = DEFAULT_DB_VERSION
 
     def download_dpm_database(self):
-        with open("dpm_database.zip","wb") as f:
+        with open("dpm_database.zip", "wb") as f:
             db_data = requests.get(self.link_db).content
             f.write(db_data)
 
     def extract_dpm_database(self):
-        with zipfile.ZipFile("dpm_database.zip", 'r') as zip_ref:
+        with zipfile.ZipFile("dpm_database.zip", "r") as zip_ref:
             zip_ref.extractall("dpm_database")
 
             if len(os.listdir("dpm_database")) > 1:
                 raise Exception("More than one file in the zip")
 
@@ -111,11 +116,11 @@
                 if file.endswith(".accdb"):
                     shutil.move(os.path.join("dpm_database", file), os.path.join("dpm_database", "dpm_database.accdb"))
 
         os.system(f"{SCRIPT_RUNNER}{SCRIPT_PATH} {EXTRACTED_DB_PATH}")
 
-    def run_application(self,extract_cleanup:bool=False,download_cleanup:bool=False,with_extract:bool=True):
+    def run_application(self, extract_cleanup: bool = False, download_cleanup: bool = False, with_extract: bool = True):
         if with_extract:
             if os.path.exists("dpm_database"):
                 shutil.rmtree("dpm_database")
 
         if with_extract:
@@ -135,44 +140,49 @@
     def write_csv_maintenance_agency(self):
         """
         Core Package
         """
 
-        with open(f"{self.output_directory}maintenance_agency.csv","w") as f:
-            f.write("""MAINTENANCE_AGENCY_ID,CODE,NAME,DESCRIPTION
-EBA,EBA,European Banking Authority,European Banking Authority""")
+        with open(f"{self.output_directory}maintenance_agency.csv", "w") as f:
+            f.write(
+                """MAINTENANCE_AGENCY_ID,CODE,NAME,DESCRIPTION
+EBA,EBA,European Banking Authority,European Banking Authority"""
+            )
 
     def map_csvs_to_sdd_exchange_format(self):
         import pybirdai.process_steps.dpm_integration.mapping_functions as new_maps
+
         """
         Core Package
         """
 
         self.write_csv_maintenance_agency()
         logging.info("Created Maintenance Agency File")
 
-        frameworks_array, framework_map = new_maps.map_frameworks() # frameworks
+        frameworks_array, framework_map = new_maps.map_frameworks()  # frameworks
         save_numpy_array_to_csv(frameworks_array, f"{self.output_directory}framework.csv", index=False)
         logging.info("Mapped Framework Entities")
 
-        domains_array, domain_map = new_maps.map_domains() # domains
+        domains_array, domain_map = new_maps.map_domains()  # domains
         save_numpy_array_to_csv(domains_array, f"{self.output_directory}domain.csv", index=False)
         logging.info("Mapped Domain Entities")
 
-        members_array, member_map = new_maps.map_members(domain_id_map=domain_map) # members
+        members_array, member_map = new_maps.map_members(domain_id_map=domain_map)  # members
         save_numpy_array_to_csv(members_array, f"{self.output_directory}member.csv", index=False)
         logging.info("Mapped Members Entities")
 
-        dimensions_array, dimension_map = new_maps.map_dimensions(domain_id_map=domain_map) # to enumerated variables
+        dimensions_array, dimension_map = new_maps.map_dimensions(domain_id_map=domain_map)  # to enumerated variables
         save_numpy_array_to_csv(dimensions_array, f"{self.output_directory}variable.csv", index=False)
         logging.info("Mapped Variables Entities")
 
-        hierarchy_array, hierarchy_map = new_maps.map_hierarchy(domain_id_map=domain_map) # member hierarchies
+        hierarchy_array, hierarchy_map = new_maps.map_hierarchy(domain_id_map=domain_map)  # member hierarchies
         save_numpy_array_to_csv(hierarchy_array, f"{self.output_directory}member_hierarchy.csv", index=False)
         logging.info("Mapped Hierarchy Entities")
 
-        hierarchy_node_array, hierarchy_node_map = new_maps.map_hierarchy_node(hierarchy_map=hierarchy_map, member_map=member_map) # member hierarchy node
+        hierarchy_node_array, hierarchy_node_map = new_maps.map_hierarchy_node(
+            hierarchy_map=hierarchy_map, member_map=member_map
+        )  # member hierarchy node
         save_numpy_array_to_csv(hierarchy_node_array, f"{self.output_directory}member_hierarchy_node.csv", index=False)
         logging.info("Mapped HierarchyNode Entities")
 
         """
         Data Definition Package
@@ -202,15 +212,24 @@
 
         cells_array, cell_map = new_maps.map_table_cell(table_map=table_map)
         save_numpy_array_to_csv(cells_array, f"{self.output_directory}table_cell.csv", index=False)
         logging.info("Mapped TableCell Entities")
 
-        cell_positions_array, cell_position_map = new_maps.map_cell_position(cell_map=cell_map,ordinate_map=ordinate_map,start_index_after_last=False)
+        cell_positions_array, cell_position_map = new_maps.map_cell_position(
+            cell_map=cell_map, ordinate_map=ordinate_map, start_index_after_last=False
+        )
         save_numpy_array_to_csv(cell_positions_array, f"{self.output_directory}cell_position.csv", index=False)
         logging.info("Mapped CellPositions Entities")
 
-        ordinate_items_array, ordinate_item_map = new_maps.map_ordinate_categorisation(member_map=member_map,dimension_map=dimension_map,ordinate_map=ordinate_map,hierarchy_map=hierarchy_map,start_index_after_last=False)
+        ordinate_items_array, ordinate_item_map = new_maps.map_ordinate_categorisation(
+            member_map=member_map,
+            dimension_map=dimension_map,
+            ordinate_map=ordinate_map,
+            hierarchy_map=hierarchy_map,
+            start_index_after_last=False,
+        )
         save_numpy_array_to_csv(ordinate_items_array, f"{self.output_directory}ordinate_item.csv", index=False)
         logging.info("Mapped OrdinateItems Entities")
 
+
 if __name__ == "__main__":
     DPMImporterService().run_application()
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/dpm_integration/dpm_integration_service.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/dpm_integration/mapping_functions/hierarchy_node.py	2025-09-21 16:24:16.538712+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/dpm_integration/mapping_functions/hierarchy_node.py	2025-09-21 17:07:36.387019+00:00
@@ -11,28 +11,36 @@
 #    Benjamin Arfa - initial API and implementation
 #
 
 import os
 from .utils import (
-    read_csv_to_dict, dict_list_to_structured_array, add_field,
-    select_fields, rename_fields, pascal_to_upper_snake
+    read_csv_to_dict,
+    dict_list_to_structured_array,
+    add_field,
+    select_fields,
+    rename_fields,
+    pascal_to_upper_snake,
 )
 
 
-def map_hierarchy_node(path=os.path.join("target", "HierarchyNode.csv"), hierarchy_map: dict = {}, member_map: dict = {}):
+def map_hierarchy_node(
+    path=os.path.join("target", "HierarchyNode.csv"), hierarchy_map: dict = {}, member_map: dict = {}
+):
     """Map hierarchy nodes from HierarchyNode.csv to the target format"""
     data_list = read_csv_to_dict(path)
 
     # Handle NaN values
     for row in data_list:
-        if not row.get("ParentMemberID") or row["ParentMemberID"] == '':
+        if not row.get("ParentMemberID") or row["ParentMemberID"] == "":
             row["ParentMemberID"] = "0"
-        if not row.get("MemberID") or row["MemberID"] == '':
+        if not row.get("MemberID") or row["MemberID"] == "":
             row["MemberID"] = "0"
 
     # Force ID fields to be strings since they will be mapped to string values
-    data = dict_list_to_structured_array(data_list, force_str_columns={'HierarchyID', 'MemberID', 'ParentHierarchyID', 'ParentMemberID'})
+    data = dict_list_to_structured_array(
+        data_list, force_str_columns={"HierarchyID", "MemberID", "ParentHierarchyID", "ParentMemberID"}
+    )
 
     column_mapping = {col: pascal_to_upper_snake(col) for col in data.dtype.names}
     data = rename_fields(data, column_mapping)
 
     # Update MEMBER_ID
@@ -77,15 +85,13 @@
     for i, row in enumerate(data):
         data[i]["HIERARCHY_ID"] = hierarchy_ids[i]
 
     data = add_field(data, "MAINTENANCE_AGENCY_ID", "EBA")
 
-    data = rename_fields(data, {
-        "HIERARCHY_ID": "MEMBER_HIERARCHY_ID",
-        "COMPARISON_OPERATOR": "COMPARATOR",
-        "UNARY_OPERATOR": "OPERATOR"
-    })
+    data = rename_fields(
+        data, {"HIERARCHY_ID": "MEMBER_HIERARCHY_ID", "COMPARISON_OPERATOR": "COMPARATOR", "UNARY_OPERATOR": "OPERATOR"}
+    )
 
     data = add_field(data, "VALID_FROM", "1900-01-01")
     data = add_field(data, "VALID_TO", "9999-12-31")
 
     # Strip spaces from COMPARATOR and OPERATOR
@@ -93,14 +99,25 @@
         data[i]["COMPARATOR"] = str(row["COMPARATOR"]).strip()
         data[i]["OPERATOR"] = str(row["OPERATOR"]).strip()
 
     # Set default COMPARATOR
     for i, row in enumerate(data):
-        if (str(row["COMPARATOR"]) == '' or str(row["COMPARATOR"]) == 'nan') and \
-           (str(row["OPERATOR"]) == '' or str(row["OPERATOR"]) == 'nan'):
+        if (str(row["COMPARATOR"]) == "" or str(row["COMPARATOR"]) == "nan") and (
+            str(row["OPERATOR"]) == "" or str(row["OPERATOR"]) == "nan"
+        ):
             data[i]["COMPARATOR"] = ">="
 
-    data = select_fields(data, [
-        "MEMBER_HIERARCHY_ID", "MEMBER_ID", "LEVEL", "PARENT_MEMBER_ID", "COMPARATOR", "OPERATOR", "VALID_FROM", "VALID_TO"
-    ])
+    data = select_fields(
+        data,
+        [
+            "MEMBER_HIERARCHY_ID",
+            "MEMBER_ID",
+            "LEVEL",
+            "PARENT_MEMBER_ID",
+            "COMPARATOR",
+            "OPERATOR",
+            "VALID_FROM",
+            "VALID_TO",
+        ],
+    )
 
-    return data, {}
\ No newline at end of file
+    return data, {}
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/dpm_integration/mapping_functions/table_cell.py	2025-09-21 16:22:53.703053+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/dpm_integration/mapping_functions/table_cell.py	2025-09-21 17:07:36.387041+00:00
@@ -11,20 +11,25 @@
 #    Benjamin Arfa - initial API and implementation
 #
 
 import os
 from .utils import (
-    read_csv_to_dict, dict_list_to_structured_array, add_field, drop_fields,
-    select_fields, rename_fields, pascal_to_upper_snake
+    read_csv_to_dict,
+    dict_list_to_structured_array,
+    add_field,
+    drop_fields,
+    select_fields,
+    rename_fields,
+    pascal_to_upper_snake,
 )
 
 
 def map_table_cell(path=os.path.join("target", "TableCell.csv"), table_map: dict = {}, dp_map: dict = {}):
     """Map table cells from TableCell.csv to the target format"""
     data = read_csv_to_dict(path)
     # Force ID fields to be strings since they will be mapped to string values
-    cells = dict_list_to_structured_array(data, force_str_columns={'CellID', 'TableVID', 'DataPointVID'})
+    cells = dict_list_to_structured_array(data, force_str_columns={"CellID", "TableVID", "DataPointVID"})
 
     column_mapping = {col: pascal_to_upper_snake(col) for col in cells.dtype.names}
     cells = rename_fields(cells, column_mapping)
     cells = add_field(cells, "MAINTENANCE_AGENCY_ID", "EBA")
 
@@ -47,13 +52,13 @@
     for row in cells:
         if "IS_SHADED" in cells.dtype.names:
             val = str(row["IS_SHADED"])
         else:
             val = "False"
-        is_shaded.append(val.lower() in ['true', '1', 'yes'])
+        is_shaded.append(val.lower() in ["true", "1", "yes"])
 
-    cells = add_field(cells, "IS_SHADED_BOOL", is_shaded, dtype='bool')
+    cells = add_field(cells, "IS_SHADED_BOOL", is_shaded, dtype="bool")
     if "IS_SHADED" in cells.dtype.names:
         cells = drop_fields(cells, "IS_SHADED")
     cells = rename_fields(cells, {"IS_SHADED_BOOL": "IS_SHADED"})
 
     # Handle DATA_POINT_VID
@@ -78,23 +83,21 @@
     for row in cells:
         id_mapping[str(row["CELL_ID"])] = str(row["NEW_CELL_ID"])
 
     cells = drop_fields(cells, "CELL_ID")
 
-    cells = rename_fields(cells, {
-        "NEW_CELL_ID": "CELL_ID",
-        "TABLE_VID": "TABLE_ID",
-        "DATA_POINT_VID": "TABLE_CELL_COMBINATION_ID"
-    })
+    cells = rename_fields(
+        cells, {"NEW_CELL_ID": "CELL_ID", "TABLE_VID": "TABLE_ID", "DATA_POINT_VID": "TABLE_CELL_COMBINATION_ID"}
+    )
 
     cells = add_field(cells, "SYSTEM_DATA_CODE", "")
 
     names = []
     for row in cells:
         names.append(str(row["CELL_ID"]))
     cells = add_field(cells, "NAME", names)
 
-    cells = select_fields(cells, [
-        "CELL_ID", "IS_SHADED", "TABLE_CELL_COMBINATION_ID", "SYSTEM_DATA_CODE", "NAME", "TABLE_ID"
-    ])
+    cells = select_fields(
+        cells, ["CELL_ID", "IS_SHADED", "TABLE_CELL_COMBINATION_ID", "SYSTEM_DATA_CODE", "NAME", "TABLE_ID"]
+    )
 
-    return cells, id_mapping
\ No newline at end of file
+    return cells, id_mapping
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/dpm_integration/mapping_functions/hierarchy_node.py
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/dpm_integration/mapping_functions/table_cell.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/generate_etl/simple_context.py	2025-09-15 13:18:11.380502+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/generate_etl/simple_context.py	2025-09-21 17:07:36.396539+00:00
@@ -10,42 +10,101 @@
 # Contributors:
 #    Neil Mackenzie - initial API and implementation
 #    Benjamin Arfa - improvements
 #
 class SimpleContext(object):
-    '''
-       
-    '''
+    """ """
+
     def get_all_related_tables(self):
         all_related_tables = [
-            'SCRTY_EXCHNG_TRDBL_DRVTV', 'CRDT_FCLTY', 'CRDT_FCLTY_ENTTY_RL_ASSGNMNT', 'INSTRMNT', 'INSTRMNT_RL',
-            'NN_FNNCL_ASST_NN_FNNCL_LBLTY', 'CSH_HND', 'PRTY', 'SCRTY_EXCHNG_TRDBL_DRVTV_PSTN',
-            'LNG_SCRTY_PSTN_PRDNTL_PRTFL_ASSGNMNT', 'LNG_SCRTY_PSTN_PRDNTL_PRTFL_ASSGNMNT_ACCNTNG_CLSSFCTN_FNNCL_ASSTS_ASSGNMNT',
-            'CLLTRL', 'INSTRMNT_ENTTY_RL_ASSGNMNT', 'FNNCL_CNTRCT', 'ENTTY_RL', 'FNDMNTL_RVW_TRDNG_BK_STNDRD_APPRCH_RSK_MSR',
-            'RSK_FAC_SA', 'ASST_PL', 'ASST_PL_DBT_SCRTY_PSTN_ASSGNMNT', 'ASST_PL_EQT_INSTRMNT_NT_SCRT_ASSGNMNT',
-            'ASST_PL_LN_ASSGNMNT', 'BLNC_SHT_NTTNG', 'CLLTRL_NN_FNNCL_ASST_ASSGNMNT', 'CRDT_FCLTY_CLLTRL_ASSGNMNT',
-            'CRDT_FCLTY_CLLTRL_RCVD_INSTRMNT_ASSGNMNT', 'CRDT_RSK_MTGTN_ASSGNMNT', 'CRDT_TRNSFR_OTHR_SCRTSTN_CVRD_BND_PRGRM',
-            'CVRD_BND_ISSNC', 'CVRD_BND_PRGRM', 'DBT_SCRTY_ISSD', 'DBT_SCRTY_ISSD_TRDTNL_SCRTSTN_ASSGNMNT',
-            'EQT_INSTRMNT_LG_EQT_INSTRMNT_NT_SCRT_ASSGNMNT', 'ETD_LBLTY_PSTN_SNTHTC_SCRTSTN_ASSGNMNT',
-            'EXCHNG_TRDBL_DRVTV_PSTN', 'FNDMNTL_RVW_TRDNG_BK_STNDRD_APPRCH_RSK_MSR_ETD_PSTNS',
-            'FNDMNTL_RVW_TRDNG_BK_STNDRD_APPRCH_RSK_MSR_FR_SCRTY_PSTNS', 'FNDMNTL_RVW_TRDNG_BK_STNDRD_APPRCH_RSK_MSR_OTC_PSTNS',
-            'FNNCL_GRNT_INSTRMNT_DBT_SCRT_DBT_SCRTY_ASSGNMNT', 'GRP', 'GRP_CLNTS_KY_MNGMNT_PRSNLL_ASSGNMNT',
-            'IMMDT_PRNT_ENTRPRS_ASSGNMNT', 'INSTRMNT_CLLTRL_RCVD_INSTRMNT_ASSGNMNT', 'INSTRMNT_HDGD_EXCHNG_TRDBL_DRVTV',
-            'INSTRMNT_HDGD_OTC_DRVTV', 'INSTRMNT_PRTCN_ARRNGMNT_ASSGNMNT', 'INTRNL_GRP_KY_MNGMNT_PRSNLL_ASSGNMNT',
-            'INTRNL_GRP_RL', 'INTRST_RT_RSK_HDG_PRTFL', 'KB_PR_BCKT', 'LN_AND_ADVNC_LG_LN_AND_ADVNC_ASSGNMNT',
-            'LN_EXCLDNG_RPRCHS_AGRMNT_CLLTRL_ASSGNMNT', 'LNG_NN_NGTBL_SCRTY_PSTN_CLLTRL_ASSGNMNT',
-            'LNG_SCRTY_PSTN_PRDNTL_PRTFL_ASSGNMNT_RSK_DT', 'LNKD_ENTRPRS_ASSGNMNT', 'MSTR_AGRMNT',
-            'MSTR_AGRMNT_ENTTY_RL_ASSGNMNT', 'MSTR_AGRMNT_FNNCL_CNTRCT_ASSGNMNT', 'NN_FNNCL_ASST', 'NN_FNNCL_LBLTY',
-            'NTRL_PRSN_KY_MNGMNT_PRSNLL_ASSGNMNT', 'OFF_BLNC_INSTRMNT_CLLTRL_ASSGNMNT',
-            'OTC_DRVTV_INSTRMNT_SNTHTC_SCRTSTN_ASSGNMNT', 'OTHR_PRTY_ID', 'PRTCTN_ARRNGMNT', 'PRTCTN_PRTCTN_PRVD_ASSGNMNT',
-            'PRTNR_ENTRPRS_ASSGNMNT', 'PRTY_CD', 'PRTY_PRVS_PRD_DT', 'RPRCHS_AGRMNT_CMPNNT', 'RTNG_AGNCY', 'RTNG_GRD',
-            'RTNG_GRD_CNTRY_ASSGNMNT', 'RTNG_GRD_ISS_BSD_RTNG_SSTM_DBT_SCRTY_ASSGNMNT', 'RTNG_SYSTM', 'RTNG_SYSTM_APPLD_LGL_PRSN',
-            'SBSDRY_JNT_VNTR_ASSCT_OTHR_ORGNSTN_ASSGNMNT', 'SCRTY_ENTTY_RL_ASSGNMNT', 'SCRTY_HDGD_EXCHNG_TRDBL_DRVTV',
-            'SCRTY_PSTN', 'SCRTY_PSTN_HDGD_OTC_DRVTV', 'SCRTY_SCRTY_RPRCHS_AGRMNT_CMPNNT_ASSGNMNT',
-            'SCTRY_BRRWNG_LNDNG_TRNSCTN_INCLDNG_CSH_CLLTRL', 'SHRT_SCRTY_PSTN_PRDNTL_PRTFL_ASSGNMNT', 'SNTHTC_SCRTSTN',
-            'SYNDCTD_CNTRCT', 'TRDTNL_SCRTSTN', 'TRNCH_SYNTHTC_SCRTSTN_WTHT_SSPE_DPST',
-            'TRNCH_SYNTHTC_SCRTSTN_WTHT_SSPE_FNNCL_GRNT', 'TRNCH_TRDTNL_SCRTSTN'
+            "SCRTY_EXCHNG_TRDBL_DRVTV",
+            "CRDT_FCLTY",
+            "CRDT_FCLTY_ENTTY_RL_ASSGNMNT",
+            "INSTRMNT",
+            "INSTRMNT_RL",
+            "NN_FNNCL_ASST_NN_FNNCL_LBLTY",
+            "CSH_HND",
+            "PRTY",
+            "SCRTY_EXCHNG_TRDBL_DRVTV_PSTN",
+            "LNG_SCRTY_PSTN_PRDNTL_PRTFL_ASSGNMNT",
+            "LNG_SCRTY_PSTN_PRDNTL_PRTFL_ASSGNMNT_ACCNTNG_CLSSFCTN_FNNCL_ASSTS_ASSGNMNT",
+            "CLLTRL",
+            "INSTRMNT_ENTTY_RL_ASSGNMNT",
+            "FNNCL_CNTRCT",
+            "ENTTY_RL",
+            "FNDMNTL_RVW_TRDNG_BK_STNDRD_APPRCH_RSK_MSR",
+            "RSK_FAC_SA",
+            "ASST_PL",
+            "ASST_PL_DBT_SCRTY_PSTN_ASSGNMNT",
+            "ASST_PL_EQT_INSTRMNT_NT_SCRT_ASSGNMNT",
+            "ASST_PL_LN_ASSGNMNT",
+            "BLNC_SHT_NTTNG",
+            "CLLTRL_NN_FNNCL_ASST_ASSGNMNT",
+            "CRDT_FCLTY_CLLTRL_ASSGNMNT",
+            "CRDT_FCLTY_CLLTRL_RCVD_INSTRMNT_ASSGNMNT",
+            "CRDT_RSK_MTGTN_ASSGNMNT",
+            "CRDT_TRNSFR_OTHR_SCRTSTN_CVRD_BND_PRGRM",
+            "CVRD_BND_ISSNC",
+            "CVRD_BND_PRGRM",
+            "DBT_SCRTY_ISSD",
+            "DBT_SCRTY_ISSD_TRDTNL_SCRTSTN_ASSGNMNT",
+            "EQT_INSTRMNT_LG_EQT_INSTRMNT_NT_SCRT_ASSGNMNT",
+            "ETD_LBLTY_PSTN_SNTHTC_SCRTSTN_ASSGNMNT",
+            "EXCHNG_TRDBL_DRVTV_PSTN",
+            "FNDMNTL_RVW_TRDNG_BK_STNDRD_APPRCH_RSK_MSR_ETD_PSTNS",
+            "FNDMNTL_RVW_TRDNG_BK_STNDRD_APPRCH_RSK_MSR_FR_SCRTY_PSTNS",
+            "FNDMNTL_RVW_TRDNG_BK_STNDRD_APPRCH_RSK_MSR_OTC_PSTNS",
+            "FNNCL_GRNT_INSTRMNT_DBT_SCRT_DBT_SCRTY_ASSGNMNT",
+            "GRP",
+            "GRP_CLNTS_KY_MNGMNT_PRSNLL_ASSGNMNT",
+            "IMMDT_PRNT_ENTRPRS_ASSGNMNT",
+            "INSTRMNT_CLLTRL_RCVD_INSTRMNT_ASSGNMNT",
+            "INSTRMNT_HDGD_EXCHNG_TRDBL_DRVTV",
+            "INSTRMNT_HDGD_OTC_DRVTV",
+            "INSTRMNT_PRTCN_ARRNGMNT_ASSGNMNT",
+            "INTRNL_GRP_KY_MNGMNT_PRSNLL_ASSGNMNT",
+            "INTRNL_GRP_RL",
+            "INTRST_RT_RSK_HDG_PRTFL",
+            "KB_PR_BCKT",
+            "LN_AND_ADVNC_LG_LN_AND_ADVNC_ASSGNMNT",
+            "LN_EXCLDNG_RPRCHS_AGRMNT_CLLTRL_ASSGNMNT",
+            "LNG_NN_NGTBL_SCRTY_PSTN_CLLTRL_ASSGNMNT",
+            "LNG_SCRTY_PSTN_PRDNTL_PRTFL_ASSGNMNT_RSK_DT",
+            "LNKD_ENTRPRS_ASSGNMNT",
+            "MSTR_AGRMNT",
+            "MSTR_AGRMNT_ENTTY_RL_ASSGNMNT",
+            "MSTR_AGRMNT_FNNCL_CNTRCT_ASSGNMNT",
+            "NN_FNNCL_ASST",
+            "NN_FNNCL_LBLTY",
+            "NTRL_PRSN_KY_MNGMNT_PRSNLL_ASSGNMNT",
+            "OFF_BLNC_INSTRMNT_CLLTRL_ASSGNMNT",
+            "OTC_DRVTV_INSTRMNT_SNTHTC_SCRTSTN_ASSGNMNT",
+            "OTHR_PRTY_ID",
+            "PRTCTN_ARRNGMNT",
+            "PRTCTN_PRTCTN_PRVD_ASSGNMNT",
+            "PRTNR_ENTRPRS_ASSGNMNT",
+            "PRTY_CD",
+            "PRTY_PRVS_PRD_DT",
+            "RPRCHS_AGRMNT_CMPNNT",
+            "RTNG_AGNCY",
+            "RTNG_GRD",
+            "RTNG_GRD_CNTRY_ASSGNMNT",
+            "RTNG_GRD_ISS_BSD_RTNG_SSTM_DBT_SCRTY_ASSGNMNT",
+            "RTNG_SYSTM",
+            "RTNG_SYSTM_APPLD_LGL_PRSN",
+            "SBSDRY_JNT_VNTR_ASSCT_OTHR_ORGNSTN_ASSGNMNT",
+            "SCRTY_ENTTY_RL_ASSGNMNT",
+            "SCRTY_HDGD_EXCHNG_TRDBL_DRVTV",
+            "SCRTY_PSTN",
+            "SCRTY_PSTN_HDGD_OTC_DRVTV",
+            "SCRTY_SCRTY_RPRCHS_AGRMNT_CMPNNT_ASSGNMNT",
+            "SCTRY_BRRWNG_LNDNG_TRNSCTN_INCLDNG_CSH_CLLTRL",
+            "SHRT_SCRTY_PSTN_PRDNTL_PRTFL_ASSGNMNT",
+            "SNTHTC_SCRTSTN",
+            "SYNDCTD_CNTRCT",
+            "TRDTNL_SCRTSTN",
+            "TRNCH_SYNTHTC_SCRTSTN_WTHT_SSPE_DPST",
+            "TRNCH_SYNTHTC_SCRTSTN_WTHT_SSPE_FNNCL_GRNT",
+            "TRNCH_TRDTNL_SCRTSTN",
         ]
-                               
-                               
+
         return all_related_tables
-    
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/generate_etl/simple_context.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/dpm_integration/mapping_functions/ordinate_categorisation.py	2025-09-21 16:24:47.791825+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/dpm_integration/mapping_functions/ordinate_categorisation.py	2025-09-21 17:07:36.409917+00:00
@@ -12,20 +12,26 @@
 #
 
 import os
 import numpy as np
 from .utils import (
-    read_csv_to_dict, dict_list_to_structured_array, add_field, drop_fields,
-    select_fields, rename_fields, pascal_to_upper_snake, merge_arrays
+    read_csv_to_dict,
+    dict_list_to_structured_array,
+    add_field,
+    drop_fields,
+    select_fields,
+    rename_fields,
+    pascal_to_upper_snake,
+    merge_arrays,
 )
 
 
 def traceback_restrictions(path=os.path.join("target", "OpenMemberRestriction.csv")):
     """Load and process open member restrictions"""
     data_list = read_csv_to_dict(path)
     # Force ID fields to be strings since they will be mapped to string values
-    restriction_df = dict_list_to_structured_array(data_list, force_str_columns={'HierarchyID', 'MemberID'})
+    restriction_df = dict_list_to_structured_array(data_list, force_str_columns={"HierarchyID", "MemberID"})
 
     # Rename columns with "Restriction" prefix
     rename_dict = {}
     for col in restriction_df.dtype.names:
         if col != "RestrictionID":
@@ -34,20 +40,33 @@
     restriction_df = rename_fields(restriction_df, rename_dict)
 
     return restriction_df
 
 
-def map_ordinate_categorisation(path=os.path.join("target", "OrdinateCategorisation.csv"), member_map: dict = {}, dimension_map: dict = {}, ordinate_map: dict = {}, hierarchy_map: dict = {}, start_index_after_last: bool = False):
+def map_ordinate_categorisation(
+    path=os.path.join("target", "OrdinateCategorisation.csv"),
+    member_map: dict = {},
+    dimension_map: dict = {},
+    ordinate_map: dict = {},
+    hierarchy_map: dict = {},
+    start_index_after_last: bool = False,
+):
     """Map ordinate categorisation from OrdinateCategorisation.csv to the target format"""
     data_list = read_csv_to_dict(path)
     # Force ID fields to be strings since they will be mapped to string values
-    data = dict_list_to_structured_array(data_list, force_str_columns={'MemberID', 'DimensionID', 'OrdinateID'})
+    data = dict_list_to_structured_array(data_list, force_str_columns={"MemberID", "DimensionID", "OrdinateID"})
 
     restrictions_arr = traceback_restrictions()
 
     if len(restrictions_arr) > 0:
-        data = merge_arrays(data, restrictions_arr, "RestrictionID", how="left", force_str_columns={'MemberID', 'DimensionID', 'OrdinateID', 'HierarchyID'})
+        data = merge_arrays(
+            data,
+            restrictions_arr,
+            "RestrictionID",
+            how="left",
+            force_str_columns={"MemberID", "DimensionID", "OrdinateID", "HierarchyID"},
+        )
 
     column_mapping = {col: pascal_to_upper_snake(col) for col in data.dtype.names}
     data = rename_fields(data, column_mapping)
     data = add_field(data, "MAINTENANCE_AGENCY_ID", "EBA")
 
@@ -82,72 +101,82 @@
 
     # Update RESTRICTION_HIERARCHY_ID
     if "RESTRICTION_HIERARCHY_ID" in data.dtype.names:
         restriction_hierarchy_ids = []
         for row in data:
-            restriction_hierarchy_ids.append(hierarchy_map.get(str(row["RESTRICTION_HIERARCHY_ID"]), str(row["RESTRICTION_HIERARCHY_ID"])))
+            restriction_hierarchy_ids.append(
+                hierarchy_map.get(str(row["RESTRICTION_HIERARCHY_ID"]), str(row["RESTRICTION_HIERARCHY_ID"]))
+            )
 
         for i, row in enumerate(data):
             data[i]["RESTRICTION_HIERARCHY_ID"] = restriction_hierarchy_ids[i]
 
     # Update RESTRICTION_MEMBER_ID
     if "RESTRICTION_MEMBER_ID" in data.dtype.names:
         restriction_member_ids = []
         for row in data:
-            restriction_member_ids.append(member_map.get(str(row["RESTRICTION_MEMBER_ID"]), str(row["RESTRICTION_MEMBER_ID"])))
+            restriction_member_ids.append(
+                member_map.get(str(row["RESTRICTION_MEMBER_ID"]), str(row["RESTRICTION_MEMBER_ID"]))
+            )
 
         for i, row in enumerate(data):
             data[i]["RESTRICTION_MEMBER_ID"] = restriction_member_ids[i]
 
-    data = rename_fields(data, {
-        "ORDINATE_ID": "AXIS_ORDINATE_ID",
-        "DIMENSION_ID": "VARIABLE_ID",
-        "MEMBER_ID": "MEMBER_ID",
-        "RESTRICTION_HIERARCHY_ID": "MEMBER_HIERARCHY_ID",
-        "RESTRICTION_MEMBER_ID": "STARTING_MEMBER_ID",
-        "RESTRICTION_MEMBER_INCLUDED": "IS_STARTING_MEMBER_INCLUDED"
-    })
+    data = rename_fields(
+        data,
+        {
+            "ORDINATE_ID": "AXIS_ORDINATE_ID",
+            "DIMENSION_ID": "VARIABLE_ID",
+            "MEMBER_ID": "MEMBER_ID",
+            "RESTRICTION_HIERARCHY_ID": "MEMBER_HIERARCHY_ID",
+            "RESTRICTION_MEMBER_ID": "STARTING_MEMBER_ID",
+            "RESTRICTION_MEMBER_INCLUDED": "IS_STARTING_MEMBER_INCLUDED",
+        },
+    )
 
     # Convert IS_STARTING_MEMBER_INCLUDED to bool
     if "IS_STARTING_MEMBER_INCLUDED" in data.dtype.names:
         is_included = []
         for row in data:
             val = str(row["IS_STARTING_MEMBER_INCLUDED"])
-            is_included.append(val.lower() in ['true', '1', 'yes'])
+            is_included.append(val.lower() in ["true", "1", "yes"])
 
-        data = add_field(data, "IS_STARTING_MEMBER_INCLUDED_BOOL", is_included, dtype='bool')
+        data = add_field(data, "IS_STARTING_MEMBER_INCLUDED_BOOL", is_included, dtype="bool")
         if "IS_STARTING_MEMBER_INCLUDED" in data.dtype.names:
             data = drop_fields(data, "IS_STARTING_MEMBER_INCLUDED")
         data = rename_fields(data, {"IS_STARTING_MEMBER_INCLUDED_BOOL": "IS_STARTING_MEMBER_INCLUDED"})
 
     data = add_field(data, "MEMBER_HIERARCHY_VALID_FROM", "")
 
     # Set IS_STARTING_MEMBER_INCLUDED to False where STARTING_MEMBER_ID is empty
     if "STARTING_MEMBER_ID" in data.dtype.names and "IS_STARTING_MEMBER_INCLUDED" in data.dtype.names:
         for i, row in enumerate(data):
-            if str(row["STARTING_MEMBER_ID"]) == '' or str(row["STARTING_MEMBER_ID"]) == 'nan':
+            if str(row["STARTING_MEMBER_ID"]) == "" or str(row["STARTING_MEMBER_ID"]) == "nan":
                 data[i]["IS_STARTING_MEMBER_INCLUDED"] = False
 
     if start_index_after_last and "ID" in data.dtype.names and len(data) > 0:
-        max_id = max(int(float(row["ID"])) for row in data if str(row["ID"]) != 'nan')
+        max_id = max(int(float(row["ID"])) for row in data if str(row["ID"]) != "nan")
         start_idx = max_id + 1 if max_id else 0
         ids = list(range(start_idx, start_idx + len(data)))
         for i, row in enumerate(data):
             data[i]["ID"] = ids[i]
     else:
         if "ID" in data.dtype.names:
             data = drop_fields(data, "ID")
         ids = list(range(len(data)))
-        data = add_field(data, "ID", ids, dtype='i8')
+        data = add_field(data, "ID", ids, dtype="i8")
 
-    data = select_fields(data, [
-        "ID",
-        "MEMBER_HIERARCHY_VALID_FROM",
-        "IS_STARTING_MEMBER_INCLUDED",
-        "AXIS_ORDINATE_ID",
-        "VARIABLE_ID",
-        "MEMBER_ID",
-        "MEMBER_HIERARCHY_ID",
-        "STARTING_MEMBER_ID"
-    ])
+    data = select_fields(
+        data,
+        [
+            "ID",
+            "MEMBER_HIERARCHY_VALID_FROM",
+            "IS_STARTING_MEMBER_INCLUDED",
+            "AXIS_ORDINATE_ID",
+            "VARIABLE_ID",
+            "MEMBER_ID",
+            "MEMBER_HIERARCHY_ID",
+            "STARTING_MEMBER_ID",
+        ],
+    )
 
-    return data, {}
\ No newline at end of file
+    return data, {}
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/dpm_integration/mapping_functions/ordinate_categorisation.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/dpm_integration/mapping_functions/tables.py	2025-09-21 16:21:52.580282+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/dpm_integration/mapping_functions/tables.py	2025-09-21 17:07:36.424667+00:00
@@ -11,23 +11,32 @@
 #    Benjamin Arfa - initial API and implementation
 #
 
 import os
 from .utils import (
-    read_csv_to_dict, dict_list_to_structured_array, add_field, drop_fields,
-    select_fields, rename_fields, pascal_to_upper_snake, merge_arrays,
-    array_to_dict, clean_spaces
+    read_csv_to_dict,
+    dict_list_to_structured_array,
+    add_field,
+    drop_fields,
+    select_fields,
+    rename_fields,
+    pascal_to_upper_snake,
+    merge_arrays,
+    array_to_dict,
+    clean_spaces,
 )
 
 
 def load_template_to_framework_mapping(base_path="target"):
     """Load mapping from templates to frameworks"""
     template_group_data = read_csv_to_dict(os.path.join(base_path, "TemplateGroup.csv"))
     template_group = dict_list_to_structured_array(template_group_data, ["FrameworkID", "TemplateGroupID"])
 
     template_group_template_data = read_csv_to_dict(os.path.join(base_path, "TemplateGroupTemplate.csv"))
-    template_group_template = dict_list_to_structured_array(template_group_template_data, ["TemplateID", "TemplateGroupID"])
+    template_group_template = dict_list_to_structured_array(
+        template_group_template_data, ["TemplateID", "TemplateGroupID"]
+    )
 
     merged = merge_arrays(template_group, template_group_template, "TemplateGroupID")
     merged = select_fields(merged, ["TemplateID", "FrameworkID"])
 
     column_mapping = {col: pascal_to_upper_snake(col) for col in merged.dtype.names}
@@ -41,11 +50,13 @@
     """Load mapping from taxonomy versions to tables"""
     taxonomy_to_table_data = read_csv_to_dict(os.path.join(base_path, "TaxonomyTableVersion.csv"))
     taxonomy_to_table_version = dict_list_to_structured_array(taxonomy_to_table_data, ["TaxonomyID", "TableVID"])
 
     taxonomy_to_package_data = read_csv_to_dict(os.path.join(base_path, "Taxonomy.csv"))
-    taxonomy_to_package_version = dict_list_to_structured_array(taxonomy_to_package_data, ["TaxonomyID", "DpmPackageCode"])
+    taxonomy_to_package_version = dict_list_to_structured_array(
+        taxonomy_to_package_data, ["TaxonomyID", "DpmPackageCode"]
+    )
 
     merged = merge_arrays(taxonomy_to_table_version, taxonomy_to_package_version, "TaxonomyID")
     merged = select_fields(merged, ["TableVID", "DpmPackageCode"])
 
     column_mapping = {col: pascal_to_upper_snake(col) for col in merged.dtype.names}
@@ -61,23 +72,23 @@
     # Remove ConceptID during reading
     for row in data:
         if "ConceptID" in row:
             del row["ConceptID"]
     # Force ID fields to be strings since they will be mapped to string values
-    tables = dict_list_to_structured_array(data, force_str_columns={'TemplateID'})
+    tables = dict_list_to_structured_array(data, force_str_columns={"TemplateID"})
 
     # Get directory and create proper path for TableVersion.csv
     path_dir = os.path.dirname(path)
     tables_versions_path = os.path.join(path_dir, "TableVersion.csv")
     tables_versions_data = read_csv_to_dict(tables_versions_path)
     for row in tables_versions_data:
         if "ConceptID" in row:
             del row["ConceptID"]
     # Force ID fields to be strings since they will be mapped to string values
-    tables_versions = dict_list_to_structured_array(tables_versions_data, force_str_columns={'TableVID'})
+    tables_versions = dict_list_to_structured_array(tables_versions_data, force_str_columns={"TableVID"})
 
-    tables = merge_arrays(tables, tables_versions, "TableID", force_str_columns={'TemplateID', 'TableVID'})
+    tables = merge_arrays(tables, tables_versions, "TableID", force_str_columns={"TemplateID", "TableVID"})
     template_to_framework_mapping = load_template_to_framework_mapping()
     table_to_taxonomy_mapping = load_taxonomy_version_to_table_mapping()
 
     # Transform column names to UPPER_SNAKE_CASE
     column_mapping = {col: pascal_to_upper_snake(col) for col in tables.dtype.names}
@@ -126,16 +137,19 @@
     for row in tables:
         id_mapping[str(row["TABLE_VID"])] = str(row["NEW_TABLE_ID"])
 
     tables = drop_fields(tables, "TABLE_ID")
 
-    tables = rename_fields(tables, {
-        "NEW_TABLE_ID": "TABLE_ID",
-        "ORIGINAL_TABLE_LABEL": "DESCRIPTION",
-        "FROM_DATE": "VALID_FROM",
-        "TO_DATE": "VALID_TO"
-    })
+    tables = rename_fields(
+        tables,
+        {
+            "NEW_TABLE_ID": "TABLE_ID",
+            "ORIGINAL_TABLE_LABEL": "DESCRIPTION",
+            "FROM_DATE": "VALID_FROM",
+            "TO_DATE": "VALID_TO",
+        },
+    )
 
     # Add NAME and CODE fields
     names = []
     codes = []
     versions = []
@@ -168,13 +182,14 @@
 
     tables = add_field(tables, "NAME", names)
     tables = add_field(tables, "CODE", codes)
     tables = add_field(tables, "VERSION", versions)
 
-    tables = select_fields(tables, [
-        "TABLE_ID", "NAME", "CODE", "DESCRIPTION", "MAINTENANCE_AGENCY_ID", "VERSION", "VALID_FROM", "VALID_TO"
-    ])
+    tables = select_fields(
+        tables,
+        ["TABLE_ID", "NAME", "CODE", "DESCRIPTION", "MAINTENANCE_AGENCY_ID", "VERSION", "VALID_FROM", "VALID_TO"],
+    )
 
     # Clean text fields
     tables = clean_spaces(tables)
 
-    return tables, id_mapping
\ No newline at end of file
+    return tables, id_mapping
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/dpm_integration/mapping_functions/tables.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/ancrdt_transformation/create_python_django_transformations_ancrdt.py	2025-09-15 13:18:11.376559+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/ancrdt_transformation/create_python_django_transformations_ancrdt.py	2025-09-21 17:07:36.429659+00:00
@@ -16,93 +16,102 @@
 import os
 from django.conf import settings
 import ast
 from pybirdai.process_steps.ancrdt_transformation.filter_buildr import TransformationBuildr
 
+
 class CreatePythonTransformations:
 
     @staticmethod
-    def create_python_joins(context, sdd_context,logger):
-        '''
+    def create_python_joins(context, sdd_context, logger):
+        """
         Read in the transformation meta data and create python classes
-        '''
+        """
 
         # CreatePythonTransformations.delete_generated_python_join_files(context)
-        CreatePythonTransformations.create_output_classes( sdd_context,logger)
-        CreatePythonTransformations.create_slice_classes(sdd_context,logger)
+        CreatePythonTransformations.create_output_classes(sdd_context, logger)
+        CreatePythonTransformations.create_slice_classes(sdd_context, logger)
         # get all the cube_links for a report
 
-    def create_output_classes(sdd_context,logger):
-
-        #get all the cubes_structure_items for that cube and make a related Python class.
-        file = open(sdd_context.output_directory + os.sep + 'generated_python_joins' + os.sep +  'ancrdt_output_tables.py', "a",  encoding='utf-8')
+    def create_output_classes(sdd_context, logger):
+
+        # get all the cubes_structure_items for that cube and make a related Python class.
+        file = open(
+            sdd_context.output_directory + os.sep + "generated_python_joins" + os.sep + "ancrdt_output_tables.py",
+            "a",
+            encoding="utf-8",
+        )
         file.write("from pybirdai.process_steps.pybird.orchestration import Orchestration\n")
         file.write("from datetime import datetime\n")
         file.write("from pybirdai.annotations.decorators import lineage\n")
         cube_link_to_foreign_cube_map__ = {
-            rolc_id:cube_links
+            rolc_id: cube_links
             for rolc_id, cube_links in sdd_context.cube_link_to_foreign_cube_map.items()
             if "ANCRDT" in rolc_id
         }
         for rolc_id, cube_links in cube_link_to_foreign_cube_map__.items():
             # logger.info(f"rolc_id: {rolc_id}")
-            file.write("from ." + rolc_id  + "_logic import *\n")
+            file.write("from ." + rolc_id + "_logic import *\n")
             file.write("\nclass " + rolc_id + ":\n")
             file.write("\tunionOfLayers = None #  " + rolc_id + "_UnionItem  unionOfLayers\n")
             cube_structure_items = sdd_context.bird_cube_structure_item_dictionary[rolc_id]
             for cube_structure_item in cube_structure_items:
                 # logger.info(f"cube_structure_item: {cube_structure_item}")
                 variable = cube_structure_item.variable_id
                 if cube_structure_item.variable_id.variable_id == "NEVS":
                     continue
                 domain = variable.domain_id.domain_id
-                file.write('\t@lineage(dependencies={"unionOfLayers.'+ variable.variable_id +'"})\n')
-                if domain == 'String':
-                    file.write('\tdef ' + variable.variable_id + '(self) -> str:\n')
-                elif domain == 'Integer':
-                    file.write('\tdef ' + variable.variable_id + '(self) -> int:\n')
-                elif domain == 'Date':
-                    file.write('\tdef ' + variable.variable_id + '(self) -> datetime:\n')
-                elif domain == 'Float':
-                    file.write('\tdef ' + variable.variable_id + '(self) -> float:\n')
-                elif domain == 'Boolean':
-                    file.write('\tdef ' + variable.variable_id + '(self) -> bool:\n')
+                file.write('\t@lineage(dependencies={"unionOfLayers.' + variable.variable_id + '"})\n')
+                if domain == "String":
+                    file.write("\tdef " + variable.variable_id + "(self) -> str:\n")
+                elif domain == "Integer":
+                    file.write("\tdef " + variable.variable_id + "(self) -> int:\n")
+                elif domain == "Date":
+                    file.write("\tdef " + variable.variable_id + "(self) -> datetime:\n")
+                elif domain == "Float":
+                    file.write("\tdef " + variable.variable_id + "(self) -> float:\n")
+                elif domain == "Boolean":
+                    file.write("\tdef " + variable.variable_id + "(self) -> bool:\n")
                 else:
-                    file.write('\tdef ' + variable.variable_id + '(self) -> str:\n')
-                    file.write('\t\t\'\'\' return string from ' + domain + ' enumeration \'\'\'\n')
-
-                file.write('\t\treturn self.unionOfLayers.' + variable.variable_id + '()\n')
-                file.write('\n')
-            file.write('\n')
-            file.write('\n')
-            file.write("class " +rolc_id + "_Table :\n" )
-            #file.write("\tunionOfLayersTable = None # " + rolc_id + "_UnionTable\n" )
-            file.write("\t" + rolc_id + "_UnionTable = None # unionOfLayersTable\n" )
-            file.write("\t" + rolc_id + "s = [] #" + rolc_id + "[]\n" )
-            file.write("\tdef  calc_" + rolc_id + "s(self) -> list[" + rolc_id + "] :\n" )
-            file.write("\t\titems = [] # " + rolc_id + "[]\n" )
-            file.write("\t\tfor item in self." +rolc_id + "_UnionTable." + rolc_id + "_UnionItems:\n" )
-            file.write("\t\t\tnewItem = " + rolc_id + "()\n" )
-            file.write("\t\t\tnewItem.unionOfLayers = item\n" )
-            file.write("\t\t\titems.append(newItem)\n" )
-            file.write("\t\treturn items\n" )
-            file.write("\tdef init(self):\n" )
-            file.write("\t\tOrchestration().init(self)\n" )
-            file.write("\t\tself." + rolc_id + "s = []\n" )
-            file.write("\t\tself." + rolc_id + "s.extend(self.calc_" + rolc_id + "s())\n" )
+                    file.write("\tdef " + variable.variable_id + "(self) -> str:\n")
+                    file.write("\t\t''' return string from " + domain + " enumeration '''\n")
+
+                file.write("\t\treturn self.unionOfLayers." + variable.variable_id + "()\n")
+                file.write("\n")
+            file.write("\n")
+            file.write("\n")
+            file.write("class " + rolc_id + "_Table :\n")
+            # file.write("\tunionOfLayersTable = None # " + rolc_id + "_UnionTable\n" )
+            file.write("\t" + rolc_id + "_UnionTable = None # unionOfLayersTable\n")
+            file.write("\t" + rolc_id + "s = [] #" + rolc_id + "[]\n")
+            file.write("\tdef  calc_" + rolc_id + "s(self) -> list[" + rolc_id + "] :\n")
+            file.write("\t\titems = [] # " + rolc_id + "[]\n")
+            file.write("\t\tfor item in self." + rolc_id + "_UnionTable." + rolc_id + "_UnionItems:\n")
+            file.write("\t\t\tnewItem = " + rolc_id + "()\n")
+            file.write("\t\t\tnewItem.unionOfLayers = item\n")
+            file.write("\t\t\titems.append(newItem)\n")
+            file.write("\t\treturn items\n")
+            file.write("\tdef init(self):\n")
+            file.write("\t\tOrchestration().init(self)\n")
+            file.write("\t\tself." + rolc_id + "s = []\n")
+            file.write("\t\tself." + rolc_id + "s.extend(self.calc_" + rolc_id + "s())\n")
             file.write("\t\tCSVConverter.persist_object_as_csv(self,True)\n")
-            file.write("\t\treturn None\n" )
-
-    def create_slice_classes( sdd_context,logger):
+            file.write("\t\treturn None\n")
+
+    def create_slice_classes(sdd_context, logger):
         cube_link_to_foreign_cube_map__ = {
-            rolc_id:cube_links
+            rolc_id: cube_links
             for rolc_id, cube_links in sdd_context.cube_link_to_foreign_cube_map.items()
             if "ANCRDT" in rolc_id
         }
 
         for rolc_id, cube_links in cube_link_to_foreign_cube_map__.items():
-            file = open(sdd_context.output_directory + os.sep + 'generated_python_joins' + os.sep +  rolc_id + '_logic.py', "a",  encoding='utf-8')
+            file = open(
+                sdd_context.output_directory + os.sep + "generated_python_joins" + os.sep + rolc_id + "_logic.py",
+                "a",
+                encoding="utf-8",
+            )
             file.write("from pybirdai.bird_data_model import *\n")
             file.write("from pybirdai.process_steps.pybird.orchestration import Orchestration\n")
             file.write("from pybirdai.process_steps.pybird.csv_converter import CSVConverter\n")
             file.write("from datetime import datetime\n")
             file.write("from pybirdai.annotations.decorators import lineage\n")
@@ -116,28 +125,27 @@
                 variable = cube_structure_item.variable_id
                 if cube_structure_item.variable_id.variable_id == "NEVS":
                     continue
 
                 domain = variable.domain_id.domain_id
-                file.write('\t@lineage(dependencies={"base.'+ variable.variable_id +'"})\n')
-                if domain == 'String':
-                    file.write('\tdef ' + variable.variable_id + '(self) -> str:\n')
-                elif domain == 'Integer':
-                    file.write('\tdef ' + variable.variable_id + '(self) -> int:\n')
-                elif domain == 'Date':
-                    file.write('\tdef ' + variable.variable_id + '(self) -> datetime:\n')
-                elif domain == 'Float':
-                    file.write('\tdef ' + variable.variable_id + '(self) -> float:\n')
-                elif domain == 'Boolean':
-                    file.write('\tdef ' + variable.variable_id + '(self) -> bool:\n')
+                file.write('\t@lineage(dependencies={"base.' + variable.variable_id + '"})\n')
+                if domain == "String":
+                    file.write("\tdef " + variable.variable_id + "(self) -> str:\n")
+                elif domain == "Integer":
+                    file.write("\tdef " + variable.variable_id + "(self) -> int:\n")
+                elif domain == "Date":
+                    file.write("\tdef " + variable.variable_id + "(self) -> datetime:\n")
+                elif domain == "Float":
+                    file.write("\tdef " + variable.variable_id + "(self) -> float:\n")
+                elif domain == "Boolean":
+                    file.write("\tdef " + variable.variable_id + "(self) -> bool:\n")
                 else:
-                    file.write('\tdef ' + variable.variable_id + '(self) -> str:\n')
-                    file.write('\t\t\'\'\' return string from ' + domain + ' enumeration \'\'\'\n')
-
-                file.write('\t\treturn self.base.' + variable.variable_id + '()')
-                file.write('\n')
-
+                    file.write("\tdef " + variable.variable_id + "(self) -> str:\n")
+                    file.write("\t\t''' return string from " + domain + " enumeration '''\n")
+
+                file.write("\t\treturn self.base." + variable.variable_id + "()")
+                file.write("\n")
 
             file.write("\nclass " + rolc_id + "_Base:\n")
             cube_structure_items = []
             cube_structure_items = sdd_context.bird_cube_structure_item_dictionary[rolc_id]
 
@@ -149,54 +157,69 @@
                 variable = cube_structure_item.variable_id
                 if cube_structure_item.variable_id.variable_id == "NEVS":
                     continue
 
                 domain = variable.domain_id.domain_id
-                if domain == 'String':
-                    file.write('\tdef ' + variable.variable_id + '() -> str:\n')
-                elif domain == 'Integer':
-                    file.write('\tdef ' + variable.variable_id + '() -> int:\n')
-                elif domain == 'Date':
-                    file.write('\tdef ' + variable.variable_id + '() -> datetime:\n')
-                elif domain == 'Float':
-                    file.write('\tdef ' + variable.variable_id + '() -> float:\n')
-                elif domain == 'Boolean':
-                    file.write('\tdef ' + variable.variable_id + '() -> bool:\n')
+                if domain == "String":
+                    file.write("\tdef " + variable.variable_id + "() -> str:\n")
+                elif domain == "Integer":
+                    file.write("\tdef " + variable.variable_id + "() -> int:\n")
+                elif domain == "Date":
+                    file.write("\tdef " + variable.variable_id + "() -> datetime:\n")
+                elif domain == "Float":
+                    file.write("\tdef " + variable.variable_id + "() -> float:\n")
+                elif domain == "Boolean":
+                    file.write("\tdef " + variable.variable_id + "() -> bool:\n")
                 else:
-                    file.write('\tdef ' + variable.variable_id + '() -> str:\n')
-                    file.write('\t\t\'\'\' return string from ' + domain + ' enumeration \'\'\'\n')
-
-                file.write('\t\tpass')
-                file.write('\n')
-
+                    file.write("\tdef " + variable.variable_id + "() -> str:\n")
+                    file.write("\t\t''' return string from " + domain + " enumeration '''\n")
+
+                file.write("\t\tpass")
+                file.write("\n")
 
             file.write("\nclass " + rolc_id + "_UnionTable :\n")
-            file.write("\t" + rolc_id + "_UnionItems = [] # " +  rolc_id + "_UnionItem []\n" )
+            file.write("\t" + rolc_id + "_UnionItems = [] # " + rolc_id + "_UnionItem []\n")
             join_ids_added = []
             cube_link_to_join_for_report_id_map__ = {
-                join_for_rolc_id:cube_links
+                join_for_rolc_id: cube_links
                 for join_for_rolc_id, cube_links in sdd_context.cube_link_to_join_for_report_id_map.items()
                 if "ANCRDT_INSTRMNT_C_1" in join_for_rolc_id
             }
             logger.info(str(cube_link_to_join_for_report_id_map__))
             for join_for_rolc_id, cube_links in cube_link_to_join_for_report_id_map__.items():
                 for cube_link in cube_links:
                     the_rolc_id = cube_link.foreign_cube_id.cube_id
                     if the_rolc_id == rolc_id:
                         if cube_link.join_identifier not in join_ids_added:
-                            file.write("\t" + rolc_id + "_" + cube_link.join_identifier.replace(' ','_') + "_Table = None # " +  cube_link.join_identifier.replace(' ','_') + "\n")
+                            file.write(
+                                "\t"
+                                + rolc_id
+                                + "_"
+                                + cube_link.join_identifier.replace(" ", "_")
+                                + "_Table = None # "
+                                + cube_link.join_identifier.replace(" ", "_")
+                                + "\n"
+                            )
                             join_ids_added.append(cube_link.join_identifier)
             file.write("\tdef calc_" + rolc_id + "_UnionItems(self) -> list[" + rolc_id + "_UnionItem] :\n")
             file.write("\t\titems = [] # " + rolc_id + "_UnionItem []\n")
 
             join_ids_added = []
             for join_for_rolc_id, cube_links in cube_link_to_join_for_report_id_map__.items():
                 for cube_link in cube_links:
                     the_rolc_id = cube_link.foreign_cube_id.cube_id
                     if the_rolc_id == rolc_id:
                         if cube_link.join_identifier not in join_ids_added:
-                            file.write("\t\tfor item in self." + rolc_id + "_" + cube_link.join_identifier.replace(' ','_') + "_Table." + cube_link.join_identifier.replace(' ','_') + "s:\n")
+                            file.write(
+                                "\t\tfor item in self."
+                                + rolc_id
+                                + "_"
+                                + cube_link.join_identifier.replace(" ", "_")
+                                + "_Table."
+                                + cube_link.join_identifier.replace(" ", "_")
+                                + "s:\n"
+                            )
                             file.write("\t\t\tnewItem = " + rolc_id + "_UnionItem()\n")
                             file.write("\t\t\tnewItem.base = item\n")
                             file.write("\t\t\titems.append(newItem)\n")
                             join_ids_added.append(cube_link.join_identifier)
             file.write("\t\treturn items\n")
@@ -215,72 +238,119 @@
                 for cube_link in cube_links:
                     the_rolc_id = cube_link.foreign_cube_id.cube_id
                     if the_rolc_id == rolc_id:
                         # only write the class header once
                         if not class_header_is_written:
-                            file.write("\nclass " + cube_link.join_identifier.replace(' ','_') + "(" + rolc_id + "_Base):\n")
+                            file.write(
+                                "\nclass " + cube_link.join_identifier.replace(" ", "_") + "(" + rolc_id + "_Base):\n"
+                            )
                             class_header_is_written = True
 
                         cube_structure_item_links = []
                         try:
-                            cube_structure_item_links = sdd_context.cube_structure_item_link_to_cube_link_map[cube_link.cube_link_id]
+                            cube_structure_item_links = sdd_context.cube_structure_item_link_to_cube_link_map[
+                                cube_link.cube_link_id
+                            ]
                         except KeyError:
                             logger.info(f"No cube structure item links for cube_link: {cube_link.cube_link_id}")
                         primary_cubes_added = []
                         if len(cube_structure_item_links) == 0:
                             file.write("\tpass\n")
                         for cube_structure_item_link in cube_structure_item_links:
                             if cube_structure_item_link.cube_link_id.primary_cube_id.cube_id not in primary_cubes_added:
-                                file.write("\t" + cube_structure_item_link.cube_link_id.primary_cube_id.cube_id  + " = None # " + cube_structure_item_link.cube_link_id.primary_cube_id.cube_id + "\n")
-                                primary_cubes_added.append(cube_structure_item_link.cube_link_id.primary_cube_id.cube_id)
+                                file.write(
+                                    "\t"
+                                    + cube_structure_item_link.cube_link_id.primary_cube_id.cube_id
+                                    + " = None # "
+                                    + cube_structure_item_link.cube_link_id.primary_cube_id.cube_id
+                                    + "\n"
+                                )
+                                primary_cubes_added.append(
+                                    cube_structure_item_link.cube_link_id.primary_cube_id.cube_id
+                                )
                         for cube_structure_item_link in cube_structure_item_links:
-                            file.write('\t@lineage(dependencies={"'+ cube_structure_item_link.cube_link_id.primary_cube_id.cube_id + '.' + cube_structure_item_link.primary_cube_variable_code.variable_id.variable_id +'"})\n')
-                            file.write("\tdef " + cube_structure_item_link.foreign_cube_variable_code.variable_id.variable_id + "(self):\n")
-                            file.write("\t\treturn self." +  cube_structure_item_link.cube_link_id.primary_cube_id.cube_id + "." + cube_structure_item_link.primary_cube_variable_code.variable_id.variable_id + "\n")
-
+                            file.write(
+                                '\t@lineage(dependencies={"'
+                                + cube_structure_item_link.cube_link_id.primary_cube_id.cube_id
+                                + "."
+                                + cube_structure_item_link.primary_cube_variable_code.variable_id.variable_id
+                                + '"})\n'
+                            )
+                            file.write(
+                                "\tdef "
+                                + cube_structure_item_link.foreign_cube_variable_code.variable_id.variable_id
+                                + "(self):\n"
+                            )
+                            file.write(
+                                "\t\treturn self."
+                                + cube_structure_item_link.cube_link_id.primary_cube_id.cube_id
+                                + "."
+                                + cube_structure_item_link.primary_cube_variable_code.variable_id.variable_id
+                                + "\n"
+                            )
 
             for join_for_rolc_id, cube_links in cube_link_to_join_for_report_id_map__.items():
-                report_and_join =   join_for_rolc_id.split(':')
+                report_and_join = join_for_rolc_id.split(":")
                 join_id = report_and_join[1]
                 if report_and_join[0] == rolc_id:
-                    file.write("\nclass " + rolc_id + "_" + join_id.replace(' ','_') + "_Table:\n" )
+                    file.write("\nclass " + rolc_id + "_" + join_id.replace(" ", "_") + "_Table:\n")
                     for cube_link in cube_links:
                         cube_structure_item_links = []
                         try:
-                            cube_structure_item_links = sdd_context.cube_structure_item_link_to_cube_link_map[cube_link.cube_link_id]
+                            cube_structure_item_links = sdd_context.cube_structure_item_link_to_cube_link_map[
+                                cube_link.cube_link_id
+                            ]
                         except KeyError:
                             logger.info(f"No cube structure item links for cube_link: {cube_link.cube_link_id}")
 
                         primary_cubes_added = []
                         for cube_structure_item_link in cube_structure_item_links:
                             if cube_structure_item_link.cube_link_id.primary_cube_id.cube_id not in primary_cubes_added:
-                                file.write("\t" + cube_structure_item_link.cube_link_id.primary_cube_id.cube_id  + "_Table = None # " + cube_structure_item_link.cube_link_id.primary_cube_id.cube_id + "\n")
-                                primary_cubes_added.append(cube_structure_item_link.cube_link_id.primary_cube_id.cube_id)
+                                file.write(
+                                    "\t"
+                                    + cube_structure_item_link.cube_link_id.primary_cube_id.cube_id
+                                    + "_Table = None # "
+                                    + cube_structure_item_link.cube_link_id.primary_cube_id.cube_id
+                                    + "\n"
+                                )
+                                primary_cubes_added.append(
+                                    cube_structure_item_link.cube_link_id.primary_cube_id.cube_id
+                                )
 
                 if report_and_join[0] == rolc_id:
                     join_id = report_and_join[1]
-                    file.write("\t" + join_id.replace(' ','_') + "s = []# " + join_id.replace(' ','_') + "[]\n")
-                    file.write("\tdef calc_" + join_id.replace(' ','_') + "s(self) :\n")
-                    file.write("\t\titems = [] # " + join_id.replace(' ','_') + "[\n")
+                    file.write("\t" + join_id.replace(" ", "_") + "s = []# " + join_id.replace(" ", "_") + "[]\n")
+                    file.write("\tdef calc_" + join_id.replace(" ", "_") + "s(self) :\n")
+                    file.write("\t\titems = [] # " + join_id.replace(" ", "_") + "[\n")
                     file.write("\t\t# Join up any refered tables that you need to join\n")
                     file.write("\t\t# loop through the main table\n")
-                    file.write("\t\t# set any references you want to on the new Item so that it can refer to themin operations\n")
+                    file.write(
+                        "\t\t# set any references you want to on the new Item so that it can refer to themin operations\n"
+                    )
                     file.write("\t\treturn items\n")
                     file.write("\tdef init(self):\n")
                     file.write("\t\tOrchestration().init(self)\n")
-                    file.write("\t\tself." + join_id.replace(' ','_') + "s = []\n")
-                    file.write("\t\tself." + join_id.replace(' ','_') + "s.extend(self.calc_" + join_id.replace(' ','_') + "s())\n")
+                    file.write("\t\tself." + join_id.replace(" ", "_") + "s = []\n")
+                    file.write(
+                        "\t\tself."
+                        + join_id.replace(" ", "_")
+                        + "s.extend(self.calc_"
+                        + join_id.replace(" ", "_")
+                        + "s())\n"
+                    )
                     file.write("\t\tCSVConverter.persist_object_as_csv(self,True)\n")
 
                     file.write("\t\treturn None\n")
                     file.write("\n")
 
             for join_for_rolc_id, cube_links in cube_link_to_join_for_report_id_map__.items():
-                report_and_join =   join_for_rolc_id.split(':')
+                report_and_join = join_for_rolc_id.split(":")
                 join_id = report_and_join[1]
                 if report_and_join[0] == rolc_id:
-                    class_inheriting_from_base = f"\nclass {rolc_id}_{join_id.replace(' ','_')}_filtered_and_aggregated({rolc_id}_Base):\n"
+                    class_inheriting_from_base = (
+                        f"\nclass {rolc_id}_{join_id.replace(' ','_')}_filtered_and_aggregated({rolc_id}_Base):\n"
+                    )
                     base_calc_function_name = f"calc_{rolc_id}_{join_id.replace(' ','_')}_filtered_and_aggregated"
                     table_class = f"\nclass {rolc_id}_{join_id.replace(' ','_')}_filtered_and_aggregated_Table:\n"
                     file.write(table_class)
                     cube_structure_items = sdd_context.bird_cube_structure_item_dictionary[rolc_id]
                     for cube_structure_item in cube_structure_items:
@@ -288,84 +358,91 @@
                         variable = cube_structure_item.variable_id
                         if cube_structure_item.variable_id.variable_id == "NEVS":
                             continue
 
                         domain = variable.domain_id.domain_id
-                        file.write('\t@lineage(dependencies={"base.'+ variable.variable_id +'"})\n')
-                        if domain == 'String':
-                            file.write('\tdef ' + variable.variable_id + '(self) -> str:\n')
-                        elif domain == 'Integer':
-                            file.write('\tdef ' + variable.variable_id + '(self) -> int:\n')
-                        elif domain == 'Date':
-                            file.write('\tdef ' + variable.variable_id + '(self) -> datetime:\n')
-                        elif domain == 'Float':
-                            file.write('\tdef ' + variable.variable_id + '(self) -> float:\n')
-                        elif domain == 'Boolean':
-                            file.write('\tdef ' + variable.variable_id + '(self) -> bool:\n')
+                        file.write('\t@lineage(dependencies={"base.' + variable.variable_id + '"})\n')
+                        if domain == "String":
+                            file.write("\tdef " + variable.variable_id + "(self) -> str:\n")
+                        elif domain == "Integer":
+                            file.write("\tdef " + variable.variable_id + "(self) -> int:\n")
+                        elif domain == "Date":
+                            file.write("\tdef " + variable.variable_id + "(self) -> datetime:\n")
+                        elif domain == "Float":
+                            file.write("\tdef " + variable.variable_id + "(self) -> float:\n")
+                        elif domain == "Boolean":
+                            file.write("\tdef " + variable.variable_id + "(self) -> bool:\n")
                         else:
-                            file.write('\tdef ' + variable.variable_id + '(self) -> str:\n')
-                            file.write('\t\t\'\'\' return string from ' + domain + ' enumeration \'\'\'\n')
-
-                        file.write('\t\treturn self.base.' + variable.variable_id + '()')
-                        file.write('\n')
-
+                            file.write("\tdef " + variable.variable_id + "(self) -> str:\n")
+                            file.write("\t\t''' return string from " + domain + " enumeration '''\n")
+
+                        file.write("\t\treturn self.base." + variable.variable_id + "()")
+                        file.write("\n")
 
                     # Generate filter for calc_ ... agregated_ function
                     generated_filter = []
                     for cube_link in cube_links:
                         cube_structure_item_link_ids = CUBE_STRUCTURE_ITEM_LINK.objects.all().filter(
-                            cube_link_id = cube_link
+                            cube_link_id=cube_link
                         )
                         for cube_structure_item_link in cube_structure_item_link_ids:
-                            generated_filter += [f"{TransformationBuildr.define_filter_from_structure_link(
+                            generated_filter += [
+                                f"{TransformationBuildr.define_filter_from_structure_link(
                                 cube_structure_item_link.cube_structure_item_link_id
-                            )}"]
+                            )}"
+                            ]
                     if not generated_filter:
                         continue
 
                     generated_filter = " and ".join(generated_filter)
 
-                    file.write(f'\tdef {base_calc_function_name}(self) -> str:\n')
-                    file.write('\t\titems = [] # ' + rolc_id + '_Loans_and_advances_filtered_and_aggregated []\n')
-                    file.write('\t\tfor item in self.' + rolc_id + '_Loans_and_advances_Table.' + rolc_id + '_Loans_and_advances:\n')
-                    file.write(f'\t\t\tif {generated_filter}:\n')
-                    file.write('\t\t\t\tnewItem = ' + rolc_id + '_Loans_and_advances_filtered_and_aggregated()\n')
-                    file.write('\t\t\t\tnewItem.source = item\n')
-                    file.write('\t\t\t\titems.append(newItem)\n')
-                    file.write('\t\treturn items\n')
-
+                    file.write(f"\tdef {base_calc_function_name}(self) -> str:\n")
+                    file.write("\t\titems = [] # " + rolc_id + "_Loans_and_advances_filtered_and_aggregated []\n")
+                    file.write(
+                        "\t\tfor item in self."
+                        + rolc_id
+                        + "_Loans_and_advances_Table."
+                        + rolc_id
+                        + "_Loans_and_advances:\n"
+                    )
+                    file.write(f"\t\t\tif {generated_filter}:\n")
+                    file.write("\t\t\t\tnewItem = " + rolc_id + "_Loans_and_advances_filtered_and_aggregated()\n")
+                    file.write("\t\t\t\tnewItem.source = item\n")
+                    file.write("\t\t\t\titems.append(newItem)\n")
+                    file.write("\t\treturn items\n")
 
                     file.write(class_inheriting_from_base)
 
                     assignment_dicts = dict()
                     for cube_link in cube_links:
                         cube_structure_item_link_ids = CUBE_STRUCTURE_ITEM_LINK.objects.all().filter(
-                            cube_link_id = cube_link
+                            cube_link_id=cube_link
                         )
                         for cube_structure_item_link in cube_structure_item_link_ids:
-                            assignment_dicts.update(TransformationBuildr.reverse_apply_member_links(
-                                cube_structure_item_link.cube_structure_item_link_id
-                            ))
+                            assignment_dicts.update(
+                                TransformationBuildr.reverse_apply_member_links(
+                                    cube_structure_item_link.cube_structure_item_link_id
+                                )
+                            )
 
                     cube_structure_items = sdd_context.bird_cube_structure_item_dictionary[rolc_id]
                     for cube_structure_item in cube_structure_items:
                         # logger.info(f"cube_structure_item: {cube_structure_item}")
                         variable = cube_structure_item.variable_id
                         if cube_structure_item.variable_id.variable_id == "NEVS":
                             continue
 
                         domain = variable.domain_id.domain_id
                         for var, source_target_dict in assignment_dicts.items():
-                            file.write('\tdef ' + var + '(self) -> str:\n')
-                            file.write(f'\t\tsource = self.{rolc_id}_{join_id.replace(' ','_')}.{var}()\n')
+                            file.write("\tdef " + var + "(self) -> str:\n")
+                            file.write(f"\t\tsource = self.{rolc_id}_{join_id.replace(' ','_')}.{var}()\n")
                             for row in source_target_dict:
-                                file.write(f"""\t\tif source == '{row.get("source")}' : return '{row.get("target")}'\n""")
-                            file.write('\n')
-
-
-
+                                file.write(
+                                    f"""\t\tif source == '{row.get("source")}' : return '{row.get("target")}'\n"""
+                                )
+                            file.write("\n")
 
     def delete_generated_python_join_files(context):
         base_dir = settings.BASE_DIR
-        python_dir = os.path.join(base_dir, 'results',  'generated_python_joins')
+        python_dir = os.path.join(base_dir, "results", "generated_python_joins")
         for file in os.listdir(python_dir):
             os.remove(os.path.join(python_dir, file))
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/ancrdt_transformation/create_python_django_transformations_ancrdt.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/dpm_integration/mapping_functions/utils.py	2025-09-21 16:20:10.818386+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/dpm_integration/mapping_functions/utils.py	2025-09-21 17:07:36.433779+00:00
@@ -24,11 +24,11 @@
 
 
 def read_csv_to_dict(path, dtype=None):
     """Read CSV file and return as list of dictionaries"""
     data = []
-    with open(path, 'r', encoding='utf-8') as f:
+    with open(path, "r", encoding="utf-8") as f:
         reader = csv.DictReader(f)
         for row in reader:
             data.append(row)
     return data
 
@@ -47,28 +47,28 @@
     # Create dtype for structured array
     dtype_list = []
     for col in columns:
         # Force certain columns to be strings
         if col in force_str_columns:
-            max_len = max(len(str(row.get(col, ''))) for row in data)
-            dtype_list.append((col, f'U{max(max_len + 50, 100)}'))  # Extra space for transformations
+            max_len = max(len(str(row.get(col, ""))) for row in data)
+            dtype_list.append((col, f"U{max(max_len + 50, 100)}"))  # Extra space for transformations
         else:
             # Check if column contains numeric data
-            sample_val = str(data[0].get(col, '')).strip()
-            if sample_val and sample_val.replace('.', '').replace('-', '').isdigit():
-                dtype_list.append((col, 'i8'))
+            sample_val = str(data[0].get(col, "")).strip()
+            if sample_val and sample_val.replace(".", "").replace("-", "").isdigit():
+                dtype_list.append((col, "i8"))
             else:
                 # Find max string length for this column
-                max_len = max(len(str(row.get(col, ''))) for row in data)
-                dtype_list.append((col, f'U{max(max_len, 1)}'))
+                max_len = max(len(str(row.get(col, ""))) for row in data)
+                dtype_list.append((col, f"U{max(max_len, 1)}"))
 
     # Create structured array
     arr = np.zeros(len(data), dtype=dtype_list)
     for i, row in enumerate(data):
         for col in columns:
-            val = row.get(col, '')
-            if arr.dtype[col].kind in ['f', 'i']:
+            val = row.get(col, "")
+            if arr.dtype[col].kind in ["f", "i"]:
                 try:
                     arr[i][col] = int(float(val)) if val else 0
                 except:
                     arr[i][col] = 0
             else:
@@ -121,11 +121,11 @@
     if len(arr) == 0:
         return arr
     return arr[fields]
 
 
-def add_field(arr, field_name, values, dtype='U100'):
+def add_field(arr, field_name, values, dtype="U100"):
     """Add a new field to structured array"""
     if len(arr) == 0:
         new_dtype = [(field_name, dtype)]
         return np.array(values if isinstance(values, list) else [values], dtype=new_dtype)
 
@@ -143,11 +143,11 @@
         new_arr[field_name][:] = values
 
     return new_arr
 
 
-def merge_arrays(left, right, left_on, right_on=None, how='inner', force_str_columns=None):
+def merge_arrays(left, right, left_on, right_on=None, how="inner", force_str_columns=None):
     """Merge two structured arrays"""
     if right_on is None:
         right_on = left_on
 
     if len(left) == 0 or len(right) == 0:
@@ -173,17 +173,17 @@
                     merged_row[field] = left_row[field]
                 for field in right.dtype.names:
                     if field != right_on and field not in merged_row:  # Avoid duplicate fields
                         merged_row[field] = right_row[field]
                 result.append(merged_row)
-        elif how == 'left':
+        elif how == "left":
             merged_row = {}
             for field in left.dtype.names:
                 merged_row[field] = left_row[field]
             for field in right.dtype.names:
                 if field != right_on and field not in merged_row:
-                    merged_row[field] = ''
+                    merged_row[field] = ""
             result.append(merged_row)
 
     if not result:
         return np.array([])
 
@@ -203,14 +203,14 @@
     if len(arr) == 0:
         return arr
 
     new_arr = arr.copy()
     for field in arr.dtype.names:
-        if arr.dtype[field].kind == 'U':  # Unicode string
+        if arr.dtype[field].kind == "U":  # Unicode string
             for i in range(len(new_arr)):
                 val = str(new_arr[i][field])
                 # Replace all types of line breaks and carriage returns with spaces
                 val = val.replace("\n", " ").replace("\r", " ").replace("\r\n", " ")
                 # Replace quotes and clean up multiple spaces
                 val = val.replace('"', "'").replace("  ", " ").strip()
                 new_arr[i][field] = val
-    return new_arr
\ No newline at end of file
+    return new_arr
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/dpm_integration/mapping_functions/utils.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/generate_test_data/enrich_ldm_with_il_links_from_fe.py	2025-09-15 13:18:11.381129+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/generate_test_data/enrich_ldm_with_il_links_from_fe.py	2025-09-21 17:07:36.453340+00:00
@@ -9,69 +9,86 @@
 #
 # Contributors:
 #    Neil Mackenzie - initial API and implementation
 #
 import csv
-#from pybirdai.process_steps.generate_test_data.ldm_utils import Utils
+
+# from pybirdai.process_steps.generate_test_data.ldm_utils import Utils
 from pybirdai.utils.utils import Utils
 import os
 
-from pybirdai.regdna import ELAttribute, ELClass, ELEnum, ELEnumLiteral, ELOperation, ELReference, ELAnnotation, ELStringToStringMapEntry
+from pybirdai.regdna import (
+    ELAttribute,
+    ELClass,
+    ELEnum,
+    ELEnumLiteral,
+    ELOperation,
+    ELReference,
+    ELAnnotation,
+    ELStringToStringMapEntry,
+)
+
 
 class InputLayerLinkEnricher(object):
-    '''
+    """
     After the Forward Engineering process has been run on the LDM,
     SQLDevelepor stores information about how whicj column in the Input
     Layer was created by forward engineering an attribute in the LDM.
     In SQLdeveloper these are accessed via the 'Impacty analysis'
     Feature..so we can see what is the equivelent Input Layer column
     for an LDM attribute.
     This class is responsable for adding an Annotation to the LDM attribute
     to show the name of the linked Input Layer column. The name
     is represented in a 'TableName.ColumnName' format.
-    '''
+    """
 
     def enrich_with_links_to_input_layer_columns(self, context):
-        '''
+        """
         Enrich the attributes of classes of our LDM package with an annotation
         To show what input layer column is related to LDM attribute.
-        '''
+        """
 
         InputLayerLinkEnricher.create_attribute_to_column_links(self, context)
 
     def create_attribute_to_column_links(self, context):
 
         file_location = context.file_directory + os.sep + "ldm" + os.sep + "DM_Mappings.csv"
         header_skipped = False
 
-        with open(file_location,  encoding='utf-8') as csvfile:
-            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
+        with open(file_location, encoding="utf-8") as csvfile:
+            filereader = csv.reader(csvfile, delimiter=",", quotechar='"')
             for row in filereader:
                 if not header_skipped:
                     header_skipped = True
                 else:
                     logical_object_name = row[5]
                     relational_model_name = row[8]
                     relational_object_Name = row[11]
                     entity_name = row[12]
                     table_name = row[13]
 
-                    if (relational_model_name == context.input_layer_name) and (table_name is not None) and (entity_name is not None) and not (table_name.strip() == "") and not (entity_name.strip() == ""):
-
+                    if (
+                        (relational_model_name == context.input_layer_name)
+                        and (table_name is not None)
+                        and (entity_name is not None)
+                        and not (table_name.strip() == "")
+                        and not (entity_name.strip() == "")
+                    ):
 
                         # annotate entites
                         if logical_object_name == entity_name:
                             ldm_entity = InputLayerLinkEnricher.get_ldm_entity(
-                                self,
-                                context,
-                                Utils.make_valid_id(entity_name))
+                                self, context, Utils.make_valid_id(entity_name)
+                            )
 
                             the_entity_annotation = Utils.get_annotation_with_source(ldm_entity, "il_mapping")
 
                             if the_entity_annotation is None:
                                 the_entity_annotation = ELAnnotation()
-                                the_entity_annotation_directive = Utils.get_annotation_directive(ldm_entity.eContainer(), "il_mapping")
+                                the_entity_annotation_directive = Utils.get_annotation_directive(
+                                    ldm_entity.eContainer(), "il_mapping"
+                                )
                                 the_entity_annotation.source = the_entity_annotation_directive
                                 ldm_entity.eAnnotations.append(the_entity_annotation)
 
                             details = the_entity_annotation.details
 
@@ -80,11 +97,11 @@
                             for detail in details:
                                 if detail.key.startswith("il_table"):
                                     il_tables_count = il_tables_count + 1
 
                             detail1 = ELStringToStringMapEntry()
-                            if il_tables_count ==0:
+                            if il_tables_count == 0:
                                 detail1.key = "il_table"
                             else:
                                 detail1.key = "il_table" + str(il_tables_count)
 
                             detail1.value = table_name
@@ -94,77 +111,85 @@
                             # annotate attributes
                             ldm_attribute = InputLayerLinkEnricher.get_ldm_attribute(
                                 self,
                                 context,
                                 Utils.make_valid_id(entity_name),
-                                Utils.make_valid_id(logical_object_name))
+                                Utils.make_valid_id(logical_object_name),
+                            )
 
                             # logical_attribute_to_relational_name[ldm_attribute] =  table_name + "." + relational_object_Name
-                            if not(ldm_attribute is None):
-                                if isinstance(ldm_attribute,ELAttribute):
-                                    the_attribute_annotation = Utils.get_annotation_with_source(ldm_attribute, "il_mapping")
+                            if not (ldm_attribute is None):
+                                if isinstance(ldm_attribute, ELAttribute):
+                                    the_attribute_annotation = Utils.get_annotation_with_source(
+                                        ldm_attribute, "il_mapping"
+                                    )
                                     if the_attribute_annotation is None:
                                         the_attribute_annotation = ELAnnotation()
-                                        the_attribute_annotation_directive = Utils.get_annotation_directive(ldm_attribute.eContainer().eContainer(), "il_mapping")
+                                        the_attribute_annotation_directive = Utils.get_annotation_directive(
+                                            ldm_attribute.eContainer().eContainer(), "il_mapping"
+                                        )
                                         the_attribute_annotation.source = the_attribute_annotation_directive
                                         ldm_attribute.eAnnotations.append(the_attribute_annotation)
 
                                     details = the_attribute_annotation.details
                                     detail1 = ELStringToStringMapEntry()
                                     detail1.key = "il_column"
                                     detail1.value = table_name + "." + relational_object_Name
                                     details.append(detail1)
 
-                                if isinstance(ldm_attribute,ELReference):
-                                    the_reference_annotation = Utils.get_annotation_with_source(ldm_attribute, "il_mapping")
+                                if isinstance(ldm_attribute, ELReference):
+                                    the_reference_annotation = Utils.get_annotation_with_source(
+                                        ldm_attribute, "il_mapping"
+                                    )
                                     if the_reference_annotation is None:
                                         the_reference_annotation = ELAnnotation()
-                                        the_reference_annotation_directive = Utils.get_annotation_directive(ldm_attribute.eContainer().eContainer(), "il_mapping")
+                                        the_reference_annotation_directive = Utils.get_annotation_directive(
+                                            ldm_attribute.eContainer().eContainer(), "il_mapping"
+                                        )
                                         the_reference_annotation.source = the_reference_annotation_directive
                                         ldm_attribute.eAnnotations.append(the_reference_annotation)
 
                                     details = the_reference_annotation.details
 
                                     detail1 = ELStringToStringMapEntry()
                                     detail1.key = "il_column"
                                     detail1.value = relational_object_Name
                                     details.append(detail1)
 
-
-
-    def get_ldm_attribute(self, context,entity_name,attribute_name):
+    def get_ldm_attribute(self, context, entity_name, attribute_name):
         for eClassifier in context.ldm_entities_package.eClassifiers:
-            if isinstance(eClassifier,ELClass):
+            if isinstance(eClassifier, ELClass):
                 for feature in eClassifier.eStructuralFeatures:
 
-                    if isinstance(feature,ELAttribute) :
+                    if isinstance(feature, ELAttribute):
                         the_entity_annotation = Utils.get_annotation_with_source(eClassifier, "long_name")
                         if the_entity_annotation is not None:
                             if the_entity_annotation.details is not None:
                                 for detail in the_entity_annotation.details:
                                     if detail.key == "long_name":
                                         if detail.value == entity_name:
-                                            the_attribute_annotation = Utils.get_annotation_with_source(feature, "long_name")
+                                            the_attribute_annotation = Utils.get_annotation_with_source(
+                                                feature, "long_name"
+                                            )
                                             if the_attribute_annotation is not None:
                                                 if the_attribute_annotation.details is not None:
                                                     for detail in the_attribute_annotation.details:
                                                         if detail.key == "long_name":
                                                             if detail.value == attribute_name:
                                                                 return feature
-                    if isinstance(feature,ELReference):
+                    if isinstance(feature, ELReference):
                         the_reference_annotation = Utils.get_annotation_with_source(feature, "long_name")
                         if the_reference_annotation is not None:
                             if the_reference_annotation.details is not None:
                                 for detail in the_reference_annotation.details:
                                     if detail.key == "long_name":
                                         if detail.value == attribute_name:
                                             return feature
 
-
-    def get_ldm_entity(self, context,entity_name):
+    def get_ldm_entity(self, context, entity_name):
         for eClassifier in context.ldm_entities_package.eClassifiers:
-            if isinstance(eClassifier,ELClass):
+            if isinstance(eClassifier, ELClass):
                 the_entity_annotation = Utils.get_annotation_with_source(eClassifier, "long_name")
                 if the_entity_annotation is not None:
                     if the_entity_annotation.details is not None:
                         for detail in the_entity_annotation.details:
                             if detail.key == "long_name":
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/generate_test_data/enrich_ldm_with_il_links_from_fe.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/import_export_join_metadata/export_join_metadata.py	2025-08-02 18:37:08.452675+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/import_export_join_metadata/export_join_metadata.py	2025-09-21 17:07:36.454783+00:00
@@ -15,21 +15,22 @@
 import os
 from django.apps import apps
 from django.utils import timezone
 from pybirdai.models.bird_meta_data_model import CUBE_LINK, CUBE_STRUCTURE_ITEM_LINK
 
+
 class ExporterJoins:
 
     @staticmethod
-    def handle(output_path:str="resources/joins_export/export_file.csv"):
+    def handle(output_path: str = "resources/joins_export/export_file.csv"):
 
         # Ensure the directory exists
         output_dir = os.path.dirname(output_path)
         if output_dir and not os.path.exists(output_dir):
             os.makedirs(output_dir)
 
-        with open(output_path.replace(".csv","cube_link.csv"), 'w', newline='', encoding='utf-8') as csvfile:
+        with open(output_path.replace(".csv", "cube_link.csv"), "w", newline="", encoding="utf-8") as csvfile:
             writer = csv.writer(csvfile)
 
             # --- Export CUBE_LINK ---
             cube_link_headers = [
                 "MAINTENANCE_AGENCY_ID",
@@ -65,27 +66,37 @@
                     link.cube_link_type,
                     link.join_identifier,
                 ]
                 writer.writerow(row)
 
-        with open(output_path.replace(".csv","cube_structure_item_link.csv"), 'w', newline='', encoding='utf-8') as csvfile:
+        with open(
+            output_path.replace(".csv", "cube_structure_item_link.csv"), "w", newline="", encoding="utf-8"
+        ) as csvfile:
             writer = csv.writer(csvfile)
 
             # --- Export CUBE_STRUCTURE_ITEM_LINK ---
             # Assuming headers correspond to field names or related names
             cube_structure_item_link_headers = [
                 "CUBE_STRUCTURE_ITEM_LINK_ID",
-                "CUBE_LINK_ID", # FK to CUBE_LINK
-                "FOREIGN_CUBE_VARIABLE_CODE", # FK to CUBE_STRUCTURE_ITEM
-                "PRIMARY_CUBE_VARIABLE_CODE", # FK to CUBE_STRUCTURE_ITEM
+                "CUBE_LINK_ID",  # FK to CUBE_LINK
+                "FOREIGN_CUBE_VARIABLE_CODE",  # FK to CUBE_STRUCTURE_ITEM
+                "PRIMARY_CUBE_VARIABLE_CODE",  # FK to CUBE_STRUCTURE_ITEM
             ]
             writer.writerow(cube_structure_item_link_headers)
 
             cube_structure_item_links = CUBE_STRUCTURE_ITEM_LINK.objects.all()
             for item_link in cube_structure_item_links:
                 row = [
                     item_link.cube_structure_item_link_id,
                     item_link.cube_link_id.pk if item_link.cube_link_id else None,
-                    item_link.foreign_cube_variable_code.cube_variable_code if item_link.foreign_cube_variable_code else None,
-                    item_link.primary_cube_variable_code.cube_variable_code if item_link.primary_cube_variable_code else None,
+                    (
+                        item_link.foreign_cube_variable_code.cube_variable_code
+                        if item_link.foreign_cube_variable_code
+                        else None
+                    ),
+                    (
+                        item_link.primary_cube_variable_code.cube_variable_code
+                        if item_link.primary_cube_variable_code
+                        else None
+                    ),
                 ]
                 writer.writerow(row)
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/import_export_join_metadata/export_join_metadata.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/filter_code/automatic_tracking_wrapper.py	2025-09-18 11:58:06.344861+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/filter_code/automatic_tracking_wrapper.py	2025-09-21 17:07:36.480346+00:00
@@ -17,322 +17,341 @@
 """
 
 import inspect
 from pybirdai.annotations.decorators import _lineage_context
 
+
 def wrap_cell_with_tracking(cell_instance):
     """
     Wrap an existing cell instance with automatic tracking.
     This adds tracking without requiring code modifications.
     """
-    
+
     # Get the calculation name from the class
     calculation_name = cell_instance.__class__.__name__
-    
+
     # Get orchestration from context
-    orchestration = _lineage_context.get('orchestration')
-    if not orchestration or not hasattr(orchestration, 'track_calculation_used_row'):
-        return cell_instance
-    
+    orchestration = _lineage_context.get("orchestration")
+    if not orchestration or not hasattr(orchestration, "track_calculation_used_row"):
+        return cell_instance
+
     # Set the calculation context
     orchestration.current_calculation = calculation_name
-    
+
     # Store original methods
-    if hasattr(cell_instance, 'calc_referenced_items'):
+    if hasattr(cell_instance, "calc_referenced_items"):
         original_calc = cell_instance.calc_referenced_items
     else:
         return cell_instance
-    
+
     # Find the filtered items attribute (usually ends with 's')
     filtered_items_attr = None
     for attr_name in dir(cell_instance):
-        if not attr_name.startswith('_') and not callable(getattr(cell_instance, attr_name)):
+        if not attr_name.startswith("_") and not callable(getattr(cell_instance, attr_name)):
             attr_value = getattr(cell_instance, attr_name)
-            if isinstance(attr_value, list) and attr_name.endswith('s'):
+            if isinstance(attr_value, list) and attr_name.endswith("s"):
                 filtered_items_attr = attr_name
                 break
-    
+
     if not filtered_items_attr:
         return cell_instance
-    
+
     # Create wrapped calc_referenced_items method
     def wrapped_calc_referenced_items():
         # Set calculation context
         orchestration.current_calculation = calculation_name
-        
+
         # Clear the filtered items list
         setattr(cell_instance, filtered_items_attr, [])
-        
+
         # Call original method - this will populate the filtered items
         original_calc()
-        
+
         # Track all items that were added (these passed the filters)
         filtered_items = getattr(cell_instance, filtered_items_attr)
         for item in filtered_items:
             orchestration.track_calculation_used_row(calculation_name, item)
-            
+
             # Track only fields that are actually used by analyzing the cell's source code
             # No longer using hardcoded common fields that cause unnecessary evaluations
-    
+
     # Replace the method
     cell_instance.calc_referenced_items = wrapped_calc_referenced_items
-    
+
     return cell_instance
 
 
 def auto_wrap_cell_execution(cell_class_name, data_point_id):
     """
     Automatically wrap a cell during execution.
     This can be called from execute_datapoint.py
     """
-    
+
     # Import the report_cells module
     import pybirdai.process_steps.filter_code.report_cells as report_cells
-    
+
     # Get the cell class
-    klass = getattr(report_cells, 'Cell_' + str(data_point_id), None)
+    klass = getattr(report_cells, "Cell_" + str(data_point_id), None)
     if not klass:
         print(f"Could not find cell class for data point {data_point_id}")
         return None
-    
+
     # Create the cell instance
     cell_instance = klass()
-    
+
     # Wrap it with tracking
     wrapped_cell = wrap_cell_with_tracking(cell_instance)
-    
+
     return wrapped_cell
 
 
 def inspect_cell_for_field_usage(cell_instance, source_code=None):
     """
     Inspect a cell's source code to determine which fields it uses.
     This provides a more accurate tracking than the heuristic approach.
     """
-    
+
     if not source_code:
         try:
             source_code = inspect.getsource(cell_instance.calc_referenced_items)
         except:
             return []
-    
+
     # Extract field names from the source code
     import re
-    
+
     # Pattern to find item.FIELD_NAME() calls
-    field_pattern = r'item\.(\w+)\(\)'
-    
+    field_pattern = r"item\.(\w+)\(\)"
+
     field_names = []
     for match in re.finditer(field_pattern, source_code):
         field_name = match.group(1)
         if field_name not in field_names:
             field_names.append(field_name)
-    
+
     return field_names
 
 
 def create_smart_tracking_wrapper(cell_instance, orchestration=None):
     """
     Create a smarter tracking wrapper that analyzes the cell's code
     to determine exactly which fields are used.
     """
-    
+
     calculation_name = cell_instance.__class__.__name__
     print(f"🛠️ Creating smart tracking wrapper for: {calculation_name}")
-    
+
     # Get orchestration from parameter or context
     if not orchestration:
-        orchestration = _lineage_context.get('orchestration')
+        orchestration = _lineage_context.get("orchestration")
         print(f"Got orchestration from context: {orchestration}")
-    
-    if not orchestration or not hasattr(orchestration, 'track_calculation_used_row'):
+
+    if not orchestration or not hasattr(orchestration, "track_calculation_used_row"):
         print(f"❌ No orchestration available for tracking in {calculation_name}")
         return cell_instance
-    
+
     # Set the calculation context
     orchestration.current_calculation = calculation_name
     print(f"✅ Orchestration ready for {calculation_name}")
-    
+
     # Get the fields used by this cell by analyzing source code
     try:
         used_fields = inspect_cell_for_field_usage(cell_instance)
     except Exception as e:
         print(f"⚠️ Failed to analyze cell source code: {e}")
         used_fields = []
     print(f"Fields to track: {used_fields}")
-    
+
     # Store original methods
-    if hasattr(cell_instance, 'calc_referenced_items'):
+    if hasattr(cell_instance, "calc_referenced_items"):
         original_calc = cell_instance.calc_referenced_items
         print(f"✅ Found calc_referenced_items method")
     else:
         print(f"❌ No calc_referenced_items method found")
         return cell_instance
-    
+
     # Find the filtered items attribute
     filtered_items_attr = None
     for attr_name in dir(cell_instance):
-        if not attr_name.startswith('_') and not callable(getattr(cell_instance, attr_name)):
+        if not attr_name.startswith("_") and not callable(getattr(cell_instance, attr_name)):
             attr_value = getattr(cell_instance, attr_name)
-            if isinstance(attr_value, list) and attr_name.endswith('s'):
+            if isinstance(attr_value, list) and attr_name.endswith("s"):
                 filtered_items_attr = attr_name
                 print(f"Found filtered items attribute: {attr_name}")
                 break
-    
+
     if not filtered_items_attr:
         print(f"❌ No filtered items attribute found")
         return cell_instance
-    
+
     # Create wrapped calc_referenced_items method
     def smart_wrapped_calc():
         # Set calculation context
         orchestration.current_calculation = calculation_name
         print(f"🚀 SMART WRAPPED CALC CALLED: {calculation_name}")
         print(f"Setting calculation context: {calculation_name}")
-        
+
         # Clear the filtered items list
         setattr(cell_instance, filtered_items_attr, [])
         print(f"Cleared {filtered_items_attr}")
-        
+
         # Call original method
         print(f"Calling original calc_referenced_items...")
         original_calc()
         print(f"Original calc_referenced_items completed")
-        
+
         # Track all items that were added and their fields
         filtered_items = getattr(cell_instance, filtered_items_attr)
         print(f"Found {len(filtered_items)} filtered items in {calculation_name}")
-        
+
         for item in filtered_items:
             try:
                 # Track the row
                 print(f"ATTEMPTING TO TRACK ROW: {type(item).__name__} for {calculation_name}")
                 orchestration.track_calculation_used_row(calculation_name, item)
                 print(f"✓ Successfully tracked row for {calculation_name}: {type(item).__name__}")
-                
+
                 # If this is a business object, also try to track any underlying data sources
-                if hasattr(item, 'base') and item.base is not None:
+                if hasattr(item, "base") and item.base is not None:
                     try:
                         print(f"ATTEMPTING TO TRACK BASE: {type(item.base).__name__}")
                         orchestration.track_calculation_used_row(calculation_name, item.base)
                         print(f"✓ Successfully tracked base object for {calculation_name}: {type(item.base).__name__}")
                     except Exception as e:
                         print(f"✗ Failed to track base object: {e}")
-                
+
                 # Track each field that this cell uses with PRECISE database field tracking
                 for field_name in used_fields:
                     if hasattr(item, field_name):
                         try:
                             # Get the method
                             method = getattr(item, field_name)
                             if callable(method):
                                 print(f"🎯 Tracking precise field access: {field_name} on {type(item).__name__}")
-                                
+
                                 # Set up database access monitoring
                                 accessed_db_fields = []
                                 original_getattr = None
-                                
+
                                 # Create a monitoring wrapper for database field access
                                 def track_db_field_access(obj, attr_name):
                                     """Monitor when Django model fields are accessed"""
-                                    if hasattr(obj, '_meta') and hasattr(obj._meta, 'model'):
+                                    if hasattr(obj, "_meta") and hasattr(obj._meta, "model"):
                                         # This is a Django model - track field access
                                         if hasattr(obj._meta.model, attr_name):
                                             model_name = type(obj).__name__
                                             accessed_db_fields.append(f"{model_name}.{attr_name}")
                                             print(f"  📊 DB field accessed: {model_name}.{attr_name}")
-                                    
+
                                     # Call the original getattr
                                     return object.__getattribute__(obj, attr_name)
-                                
+
                                 # Temporarily replace __getattribute__ on Django models if the item has access to them
                                 # This is complex, so let's use a simpler approach first
-                                
+
                                 # Execute the method and capture the value
                                 field_value = method()
-                                
+
                                 # Track the business logic field access
                                 orchestration.track_calculation_used_field(calculation_name, field_name, item)
                                 print(f"Tracked field {field_name} for {calculation_name} (value: {field_value})")
-                                
+
                                 # Try to trace what database sources this business method used
                                 # Look for database model attributes in the business object
                                 db_sources_found = []
                                 for attr_name in dir(item):
-                                    if not attr_name.startswith('_'):
+                                    if not attr_name.startswith("_"):
                                         try:
                                             attr_value = getattr(item, attr_name)
-                                            if attr_value and hasattr(attr_value, '_meta') and hasattr(attr_value._meta, 'model'):
+                                            if (
+                                                attr_value
+                                                and hasattr(attr_value, "_meta")
+                                                and hasattr(attr_value._meta, "model")
+                                            ):
                                                 # This is a Django model instance
                                                 model_name = type(attr_value).__name__
                                                 db_sources_found.append((model_name, attr_value))
                                         except:
                                             pass
-                                
+
                                 # For each Django model source, track it as a used row and try to identify relevant fields
                                 for model_name, model_instance in db_sources_found:
                                     try:
                                         # Track the model instance as a used row
                                         orchestration.track_calculation_used_row(calculation_name, model_instance)
                                         print(f"  📋 Tracked source model: {model_name}")
-                                        
+
                                         # Try to identify which specific fields of this model might contain the computed value
                                         # Look for fields with similar names or that might contribute to the calculation
-                                        if hasattr(model_instance, '_meta'):
+                                        if hasattr(model_instance, "_meta"):
                                             for model_field in model_instance._meta.fields:
                                                 field_name_lower = field_name.lower()
                                                 model_field_lower = model_field.name.lower()
-                                                
+
                                                 # Check if the model field might be related to the business field
-                                                if (field_name_lower in model_field_lower or 
-                                                    model_field_lower in field_name_lower or
-                                                    'amnt' in model_field_lower):  # Amount fields are often key
-                                                    
+                                                if (
+                                                    field_name_lower in model_field_lower
+                                                    or model_field_lower in field_name_lower
+                                                    or "amnt" in model_field_lower
+                                                ):  # Amount fields are often key
+
                                                     try:
                                                         # Get the actual database field object for tracking
-                                                        db_table = orchestration.current_populated_tables.get(model_name)
-                                                        if db_table and hasattr(db_table, 'table'):
-                                                            db_fields = db_table.table.database_fields.filter(name=model_field.name)
+                                                        db_table = orchestration.current_populated_tables.get(
+                                                            model_name
+                                                        )
+                                                        if db_table and hasattr(db_table, "table"):
+                                                            db_fields = db_table.table.database_fields.filter(
+                                                                name=model_field.name
+                                                            )
                                                             if db_fields.exists():
                                                                 db_field = db_fields.first()
-                                                                orchestration.track_calculation_used_field(calculation_name, model_field.name, model_instance)
-                                                                print(f"    🎯 Precise field: {model_name}.{model_field.name}")
+                                                                orchestration.track_calculation_used_field(
+                                                                    calculation_name, model_field.name, model_instance
+                                                                )
+                                                                print(
+                                                                    f"    🎯 Precise field: {model_name}.{model_field.name}"
+                                                                )
                                                     except Exception as e:
-                                                        print(f"    ⚠️ Could not track precise field {model_field.name}: {e}")
-                                    
+                                                        print(
+                                                            f"    ⚠️ Could not track precise field {model_field.name}: {e}"
+                                                        )
+
                                     except Exception as e:
                                         print(f"  ❌ Failed to track source model {model_name}: {e}")
-                                        
+
                         except Exception as e:
                             print(f"Could not track field {field_name}: {e}")
-                
+
                 # Also track any Django ORM objects that this business object references
                 # These are typically the underlying database records
                 for attr_name in dir(item):
-                    if not attr_name.startswith('_') and not callable(getattr(item, attr_name)):
+                    if not attr_name.startswith("_") and not callable(getattr(item, attr_name)):
                         attr_value = getattr(item, attr_name)
-                        
+
                         # Check if this looks like a Django model instance
-                        if attr_value and hasattr(attr_value, '_meta') and hasattr(attr_value._meta, 'model'):
+                        if attr_value and hasattr(attr_value, "_meta") and hasattr(attr_value._meta, "model"):
                             try:
                                 # This is a Django model instance - track it as a database row
                                 orchestration.track_calculation_used_row(calculation_name, attr_value)
                                 print(f"Tracked Django model {type(attr_value).__name__} for {calculation_name}")
-                                
+
                                 # Also track the fields of this Django object that are accessed
                                 model_class_name = type(attr_value).__name__
                                 for used_field in used_fields:
                                     # Check if this field might belong to this model
                                     if hasattr(attr_value, used_field):
-                                        orchestration.track_calculation_used_field(calculation_name, used_field, attr_value)
+                                        orchestration.track_calculation_used_field(
+                                            calculation_name, used_field, attr_value
+                                        )
                                         print(f"Tracked model field {used_field} on {model_class_name}")
                             except Exception as e:
                                 print(f"Could not track Django model {type(attr_value).__name__}: {e}")
             except Exception as e:
                 print(f"Error tracking item in {calculation_name}: {e}")
-    
+
     # Replace the method
     print(f"🔄 Replacing calc_referenced_items method on {calculation_name}")
     cell_instance.calc_referenced_items = smart_wrapped_calc
     print(f"✅ Method replacement complete. New method: {cell_instance.calc_referenced_items}")
-    
-    return cell_instance
\ No newline at end of file
+
+    return cell_instance
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/filter_code/automatic_tracking_wrapper.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/hierarchy_conversion/convert_ldm_to_sdd_hierarchies.py	2025-08-02 18:37:08.452486+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/hierarchy_conversion/convert_ldm_to_sdd_hierarchies.py	2025-09-21 17:07:36.490430+00:00
@@ -16,101 +16,102 @@
 from datetime import datetime
 from django.apps import apps
 from django.db import models
 from difflib import get_close_matches
 
+
 class ConvertLDMToSDDHierarchies:
     """Class for converting LDM hierarchies to SDD hierarchies."""
 
     def find_closest_member(self, member_id):
         """
         Find the closest matching existing member name.
-        
+
         Args:
             member_id: The member ID to find matches for
-            
+
         Returns:
             tuple: (name, member_id) of closest match, or (None, None) if no matches
         """
-        MEMBER = apps.get_model('pybirdai', 'MEMBER')
-        
+        MEMBER = apps.get_model("pybirdai", "MEMBER")
+
         # Check for exact match with underscores
-        member_with_underscores = member_id.replace(' ', '_')
+        member_with_underscores = member_id.replace(" ", "_")
         member = MEMBER.objects.filter(name=member_with_underscores).first()
         if member:
             return (member.name, member.member_id)
-            
+
         # If no underscore match, find closest match
-        existing_members = list(MEMBER.objects.values_list('name', flat=True))
+        existing_members = list(MEMBER.objects.values_list("name", flat=True))
         matches = get_close_matches(member_id, existing_members, n=1, cutoff=0.6)
         if matches:
             member = MEMBER.objects.filter(name=matches[0]).first()
             return (member.name, member.member_id)
         return (None, None)
 
     def check_member_exists(self, member_id):
         """
         Check if a member exists in the MEMBER table.
-        
+
         Args:
             member_id: The member ID to check (with spaces)
-            
+
         Returns:
             tuple: (exists, member_id) where exists is bool and member_id is the matched ID or None
         """
-        MEMBER = apps.get_model('pybirdai', 'MEMBER')
+        MEMBER = apps.get_model("pybirdai", "MEMBER")
         # Check with spaces
         member = MEMBER.objects.filter(name=member_id).first()
         if member:
             return (True, member.member_id)
         # Check with underscores
-        member = MEMBER.objects.filter(name=member_id.replace(' ', '_')).first()
+        member = MEMBER.objects.filter(name=member_id.replace(" ", "_")).first()
         if member:
             return (True, member.member_id)
         return (False, None)
 
     def get_all_subclasses_and_delegates(self, cls, processed=None, parent=None, level=1):
         """
         Recursively get all subclasses and delegate relationships of a class.
-        
+
         Args:
             cls: The class to get subclasses and delegates for
             processed: Set of already processed classes to avoid cycles
             parent: Parent class for tracking hierarchy
             level: Current level in hierarchy
-            
+
         Returns:
             list: List of tuples (class, parent_class, is_delegate, level)
         """
         if processed is None:
             processed = set()
-            
+
         if cls in processed:
             return []
-            
+
         processed.add(cls)
         result = []
-        
+
         # Get direct subclasses
         for subclass in cls.__subclasses__():
             result.append((subclass, cls, False, level))
             result.extend(self.get_all_subclasses_and_delegates(subclass, processed, cls, level + 1))
-            
+
         # Get delegate relationships
         for field in cls._meta.get_fields():
-            if isinstance(field, models.ForeignKey) and field.name.endswith('_delegate'):
+            if isinstance(field, models.ForeignKey) and field.name.endswith("_delegate"):
                 delegate_class = field.related_model
                 if delegate_class not in processed:
                     result.append((delegate_class, cls, True, level))
                     result.extend(self.get_all_subclasses_and_delegates(delegate_class, processed, cls, level + 1))
-                    
+
         return result
 
     def convert_hierarchies(self, context, sdd_context):
         """
         Convert LDM hierarchies to SDD hierarchies.
-        
+
         Args:
             context: The general context containing file paths and settings
             sdd_context: The SDD-specific context containing SDD-related settings
         """
         # Constants for the hierarchy
@@ -119,119 +120,120 @@
         DOMAIN_ID = "INSTRMNT_DOMAIN"
         VALID_FROM = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
         VALID_TO = "9999-12-31"
 
         # Get the INSTRMNT class
-        INSTRMNT = apps.get_model('pybirdai', 'INSTRMNT')
-        
+        INSTRMNT = apps.get_model("pybirdai", "INSTRMNT")
+
         # Get all subclasses and delegates recursively
         class_relationships = self.get_all_subclasses_and_delegates(INSTRMNT)
-        
+
         # Track missing members and their suggestions
         missing_members = {}
-        
+
         # Create output directory if it doesn't exist
-        output_dir = os.path.join(context.output_directory, 'ldm_to_sdd_hierarchies')
+        output_dir = os.path.join(context.output_directory, "ldm_to_sdd_hierarchies")
         os.makedirs(output_dir, exist_ok=True)
-        
+
         # Create member_hierarchy.csv
-        hierarchy_file = os.path.join(output_dir, 'member_hierarchy.csv')
-        with open(hierarchy_file, 'w', newline='') as f:
+        hierarchy_file = os.path.join(output_dir, "member_hierarchy.csv")
+        with open(hierarchy_file, "w", newline="") as f:
             writer = csv.writer(f)
             # Write header
-            writer.writerow([
-                'MAINTENANCE_AGENCY_ID', 
-                'MEMBER_HIERARCHY_ID', 
-                'CODE', 
-                'DOMAIN_ID', 
-                'NAME', 
-                'DESCRIPTION', 
-                'IS_MAIN_HIERARCHY'
-            ])
+            writer.writerow(
+                [
+                    "MAINTENANCE_AGENCY_ID",
+                    "MEMBER_HIERARCHY_ID",
+                    "CODE",
+                    "DOMAIN_ID",
+                    "NAME",
+                    "DESCRIPTION",
+                    "IS_MAIN_HIERARCHY",
+                ]
+            )
             # Write data
-            writer.writerow([
-                MAINTENANCE_AGENCY_ID,
-                HIERARCHY_ID,
-                '1',
-                DOMAIN_ID,
-                'Instrument type hierarchy',
-                'Hierarchical structure of instrument types and delegates',
-                'true'
-            ])
+            writer.writerow(
+                [
+                    MAINTENANCE_AGENCY_ID,
+                    HIERARCHY_ID,
+                    "1",
+                    DOMAIN_ID,
+                    "Instrument type hierarchy",
+                    "Hierarchical structure of instrument types and delegates",
+                    "true",
+                ]
+            )
 
         # Create member_hierarchy_node.csv
-        nodes_file = os.path.join(output_dir, 'member_hierarchy_node.csv')
-        with open(nodes_file, 'w', newline='') as f:
+        nodes_file = os.path.join(output_dir, "member_hierarchy_node.csv")
+        with open(nodes_file, "w", newline="") as f:
             writer = csv.writer(f)
             # Write header
-            writer.writerow([
-                'MEMBER_HIERARCHY_ID',
-                'MEMBER_ID',
-                'LEVEL',
-                'PARENT_MEMBER_ID',
-                'COMPARATOR',
-                'OPERATOR',
-                'VALID_FROM',
-                'VALID_TO'
-            ])
-            
+            writer.writerow(
+                [
+                    "MEMBER_HIERARCHY_ID",
+                    "MEMBER_ID",
+                    "LEVEL",
+                    "PARENT_MEMBER_ID",
+                    "COMPARATOR",
+                    "OPERATOR",
+                    "VALID_FROM",
+                    "VALID_TO",
+                ]
+            )
+
             # Check root node
-            root_member_id = INSTRMNT._meta.verbose_name.replace('_', ' ')
+            root_member_id = INSTRMNT._meta.verbose_name.replace("_", " ")
             exists, matched_id = self.check_member_exists(root_member_id)
             if not exists:
                 suggestion = self.find_closest_member(root_member_id)
                 missing_members[root_member_id] = suggestion
-            
+
             # Write root node
-            writer.writerow([
-                HIERARCHY_ID,
-                matched_id if matched_id else root_member_id,
-                1,
-                '',
-                '=',
-                '',
-                VALID_FROM,
-                VALID_TO
-            ])
-            
+            writer.writerow(
+                [HIERARCHY_ID, matched_id if matched_id else root_member_id, 1, "", "=", "", VALID_FROM, VALID_TO]
+            )
+
             # Write all nodes (both inheritance and delegate relationships)
             for cls, parent_cls, is_delegate, level in class_relationships:
-                member_name = cls._meta.verbose_name.replace('_', ' ')
-                parent_member_name = parent_cls._meta.verbose_name.replace('_', ' ')
-                
+                member_name = cls._meta.verbose_name.replace("_", " ")
+                parent_member_name = parent_cls._meta.verbose_name.replace("_", " ")
+
                 # Check if members exist and get their IDs
                 exists, member_matched_id = self.check_member_exists(member_name)
                 if not exists:
                     suggestion = self.find_closest_member(member_name)
                     missing_members[member_name] = suggestion
-                
+
                 exists, parent_matched_id = self.check_member_exists(parent_member_name)
                 if not exists:
                     suggestion = self.find_closest_member(parent_member_name)
                     missing_members[parent_member_name] = suggestion
-                
-                writer.writerow([
-                    HIERARCHY_ID,
-                    member_matched_id if member_matched_id else member_name,
-                    level + 1,  # Add 1 since root is level 1
-                    parent_matched_id if parent_matched_id else parent_member_name,
-                    '=' if not is_delegate else 'D',  # Use 'D' comparator for delegate relationships
-                    '',
-                    VALID_FROM,
-                    VALID_TO
-                ])
+
+                writer.writerow(
+                    [
+                        HIERARCHY_ID,
+                        member_matched_id if member_matched_id else member_name,
+                        level + 1,  # Add 1 since root is level 1
+                        parent_matched_id if parent_matched_id else parent_member_name,
+                        "=" if not is_delegate else "D",  # Use 'D' comparator for delegate relationships
+                        "",
+                        VALID_FROM,
+                        VALID_TO,
+                    ]
+                )
 
         # Save missing members information to CSV
         if missing_members:
-            missing_members_file = os.path.join(output_dir, 'missing_members.csv')
-            with open(missing_members_file, 'w', newline='') as f:
+            missing_members_file = os.path.join(output_dir, "missing_members.csv")
+            with open(missing_members_file, "w", newline="") as f:
                 writer = csv.writer(f)
-                writer.writerow(['Missing Member', 'Match Type', 'Matched Name', 'Matched Member ID'])
+                writer.writerow(["Missing Member", "Match Type", "Matched Name", "Matched Member ID"])
                 for member, suggestion in sorted(missing_members.items()):
                     if suggestion:
                         name, member_id = suggestion
-                        match_type = 'Underscore Match' if name == member.replace(' ', '_') else 'Closest Match'
+                        match_type = "Underscore Match" if name == member.replace(" ", "_") else "Closest Match"
                         writer.writerow([member, match_type, name, member_id])
                     else:
-                        writer.writerow([member, 'No Match', '', ''])
-
-        return f"Created hierarchy files in {output_dir} including both inheritance and delegate relationships" 
\ No newline at end of file
+                        writer.writerow([member, "No Match", "", ""])
+
+        return f"Created hierarchy files in {output_dir} including both inheritance and delegate relationships"
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/hierarchy_conversion/convert_ldm_to_sdd_hierarchies.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/import_export_join_metadata/import_join_metadata.py	2025-08-02 18:37:08.452852+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/import_export_join_metadata/import_join_metadata.py	2025-09-21 17:07:36.504073+00:00
@@ -18,40 +18,47 @@
 from pybirdai.models.bird_meta_data_model import (
     CUBE_LINK,
     CUBE_STRUCTURE_ITEM_LINK,
     MAINTENANCE_AGENCY,
     CUBE,
-    CUBE_STRUCTURE_ITEM
+    CUBE_STRUCTURE_ITEM,
 )
+
 
 class ImporterJoins:
 
     @staticmethod
     def _get_instance_or_skip(model, pk_value, row_id, field_name):
         """Looks up FK instance, returns instance or None if pk_value is empty or object not found.
-           Prints error message for critical missing items and signals caller to skip."""
+        Prints error message for critical missing items and signals caller to skip."""
         if not pk_value:
-            print(f"Error: Missing required {model.__name__} pk for {row_id} related to field '{field_name}'. Skipping row.")
+            print(
+                f"Error: Missing required {model.__name__} pk for {row_id} related to field '{field_name}'. Skipping row."
+            )
             return None
         try:
             if model == CUBE_STRUCTURE_ITEM:
                 return model.objects.get(cube_variable_code=pk_value)
             return model.objects.get(pk=pk_value)
         except ObjectDoesNotExist:
-            print(f"Error: {model.__name__} with pk '{pk_value}' not found for {row_id} related to field '{field_name}'. Skipping row.")
+            print(
+                f"Error: {model.__name__} with pk '{pk_value}' not found for {row_id} related to field '{field_name}'. Skipping row."
+            )
             return None
 
     @staticmethod
     def _get_instance_or_none(model, pk_value, row_id, field_name):
         """Looks up FK instance, returns instance or None if pk_value is empty or object not found.
-           Prints warning for non-critical missing items."""
+        Prints warning for non-critical missing items."""
         if not pk_value:
-            return None # Field is optional, missing PK is fine
+            return None  # Field is optional, missing PK is fine
         try:
             return model.objects.get(pk=pk_value)
         except ObjectDoesNotExist:
-            print(f"Warning: {model.__name__} with pk '{pk_value}' not found for {row_id} related to field '{field_name}'. Setting FK to None.")
+            print(
+                f"Warning: {model.__name__} with pk '{pk_value}' not found for {row_id} related to field '{field_name}'. Setting FK to None."
+            )
             return None
 
     @staticmethod
     def _parse_date_or_none(date_str, row_id, field_name):
         """Parses ISO date string, returns datetime object or None on error or empty string."""
@@ -63,11 +70,11 @@
         except ValueError:
             print(f"Warning: Invalid date format for {field_name} '{date_str}' for {row_id}. Setting field to None.")
             return None
 
     @staticmethod
-    def handle(input_path:str="resources/joins_export/export_file.csv"):
+    def handle(input_path: str = "resources/joins_export/export_file.csv"):
         """
         Imports CUBE_LINK and CUBE_STRUCTURE_ITEM_LINK data from CSV files
         generated by the export process.
 
         Args:
@@ -85,50 +92,55 @@
         # --- Import CUBE_LINK ---
         print(f"\nImporting CUBE_LINK from {cube_link_path}")
         imported_cube_links_count = 0
 
         try:
-            with open(cube_link_path, 'r', newline='', encoding='utf-8') as csvfile:
+            with open(cube_link_path, "r", newline="", encoding="utf-8") as csvfile:
                 reader = csv.DictReader(csvfile)
                 if not reader.fieldnames:
                     print("Warning: CUBE_LINK CSV file is empty or has no headers.")
                     # Continue to try the second file
-                else: # Only process if there are fieldnames
+                else:  # Only process if there are fieldnames
                     for row in reader:
                         link_id = row.get("CUBE_LINK_ID")
                         if not link_id:
                             print(f"Warning: Skipping CUBE_LINK row with no CUBE_LINK_ID: {row}")
                             continue
 
-                        try: # Keep a general try/except for unexpected row processing errors
+                        try:  # Keep a general try/except for unexpected row processing errors
                             # Prepare data dictionary for update_or_create defaults
                             link_defaults = {
                                 "code": row.get("CODE"),
                                 "name": row.get("NAME"),
                                 "description": row.get("DESCRIPTION"),
                                 "version": row.get("VERSION"),
-                                "order_relevance": row.get("ORDER_RELEVANCE") if isinstance(row.get("ORDER_RELEVANCE"),int) else 0, # Assuming model field handles type conversion
+                                "order_relevance": (
+                                    row.get("ORDER_RELEVANCE") if isinstance(row.get("ORDER_RELEVANCE"), int) else 0
+                                ),  # Assuming model field handles type conversion
                                 "cube_link_type": row.get("CUBE_LINK_TYPE"),
                                 "join_identifier": row.get("JOIN_IDENTIFIER"),
                             }
 
                             # Handle FK lookups using helpers
                             link_defaults["maintenance_agency_id"] = ImporterJoins._get_instance_or_none(
-                                MAINTENANCE_AGENCY, row.get("MAINTENANCE_AGENCY_ID"), f"CUBE_LINK '{link_id}'", "MAINTENANCE_AGENCY_ID"
+                                MAINTENANCE_AGENCY,
+                                row.get("MAINTENANCE_AGENCY_ID"),
+                                f"CUBE_LINK '{link_id}'",
+                                "MAINTENANCE_AGENCY_ID",
                             )
 
                             primary_cube = ImporterJoins._get_instance_or_skip(
                                 CUBE, row.get("PRIMARY_CUBE_ID"), f"CUBE_LINK '{link_id}'", "PRIMARY_CUBE_ID"
                             )
                             if primary_cube is None:
-                                continue # Skip row if mandatory FK not found/missing
+                                continue  # Skip row if mandatory FK not found/missing
 
                             foreign_cube = ImporterJoins._get_instance_or_skip(
                                 CUBE, row.get("FOREIGN_CUBE_ID"), f"CUBE_LINK '{link_id}'", "FOREIGN_CUBE_ID"
                             )
                             if foreign_cube is None:
-                                continue # Skip row if mandatory FK not found/missing
+                                continue  # Skip row if mandatory FK not found/missing
 
                             link_defaults["primary_cube_id"] = primary_cube
                             link_defaults["foreign_cube_id"] = foreign_cube
 
                             # Handle dates using helper
@@ -139,12 +151,11 @@
                                 row.get("VALID_TO"), f"CUBE_LINK '{link_id}'", "VALID_TO"
                             )
 
                             # Use update_or_create based on CUBE_LINK_ID
                             obj, created = CUBE_LINK.objects.update_or_create(
-                                cube_link_id=link_id,
-                                defaults=link_defaults
+                                cube_link_id=link_id, defaults=link_defaults
                             )
                             imported_cube_links_count += 1
                             # print(f"{'Created' if created else 'Updated'} CUBE_LINK: {obj.cube_link_id}") # Uncomment for verbose output
 
                         except Exception as e:
@@ -154,73 +165,82 @@
         except FileNotFoundError:
             print(f"Error: CUBE_LINK file not found at {cube_link_path}. Skipping CUBE_LINK import.")
         except Exception as e:
             print(f"Error reading CUBE_LINK file {cube_link_path}: {e}")
 
-
         print(f"Finished importing {imported_cube_links_count} CUBE_LINK records.")
 
         # --- Import CUBE_STRUCTURE_ITEM_LINK ---
         print(f"\nImporting CUBE_STRUCTURE_ITEM_LINK from {cube_structure_item_link_path}")
         imported_item_links_count = 0
 
         try:
-            with open(cube_structure_item_link_path, 'r', newline='', encoding='utf-8') as csvfile:
+            with open(cube_structure_item_link_path, "r", newline="", encoding="utf-8") as csvfile:
                 reader = csv.DictReader(csvfile)
                 if not reader.fieldnames:
                     print("Warning: CUBE_STRUCTURE_ITEM_LINK CSV file is empty or has no headers.")
                     # Don't return here
-                else: # Only process if there are fieldnames
+                else:  # Only process if there are fieldnames
                     for row in reader:
                         item_link_id = row.get("CUBE_STRUCTURE_ITEM_LINK_ID")
                         if not item_link_id:
-                            print(f"Warning: Skipping CUBE_STRUCTURE_ITEM_LINK row with no CUBE_STRUCTURE_ITEM_LINK_ID: {row}")
+                            print(
+                                f"Warning: Skipping CUBE_STRUCTURE_ITEM_LINK row with no CUBE_STRUCTURE_ITEM_LINK_ID: {row}"
+                            )
                             continue
 
-                        try: # Keep a general try/except for unexpected row processing errors
+                        try:  # Keep a general try/except for unexpected row processing errors
                             # Prepare data dictionary for update_or_create defaults
                             item_link_defaults = {}
 
                             # Handle FK lookups using helper
                             cube_link = ImporterJoins._get_instance_or_skip(
-                                CUBE_LINK, row.get("CUBE_LINK_ID"), f"CUBE_STRUCTURE_ITEM_LINK '{item_link_id}'", "CUBE_LINK_ID"
+                                CUBE_LINK,
+                                row.get("CUBE_LINK_ID"),
+                                f"CUBE_STRUCTURE_ITEM_LINK '{item_link_id}'",
+                                "CUBE_LINK_ID",
                             )
                             if cube_link is None:
-                                continue # Skip row if mandatory FK not found/missing
+                                continue  # Skip row if mandatory FK not found/missing
 
                             foreign_variable = ImporterJoins._get_instance_or_skip(
-                                CUBE_STRUCTURE_ITEM, row.get("FOREIGN_CUBE_VARIABLE_CODE"), f"CUBE_STRUCTURE_ITEM_LINK '{item_link_id}'", "FOREIGN_CUBE_VARIABLE_CODE"
+                                CUBE_STRUCTURE_ITEM,
+                                row.get("FOREIGN_CUBE_VARIABLE_CODE"),
+                                f"CUBE_STRUCTURE_ITEM_LINK '{item_link_id}'",
+                                "FOREIGN_CUBE_VARIABLE_CODE",
                             )
                             if foreign_variable is None:
-                                continue # Skip row if mandatory FK not found/missing
+                                continue  # Skip row if mandatory FK not found/missing
 
                             primary_variable = ImporterJoins._get_instance_or_skip(
-                                CUBE_STRUCTURE_ITEM, row.get("PRIMARY_CUBE_VARIABLE_CODE"), f"CUBE_STRUCTURE_ITEM_LINK '{item_link_id}'", "PRIMARY_CUBE_VARIABLE_CODE"
+                                CUBE_STRUCTURE_ITEM,
+                                row.get("PRIMARY_CUBE_VARIABLE_CODE"),
+                                f"CUBE_STRUCTURE_ITEM_LINK '{item_link_id}'",
+                                "PRIMARY_CUBE_VARIABLE_CODE",
                             )
                             if primary_variable is None:
-                                continue # Skip row if mandatory FK not found/missing
+                                continue  # Skip row if mandatory FK not found/missing
 
                             item_link_defaults["cube_link_id"] = cube_link
                             item_link_defaults["foreign_cube_variable_code"] = foreign_variable
                             item_link_defaults["primary_cube_variable_code"] = primary_variable
 
-
                             # Use update_or_create based on CUBE_STRUCTURE_ITEM_LINK_ID
                             obj, created = CUBE_STRUCTURE_ITEM_LINK.objects.update_or_create(
-                                cube_structure_item_link_id=item_link_id,
-                                defaults=item_link_defaults
+                                cube_structure_item_link_id=item_link_id, defaults=item_link_defaults
                             )
                             imported_item_links_count += 1
                             # print(f"{'Created' if created else 'Updated'} CUBE_STRUCTURE_ITEM_LINK: {obj.cube_structure_item_link_id}") # Uncomment for verbose output
 
                         except Exception as e:
                             print(f"Error processing CUBE_STRUCTURE_ITEM_LINK row with ID '{item_link_id}': {e}")
                             # Continue to the next row
 
         except FileNotFoundError:
-            print(f"Error: CUBE_STRUCTURE_ITEM_LINK file not found at {cube_structure_item_link_path}. Skipping CUBE_STRUCTURE_ITEM_LINK import.")
+            print(
+                f"Error: CUBE_STRUCTURE_ITEM_LINK file not found at {cube_structure_item_link_path}. Skipping CUBE_STRUCTURE_ITEM_LINK import."
+            )
         except Exception as e:
             print(f"Error reading CUBE_STRUCTURE_ITEM_LINK file {cube_structure_item_link_path}: {e}")
 
-
         print(f"Finished importing {imported_item_links_count} CUBE_STRUCTURE_ITEM_LINK records.")
         print("\nImport process finished.")
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/import_export_join_metadata/import_join_metadata.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/joins_meta_data/ldm_search.py	2025-09-15 13:18:11.384758+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/joins_meta_data/ldm_search.py	2025-09-21 17:07:36.540201+00:00
@@ -11,10 +11,11 @@
 #    Neil Mackenzie - initial API and implementation
 #
 
 from django.db.models.fields.related import ForeignKey
 from django.apps import apps
+
 
 class ELDMSearch:
     """
     A class for searching and retrieving related entities in a Django model hierarchy.
     """
@@ -32,11 +33,13 @@
         """
         entities = set()
         ELDMSearch._get_superclasses_and_associated_entities(
             context, entity, entities, 0, 4, memoization_parents_from_disjoint_subtyping_eldm_search
         )
-        ELDMSearch._get_associated_entities(context, entity, entities, 0, 4, memoization_parents_from_disjoint_subtyping_eldm_search)
+        ELDMSearch._get_associated_entities(
+            context, entity, entities, 0, 4, memoization_parents_from_disjoint_subtyping_eldm_search
+        )
         return list(entities)
 
     def _get_associated_entities(
         context, entity, entities, link_count, link_limit, memoization_parents_from_disjoint_subtyping_eldm_search
     ):
@@ -61,14 +64,24 @@
             ):
                 related_model = feature.related_model
                 entities.add(related_model)
 
                 ELDMSearch._get_superclasses_and_associated_entities(
-                    context, related_model, entities, link_count + 1, link_limit, memoization_parents_from_disjoint_subtyping_eldm_search
+                    context,
+                    related_model,
+                    entities,
+                    link_count + 1,
+                    link_limit,
+                    memoization_parents_from_disjoint_subtyping_eldm_search,
                 )
                 ELDMSearch._get_associated_entities(
-                    context, related_model, entities, link_count + 1, link_limit, memoization_parents_from_disjoint_subtyping_eldm_search
+                    context,
+                    related_model,
+                    entities,
+                    link_count + 1,
+                    link_limit,
+                    memoization_parents_from_disjoint_subtyping_eldm_search,
                 )
 
     def _get_superclasses_and_associated_entities(
         context, entity, entities, link_count, link_limit, memoization_parents_from_disjoint_subtyping_eldm_search
     ):
@@ -81,35 +94,57 @@
             entities (set): The set to store found entities.
             link_count (int): The current depth of the recursive search.
             link_limit (int): The maximum depth of the recursive search.
         """
 
-        parents_from_disjoint_subtyping = ELDMSearch._get_parents_from_disjoint_subtyping(entity, memoization_parents_from_disjoint_subtyping_eldm_search)
+        parents_from_disjoint_subtyping = ELDMSearch._get_parents_from_disjoint_subtyping(
+            entity, memoization_parents_from_disjoint_subtyping_eldm_search
+        )
         for parent in parents_from_disjoint_subtyping:
             if parent not in entities:
                 entities.add(parent)
                 ELDMSearch._get_associated_entities(
-                context, parent, entities, link_count, link_limit, memoization_parents_from_disjoint_subtyping_eldm_search
+                    context,
+                    parent,
+                    entities,
+                    link_count,
+                    link_limit,
+                    memoization_parents_from_disjoint_subtyping_eldm_search,
                 )
                 ELDMSearch._get_superclasses_and_associated_entities(
-                    context, parent, entities, link_count, link_limit, memoization_parents_from_disjoint_subtyping_eldm_search
+                    context,
+                    parent,
+                    entities,
+                    link_count,
+                    link_limit,
+                    memoization_parents_from_disjoint_subtyping_eldm_search,
                 )
 
         parent_list = entity._meta.get_parent_list()
 
         if parent_list:
             super_entity = parent_list[0]
             if super_entity not in entities:
                 entities.add(super_entity)
                 ELDMSearch._get_associated_entities(
-                    context, super_entity, entities, link_count, link_limit,memoization_parents_from_disjoint_subtyping_eldm_search
+                    context,
+                    super_entity,
+                    entities,
+                    link_count,
+                    link_limit,
+                    memoization_parents_from_disjoint_subtyping_eldm_search,
                 )
                 ELDMSearch._get_superclasses_and_associated_entities(
-                    context, super_entity, entities, link_count, link_limit,memoization_parents_from_disjoint_subtyping_eldm_search
+                    context,
+                    super_entity,
+                    entities,
+                    link_count,
+                    link_limit,
+                    memoization_parents_from_disjoint_subtyping_eldm_search,
                 )
 
-        #for feature in entity._meta.get_fields():
+        # for feature in entity._meta.get_fields():
         #    if (
         #        isinstance(feature, ForeignKey)
         #        and feature.name.startswith("parent_")
         #        and feature.name not in ("parent_member_id", "parent_axis_ordinate_id")
         #    ):
@@ -120,11 +155,10 @@
         #            context, super_entity, entities, link_count, link_limit
         #        )
         #       ELDMSearch._get_superclasses_and_associated_entities(
         #            context, super_entity, entities, link_count, link_limit
         #        )
-
 
     def _get_parents_from_disjoint_subtyping(entity, memoization_parents_from_disjoint_subtyping_eldm_search):
         """
         Retrieve parents from disjoint subtyping relationships.
 
@@ -138,15 +172,14 @@
         # get a link tot the full django model, then loop trhought all tables inthe model
         list_of_results = []
         if hash(entity) in memoization_parents_from_disjoint_subtyping_eldm_search:
             return memoization_parents_from_disjoint_subtyping_eldm_search[hash(entity)]
         for model in apps.get_models():
-            if model._meta.app_label == 'pybirdai':
-                #print(f"{model._meta.app_label}  -> {model.__name__}")
+            if model._meta.app_label == "pybirdai":
+                # print(f"{model._meta.app_label}  -> {model.__name__}")
                 for feature in model._meta.get_fields():
-                    if (
-                        isinstance(feature, ForeignKey)
-                        and feature.name.endswith("_delegate")
-                    ) and feature.name[0:len(feature.name)-9] == entity.__name__:
+                    if (isinstance(feature, ForeignKey) and feature.name.endswith("_delegate")) and feature.name[
+                        0 : len(feature.name) - 9
+                    ] == entity.__name__:
                         list_of_results.append(model)
         memoization_parents_from_disjoint_subtyping_eldm_search[hash(entity)] = list_of_results
         return list_of_results
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/joins_meta_data/ldm_search.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/generate_etl/generate_etl.py	2025-09-15 13:18:11.380051+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/generate_etl/generate_etl.py	2025-09-21 17:07:36.574145+00:00
@@ -16,44 +16,38 @@
 import shutil
 from .simple_context import SimpleContext
 
 import math
 
+
 class GenerateETL(object):
 
     def __init__(self):
         self.entity_columns_by_schema = {}
         self.slice_configurations_by_schema = {}
-        
-    def create_etl_guide(self, csv_dir, output_file, context): 
+
+    def create_etl_guide(self, csv_dir, output_file, context):
         self.process_all_tables()
 
     def process_table(self, table_name, source_schema):
-        
+
         self._initialize_storage_for_schema_and_table(source_schema, table_name)
-        
-        os.makedirs('etl_results', exist_ok=True)
+
+        os.makedirs("etl_results", exist_ok=True)
 
         leaf_entities_hierarchy = self._load_entity_hierarchy(table_name, leafs_only=True)
         all_entities_hierarchy = self._load_entity_hierarchy(table_name, leafs_only=False)
         entity_column_mappings = self._load_entity_column_mappings(table_name, leaf_entities_hierarchy)
-        
+
         leaf_combinations = self._extract_leaf_combinations(table_name, leaf_entities_hierarchy)
-        
-        self._process_entity_columns(
-            source_schema,
-            all_entities_hierarchy,
-            entity_column_mappings)
-        
-        self._generate_slice_configurations(
-            table_name,
-            leaf_combinations,
-            leaf_entities_hierarchy,
-            source_schema)
-        
+
+        self._process_entity_columns(source_schema, all_entities_hierarchy, entity_column_mappings)
+
+        self._generate_slice_configurations(table_name, leaf_combinations, leaf_entities_hierarchy, source_schema)
+
         self._write_consolidated_slices_file(table_name, source_schema)
-        
+
         self._create_filter_functions(table_name, source_schema, leaf_entities_hierarchy)
 
     def _initialize_storage_for_schema_and_table(self, source_schema, table_name):
         if source_schema not in self.entity_columns_by_schema:
             self.entity_columns_by_schema[source_schema] = {}
@@ -70,124 +64,106 @@
 
     def _extract_leaf_combinations(self, table_name, entity_hierarchy):
         return self.get_leaf_list_list(table_name, entity_hierarchy)
 
     def _process_entity_columns(self, source_schema, all_entities_hierarchy, entity_column_mappings):
-        return self.generate_part_for_each_entity(
-            source_schema, all_entities_hierarchy, entity_column_mappings, {})
+        return self.generate_part_for_each_entity(source_schema, all_entities_hierarchy, entity_column_mappings, {})
 
     def _generate_slice_configurations(self, table_name, leaf_combinations, leaf_entities_hierarchy, source_schema):
-        return self.create_slice_per_leaf_list(
-            table_name, leaf_combinations, leaf_entities_hierarchy, source_schema)
+        return self.create_slice_per_leaf_list(table_name, leaf_combinations, leaf_entities_hierarchy, source_schema)
 
     def _write_consolidated_slices_file(self, table_name, source_schema):
         return self.write_final_slices_file("", table_name, source_schema)
 
     def _create_filter_functions(self, table_name, source_schema, leaf_entities_hierarchy):
-        output_directory = f'etl_results'
+        output_directory = f"etl_results"
         return self.create_filter_function_per_leaf(output_directory, leaf_entities_hierarchy, source_schema)
-    
-    
+
     def generate(self, table_name, source_schema):
         return self.process_table(table_name, source_schema)
-    
-    def generate_part_for_each_entity(self, 
-                                              source_schema,
-                                              entity_to_hierarchy_dict_all_nodes,
-                                              entity_to_columns_dict,
-                                              populated_mappings_dict):
-        
+
+    def generate_part_for_each_entity(
+        self, source_schema, entity_to_hierarchy_dict_all_nodes, entity_to_columns_dict, populated_mappings_dict
+    ):
+
         if source_schema not in self.entity_columns_by_schema:
             self.entity_columns_by_schema[source_schema] = {}
-            
+
         for entity, column_list in entity_to_columns_dict.items():
             csv_columns = []
             for column in column_list:
-                parts = column.split('.')
+                parts = column.split(".")
                 col = ""
                 if len(parts) == 1:
                     col = parts[0]
                 elif len(parts) == 2:
                     col = parts[1]
 
-                if not(self._is_column_inherited_from_parent(
-                                            col,
-                                            entity,
-                                            entity_to_hierarchy_dict_all_nodes,
-                                            entity_to_columns_dict)):
+                if not (
+                    self._is_column_inherited_from_parent(
+                        col, entity, entity_to_hierarchy_dict_all_nodes, entity_to_columns_dict
+                    )
+                ):
                     csv_columns.append(col)
-            
-            
-            csv_content = ','.join(csv_columns)
-            
-            
-            self.entity_columns_by_schema[source_schema][entity] = {
-                'csv': csv_content
-            }
-
-    def create_slice_per_leaf_list(self,
-                                           table,
-                                           leaf_list_list,
-                                           entity_to_hierarchy_dict,
-                                           source_schema):   
+
+            csv_content = ",".join(csv_columns)
+
+            self.entity_columns_by_schema[source_schema][entity] = {"csv": csv_content}
+
+    def create_slice_per_leaf_list(self, table, leaf_list_list, entity_to_hierarchy_dict, source_schema):
         used_names_count = {}
-        
-        
+
         all_entity_columns = set()
         for entity, entity_data in self.entity_columns_by_schema.get(source_schema, {}).items():
-            if entity_data['csv']:
-                columns = [col.strip() for col in entity_data['csv'].split(',') if col.strip()]
+            if entity_data["csv"]:
+                columns = [col.strip() for col in entity_data["csv"].split(",") if col.strip()]
                 all_entity_columns.update(columns)
-        
+
         sorted_column_list = sorted(list(all_entity_columns))
-        
+
         for leaf_combination in leaf_list_list:
             if len(leaf_combination) > 0:
                 parent_entities = []
                 slice_name = self._generate_slice_name(table, leaf_combination, used_names_count)
-                
-                
+
                 for leaf_entity in leaf_combination:
                     hierarchy = entity_to_hierarchy_dict[leaf_entity]
-                    hierarchy_parts = hierarchy.split('.')
+                    hierarchy_parts = hierarchy.split(".")
                     for entity_part in hierarchy_parts:
                         if not self._is_internal_entity(entity_part):
                             if entity_part not in parent_entities:
                                 parent_entities.append(entity_part)
-                
-                
+
                 header_content = self._generate_csv_header(sorted_column_list)
-                
-                
+
                 csv_filter = self._generate_csv_filter(leaf_combination)
-                
-                
+
                 relevant_columns = self._collect_relevant_columns(source_schema, parent_entities)
-                
-                
+
                 csv_row = self._build_csv_row(csv_filter, sorted_column_list, relevant_columns)
-                
-                
+
                 self.slice_configurations_by_schema[source_schema][table][slice_name] = {
-                    'csv': csv_row,
-                    'header': header_content
+                    "csv": csv_row,
+                    "header": header_content,
                 }
 
     def _generate_slice_name(self, table, leaf_combination, used_names_count):
         return self.get_leaf_combination_name_under_150_chars(table, leaf_combination, used_names_count)
 
     def _is_internal_entity(self, entity_part):
-        return (entity_part.endswith('delegate') or
-                entity_part.endswith('_disc') or
-                entity_part.endswith('_association') or
-                entity_part.endswith('_composition'))
+        return (
+            entity_part.endswith("delegate")
+            or entity_part.endswith("_disc")
+            or entity_part.endswith("_association")
+            or entity_part.endswith("_composition")
+        )
 
     def _generate_csv_header(self, sorted_column_list):
         header_content = "filter,"
         for column in sorted_column_list:
             header_content += column + ","
-        
+
         if sorted_column_list:
             header_content += sorted_column_list[-1]
         header_content += "\n"
         return header_content
 
@@ -200,12 +176,12 @@
 
     def _collect_relevant_columns(self, source_schema, parent_entities):
         relevant_columns = set()
         for parent_entity in parent_entities:
             entity_data = self.entity_columns_by_schema.get(source_schema, {}).get(parent_entity, {})
-            if entity_data.get('csv'):
-                columns = [col.strip() for col in entity_data['csv'].split(',') if col.strip()]
+            if entity_data.get("csv"):
+                columns = [col.strip() for col in entity_data["csv"].split(",") if col.strip()]
                 relevant_columns.update(columns)
         return relevant_columns
 
     def _build_csv_row(self, csv_filter, sorted_column_list, relevant_columns):
         csv_row = csv_filter + ","
@@ -218,403 +194,356 @@
 
     def _is_column_inherited_from_parent(self, col, the_entity, entity_to_hierarchy_dict, entity_to_columns_dict):
         return self.super_type_contains_column(col, the_entity, entity_to_hierarchy_dict, entity_to_columns_dict)
 
     def write_final_slices_file(self, output_directory, table, source_schema):
-        
-        target_file = f'etl_results{os.sep}{table}_all_slices.csv'
-        
-        
+
+        target_file = f"etl_results{os.sep}{table}_all_slices.csv"
+
         slice_configurations = self.slice_configurations_by_schema.get(source_schema, {}).get(table, {})
         header = None
-        
-        
+
         for slice_data in slice_configurations.values():
-            if slice_data.get('header'):
-                header = slice_data['header']
+            if slice_data.get("header"):
+                header = slice_data["header"]
                 break
-        
-        with open(target_file, 'w', encoding='utf-8') as f:
-            
+
+        with open(target_file, "w", encoding="utf-8") as f:
+
             if header:
                 f.write(header)
-            
-            
+
             for slice_data in slice_configurations.values():
-                if slice_data.get('csv'):
-                    
-                    f.write(slice_data['csv'] + '\n')
-
-    
+                if slice_data.get("csv"):
+
+                    f.write(slice_data["csv"] + "\n")
+
     def get_leaf_list_list(self, table, entity_to_hierarchy_dict_leafs_only):
-        input_file_location = 'results' + os.sep + 'csv' + os.sep + '' + \
-                                table + \
-                                '_discrimitor_combinations_summary.csv'
+        input_file_location = "results" + os.sep + "csv" + os.sep + "" + table + "_discrimitor_combinations_summary.csv"
         header_skipped = False
         leaf_list_list = []
-        with open(input_file_location, encoding='utf-8') as csvfile:
-            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
+        with open(input_file_location, encoding="utf-8") as csvfile:
+            filereader = csv.reader(csvfile, delimiter=",", quotechar='"')
             for row in filereader:
                 row_is_valid = True
-                
-                if (not header_skipped):
+
+                if not header_skipped:
                     header_skipped = True
                 else:
                     leaf_list = []
                     for item in row:
-                        if not(item == '') and not(item is None):
-                            if item in entity_to_hierarchy_dict_leafs_only.\
-                                                                    keys():
+                        if not (item == "") and not (item is None):
+                            if item in entity_to_hierarchy_dict_leafs_only.keys():
                                 leaf_list.append(item)
                             else:
-                                
-                                
-                                
-                                
-                                
-                                has_leaf = \
-                                    self.non_leaf_has_corresponding_leaf_in_row(
-                                            item,
-                                            row,
-                                            entity_to_hierarchy_dict_leafs_only)
+
+                                has_leaf = self.non_leaf_has_corresponding_leaf_in_row(
+                                    item, row, entity_to_hierarchy_dict_leafs_only
+                                )
 
                     if row_is_valid:
                         leaf_list_list.append(leaf_list)
         return leaf_list_list
 
-    def non_leaf_has_corresponding_leaf_in_row(
-                                        self,
-                                        item,
-                                        row,
-                                        entity_to_hierarchy_dict_leafs_only):
+    def non_leaf_has_corresponding_leaf_in_row(self, item, row, entity_to_hierarchy_dict_leafs_only):
         return_value = False
         for entity, hierarchy in entity_to_hierarchy_dict_leafs_only.items():
-            hierarchy_parts = hierarchy.split('.')
+            hierarchy_parts = hierarchy.split(".")
             if item in hierarchy_parts:
                 if entity in row:
                     return_value = True
 
         return return_value
-    
+
     def get_entity_to_columns_dict(self, table, entity_to_hierarchy_dict):
-        input_file_location = 'results' + os.sep + 'csv' + os.sep + '' + table + \
-                    '_discrimitor_combinations_full.csv'
+        input_file_location = "results" + os.sep + "csv" + os.sep + "" + table + "_discrimitor_combinations_full.csv"
         column_index_to_full_ldm_column_name_list = []
         column_index_to_il_column_name_list = []
         subset_column_index_to_full_ldm_column_name_list = []
         subset_column_index_to_il_column_name_list = []
         entity_to_columns_dict = {}
 
         header_skipped = False
         header2_skipped = False
-        with open(input_file_location, encoding='utf-8') as csvfile:
-            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
+        with open(input_file_location, encoding="utf-8") as csvfile:
+            filereader = csv.reader(csvfile, delimiter=",", quotechar='"')
             for row in filereader:
-                if (not header_skipped):
+                if not header_skipped:
                     header_skipped = True
                     column_index_to_full_ldm_column_name_list = row
-                elif(not header2_skipped):
+                elif not header2_skipped:
                     column_index_to_il_column_name_list = row
                     header2_skipped = True
                 else:
                     pass
 
             count = 0
             for il_item in column_index_to_il_column_name_list:
 
                 ldm_item = column_index_to_full_ldm_column_name_list[count]
-                subset_column_index_to_full_ldm_column_name_list.append(
-                                                                    ldm_item)
+                subset_column_index_to_full_ldm_column_name_list.append(ldm_item)
                 subset_column_index_to_il_column_name_list.append(il_item)
-               
-                count = count + 1   
+
+                count = count + 1
 
             count = 0
             for il_item in subset_column_index_to_il_column_name_list:
-                ldm_item = \
-                    subset_column_index_to_full_ldm_column_name_list[count]
+                ldm_item = subset_column_index_to_full_ldm_column_name_list[count]
 
                 table_name = self.get_table_name_from_ldm_column(ldm_item)
                 il_column_list = []
                 try:
                     il_column_list = entity_to_columns_dict[table_name]
                 except KeyError:
                     entity_to_columns_dict[table_name] = il_column_list
 
-                if not(il_item == 'UNKNOWN'):    
-                    
-                    if not(il_item in il_column_list):
+                if not (il_item == "UNKNOWN"):
+
+                    if not (il_item in il_column_list):
                         il_column_list.append(il_item)
                 count = count + 1
 
             for item in entity_to_hierarchy_dict.keys():
-                if not (item in entity_to_columns_dict.keys()) \
-                            and not (item is None):
-                    entity_to_columns_dict[item] = []  
-
-        return entity_to_columns_dict  
-        
-    def super_type_contains_column(self, col,
-                                   the_entity,
-                                   entity_to_hierarchy_dict,
-                                   entity_to_columns_dict):
+                if not (item in entity_to_columns_dict.keys()) and not (item is None):
+                    entity_to_columns_dict[item] = []
+
+        return entity_to_columns_dict
+
+    def super_type_contains_column(self, col, the_entity, entity_to_hierarchy_dict, entity_to_columns_dict):
         super_type_contains_column = False
-        
+
         for entity, hierarchy in entity_to_hierarchy_dict.items():
             if the_entity == entity:
-                hierarchy_parts = hierarchy.split('.')
-                
+                hierarchy_parts = hierarchy.split(".")
+
                 for leaf in hierarchy_parts:
                     if leaf == entity:
                         break
                     else:
-                        if not (leaf.endswith('delegate') or 
-                                leaf.endswith('_disc') or 
-                                leaf.endswith('_association') or 
-                                leaf.endswith('_composition')):
+                        if not (
+                            leaf.endswith("delegate")
+                            or leaf.endswith("_disc")
+                            or leaf.endswith("_association")
+                            or leaf.endswith("_composition")
+                        ):
                             try:
                                 columns = entity_to_columns_dict[leaf]
                                 for column in columns:
                                     main_column_part = ""
-                                    parts = column.split('.')                                        
+                                    parts = column.split(".")
                                     if len(parts) == 1:
                                         main_column_part = parts[0]
                                     elif len(parts) == 2:
                                         main_column_part = parts[1]
                                     if main_column_part == col:
                                         super_type_contains_column = True
                             except:
                                 pass
 
         return super_type_contains_column
-        
+
     def get_entity_to_hierarchy_dict(self, table, leafs_only):
-        input_file_location = 'results' + os.sep + 'csv' + os.sep + '' + table + \
-                '_discrimitor_combinations_summary.csv'
+        input_file_location = "results" + os.sep + "csv" + os.sep + "" + table + "_discrimitor_combinations_summary.csv"
         column_index_to_full_ldm_column_name_list = []
         entity_to_hierarchy_dict = {}
         items_to_pop = []
         header_skipped = False
         header2_skipped = False
-        with open(input_file_location, encoding='utf-8') as csvfile:
-            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
+        with open(input_file_location, encoding="utf-8") as csvfile:
+            filereader = csv.reader(csvfile, delimiter=",", quotechar='"')
             for row in filereader:
-                if (not header_skipped):
+                if not header_skipped:
                     header_skipped = True
                     column_index_to_full_ldm_column_name_list = row
-                elif(not header2_skipped):
+                elif not header2_skipped:
                     pass
                     header2_skipped = True
                 else:
                     counter = 0
                     for item in row:
-                        if not(item == ""):
-                            entity_to_hierarchy_dict[item] = \
-                                column_index_to_full_ldm_column_name_list\
-                                    [counter] + "." + item
-                                                                     
-                            hierarchy_parts = \
-                                column_index_to_full_ldm_column_name_list\
-                                    [counter].split('.')
-                            
+                        if not (item == ""):
+                            entity_to_hierarchy_dict[item] = (
+                                column_index_to_full_ldm_column_name_list[counter] + "." + item
+                            )
+
+                            hierarchy_parts = column_index_to_full_ldm_column_name_list[counter].split(".")
+
                             for leaf in hierarchy_parts:
-                                if not (leaf.endswith('delegate') or 
-                                        leaf.endswith('_disc') or 
-                                        leaf.endswith('_association') or 
-                                        leaf.endswith('_composition')) \
-                                            and not(leaf == item):
+                                if not (
+                                    leaf.endswith("delegate")
+                                    or leaf.endswith("_disc")
+                                    or leaf.endswith("_association")
+                                    or leaf.endswith("_composition")
+                                ) and not (leaf == item):
                                     items_to_pop.append(leaf)
-                        counter = counter + 1    
+                        counter = counter + 1
 
         if leafs_only:
             for item in items_to_pop:
                 try:
                     entity_to_hierarchy_dict.pop(item)
                 except:
                     pass
 
-                    
         return entity_to_hierarchy_dict
-    
-    def get_leaf_combination_name_under_64_chars(self,
-                                                 table,
-                                                 leaf_list,
-                                                 used_names_count):
+
+    def get_leaf_combination_name_under_64_chars(self, table, leaf_list, used_names_count):
         table_length = len(table)
         remaining = 61 - table_length
         num_of_leafs = len(leaf_list)
         target_length_per_leaf = math.floor(remaining / num_of_leafs)
         leaf_combination = ""
         leafs_left = num_of_leafs
         for leaf in leaf_list:
             used_leaf_name = leaf
             if len(leaf) > target_length_per_leaf:
-                used_leaf_name = used_leaf_name[0:7] + \
-                used_leaf_name[len(leaf) - target_length_per_leaf + 9:len(leaf)]
+                used_leaf_name = (
+                    used_leaf_name[0:7] + used_leaf_name[len(leaf) - target_length_per_leaf + 9 : len(leaf)]
+                )
                 remaining = remaining - len(used_leaf_name)
             else:
                 leafs_left = leafs_left - 1
                 remaining = remaining - len(used_leaf_name)
-                if (leafs_left > 0):
-                    target_length_per_leaf = \
-                        math.floor(remaining / leafs_left)
+                if leafs_left > 0:
+                    target_length_per_leaf = math.floor(remaining / leafs_left)
 
             leaf_combination = leaf_combination + ":" + used_leaf_name
         try:
             used_count = used_names_count[leaf_combination]
-            used_names_count[leaf_combination] = \
-                used_names_count[leaf_combination] + 1
+            used_names_count[leaf_combination] = used_names_count[leaf_combination] + 1
             leaf_combination = leaf_combination + str(used_count)
         except KeyError:
             used_names_count[leaf_combination] = 1
 
         return leaf_combination
-    
-    def get_leaf_combination_name_under_150_chars(self,
-                                                 table,
-                                                 leaf_list,
-                                                 used_names_count):
+
+    def get_leaf_combination_name_under_150_chars(self, table, leaf_list, used_names_count):
         table_length = len(table)
         remaining = 130 - table_length
         num_of_leafs = len(leaf_list)
         target_length_per_leaf = math.floor(remaining / num_of_leafs)
         leaf_combination = ""
         leafs_left = num_of_leafs
         for leaf in leaf_list:
             used_leaf_name = leaf
             if len(leaf) > target_length_per_leaf:
-                used_leaf_name = used_leaf_name[0:7] + \
-                used_leaf_name[len(leaf) - target_length_per_leaf + 9:len(leaf)]
+                used_leaf_name = (
+                    used_leaf_name[0:7] + used_leaf_name[len(leaf) - target_length_per_leaf + 9 : len(leaf)]
+                )
                 remaining = remaining - len(used_leaf_name)
             else:
                 leafs_left = leafs_left - 1
                 remaining = remaining - len(used_leaf_name)
-                if (leafs_left > 0):
-                    target_length_per_leaf = \
-                        math.floor(remaining / leafs_left)
+                if leafs_left > 0:
+                    target_length_per_leaf = math.floor(remaining / leafs_left)
 
             leaf_combination = leaf_combination + ":" + used_leaf_name
         try:
             used_count = used_names_count[leaf_combination]
-            used_names_count[leaf_combination] = \
-                used_names_count[leaf_combination] + 1
+            used_names_count[leaf_combination] = used_names_count[leaf_combination] + 1
             leaf_combination = leaf_combination + str(used_count)
         except KeyError:
             used_names_count[leaf_combination] = 1
 
         return leaf_combination
 
-    def create_filter_function_per_leaf(self,
-                                        output_directory,
-                                        entity_to_hierarchy_dict,
-                                        source_schema):            
-        filter_definitions_file =\
-            open(output_directory + os.sep + 'python_functions' + os.sep + '' +
-                 'python_filter_definitions' + os.sep + 'filter_definitions.py',
-                 "a", encoding='utf-8')
-        
+    def create_filter_function_per_leaf(self, output_directory, entity_to_hierarchy_dict, source_schema):
+        filter_definitions_file = open(
+            output_directory
+            + os.sep
+            + "python_functions"
+            + os.sep
+            + ""
+            + "python_filter_definitions"
+            + os.sep
+            + "filter_definitions.py",
+            "a",
+            encoding="utf-8",
+        )
+
         for entity, hierarchy in entity_to_hierarchy_dict.items():
-           
+
             if entity is not None:
-                filter_definitions_file.write(
-                    "def is_" + entity + "():\n")
+                filter_definitions_file.write("def is_" + entity + "():\n")
                 filter_definitions_file.write("\treturn False\n\n")
-                
-             
-                    
+
         filter_definitions_file.close()
-       
-        
 
     def get_table_name_from_ldm_column(self, ldm_item):
-        qualified_name_list = ldm_item.split('.')
+        qualified_name_list = ldm_item.split(".")
         table_name = qualified_name_list[len(qualified_name_list) - 2]
 
         return table_name
-    
+
     def process_all_tables(self):
-        
+
         context = SimpleContext()
         all_tables_to_process = context.get_all_related_tables()
-        
-        
+
         self._setup_output_directories()
-        
-        
-        source_schema = 'all'
+
+        source_schema = "all"
         self._create_schema_directories(source_schema)
 
-        
         for table_name in all_tables_to_process:
             self._create_mapping_template(table_name, source_schema)
 
-        
         for table_name in all_tables_to_process:
             self.process_table(table_name, source_schema)
 
     def generate_all(self):
         return self.process_all_tables()
 
     def _setup_output_directories(self):
-        
-        if os.path.exists('etl_results'):
-            shutil.rmtree('etl_results')
-        os.mkdir('etl_results')
-        os.mkdir('etl_results' + os.sep + 'mapping_templates')
-        os.mkdir('etl_results' + os.sep + 'python_functions')
-        os.mkdir('etl_results' + os.sep + 'python_functions' + os.sep + 'python_filter_definitions')
+
+        if os.path.exists("etl_results"):
+            shutil.rmtree("etl_results")
+        os.mkdir("etl_results")
+        os.mkdir("etl_results" + os.sep + "mapping_templates")
+        os.mkdir("etl_results" + os.sep + "python_functions")
+        os.mkdir("etl_results" + os.sep + "python_functions" + os.sep + "python_filter_definitions")
 
     def _create_schema_directories(self, source_schema):
-        os.mkdir('etl_results' + os.sep + 'mapping_templates' + os.sep + source_schema)
+        os.mkdir("etl_results" + os.sep + "mapping_templates" + os.sep + source_schema)
 
     def _create_mapping_template(self, table_name, source_schema):
         return self.generate_mapping_template(table_name, source_schema)
 
-        
-       
-
     def generate_mapping_template(self, table, source_schema):
         output_directory = source_schema + os.sep + table
 
-        entity_to_hierarchy_dict_leafs_only = \
-            self.get_entity_to_hierarchy_dict(table, True)
-        entity_to_columns_dict = \
-            self.get_entity_to_columns_dict(table,
-                                           entity_to_hierarchy_dict_leafs_only)
-        self.create_mapping_templates(table,
-                                     'etl_results' + os.sep + 'mapping_templates' + os.sep + '' +
-                                     source_schema, source_schema, entity_to_columns_dict)
-
-    def create_mapping_templates(self,
-                                 table,
-                                 output_directory,
-                                 source_schema,
-                                 entity_to_columns_dict):
-        
-        file = open(output_directory + os.sep + 
-                    source_schema + '_to_' + 
-                    table + ".csv", "a", encoding='utf-8')
+        entity_to_hierarchy_dict_leafs_only = self.get_entity_to_hierarchy_dict(table, True)
+        entity_to_columns_dict = self.get_entity_to_columns_dict(table, entity_to_hierarchy_dict_leafs_only)
+        self.create_mapping_templates(
+            table,
+            "etl_results" + os.sep + "mapping_templates" + os.sep + "" + source_schema,
+            source_schema,
+            entity_to_columns_dict,
+        )
+
+    def create_mapping_templates(self, table, output_directory, source_schema, entity_to_columns_dict):
+
+        file = open(output_directory + os.sep + source_schema + "_to_" + table + ".csv", "a", encoding="utf-8")
         for entity, column_list in entity_to_columns_dict.items():
             for column in column_list:
-                file.write(entity + "," + column + ",\'?\',\n")
+                file.write(entity + "," + column + ",'?',\n")
 
     def get_populated_mappings(self, source_schema, table):
-        mappings_selection = 'source_mappings'
-        
-        file_location = mappings_selection + os.sep + \
-                        source_schema + os.sep + source_schema + \
-                        '_to_' + table + ".csv"
+        mappings_selection = "source_mappings"
+
+        file_location = mappings_selection + os.sep + source_schema + os.sep + source_schema + "_to_" + table + ".csv"
         mappings_dict = {}
 
-        with open(file_location, encoding='utf-8') as csvfile:
-            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
+        with open(file_location, encoding="utf-8") as csvfile:
+            filereader = csv.reader(csvfile, delimiter=",", quotechar='"')
             for row in filereader:
                 entity = row[0]
                 column = row[1]
                 mapped_val = row[2]
                 mappings_dict[entity + column] = mapped_val
 
         return mappings_dict
-            
+
+
 if __name__ == "__main__":
-   gen = GenerateETL() 
-   gen.generate_all()
\ No newline at end of file
+    gen = GenerateETL()
+    gen.generate_all()
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/generate_etl/generate_etl.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/joins_meta_data/create_joins_meta_data.py	2025-09-15 13:18:11.383870+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/joins_meta_data/create_joins_meta_data.py	2025-09-21 17:07:36.583026+00:00
@@ -11,16 +11,17 @@
 #    Neil Mackenzie - initial API and implementation
 #
 
 from pybirdai.models.bird_meta_data_model import *
 from django.apps import apps
-from django.db.models.fields import CharField,DateTimeField,BooleanField,FloatField,BigIntegerField
+from django.db.models.fields import CharField, DateTimeField, BooleanField, FloatField, BigIntegerField
 import os
 import csv
 from typing import List, Any
 
 from pybirdai.process_steps.joins_meta_data.ldm_search import ELDMSearch
+
 
 class JoinsMetaDataCreator:
     """
     A class for creating generation rules for reports and tables.
     """
@@ -43,53 +44,43 @@
         Args:
             context (Any): The context object containing necessary data.
             sdd_context (Any): The SDD context object.
             framework (str): The framework being used (e.g., "FINREP_REF").
         """
-        file_location = os.path.join(context.file_directory, "joins_configuration",
-                                     f"in_scope_reports_{framework}.csv")
+        file_location = os.path.join(context.file_directory, "joins_configuration", f"in_scope_reports_{framework}.csv")
         self.create_ldm_entity_to_linked_entities_map(context, sdd_context)
 
-        with open(file_location, encoding='utf-8') as csvfile:
-            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
+        with open(file_location, encoding="utf-8") as csvfile:
+            filereader = csv.reader(csvfile, delimiter=",", quotechar='"')
             next(filereader)  # Skip header
             for row in filereader:
                 report_template = row[0]
-                generated_output_layer = self.find_output_layer_cube(
-                    sdd_context, report_template, framework)
+                generated_output_layer = self.find_output_layer_cube(sdd_context, report_template, framework)
                 if generated_output_layer:
-                    self.add_join_for_products_il(context, sdd_context,
-                                         generated_output_layer, framework)
-
-    def create_ldm_entity_to_linked_entities_map(self, context: Any,
-                                                 sdd_context: Any) -> None:
+                    self.add_join_for_products_il(context, sdd_context, generated_output_layer, framework)
+
+    def create_ldm_entity_to_linked_entities_map(self, context: Any, sdd_context: Any) -> None:
         """
         Create a mapping of LDM entities to their linked entities.
 
         Args:
             context (Any): The context object containing necessary data.
             sdd_context (Any): The SDD context object.
         """
-        output_file = os.path.join(context.output_directory, "csv",
-                                   "ldm_entity_related_entities.csv")
-        with open(output_file, "w", encoding='utf-8') as f:
+        output_file = os.path.join(context.output_directory, "csv", "ldm_entity_related_entities.csv")
+        with open(output_file, "w", encoding="utf-8") as f:
             f.write("ldm_entity,related_entities\n")
             for model in apps.get_models():
-                if model._meta.app_label == 'pybirdai':
-                    entities = ELDMSearch.get_all_related_entities(
-                        self, context, model
-                    )
-                    related_entities_string = ":".join(
-                        entity.__name__ for entity in entities
-                    )
+                if model._meta.app_label == "pybirdai":
+                    entities = ELDMSearch.get_all_related_entities(self, context, model)
+                    related_entities_string = ":".join(entity.__name__ for entity in entities)
                     f.write(f"{model.__name__},{related_entities_string}\n")
-                    context.ldm_entity_to_linked_tables_map[model.__name__] = \
-                        related_entities_string
-
-    def add_join_for_products_il(self, context: Any, sdd_context: Any,
-                           generated_output_layer: Any,
-                           framework: str) -> None:
+                    context.ldm_entity_to_linked_tables_map[model.__name__] = related_entities_string
+
+    def add_join_for_products_il(
+        self, context: Any, sdd_context: Any, generated_output_layer: Any, framework: str
+    ) -> None:
         """
         Add join for products for the input layer.
 
         Args:
             context (Any): The context object containing necessary data.
@@ -120,31 +111,28 @@
             main_categories = context.report_to_main_category_map[report_template]
             for mc in main_categories:
                 try:
                     tables = tables_for_main_category_map[mc]
                     for table in tables:
-                        inputLayerTable = self.find_input_layer_cube(
-                            sdd_context, table, framework
-                        )
+                        inputLayerTable = self.find_input_layer_cube(sdd_context, table, framework)
                         join_for_products = table_and_part_tuple_map[mc]
 
                         for join_for_product in join_for_products:
                             print(f"join_for_product:{join_for_product}")
                             print(inputLayerTable)
                             input_entity_list = [inputLayerTable]
                             linked_tables = join_for_products_to_linked_tables_map[join_for_product]
                             linked_tables_list = linked_tables.split(":")
-                            if (inputLayerTable and
-                                inputLayerTable.cube_structure_id not in linked_tables_list):
+                            if inputLayerTable and inputLayerTable.cube_structure_id not in linked_tables_list:
                                 linked_tables_list.append(inputLayerTable.cube_structure_id)
                             extra_tables = []
                             for the_table in linked_tables_list:
                                 extra_linked_tables = []
                                 try:
-                                    #if the_table.endswith("_ELDM"):
+                                    # if the_table.endswith("_ELDM"):
                                     extra_linked_tables_string = context.ldm_entity_to_linked_tables_map[the_table]
-                                    #else:
+                                    # else:
                                     #    extra_linked_tables_string = context.ldm_entity_to_linked_tables_map[the_table + "_ELDM"]
                                     extra_linked_tables = extra_linked_tables_string.split(":")
                                 except KeyError:
                                     pass
 
@@ -155,67 +143,83 @@
                             for extra_table in extra_tables:
                                 if extra_table not in linked_tables_list:
                                     linked_tables_list.append(extra_table)
 
                             for the_table in linked_tables_list:
-                                the_input_table = self.find_input_layer_cube(
-                                    sdd_context, the_table, framework
-                                )
+                                the_input_table = self.find_input_layer_cube(sdd_context, the_table, framework)
                                 if the_input_table:
                                     input_entity_list.append(the_input_table)
 
                             if join_for_product[0] == table:
                                 for input_entity in input_entity_list:
                                     print(f"input_entity:{input_entity}")
                                     cube_link = CUBE_LINK()
                                     cube_link.description = f"{join_for_product[0]}:{mc}:{join_for_product[1]}:{input_entity.cube_structure_id}"
-                                    cube_link.name = f"{join_for_product[0]}:{join_for_product[1]}:{input_entity.cube_structure_id}"
+                                    cube_link.name = (
+                                        f"{join_for_product[0]}:{join_for_product[1]}:{input_entity.cube_structure_id}"
+                                    )
                                     cube_link.join_identifier = join_for_product[1]
                                     primary_cube = sdd_context.bird_cube_dictionary.get(input_entity.cube_structure_id)
                                     if primary_cube:
                                         cube_link.primary_cube_id = primary_cube
                                         cube_link.cube_link_id = (
                                             f"{report_template}:"
                                             f"{input_entity.cube_structure_id}:{join_for_product[1]}"
                                         )
                                     else:
-                                        cube_link.cube_link_id = f"{input_entity.cube_structure_id}:{join_for_product[1]}"
+                                        cube_link.cube_link_id = (
+                                            f"{input_entity.cube_structure_id}:{join_for_product[1]}"
+                                        )
                                         print(f"cube_link.primary_cube_id not found for {table}")
                                     cube_link.foreign_cube_id = generated_output_layer
-
 
                                     if cube_link.cube_link_id not in sdd_context.cube_link_dictionary:
                                         sdd_context.cube_link_dictionary[cube_link.cube_link_id] = cube_link
                                         foreign_cube = cube_link.foreign_cube_id
                                         try:
-                                            sdd_context.cube_link_to_foreign_cube_map[foreign_cube.cube_id].append(cube_link)
+                                            sdd_context.cube_link_to_foreign_cube_map[foreign_cube.cube_id].append(
+                                                cube_link
+                                            )
                                         except KeyError:
-                                            sdd_context.cube_link_to_foreign_cube_map[foreign_cube.cube_id] = [cube_link]
+                                            sdd_context.cube_link_to_foreign_cube_map[foreign_cube.cube_id] = [
+                                                cube_link
+                                            ]
                                         join_identifier = cube_link.join_identifier
                                         try:
-                                            sdd_context.cube_link_to_join_identifier_map[join_identifier].append(cube_link)
+                                            sdd_context.cube_link_to_join_identifier_map[join_identifier].append(
+                                                cube_link
+                                            )
                                         except KeyError:
                                             sdd_context.cube_link_to_join_identifier_map[join_identifier] = [cube_link]
 
                                         join_for_report_id = foreign_cube.cube_id + ":" + cube_link.join_identifier
                                         try:
-                                            sdd_context.cube_link_to_join_for_report_id_map[join_for_report_id].append(cube_link)
+                                            sdd_context.cube_link_to_join_for_report_id_map[join_for_report_id].append(
+                                                cube_link
+                                            )
                                         except KeyError:
-                                            sdd_context.cube_link_to_join_for_report_id_map[join_for_report_id] = [cube_link]
-
-
-
-                                        num_of_cube_link_items = self.add_field_to_field_lineage_to_rules_for_join_for_product(
-                                            context, sdd_context, generated_output_layer,
-                                            input_entity, mc, report_template,
-                                            framework, cube_link, cube_structure_item_links_to_create
+                                            sdd_context.cube_link_to_join_for_report_id_map[join_for_report_id] = [
+                                                cube_link
+                                            ]
+
+                                        num_of_cube_link_items = (
+                                            self.add_field_to_field_lineage_to_rules_for_join_for_product(
+                                                context,
+                                                sdd_context,
+                                                generated_output_layer,
+                                                input_entity,
+                                                mc,
+                                                report_template,
+                                                framework,
+                                                cube_link,
+                                                cube_structure_item_links_to_create,
+                                            )
                                         )
 
                                         if context.save_derived_sdd_items:
                                             if num_of_cube_link_items > 0:
                                                 cube_links_to_create.append(cube_link)
-
 
                 except KeyError:
                     print(f"no tables for main category:{mc}")
         except KeyError:
             print(f"no main category for report :{report_template}")
@@ -225,22 +229,28 @@
             CUBE_LINK.objects.bulk_create(cube_links_to_create, batch_size=1000)
 
         # Bulk create all collected CUBE_STRUCTURE_ITEM_LINK objects
         for item in cube_structure_item_links_to_create:
             print(item.foreign_cube_variable_code.variable_id)
-            #import pdb;pdb.set_trace()
+            # import pdb;pdb.set_trace()
             item.save()
-        #import pdb;pdb.set_trace()
-        #if context.save_derived_sdd_items and cube_structure_item_links_to_create:
+        # import pdb;pdb.set_trace()
+        # if context.save_derived_sdd_items and cube_structure_item_links_to_create:
         #    CUBE_STRUCTURE_ITEM_LINK.objects.bulk_create(cube_structure_item_links_to_create, batch_size=1000)
 
     def add_field_to_field_lineage_to_rules_for_join_for_product(
-            self, context: Any, sdd_context: Any,
-            output_entity: Any, input_entity: Any,
-            category: str, report_template: str,
-            framework: str, cube_link: Any,
-            cube_structure_item_links_to_create: List) -> None:
+        self,
+        context: Any,
+        sdd_context: Any,
+        output_entity: Any,
+        input_entity: Any,
+        category: str,
+        report_template: str,
+        framework: str,
+        cube_link: Any,
+        cube_structure_item_links_to_create: List,
+    ) -> None:
         """
         Add field-to-field lineage rules for a join for product.
 
         Args:
             context (Any): The context object containing necessary data.
@@ -251,30 +261,26 @@
             report_template (str): The report template name.
             framework (str): The framework being used (e.g., "FINREP_REF").
             cube_link (Any): The cube link object.
         """
         num_of_cube_link_items = 0
-        for output_item in sdd_context.bird_cube_structure_item_dictionary[
-                output_entity.cube_id + '_cube_structure']:
-            if self.valid_operation(context, output_item, framework,
-                                    category, report_template):
-                input_columns = self.find_variables_with_same_domain_then_name(
-                    sdd_context, output_item, input_entity)
+        for output_item in sdd_context.bird_cube_structure_item_dictionary[output_entity.cube_id + "_cube_structure"]:
+            if self.valid_operation(context, output_item, framework, category, report_template):
+                input_columns = self.find_variables_with_same_domain_then_name(sdd_context, output_item, input_entity)
 
                 if input_columns:
                     for input_column in input_columns:
                         csil = CUBE_STRUCTURE_ITEM_LINK()
                         csil.foreign_cube_variable_code = output_item
                         csil.cube_structure_item_link_id = (
-                            f"{cube_link.cube_link_id}:"
-                            f"{csil.foreign_cube_variable_code.variable_id.variable_id}"
+                            f"{cube_link.cube_link_id}:" f"{csil.foreign_cube_variable_code.variable_id.variable_id}"
                         )
                         csil.primary_cube_variable_code = input_column
-                        if not(csil.primary_cube_variable_code.variable_id is None):
+                        if not (csil.primary_cube_variable_code.variable_id is None):
                             csil.cube_structure_item_link_id += (
-                            f":{csil.primary_cube_variable_code.variable_id.variable_id}"
-                        )
+                                f":{csil.primary_cube_variable_code.variable_id.variable_id}"
+                            )
                         csil.cube_link_id = cube_link
                         sdd_context.cube_structure_item_links_dictionary[csil.cube_structure_item_link_id] = csil
 
                         try:
                             sdd_context.cube_structure_item_link_to_cube_link_map[cube_link.cube_link_id].append(csil)
@@ -284,11 +290,13 @@
                         if context.save_derived_sdd_items:
                             cube_structure_item_links_to_create.append(csil)
                             num_of_cube_link_items = num_of_cube_link_items + 1
         return num_of_cube_link_items
 
-    def valid_operation(self, context: Any, output_item: Any, framework: str, category: str, report_template: str) -> bool:
+    def valid_operation(
+        self, context: Any, output_item: Any, framework: str, category: str, report_template: str
+    ) -> bool:
         """
         Check if the operation is valid for the given output item and context.
 
         Args:
             context (Any): The context object containing necessary data.
@@ -300,11 +308,20 @@
         Returns:
             bool: True if the operation is valid, False otherwise.
         """
         return True
 
-    def operation_exists_in_cell_for_report_with_category(self, context: Any, sdd_context: Any, output_item: Any, framework: str, input_cube_type: str, category: str, report_template: str) -> bool:
+    def operation_exists_in_cell_for_report_with_category(
+        self,
+        context: Any,
+        sdd_context: Any,
+        output_item: Any,
+        framework: str,
+        input_cube_type: str,
+        category: str,
+        report_template: str,
+    ) -> bool:
         """
         Check if an operation exists in a cell for a report with a specific category.
 
         Args:
             context (Any): The context object containing necessary data.
@@ -321,17 +338,20 @@
         combinations = sdd_context.combination_to_rol_cube_map.get(report_template, [])
         for combination in combinations:
             if combination in context.cell_to_typ_instrmnt_map[category]:
                 combination_items = sdd_context.combination_item_dictionary.get(combination.combination_id, [])
                 for combination_item in combination_items:
-                    if combination_item.variable_id.name == output_item.name and combination_item.member_id.member_id == category:
+                    if (
+                        combination_item.variable_id.name == output_item.name
+                        and combination_item.member_id.member_id == category
+                    ):
                         return True
         return False
 
-
-
-    def find_variables_with_same_domain_then_name(self, sdd_context: Any, output_item: Any, input_entity: Any) -> List[Any]:
+    def find_variables_with_same_domain_then_name(
+        self, sdd_context: Any, output_item: Any, input_entity: Any
+    ) -> List[Any]:
         """
         Find variables with the same domain and then name as the output item.
 
         Args:
             sdd_context (Any): The SDD context object.
@@ -342,21 +362,27 @@
             List[Any]: A list of matching variables.
         """
         related_variables = []
         target_domain = output_item.variable_id.domain_id
 
-        if target_domain and  not ((target_domain.domain_id == "String") or (target_domain.domain_id == "Date") or (target_domain.domain_id == "Integer") or (target_domain.domain_id == "Boolean") or (target_domain.domain_id == "Float")     ):
+        if target_domain and not (
+            (target_domain.domain_id == "String")
+            or (target_domain.domain_id == "Date")
+            or (target_domain.domain_id == "Integer")
+            or (target_domain.domain_id == "Boolean")
+            or (target_domain.domain_id == "Float")
+        ):
             if input_entity:
                 field_list = []
                 try:
                     field_list = sdd_context.bird_cube_structure_item_dictionary[input_entity.cube_structure_id]
                 except KeyError:
                     pass
                 for csi in field_list:
                     variable = csi.variable_id
                     if variable and variable.domain_id.domain_id == target_domain.domain_id:
-                            related_variables.append(csi)
+                        related_variables.append(csi)
         else:
             output_variable_name = output_item.variable_id.variable_id
             if output_variable_name:
                 if input_entity:
                     field_list = []
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/joins_meta_data/create_joins_meta_data.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/generate_test_data/traverser.py	2025-09-15 13:18:11.381923+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/generate_test_data/traverser.py	2025-09-21 17:07:36.591887+00:00
@@ -9,16 +9,16 @@
 #
 # Contributors:
 #    Neil Mackenzie - initial API and implementation
 #
 import os
-from pybirdai.regdna import ELAttribute, ELClass,ELReference,ELEnum
+from pybirdai.regdna import ELAttribute, ELClass, ELReference, ELEnum
 from pybirdai.process_steps.generate_test_data.ldm_utils import Utils
 
 
 class SubtypeExploder(object):
-    '''
+    """
     To make input layer test data for transformations, we want to have
     some ’useful concrete things’ like an example of ‘credit credit debt’,
     and  not  ’abstract things’ like ‘loans and advances’. We also want
     to have concise things, we don’t want to fill in every column of
     every IL table for a credit card debt, we want to fill in just the
@@ -51,15 +51,14 @@
     loan acting in the role of an off-balance sheet item), these non-useful
     combinations are a known issue in the LDM, and there is an attempt to
     reduce them or at least document which combinations do not really
     relate to real world concepts.
 
-    '''
-
-    def traverse(self, context, entity_name,
-                 show_all_columns_for_subtype_explosion):
-        '''
+    """
+
+    def traverse(self, context, entity_name, show_all_columns_for_subtype_explosion):
+        """
         Traverse through the LDM and recursively, for each delegate,
         show what are the subclasses of the delegate.
         Note that delegates represent an arc that represents a disjoint
         subtype in the SQLDeveloper LDM.
         We also imagine that any subclasses that are not in a delegated class
@@ -68,54 +67,72 @@
         The way we list this is using the concept of a discriminator which shows
         A chosen subclass, this in fact is the way that the forward engineered
         Input Layer represents a particular set of combined subclasses.
         If we have set the show_all_columns flag to be true, then we also
         show the columns required by each subclass.
-        '''
-        column_headers= []
-        input_layer_column_headers =[]
+        """
+        column_headers = []
+        input_layer_column_headers = []
         rows = []
         row = {}
-        entity = SubtypeExploder.find_class_with_name(self, context,entity_name)
+        entity = SubtypeExploder.find_class_with_name(self, context, entity_name)
         #  Instrument_role Over_the_counter_OTC_Derivative_role
         entity_list = []
         entity_list.append(entity)
 
-        SubtypeExploder.process_entity(self, context, [], entity, "", entity_list,column_headers,
-                                       input_layer_column_headers,row, rows,
-                                       show_all_columns_for_subtype_explosion)
+        SubtypeExploder.process_entity(
+            self,
+            context,
+            [],
+            entity,
+            "",
+            entity_list,
+            column_headers,
+            input_layer_column_headers,
+            row,
+            rows,
+            show_all_columns_for_subtype_explosion,
+        )
         full_or_summary = "_summary"
 
         if show_all_columns_for_subtype_explosion:
             full_or_summary = "_full"
 
         if show_all_columns_for_subtype_explosion:
 
-            f = open(context.output_directory + os.sep + 'csv' +
-                    os.sep + entity_name + '_discrimitor_combinations' +
-                    full_or_summary + '.csv',
-                    "a",  encoding='utf-8')
+            f = open(
+                context.output_directory
+                + os.sep
+                + "csv"
+                + os.sep
+                + entity_name
+                + "_discrimitor_combinations"
+                + full_or_summary
+                + ".csv",
+                "a",
+                encoding="utf-8",
+            )
             counter = 0
             # write the columns, which are all LDM attributes
             for column in column_headers:
                 if counter == 0:
                     f.write(column)
                     counter = 1
                 else:
 
-                    f.write(',' + column)
+                    f.write("," + column)
             f.write("\n")
             counter = 0
             # write the corresponding input layer columns also
             for column in input_layer_column_headers:
                 if counter == 0:
                     f.write(column)
                     counter = 1
                 else:
-                    f.write(',' + column)
+                    f.write("," + column)
             f.write("\n")
-            for the_row  in rows:
+            for the_row in rows:
                 counter = 0
                 for column in column_headers:
                     if counter == 0:
                         try:
                             f.write(the_row[column])
@@ -123,78 +140,83 @@
                             pass
                         counter = 1
                     else:
 
                         try:
-                            f.write(',' + the_row[column])
+                            f.write("," + the_row[column])
                         except KeyError:
-                            f.write(',')
+                            f.write(",")
 
                 f.write("\n")
 
         else:
-            f = open(context.output_directory + os.sep + 'csv' +
-                    os.sep + entity_name + '_discrimitor_combinations' +
-                    full_or_summary + '.csv',
-                    "a",  encoding='utf-8')
+            f = open(
+                context.output_directory
+                + os.sep
+                + "csv"
+                + os.sep
+                + entity_name
+                + "_discrimitor_combinations"
+                + full_or_summary
+                + ".csv",
+                "a",
+                encoding="utf-8",
+            )
             counter = 0
             # write the columns, which are all LDM attributes
             for column in column_headers:
                 if counter == 0:
                     f.write(column)
                     counter = 1
                 else:
 
-                    f.write(',' + column)
+                    f.write("," + column)
             f.write("\n")
             counter = 0
             # write the corresponding input layer columns also
             for column in input_layer_column_headers:
-                if column.endswith('_disc') or column.endswith('_delegate'):
+                if column.endswith("_disc") or column.endswith("_delegate"):
 
                     if counter == 0:
                         f.write(column)
                         counter = 1
                     else:
-                        f.write(',' + column)
+                        f.write("," + column)
             f.write("\n")
-            for the_row  in rows:
+            for the_row in rows:
                 counter = 0
                 for column in column_headers:
-                    if column.endswith('_disc') or column.endswith('_delegate'):
+                    if column.endswith("_disc") or column.endswith("_delegate"):
                         if counter == 0:
                             try:
                                 f.write(the_row[column])
                             except KeyError:
                                 pass
                             counter = 1
                         else:
                             try:
-                                f.write(',' + the_row[column])
+                                f.write("," + the_row[column])
                             except KeyError:
-                                f.write(',')
+                                f.write(",")
 
                 f.write("\n")
 
         il_table_names = SubtypeExploder.get_tables_from_column_name(self, input_layer_column_headers)
 
-
-
-
-
     def strip_special_characters(self, text):
-        if '$' in text:
+        if "$" in text:
             splitted = text.split("$")
-            if splitted[0] == 'X':
-                return '0'
+            if splitted[0] == "X":
+                return "0"
             else:
                 return splitted[0]
-        elif ':' in text:
+        elif ":" in text:
             splitted = text.split(":")
-            if splitted[0] == 'X':
-                return '0'
-            else:return splitted[0]
+            if splitted[0] == "X":
+                return "0"
+            else:
+                return splitted[0]
         else:
             return text
 
     def get_tables_from_column_name(self, columns):
 
@@ -216,115 +238,120 @@
 
         splitted = text.split(".")
 
         return splitted[1]
 
-    def post_process_row(self, context,column_headers,
-                                       input_layer_column_headers, the_row):
+    def post_process_row(self, context, column_headers, input_layer_column_headers, the_row):
 
         map = {}
-        identifier = ''
+        identifier = ""
         for column in column_headers:
-            if column.endswith('_delegate'):
-
-                column_prefix = column[0:column.index('_delegate')]
+            if column.endswith("_delegate"):
+
+                column_prefix = column[0 : column.index("_delegate")]
                 try:
                     identifier = identifier + the_row[column]
                     the_row[column_prefix] = the_row[column]
                 except KeyError:
-                        pass
+                    pass
 
         for column in column_headers:
-            if column.endswith('_disc'):
-
-                column_prefix = column[0:column.index('_disc')]
+            if column.endswith("_disc"):
+
+                column_prefix = column[0 : column.index("_disc")]
                 try:
                     identifier = identifier + the_row[column]
                     the_row[column_prefix] = the_row[column]
                 except KeyError:
-                        pass
-
-        counter =0
+                    pass
+
+        counter = 0
         for column in column_headers:
-                input_layer_column_header = input_layer_column_headers[counter]
-                try:
-                    map[input_layer_column_header] = the_row[column]
-                except KeyError:
-                        pass
-                counter = counter +1
-        map['IDENTIFIER'] = identifier
+            input_layer_column_header = input_layer_column_headers[counter]
+            try:
+                map[input_layer_column_header] = the_row[column]
+            except KeyError:
+                pass
+            counter = counter + 1
+        map["IDENTIFIER"] = identifier
         return map
 
-
-
-
-    def process_entity(self, context, discriminator_list, parent_entity, parent_entity_prefix, entity_combination,column_headers,input_layer_column_headers,
-                       row,rows,show_all_columns_for_subtype_explosion):
-        '''
+    def process_entity(
+        self,
+        context,
+        discriminator_list,
+        parent_entity,
+        parent_entity_prefix,
+        entity_combination,
+        column_headers,
+        input_layer_column_headers,
+        row,
+        rows,
+        show_all_columns_for_subtype_explosion,
+    ):
+        """
         For a specific entity, append the attributes of that entity
         to required column headers (if show_all_columns = True ).
         Also append to the row any discrimitars, showing that we have some
         subclasses to deal with or that we have delegates that can have
         subclasses that we need to deal with.
         Also, recursively deal with any of the subclasses or any subclasses
         of delegates
-        '''
+        """
         current_row = row
 
         count = 0
         for discriminator in discriminator_list:
             qualified_attribute_name = parent_entity_prefix + "." + discriminator.name
 
-            if not(qualified_attribute_name in column_headers):
+            if not (qualified_attribute_name in column_headers):
                 column_headers.append(qualified_attribute_name)
-                input_layer_column_name = SubtypeExploder.get_input_layer_column(self,discriminator)
+                input_layer_column_name = SubtypeExploder.get_input_layer_column(self, discriminator)
                 input_layer_column_headers.append(input_layer_column_name)
             current_row[qualified_attribute_name] = entity_combination[count].name
-            count = count +1
+            count = count + 1
         for entity in entity_combination:
             if show_all_columns_for_subtype_explosion:
                 references = SubtypeExploder.get_non_discriminator_references(self, context, entity)
                 for ref in references:
                     qualified_attribute_name = ""
                     if parent_entity_prefix == "":
                         qualified_attribute_name = entity.name + "." + ref.name
                     else:
                         qualified_attribute_name = parent_entity_prefix + "." + entity.name + "." + ref.name
 
-                    if not(qualified_attribute_name in column_headers):
+                    if not (qualified_attribute_name in column_headers):
                         column_headers.append(qualified_attribute_name)
-                        input_layer_column_name = SubtypeExploder.get_input_layer_column(self,ref)
+                        input_layer_column_name = SubtypeExploder.get_input_layer_column(self, ref)
                         input_layer_column_headers.append(input_layer_column_name)
-                        #current_row[qualified_attribute_name] = 'X'
-                        current_row[qualified_attribute_name] = SubtypeExploder.get_valid_example_value(self,ref)
+                        # current_row[qualified_attribute_name] = 'X'
+                        current_row[qualified_attribute_name] = SubtypeExploder.get_valid_example_value(self, ref)
 
                 attributes = SubtypeExploder.get_attributes(self, context, entity)
 
                 for attribute in attributes:
                     if parent_entity_prefix == "":
                         qualified_attribute_name = entity.name + "." + attribute.name
                     else:
                         qualified_attribute_name = parent_entity_prefix + "." + entity.name + "." + attribute.name
 
-                    if not(qualified_attribute_name in column_headers):
+                    if not (qualified_attribute_name in column_headers):
                         column_headers.append(qualified_attribute_name)
-                        input_layer_column_name = SubtypeExploder.get_input_layer_column(self,attribute)
+                        input_layer_column_name = SubtypeExploder.get_input_layer_column(self, attribute)
                         input_layer_column_headers.append(input_layer_column_name)
-                        #current_row[qualified_attribute_name] = 'X'
-                        current_row[qualified_attribute_name] = SubtypeExploder.get_valid_example_value(self,attribute)
+                        # current_row[qualified_attribute_name] = 'X'
+                        current_row[qualified_attribute_name] = SubtypeExploder.get_valid_example_value(self, attribute)
 
             discriminators = SubtypeExploder.get_discriminators(self, context, entity)
             columns = []
             # here we work out the possible combinations of entities for the discriminotors
             # these are stored as a set of columns, these reresent a grid that can be read
             # row by row, where 1 row represents one possible combination of entities.
 
             for discriminator in discriminators:
-                SubtypeExploder.enrich_discrimitor_columns(self, context, discriminator,columns)
-                #SubtypeExploder.print_combination_grid(self, columns)
-
-
+                SubtypeExploder.enrich_discrimitor_columns(self, context, discriminator, columns)
+                # SubtypeExploder.print_combination_grid(self, columns)
 
             if len(columns) > 0:
                 count = 0
                 for each_entity in columns[0]:
                     entity_combination = []
@@ -337,56 +364,60 @@
 
                     # for our row of csv data we have already worked out the value for sme columns
                     # we are going to copy/clone that row and add further information to it
                     # not that this is a recursive process.
                     current_row_detached_clone = current_row.copy()
-                    #current_row_detached_clone[qualified_attribute_name] = each_entity.name
+                    # current_row_detached_clone[qualified_attribute_name] = each_entity.name
 
                     new_parent_entity_prefix = ""
-                    if (parent_entity_prefix == ""):
+                    if parent_entity_prefix == "":
                         new_parent_entity_prefix = entity.name
                     else:
                         new_parent_entity_prefix = parent_entity_prefix + "." + entity.name
 
-
-                    SubtypeExploder.process_entity(self, context, discriminators, entity, new_parent_entity_prefix , entity_combination,
-                                                   column_headers, input_layer_column_headers,
-                                                   current_row_detached_clone,
-                                                   rows,
-                                                   show_all_columns_for_subtype_explosion)
+                    SubtypeExploder.process_entity(
+                        self,
+                        context,
+                        discriminators,
+                        entity,
+                        new_parent_entity_prefix,
+                        entity_combination,
+                        column_headers,
+                        input_layer_column_headers,
+                        current_row_detached_clone,
+                        rows,
+                        show_all_columns_for_subtype_explosion,
+                    )
                     count = count + 1
 
-
         rows.append(current_row)
-
 
     def get_entity_domain_code(self, context, entity):
         for domain in context.ldm_domains_package.eClassifiers:
-            if domain.name.endswith('Input_Layer__domain'):
+            if domain.name.endswith("Input_Layer__domain"):
                 for member in domain.eLiterals:
-                    if member.name ==  entity.name:
+                    if member.name == entity.name:
                         return member.literal
         for domain in context.ldm_domains_package.eClassifiers:
             for member in domain.eLiterals:
-                if member.name ==  entity.name:
+                if member.name == entity.name:
                     return member.literal
         for domain in context.ldm_domains_package.eClassifiers:
-            if domain.name.endswith('Input_Layer__domain'):
+            if domain.name.endswith("Input_Layer__domain"):
                 for member in domain.eLiterals:
-                    if member.name ==  entity.name + 's':
+                    if member.name == entity.name + "s":
                         return member.literal
         for domain in context.ldm_domains_package.eClassifiers:
             for member in domain.eLiterals:
-                if member.name ==  entity.name + 's':
+                if member.name == entity.name + "s":
                     return member.literal
-        return 'X'
-
+        return "X"
 
     def print_combination_grid(self, columns):
-        '''
+        """
         print the grid of combinations
-        '''
+        """
         column_count = 0
         for column in columns:
             row_count = 0
             for item in column:
                 row_count = row_count + 1
@@ -408,49 +439,48 @@
             for the_list in list_of_lists:
                 list_copy = the_list.copy()
                 first_one = True
                 for entity in entities:
                     for item in list_copy:
-                        if not(first_one):
+                        if not (first_one):
                             the_list.append(item)
                         discrimitors_entity_list.append(entity)
                     first_one = False
 
         list_of_lists.append(discrimitors_entity_list)
 
-
     def get_non_discriminator_references(self, context, entity):
-        '''
+        """
         get any non-containment references from the entity, which are not delegates.
         Note that the delegates can represent the arcs of the
         BIRD SQLDevelope model used to describe disjoint subtyping
-        '''
+        """
         reference_list = []
         for ref in entity.eStructuralFeatures:
-            if isinstance(ref,ELReference):
-                if not(ref.name.endswith('_delegate')) and not(SubtypeExploder.reference_is_containment(self,ref)):
-                        reference_list.append(ref)
+            if isinstance(ref, ELReference):
+                if not (ref.name.endswith("_delegate")) and not (SubtypeExploder.reference_is_containment(self, ref)):
+                    reference_list.append(ref)
 
         return reference_list
 
-    def reference_is_containment(self,ref):
-        '''
+    def reference_is_containment(self, ref):
+        """
         check if the reference is a containment reference
-        '''
+        """
         return_value = False
         annotation = Utils.get_annotation_with_source(ref, "relationship_type")
 
-        if not(annotation is None):
+        if not (annotation is None):
             details = annotation.details
 
             for detail in details.items:
                 if detail.key == "is_identifying_relationship":
                     return_value = True
 
         return return_value
 
-    #def get_non_discriminator_containment_references(self, context, entity):
+    # def get_non_discriminator_containment_references(self, context, entity):
     #    '''
     #    get any containment references from the entity, which are not delegates.
     #    Note that the delegates can represent the arcs of the
     #    BIRD SQLDevelope model used to describe disjoint subtyping
     #    '''
@@ -460,107 +490,107 @@
     #            if not(ref.name.endswith('_delegate')):
     #                reference_list.append(ref)
     #
     #    return reference_list
 
-    def get_input_layer_column(self,feature):
-        '''
+    def get_input_layer_column(self, feature):
+        """
         From the annotation find the the link to input layer column
-        '''
+        """
         return_value = "UNKNOWN"
         annotation = Utils.get_annotation_with_source(feature, "il_mapping")
 
-        if not(annotation is None):
+        if not (annotation is None):
             details = annotation.details
 
             for detail in details.items:
                 if detail.key == "il_column":
                     return_value = detail.value
 
         return return_value
 
-    def get_valid_example_value(self,feature):
-        '''
+    def get_valid_example_value(self, feature):
+        """
         From the annotation find the the link to input layer column
-        '''
+        """
         if isinstance(feature, ELAttribute):
             type = feature.eType
             if isinstance(type, ELEnum):
-                if len(type.eLiterals)>0:
-                    return str(type.eLiterals[0].literal) + '$' +  type.eLiterals[0].name
+                if len(type.eLiterals) > 0:
+                    return str(type.eLiterals[0].literal) + "$" + type.eLiterals[0].name
                 else:
-                    return 'X'
+                    return "X"
             else:
-                if type.name == "String" :
+                if type.name == "String":
                     return "EXAMPLE"
-                elif type.name == "double" :
+                elif type.name == "double":
                     return "123.00"
-                elif type.name == "int" :
+                elif type.name == "int":
                     return "345"
-                elif type.name == "Date" :
+                elif type.name == "Date":
                     return "2018-09-30"
-                elif type.name == "boolean" :
+                elif type.name == "boolean":
                     return "True"
                 else:
-                    return 'X'
+                    return "X"
         else:
-            return 'X'
-
-
+            return "X"
 
     def get_attributes(self, context, entity):
-        '''
+        """
         get the attributes of an entity
-        '''
+        """
         attribute_list = []
         for attribute in entity.eStructuralFeatures:
-            if isinstance(attribute,ELAttribute):
+            if isinstance(attribute, ELAttribute):
                 attribute_list.append(attribute)
         return attribute_list
 
     def get_discriminators(self, context, entity):
-        '''
+        """
         get any containment references, these represent identifying relationships
         in the LDM, and also arcs for disjoint subtyping.
-        '''
+        """
         reference_list = []
         for ref in entity.eStructuralFeatures:
-            if isinstance(ref,ELReference):
-                if SubtypeExploder.reference_is_containment(self,ref):
+            if isinstance(ref, ELReference):
+                if SubtypeExploder.reference_is_containment(self, ref):
                     # if we are refering to an entity in a differnt hierarchy then
                     # we don't consider it a discriminator.
-                    if not(SubtypeExploder.different_il_tables(self, context, entity,ref.eType)):
+                    if not (SubtypeExploder.different_il_tables(self, context, entity, ref.eType)):
                         reference_list.append(ref)
 
         # if there are any direct subclasses of this entity
         # (not including disjoint subclasses) then we create a
         # dummy discriminator for those
-        direct_subclasses = SubtypeExploder.get_subclasses(self,context,entity);
+        direct_subclasses = SubtypeExploder.get_subclasses(self, context, entity)
         if len(direct_subclasses) > 0:
             dummy_discrimitory = ELReference()
             try:
-                dummy_discrimitory.name = context.entity_to_arc_dictionary[entity.original_name.replace(',','_')][0] + "_disc"
+                dummy_discrimitory.name = (
+                    context.entity_to_arc_dictionary[entity.original_name.replace(",", "_")][0] + "_disc"
+                )
             except KeyError:
                 dummy_discrimitory.name = entity.name + "_disc"
             dummy_discrimitory.eType = entity
-            reference_list.append(dummy_discrimitory);
+            reference_list.append(dummy_discrimitory)
         return reference_list
 
     def different_hierarchies(self, context, class1, class2):
         annotation1 = Utils.get_annotation_with_source(class1, "entity_hierarchy")
         annotation2 = Utils.get_annotation_with_source(class2, "entity_hierarchy")
         hierarchy1 = "NA"
         hierarchy2 = "NA"
         if not (annotation1 is None):
             details1 = annotation1.details
             for map_entry in details1:
-                if map_entry.key == 'entity_hierarchy':
+                if map_entry.key == "entity_hierarchy":
                     hierarchy1 = map_entry.value
         if not (annotation2 is None):
             details2 = annotation2.details
             for map_entry in details2:
-                if map_entry.key == 'entity_hierarchy':
+                if map_entry.key == "entity_hierarchy":
                     hierarchy2 = map_entry.value
 
         if (hierarchy1 == "NA") or (hierarchy2 == "NA"):
             return False
         elif hierarchy1 == hierarchy2:
@@ -574,77 +604,75 @@
         il_table1 = "NA"
         il_table2 = "NA"
         if not (annotation1 is None):
             details1 = annotation1.details
             for map_entry in details1:
-                if map_entry.key == 'il_table':
+                if map_entry.key == "il_table":
                     il_table1 = map_entry.value
         if not (annotation2 is None):
             details2 = annotation2.details
             for map_entry in details2:
-                if map_entry.key == 'il_table':
+                if map_entry.key == "il_table":
                     il_table2 = map_entry.value
 
         if (il_table1 == "NA") or (il_table2 == "NA"):
             return False
         elif il_table1 == il_table2:
             return False
         else:
             return True
 
-
-    def get_possible_entities(self,context, discriminator):
-        '''
+    def get_possible_entities(self, context, discriminator):
+        """
         get any subclasses related to a delegate
         Note that the delegates can represent the arcs of the
         BIRD SQLDevelope model used to describe disjoint subtyping
         The discriminator is the name of the attribute holding the
         delegated class (or more likely, its subclasses)
-        '''
+        """
         entity_type = discriminator.eType
         class_list = []
         # for disjoint subtypes we always delegate to an abstract
         # class and provide concrete subclasses for each disjoint subclasses
         # we dont need to consider the abstract subclass in the processing.
         # For basic identifying (composition/containment) relationships
         # the entity may not be abstract, and so we should include it
         # in processing and its subtypes
-        if not (entity_type.eAbstract) and not(discriminator.name.endswith("_disc")):
+        if not (entity_type.eAbstract) and not (discriminator.name.endswith("_disc")):
             class_list.append(entity_type)
 
         # get the subclasses, for disjoint subtyping there will
         # only be direct subtypes. we should consider that for
         # identifying relationships there might be subtypes that have
         # subtypes, so we may need to amend this code to
         # deal with that situation....need to think exactly how the
         # combinations are and should be made in this case with a
         # a clear test
         for eclassifier in context.ldm_entities_package.eClassifiers:
-            if isinstance(eclassifier,ELClass):
+            if isinstance(eclassifier, ELClass):
                 if len(eclassifier.eSuperTypes) > 0:
                     if eclassifier.eSuperTypes[0] == entity_type:
 
                         class_list.append(eclassifier)
         return class_list
 
-
-    def get_subclasses(self,context, entity_type):
-        '''
+    def get_subclasses(self, context, entity_type):
+        """
         Get the subclasses of a class
-        '''
+        """
         subclass_list = []
         for eclassifier in context.ldm_entities_package.eClassifiers:
-            if isinstance(eclassifier,ELClass):
+            if isinstance(eclassifier, ELClass):
                 if len(eclassifier.eSuperTypes) > 0:
                     if eclassifier.eSuperTypes[0] == entity_type:
 
                         subclass_list.append(eclassifier)
         return subclass_list
 
     def find_class_with_name(self, context, name):
-        '''
+        """
         get the class with this name from the input tables package
-        '''
+        """
         for eclassifier in context.ldm_entities_package.eClassifiers:
             if isinstance(eclassifier, ELClass):
                 if eclassifier.name == name:
                     return eclassifier
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/generate_test_data/traverser.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/joins_meta_data/delete_joins_meta_data.py	2025-09-15 13:18:11.384389+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/joins_meta_data/delete_joins_meta_data.py	2025-09-21 17:07:36.599126+00:00
@@ -12,17 +12,18 @@
 #
 from pybirdai.context.sdd_context_django import SDDContext
 from pybirdai.models.bird_meta_data_model import *
 from django.apps import apps
 from django.db import connection
-from django.db.models.fields import CharField,DateTimeField,BooleanField,FloatField,BigIntegerField
+from django.db.models.fields import CharField, DateTimeField, BooleanField, FloatField, BigIntegerField
 import os
 import csv
 from typing import List, Any
 from django.db import connection
 
 from pybirdai.process_steps.joins_meta_data.ldm_search import ELDMSearch
+
 
 class TransformationMetaDataDestroyer:
     """
     A class for creating generation rules for reports and tables.
     """
@@ -45,24 +46,23 @@
             cursor.execute("DELETE FROM pybirdai_cube_structure where cube_structure_id like '%structure'")
             print("DELETE FROM pybirdai_cube_structure where cube_structure_id like '%structure'")
 
         # check if we should really delete all of these or just some.
 
-        for key,value in sdd_context.bird_cube_dictionary.items():
-            if key.endswith('_cube_structure'):
+        for key, value in sdd_context.bird_cube_dictionary.items():
+            if key.endswith("_cube_structure"):
                 del sdd_context.bird_cube_dictionary[key]
-        for key,value in sdd_context.bird_cube_structure_item_dictionary.items():
-            if key.endswith('_cube_structure'):
+        for key, value in sdd_context.bird_cube_structure_item_dictionary.items():
+            if key.endswith("_cube_structure"):
                 del sdd_context.bird_cube_structure_item_dictionary[key]
-        for key,value in sdd_context.bird_cube_structure_dictionary.items():
-            if key.endswith('_cube_structure'):
+        for key, value in sdd_context.bird_cube_structure_dictionary.items():
+            if key.endswith("_cube_structure"):
                 del sdd_context.bird_cube_structure_dictionary[key]
 
         sdd_context.combination_item_dictionary = {}
         sdd_context.combination_dictionary = {}
         sdd_context.combination_to_rol_cube_map = {}
-
 
     def delete_joins_meta_data(self, context: Any, sdd_context: Any, framework: str) -> None:
         """
         Generate generation rules for the given context and framework.
 
@@ -70,93 +70,89 @@
             context (Any): The context object containing necessary data.
             sdd_context (Any): The SDD context object.
             framework (str): The framework being used (e.g., "FINREP_REF").
         """
 
-        model_classes = [CUBE_LINK,
-        CUBE_STRUCTURE_ITEM_LINK]
+        model_classes = [CUBE_LINK, CUBE_STRUCTURE_ITEM_LINK]
 
         for model_cls in model_classes:
             self.delete_items_for_sqlite(model_cls)
-
-
 
         sdd_context.cube_link_dictionary = {}
         sdd_context.cube_link_to_foreign_cube_map = {}
         sdd_context.cube_link_to_join_identifier_map = {}
         sdd_context.cube_link_to_join_for_report_id_map = {}
         sdd_context.cube_structure_item_links_dictionary = {}
         sdd_context.cube_structure_item_link_to_cube_link_map = {}
 
-
     def delete_semantic_integration_meta_data(self, context: Any, sdd_context: Any, framework: str) -> None:
         """
         Generate generation rules for the given context and framework.
 
         Args:
             context (Any): The context object containing necessary data.
             sdd_context (Any): The SDD context object.
             framework (str): The framework being used (e.g., "FINREP_REF").
         """
 
-        model_classes = [MAPPING_TO_CUBE,
-        MAPPING_DEFINITION,
-        VARIABLE_MAPPING_ITEM,
-        VARIABLE_MAPPING,
-        MEMBER_MAPPING_ITEM,
-        MEMBER_MAPPING]
+        model_classes = [
+            MAPPING_TO_CUBE,
+            MAPPING_DEFINITION,
+            VARIABLE_MAPPING_ITEM,
+            VARIABLE_MAPPING,
+            MEMBER_MAPPING_ITEM,
+            MEMBER_MAPPING,
+        ]
 
         for model_cls in model_classes:
             self.delete_items_for_sqlite(model_cls)
-
-
 
         sdd_context.mapping_definition_dictionary = {}
         sdd_context.variable_mapping_dictionary = {}
         sdd_context.variable_mapping_item_dictionary = {}
         sdd_context.member_mapping_dictionary = {}
         sdd_context.member_mapping_items_dictionary = {}
         sdd_context.mapping_to_cube_dictionary = {}
 
-        TransformationMetaDataDestroyer.delete_joins_meta_data(self,context,sdd_context,framework)
-
-    def delete_items_for_sqlite(self,model_clss):
+        TransformationMetaDataDestroyer.delete_joins_meta_data(self, context, sdd_context, framework)
+
+    def delete_items_for_sqlite(self, model_clss):
         # Define allowed table names to prevent SQL injection
         ALLOWED_TABLES = {
-            'pybirdai_cube_link',
-            'pybirdai_cube_structure_item_link',
-            'pybirdai_cube_structure_item',
-            'pybirdai_cube_structure',
-            'pybirdai_cube',
-            'pybirdai_domain',
-            'pybirdai_variable',
-            'pybirdai_member',
-            'pybirdai_member_mapping',
-            'pybirdai_member_mapping_item',
-            'pybirdai_variable_mapping',
-            'pybirdai_variable_mapping_item',
-            'pybirdai_table_cell',
-            'pybirdai_cell_position',
-            'pybirdai_axis_ordinate',
-            'pybirdai_ordinate_item',
-            'pybirdai_mapping_definition',
-            'pybirdai_mapping_to_cube',
-            'pybirdai_table',
-            'pybirdai_axis',
-            'pybirdai_axis_ordinate',
-            'pybirdai_subdomain',
-            'pybirdai_subdomain_enumeration',
-            'pybirdai_facet_collection',
-            'pybirdai_maintenance_agency',
-            'pybirdai_framework',
-            'pybirdai_member_hierarchy',
-            'pybirdai_member_hierarchy_node',
-            'pybirdai_combination',
-            'pybirdai_combination_item',
-            'pybirdai_cube_to_combination'
+            "pybirdai_cube_link",
+            "pybirdai_cube_structure_item_link",
+            "pybirdai_cube_structure_item",
+            "pybirdai_cube_structure",
+            "pybirdai_cube",
+            "pybirdai_domain",
+            "pybirdai_variable",
+            "pybirdai_member",
+            "pybirdai_member_mapping",
+            "pybirdai_member_mapping_item",
+            "pybirdai_variable_mapping",
+            "pybirdai_variable_mapping_item",
+            "pybirdai_table_cell",
+            "pybirdai_cell_position",
+            "pybirdai_axis_ordinate",
+            "pybirdai_ordinate_item",
+            "pybirdai_mapping_definition",
+            "pybirdai_mapping_to_cube",
+            "pybirdai_table",
+            "pybirdai_axis",
+            "pybirdai_axis_ordinate",
+            "pybirdai_subdomain",
+            "pybirdai_subdomain_enumeration",
+            "pybirdai_facet_collection",
+            "pybirdai_maintenance_agency",
+            "pybirdai_framework",
+            "pybirdai_member_hierarchy",
+            "pybirdai_member_hierarchy_node",
+            "pybirdai_combination",
+            "pybirdai_combination_item",
+            "pybirdai_cube_to_combination",
         }
-        
+
         with connection.cursor() as cursor:
             cursor.execute("PRAGMA foreign_keys = 0;")
             for model_cls in model_clss:
                 table_name = f"pybirdai_{model_cls.__name__.lower()}"
                 if table_name in ALLOWED_TABLES:
@@ -200,11 +196,11 @@
             FRAMEWORK,
             MEMBER_HIERARCHY,
             MEMBER_HIERARCHY_NODE,
             COMBINATION,
             COMBINATION_ITEM,
-            CUBE_TO_COMBINATION
+            CUBE_TO_COMBINATION,
         ]
         self.delete_items_for_sqlite(model_classes)
 
         sdd_context.mapping_definition_dictionary = {}
         sdd_context.variable_mapping_dictionary = {}
@@ -242,17 +238,15 @@
         sdd_context.cube_link_to_join_identifier_map = {}
         sdd_context.cube_link_to_join_for_report_id_map = {}
         sdd_context.cube_structure_item_links_dictionary = {}
         sdd_context.cube_structure_item_link_to_cube_link_map = {}
         sdd_context.subdomain_dictionary = {}
-        sdd_context.subdomain_to_domain_map ={}
+        sdd_context.subdomain_to_domain_map = {}
         sdd_context.subdomain_enumeration_dictionary = {}
         sdd_context.members_that_are_nodes = {}
         sdd_context.member_plus_hierarchy_to_child_literals = {}
         sdd_context.domain_to_hierarchy_dictionary = {}
-
-
 
         SDDContext.mapping_definition_dictionary = {}
         SDDContext.variable_mapping_dictionary = {}
         SDDContext.variable_mapping_item_dictionary = {}
         SDDContext.bird_cube_structure_dictionary = {}
@@ -288,10 +282,10 @@
         SDDContext.cube_link_to_join_identifier_map = {}
         SDDContext.cube_link_to_join_for_report_id_map = {}
         SDDContext.cube_structure_item_links_dictionary = {}
         SDDContext.cube_structure_item_link_to_cube_link_map = {}
         SDDContext.subdomain_dictionary = {}
-        SDDContext.subdomain_to_domain_map ={}
+        SDDContext.subdomain_to_domain_map = {}
         SDDContext.subdomain_enumeration_dictionary = {}
         SDDContext.members_that_are_nodes = {}
         SDDContext.member_plus_hierarchy_to_child_literals = {}
         SDDContext.domain_to_hierarchy_dictionary = {}
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/joins_meta_data/delete_joins_meta_data.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/metadata_lineage/__init__.py	2025-09-02 15:09:38.716492+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/metadata_lineage/__init__.py	2025-09-21 17:07:36.601636+00:00
@@ -7,6 +7,6 @@
 #
 # SPDX-License-Identifier: EPL-2.0
 #
 # Contributors:
 #    Neil Mackenzie - initial API and implementation
-#
\ No newline at end of file
+#
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/metadata_lineage/__init__.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/joins_meta_data/member_hierarchy_service.py	2025-08-02 18:37:08.454693+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/joins_meta_data/member_hierarchy_service.py	2025-09-21 17:07:36.623323+00:00
@@ -16,28 +16,31 @@
 
 class MemberHierarchyService:
     def __init__(self):
         self._member_list_cache = {}
 
-    def is_member_a_node(self, sdd_context,member):
+    def is_member_a_node(self, sdd_context, member):
         return member.member_id in sdd_context.members_that_are_nodes
 
     def prepare_node_dictionaries_and_lists(self, sdd_context):
         sdd_context.members_that_are_nodes = set()
         sdd_context.member_plus_hierarchy_to_child_literals = {}
 
-
         for node in sdd_context.member_hierarchy_node_dictionary.values():
-            if node.parent_member_id and node.parent_member_id != '':
+            if node.parent_member_id and node.parent_member_id != "":
                 sdd_context.members_that_are_nodes.add(node.parent_member_id)
-                member_plus_hierarchy = f"{node.parent_member_id.member_id}:{node.member_hierarchy_id.member_hierarchy_id}"
+                member_plus_hierarchy = (
+                    f"{node.parent_member_id.member_id}:{node.member_hierarchy_id.member_hierarchy_id}"
+                )
 
                 if member_plus_hierarchy not in sdd_context.member_plus_hierarchy_to_child_literals:
                     sdd_context.member_plus_hierarchy_to_child_literals[member_plus_hierarchy] = [node.member_id]
                 else:
                     if node.member_id not in sdd_context.member_plus_hierarchy_to_child_literals[member_plus_hierarchy]:
-                        sdd_context.member_plus_hierarchy_to_child_literals[member_plus_hierarchy].append(node.member_id)
+                        sdd_context.member_plus_hierarchy_to_child_literals[member_plus_hierarchy].append(
+                            node.member_id
+                        )
 
         sdd_context.domain_to_hierarchy_dictionary = {}
         for hierarchy in MEMBER_HIERARCHY.objects.all():
             domain_id = hierarchy.domain_id
             if domain_id not in sdd_context.domain_to_hierarchy_dictionary:
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/joins_meta_data/member_hierarchy_service.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/input_model/import_database_to_sdd_model.py	2025-09-15 13:18:11.382797+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/input_model/import_database_to_sdd_model.py	2025-09-21 17:07:36.627209+00:00
@@ -12,21 +12,24 @@
 #
 
 from pybirdai.models.bird_meta_data_model import *
 from concurrent.futures import ThreadPoolExecutor
 from datetime import datetime
+
+
 class ImportDatabaseToSDDModel(object):
-    '''
+    """
     Class responsible for the import of  SDD csv files
     into an instance of the analaysis model
-    '''
+    """
+
     def import_sdd(self, sdd_context):
-        '''
+        """
         Import SDD csv files into an instance of the analysis model, using parallel execution
         where possible for better performance.
-        '''
-        #print the current time
+        """
+        # print the current time
         print("Starting import at:")
         print(datetime.now())
         # Basic setup - these need to run sequentially as later steps depend on them
         ImportDatabaseToSDDModel.create_maintenance_agencies(self, sdd_context)
         ImportDatabaseToSDDModel.create_frameworks(self, sdd_context)
@@ -36,11 +39,11 @@
         with ThreadPoolExecutor(max_workers=4) as executor:
             futures = [
                 executor.submit(ImportDatabaseToSDDModel.create_all_members, self, sdd_context),
                 executor.submit(ImportDatabaseToSDDModel.create_all_variables, self, sdd_context),
                 executor.submit(ImportDatabaseToSDDModel.create_all_rol_cube_structures, self, sdd_context),
-                executor.submit(ImportDatabaseToSDDModel.create_all_rol_cubes, self, sdd_context)
+                executor.submit(ImportDatabaseToSDDModel.create_all_rol_cubes, self, sdd_context),
             ]
             # Wait for all tasks to complete
             for future in futures:
                 future.result()
 
@@ -50,11 +53,11 @@
                 executor.submit(ImportDatabaseToSDDModel.create_all_rol_cube_structure_items, self, sdd_context),
                 executor.submit(ImportDatabaseToSDDModel.create_all_nonref_member_hierarchies, self, sdd_context),
                 executor.submit(ImportDatabaseToSDDModel.create_member_mappings, self, sdd_context),
                 executor.submit(ImportDatabaseToSDDModel.create_all_mapping_definitions, self, sdd_context),
                 executor.submit(ImportDatabaseToSDDModel.create_all_variable_mappings, self, sdd_context),
-                executor.submit(ImportDatabaseToSDDModel.create_combinations, self, sdd_context)
+                executor.submit(ImportDatabaseToSDDModel.create_combinations, self, sdd_context),
             ]
             for future in futures:
                 future.result()
 
         # Group 3 - Dependent on previous groups but independent of each other
@@ -63,184 +66,183 @@
                 executor.submit(ImportDatabaseToSDDModel.create_all_nonref_member_hierarchies_nodes, self, sdd_context),
                 executor.submit(ImportDatabaseToSDDModel.create_all_member_mapping_items, self, sdd_context),
                 executor.submit(ImportDatabaseToSDDModel.create_all_mapping_to_cubes, self, sdd_context),
                 executor.submit(ImportDatabaseToSDDModel.create_all_variable_mapping_items, self, sdd_context),
                 executor.submit(ImportDatabaseToSDDModel.create_combination_items, self, sdd_context),
-                executor.submit(ImportDatabaseToSDDModel.create_cube_to_combination, self, sdd_context)
+                executor.submit(ImportDatabaseToSDDModel.create_cube_to_combination, self, sdd_context),
             ]
             for future in futures:
                 future.result()
 
         # Group 4 - Report-related items that can run in parallel
         with ThreadPoolExecutor(max_workers=4) as executor:
             futures = [
                 executor.submit(ImportDatabaseToSDDModel.create_report_tables, self, sdd_context),
                 executor.submit(ImportDatabaseToSDDModel.create_axis, self, sdd_context),
                 executor.submit(ImportDatabaseToSDDModel.create_table_cells, self, sdd_context),
-                executor.submit(ImportDatabaseToSDDModel.create_cube_links, self, sdd_context)
+                executor.submit(ImportDatabaseToSDDModel.create_cube_links, self, sdd_context),
             ]
             for future in futures:
                 future.result()
 
         # Group 5 - Final dependent items
         with ThreadPoolExecutor(max_workers=4) as executor:
             futures = [
                 executor.submit(ImportDatabaseToSDDModel.create_axis_ordinates, self, sdd_context),
                 executor.submit(ImportDatabaseToSDDModel.create_ordinate_items, self, sdd_context),
                 executor.submit(ImportDatabaseToSDDModel.create_cell_positions, self, sdd_context),
-                executor.submit(ImportDatabaseToSDDModel.create_cube_structure_item_links, self, sdd_context)
+                executor.submit(ImportDatabaseToSDDModel.create_cube_structure_item_links, self, sdd_context),
             ]
             for future in futures:
                 future.result()
 
-
-            #print the current time
+            # print the current time
         print("Ending import at:")
         print(datetime.now())
 
     def import_sdd_for_filters(self, sdd_context, tables_to_import):
-        '''
+        """
         Import only the necessary tables for filters from the database.
         This is a faster version of import_sdd that only imports tables needed for filters.
         Tables are imported in groups based on their dependencies.
-        '''
+        """
         print("Starting selective import at:")
         print(datetime.now())
 
         # Group 1 - Base tables with no dependencies
-        if 'MAINTENANCE_AGENCY' in tables_to_import:
+        if "MAINTENANCE_AGENCY" in tables_to_import:
             ImportDatabaseToSDDModel.create_maintenance_agencies(self, sdd_context)
-        if 'DOMAIN' in tables_to_import:
+        if "DOMAIN" in tables_to_import:
             ImportDatabaseToSDDModel.create_all_domains(self, sdd_context)
 
         # Group 2 - Tables that depend only on DOMAIN and MAINTENANCE_AGENCY
         with ThreadPoolExecutor(max_workers=4) as executor:
             futures = []
-            if 'MEMBER' in tables_to_import:
+            if "MEMBER" in tables_to_import:
                 futures.append(executor.submit(ImportDatabaseToSDDModel.create_all_members, self, sdd_context))
-            if 'VARIABLE' in tables_to_import:
+            if "VARIABLE" in tables_to_import:
                 futures.append(executor.submit(ImportDatabaseToSDDModel.create_all_variables, self, sdd_context))
-            if 'MEMBER_HIERARCHY' in tables_to_import:
-                futures.append(executor.submit(ImportDatabaseToSDDModel.create_all_nonref_member_hierarchies, self, sdd_context))
-            if 'CUBE' in tables_to_import:
+            if "MEMBER_HIERARCHY" in tables_to_import:
+                futures.append(
+                    executor.submit(ImportDatabaseToSDDModel.create_all_nonref_member_hierarchies, self, sdd_context)
+                )
+            if "CUBE" in tables_to_import:
                 futures.append(executor.submit(ImportDatabaseToSDDModel.create_all_rol_cubes, self, sdd_context))
             for future in futures:
                 future.result()
 
         # Group 3 - Tables that depend on MEMBER and MEMBER_HIERARCHY
-        if 'MEMBER_HIERARCHY_NODE' in tables_to_import:
+        if "MEMBER_HIERARCHY_NODE" in tables_to_import:
             ImportDatabaseToSDDModel.create_all_nonref_member_hierarchies_nodes(self, sdd_context)
 
         # Group 4 - Tables that depend on VARIABLE and MAINTENANCE_AGENCY
-        if 'COMBINATION' in tables_to_import:
+        if "COMBINATION" in tables_to_import:
             ImportDatabaseToSDDModel.create_combinations(self, sdd_context)
 
         # Group 5 - Tables that depend on COMBINATION
-        if 'COMBINATION_ITEM' in tables_to_import:
+        if "COMBINATION_ITEM" in tables_to_import:
             ImportDatabaseToSDDModel.create_combination_items(self, sdd_context)
 
         # Group 6 - Tables that depend on both CUBE and COMBINATION
-        if 'CUBE_TO_COMBINATION' in tables_to_import:
+        if "CUBE_TO_COMBINATION" in tables_to_import:
             ImportDatabaseToSDDModel.create_cube_to_combination(self, sdd_context)
 
         print("Ending selective import at:")
         print(datetime.now())
 
     def import_sdd_for_joins(self, sdd_context, tables_to_import):
-        '''
+        """
         Import only the necessary tables for joins from the database.
         This is a faster version of import_sdd that only imports tables needed for joins.
         Tables are imported in groups based on their dependencies.
-        '''
+        """
         print("Starting selective import at:")
         print(datetime.now())
 
         # Group 1 - Base tables with no dependencies
-        if 'MAINTENANCE_AGENCY' in tables_to_import:
+        if "MAINTENANCE_AGENCY" in tables_to_import:
             ImportDatabaseToSDDModel.create_maintenance_agencies(self, sdd_context)
-        if 'DOMAIN' in tables_to_import:
+        if "DOMAIN" in tables_to_import:
             ImportDatabaseToSDDModel.create_all_domains(self, sdd_context)
 
         # Group 2 - Tables that depend only on DOMAIN and MAINTENANCE_AGENCY
         with ThreadPoolExecutor(max_workers=2) as executor:
             futures = []
-            if 'VARIABLE' in tables_to_import:
+            if "VARIABLE" in tables_to_import:
                 futures.append(executor.submit(ImportDatabaseToSDDModel.create_all_variables, self, sdd_context))
-            if 'CUBE' in tables_to_import:
+            if "CUBE" in tables_to_import:
                 futures.append(executor.submit(ImportDatabaseToSDDModel.create_all_rol_cubes, self, sdd_context))
             for future in futures:
                 future.result()
 
         # Group 3 - Tables that depend on CUBE_STRUCTURE and VARIABLE
-        if 'CUBE_STRUCTURE_ITEM' in tables_to_import:
+        if "CUBE_STRUCTURE_ITEM" in tables_to_import:
             ImportDatabaseToSDDModel.create_all_rol_cube_structure_items(self, sdd_context)
 
         # Group 4 - Tables that depend on CUBE
-        if 'CUBE_LINK' in tables_to_import:
+        if "CUBE_LINK" in tables_to_import:
             ImportDatabaseToSDDModel.create_cube_links(self, sdd_context)
 
         # Group 5 - Tables that depend on CUBE_LINK and CUBE_STRUCTURE_ITEM
-        if 'CUBE_STRUCTURE_ITEM_LINK' in tables_to_import:
+        if "CUBE_STRUCTURE_ITEM_LINK" in tables_to_import:
             ImportDatabaseToSDDModel.create_cube_structure_item_links(self, sdd_context)
 
         print("Ending selective import at:")
         print(datetime.now())
 
     def create_all_mapping_definitions(self, context):
-        '''
+        """
         import all the mapping definitions
-        '''
+        """
         context.mapping_definition_dictionary = {}
         for mapping_definition in MAPPING_DEFINITION.objects.all():
-            context.mapping_definition_dictionary[
-                mapping_definition.mapping_id] = mapping_definition
+            context.mapping_definition_dictionary[mapping_definition.mapping_id] = mapping_definition
 
     def create_all_variable_mappings(self, context):
-        '''
+        """
         import all the variable mappings
-        '''
+        """
         context.variable_mapping_dictionary = {}
         for variable_mapping in VARIABLE_MAPPING.objects.all():
-            context.variable_mapping_dictionary[
-                variable_mapping.variable_mapping_id] = variable_mapping
+            context.variable_mapping_dictionary[variable_mapping.variable_mapping_id] = variable_mapping
 
     def create_all_variable_mapping_items(self, context):
-        '''
+        """
         import all the variable mapping items
-        '''
+        """
         context.variable_mapping_item_dictionary = {}
         for variable_mapping_item in VARIABLE_MAPPING_ITEM.objects.all():
             try:
                 variable_mapping_list = context.variable_mapping_item_dictionary[
-                    variable_mapping_item.variable_mapping_id.variable_mapping_id]
+                    variable_mapping_item.variable_mapping_id.variable_mapping_id
+                ]
                 variable_mapping_list.append(variable_mapping_item)
             except KeyError:
                 context.variable_mapping_item_dictionary[
                     variable_mapping_item.variable_mapping_id.variable_mapping_id
                 ] = [variable_mapping_item]
 
     def create_all_rol_cube_structures(self, context):
-        '''
+        """
         import all the rol cube structures
-        '''
+        """
         context.bird_cube_structure_dictionary = {}
         for rol_cube_structure in CUBE_STRUCTURE.objects.all():
-            context.bird_cube_structure_dictionary[
-                rol_cube_structure.cube_structure_id] = rol_cube_structure
+            context.bird_cube_structure_dictionary[rol_cube_structure.cube_structure_id] = rol_cube_structure
 
     def create_all_rol_cubes(self, context):
-        '''
+        """
         import all the rol cubes
-        '''
+        """
         context.bird_cube_dictionary = {}
         for rol_cube in CUBE.objects.all():
             context.bird_cube_dictionary[rol_cube.cube_id] = rol_cube
 
     def create_all_rol_cube_structure_items(self, context):
-        '''
+        """
         import all the rol cube structure items
-        '''
+        """
         context.bird_cube_structure_item_dictionary = {}
         for rol_cube_structure_item in CUBE_STRUCTURE_ITEM.objects.all():
             try:
                 context.bird_cube_structure_item_dictionary[
                     rol_cube_structure_item.cube_structure_id.cube_structure_id
@@ -249,64 +251,61 @@
                 context.bird_cube_structure_item_dictionary[
                     rol_cube_structure_item.cube_structure_id.cube_structure_id
                 ] = [rol_cube_structure_item]
 
     def create_all_mapping_to_cubes(self, context):
-        '''
+        """
         import all the mapping to cubes
-        '''
+        """
         context.mapping_to_cube_dictionary = {}
         for mapping_to_cube in MAPPING_TO_CUBE.objects.all():
             try:
-                mapping_to_cube_list = context.mapping_to_cube_dictionary[
-                    mapping_to_cube.cube_mapping_id]
+                mapping_to_cube_list = context.mapping_to_cube_dictionary[mapping_to_cube.cube_mapping_id]
                 mapping_to_cube_list.append(mapping_to_cube)
             except KeyError:
-                context.mapping_to_cube_dictionary[
-                    mapping_to_cube.cube_mapping_id] = [mapping_to_cube]
+                context.mapping_to_cube_dictionary[mapping_to_cube.cube_mapping_id] = [mapping_to_cube]
 
     def create_maintenance_agencies(self, context):
-        '''
+        """
         Import all maintenance agencies
-        '''
+        """
         context.agency_dictionary = {}
         for agency in MAINTENANCE_AGENCY.objects.all():
             context.agency_dictionary[agency.maintenance_agency_id] = agency
 
     def create_frameworks(self, context):
-        '''
+        """
         Import all frameworks
-        '''
+        """
         context.framework_dictionary = {}
         for framework in FRAMEWORK.objects.all():
             context.framework_dictionary[framework.framework_id] = framework
 
     def create_all_domains(self, context):
-        '''
+        """
         import all the domains
-        '''
+        """
         context.domain_dictionary = {}
         for domain in DOMAIN.objects.all():
             context.domain_dictionary[domain.domain_id] = domain
 
-
     def create_all_members(self, context):
-        '''
+        """
         Import all the members
-        '''
+        """
         context.member_dictionary = {}
         context.member_id_to_domain_map = {}
         context.member_id_to_member_code_map = {}
         for member in MEMBER.objects.all():
             context.member_dictionary[member.member_id] = member
             context.member_id_to_domain_map[member] = member.domain_id
             context.member_id_to_member_code_map[member.member_id] = member.code
 
     def create_all_variables(self, context):
-        '''
+        """
         import all the variables
-        '''
+        """
         context.variable_dictionary = {}
         context.variable_to_domain_map = {}
         context.variable_to_long_names_map = {}
         context.variable_to_primary_concept_map = {}
         for variable in VARIABLE.objects.all():
@@ -314,175 +313,160 @@
             context.variable_to_domain_map[variable.variable_id] = variable.domain_id
             context.variable_to_long_names_map[variable.variable_id] = variable.name
             context.variable_to_primary_concept_map[variable.variable_id] = variable.primary_concept
 
     def create_all_nonref_member_hierarchies(self, context):
-        '''
+        """
         Import all non-reference member hierarchies
-        '''
+        """
         context.member_hierarchy_dictionary = {}
         for hierarchy in MEMBER_HIERARCHY.objects.all():
-            context.member_hierarchy_dictionary[
-                hierarchy.member_hierarchy_id] = hierarchy
-
+            context.member_hierarchy_dictionary[hierarchy.member_hierarchy_id] = hierarchy
 
     def create_all_nonref_member_hierarchies_nodes(self, context):
-        '''
+        """
         Import all non-reference member hierarchy nodes
-        '''
+        """
         context.member_hierarchy_node_dictionary = {}
         for hierarchy_node in MEMBER_HIERARCHY_NODE.objects.all():
             member = hierarchy_node.member_id
-            member_name = 'None'
-            if not(member is None):
+            member_name = "None"
+            if not (member is None):
                 member_name = member.member_id
             context.member_hierarchy_node_dictionary[
                 hierarchy_node.member_hierarchy_id.member_hierarchy_id + ":" + member_name
             ] = hierarchy_node
 
-    def create_report_tables (self, context):
-        '''
+    def create_report_tables(self, context):
+        """
         import all the tables from the rendering package
-        '''
+        """
         context.report_tables_dictionary = {}
         for table in TABLE.objects.all():
             context.report_tables_dictionary[table.table_id] = table
 
-    def create_axis (self, context):
-        '''
+    def create_axis(self, context):
+        """
         import all the axes from the rendering package
-        '''
+        """
         context.axis_dictionary = {}
         for axis in AXIS.objects.all():
             context.axis_dictionary[axis.axis_id] = axis
 
     def create_axis_ordinates(self, context):
-        '''
+        """
         import all the axis_ordinate from the rendering package
-        '''
+        """
         context.axis_ordinate_dictionary = {}
         for axis_ordinate in AXIS_ORDINATE.objects.all():
-            context.axis_ordinate_dictionary[
-                axis_ordinate.axis_ordinate_id] = axis_ordinate
+            context.axis_ordinate_dictionary[axis_ordinate.axis_ordinate_id] = axis_ordinate
 
     def create_ordinate_items(self, sdd_context):
-        '''
+        """
         import all the axis_ordinate from the rendering package
-        '''
+        """
         sdd_context.axis_ordinate_to_ordinate_items_map = {}
         for ordinate_item in ORDINATE_ITEM.objects.all():
             try:
                 ordinate_item_list = sdd_context.axis_ordinate_to_ordinate_items_map[
-                    ordinate_item.axis_ordinate_id.axis_ordinate_id]
+                    ordinate_item.axis_ordinate_id.axis_ordinate_id
+                ]
                 ordinate_item_list.append(ordinate_item)
             except KeyError:
-                sdd_context.axis_ordinate_to_ordinate_items_map[
-                    ordinate_item.axis_ordinate_id.axis_ordinate_id] = [ordinate_item]
+                sdd_context.axis_ordinate_to_ordinate_items_map[ordinate_item.axis_ordinate_id.axis_ordinate_id] = [
+                    ordinate_item
+                ]
 
     def create_table_cells(self, context):
-        '''
+        """
         import all the axis_ordinate from the rendering package
-        '''
+        """
         context.table_cell_dictionary = {}
         context.table_to_table_cell_dictionary = {}
         for table_cell in TABLE_CELL.objects.all():
             context.table_cell_dictionary[table_cell.cell_id] = table_cell
 
-
             table_cell_list = []
             try:
-                table_cell_list = context.table_to_table_cell_dictionary[
-                    table_cell.table_id]
-            except KeyError:
-                context.table_to_table_cell_dictionary[
-                    table_cell.table_id] = table_cell_list
+                table_cell_list = context.table_to_table_cell_dictionary[table_cell.table_id]
+            except KeyError:
+                context.table_to_table_cell_dictionary[table_cell.table_id] = table_cell_list
 
             table_cell_list.append(table_cell)
 
-
     def create_cell_positions(self, context):
-        '''
+        """
         import all the axis_ordinate from the rendering package
-        '''
+        """
         context.cell_positions_dictionary = {}
         for cell_position in CELL_POSITION.objects.all():
             try:
-                cell_position_list = context.cell_positions_dictionary[
-                    cell_position.cell_id.cell_id]
+                cell_position_list = context.cell_positions_dictionary[cell_position.cell_id.cell_id]
                 cell_position_list.append(cell_position)
             except KeyError:
 
-                context.cell_positions_dictionary[
-                    cell_position.cell_id.cell_id] = [cell_position]
-
+                context.cell_positions_dictionary[cell_position.cell_id.cell_id] = [cell_position]
 
     def create_member_mappings(self, context):
-        '''
+        """
         Import all the member mappings from the rendering package
-        '''
+        """
         context.member_mapping_dictionary = {}
         for member_mapping in MEMBER_MAPPING.objects.all():
-            context.member_mapping_dictionary[
-                member_mapping.member_mapping_id] = member_mapping
-
+            context.member_mapping_dictionary[member_mapping.member_mapping_id] = member_mapping
 
     def create_all_member_mapping_items(self, context):
-        ''' import all the member mappings from the rendering package'''
+        """import all the member mappings from the rendering package"""
         context.member_mapping_items_dictionary = {}
         for member_mapping_item in MEMBER_MAPPING_ITEM.objects.all():
             try:
                 member_mapping_list = context.member_mapping_items_dictionary[
-                    member_mapping_item.member_mapping_id.member_mapping_id]
+                    member_mapping_item.member_mapping_id.member_mapping_id
+                ]
                 member_mapping_list.append(member_mapping_item)
             except KeyError:
-                context.member_mapping_items_dictionary[
-                    member_mapping_item.member_mapping_id.member_mapping_id
-                ] = [member_mapping_item]
+                context.member_mapping_items_dictionary[member_mapping_item.member_mapping_id.member_mapping_id] = [
+                    member_mapping_item
+                ]
 
     def create_combination_items(self, context):
-        '''
+        """
         Import all the combination items
-        '''
+        """
         context.combination_item_dictionary = {}
         for combination_item in COMBINATION_ITEM.objects.all():
             try:
                 combination_item_list = context.combination_item_dictionary[
-                    combination_item.combination_id.combination_id]
+                    combination_item.combination_id.combination_id
+                ]
                 combination_item_list.append(combination_item)
             except KeyError:
-                context.combination_item_dictionary[
-                    combination_item.combination_id.combination_id
-                ] = [combination_item]
+                context.combination_item_dictionary[combination_item.combination_id.combination_id] = [combination_item]
 
     def create_combinations(self, context):
-        '''
+        """
         Import all the combinations
-        '''
+        """
         context.combination_dictionary = {}
         for combination in COMBINATION.objects.all():
-            context.combination_dictionary[
-                combination.combination_id] = combination
+            context.combination_dictionary[combination.combination_id] = combination
 
     def create_cube_to_combination(self, context):
-        '''
+        """
         Import all the cube to combination
-        '''
+        """
         context.combination_to_rol_cube_map = {}
         for cube_to_combination in CUBE_TO_COMBINATION.objects.all():
             try:
-                context.combination_to_rol_cube_map[
-                    cube_to_combination.cube_id.cube_id
-                ].append(cube_to_combination)
-            except KeyError:
-                context.combination_to_rol_cube_map[
-                    cube_to_combination.cube_id.cube_id
-                ] = [cube_to_combination]
+                context.combination_to_rol_cube_map[cube_to_combination.cube_id.cube_id].append(cube_to_combination)
+            except KeyError:
+                context.combination_to_rol_cube_map[cube_to_combination.cube_id.cube_id] = [cube_to_combination]
 
     def create_cube_links(self, context):
-        '''
+        """
         Import all the cube links
-        '''
+        """
         context.cube_link_dictionary = {}
         context.cube_link_to_foreign_cube_map = {}
         context.cube_link_to_join_identifier_map = {}
         context.cube_link_to_join_for_report_id_map = {}
         for cube_link in CUBE_LINK.objects.all():
@@ -502,19 +486,22 @@
             try:
                 context.cube_link_to_join_for_report_id_map[join_for_report_id].append(cube_link)
             except KeyError:
                 context.cube_link_to_join_for_report_id_map[join_for_report_id] = [cube_link]
 
-
     def create_cube_structure_item_links(self, context):
-        '''
+        """
         Import all the cube structure item links
-        '''
+        """
         context.cube_structure_item_links_dictionary = {}
         context.cube_structure_item_link_to_cube_link_map = {}
         for cube_structure_item_link in CUBE_STRUCTURE_ITEM_LINK.objects.all():
-            context.cube_structure_item_links_dictionary[cube_structure_item_link.cube_structure_item_link_id] = cube_structure_item_link
+            context.cube_structure_item_links_dictionary[cube_structure_item_link.cube_structure_item_link_id] = (
+                cube_structure_item_link
+            )
             cube_link = cube_structure_item_link.cube_link_id
             try:
-                context.cube_structure_item_link_to_cube_link_map[cube_link.cube_link_id].append(cube_structure_item_link)
+                context.cube_structure_item_link_to_cube_link_map[cube_link.cube_link_id].append(
+                    cube_structure_item_link
+                )
             except KeyError:
                 context.cube_structure_item_link_to_cube_link_map[cube_link.cube_link_id] = [cube_structure_item_link]
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/input_model/import_database_to_sdd_model.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/joins_meta_data/main_category_finder.py	2025-09-15 13:18:11.385114+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/joins_meta_data/main_category_finder.py	2025-09-21 17:07:36.639451+00:00
@@ -21,50 +21,49 @@
 # SPDE-License-Identifier: EPL-2.0
 #
 # Contributors:
 #    Neil Mackenzie - initial API and implementation
 #
-'''
+"""
 @author: Neil
-'''
+"""
 import csv
 import os
 from pybirdai.process_steps.website_to_sddmodel.import_website_to_sdd_model_django import ImportWebsiteToSDDModel
 
+
 class MainCategoryFinder(object):
-    '''
+    """
     This class is responsible for creating maps of information
     related to the EBA main category
-    '''
-    def create_report_to_main_category_maps(self, context, sdd_context, framework,
-                                            reporting_framework_version):
-        '''
+    """
+
+    def create_report_to_main_category_maps(self, context, sdd_context, framework, reporting_framework_version):
+        """
         Create maps of information related to the EBA main category
-        '''
-
-        MainCategoryFinder.create_main_category_to_name_map(self, context,
-                                                            sdd_context, framework)
+        """
+
+        MainCategoryFinder.create_main_category_to_name_map(self, context, sdd_context, framework)
         MainCategoryFinder.create_report_to_main_category_map(
-            self, context, sdd_context, framework, reporting_framework_version)
-        #MainCategoryFinder.create_draft_join_for_product_file(
+            self, context, sdd_context, framework, reporting_framework_version
+        )
+        # MainCategoryFinder.create_draft_join_for_product_file(
         #    self, context, sdd_context, framework)
-        MainCategoryFinder.create_join_for_product_to_main_category_map(
-            self, context, sdd_context, framework)
-        MainCategoryFinder.create_il_tables_for_main_category_map(
-            self, context, sdd_context, framework)
-        MainCategoryFinder.create_join_for_products_for_main_category_map(
-            self, context, sdd_context, framework)
+        MainCategoryFinder.create_join_for_product_to_main_category_map(self, context, sdd_context, framework)
+        MainCategoryFinder.create_il_tables_for_main_category_map(self, context, sdd_context, framework)
+        MainCategoryFinder.create_join_for_products_for_main_category_map(self, context, sdd_context, framework)
 
     def create_main_category_to_name_map(self, context, sdd_context, framework):
-        '''
+        """
         Create a map of EBA main category code to its user-friendly display name
-        '''
-        file_location = os.path.join(context.file_directory, "joins_configuration",
-                                     f"join_for_product_to_reference_category_{framework}.csv")
-
-        with open(file_location, encoding='utf-8') as csvfile:
-            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
+        """
+        file_location = os.path.join(
+            context.file_directory, "joins_configuration", f"join_for_product_to_reference_category_{framework}.csv"
+        )
+
+        with open(file_location, encoding="utf-8") as csvfile:
+            filereader = csv.reader(csvfile, delimiter=",", quotechar='"')
             next(filereader)  # Skip header
             for main_category, main_category_name, *_ in filereader:
                 if framework == "FINREP_REF":
                     context.main_category_to_name_map_finrep[main_category] = main_category_name
                 elif framework == "AE_REF":
@@ -82,70 +81,69 @@
             list: Deduplicated list of members.
         """
         return list({item.member_id for item in member_mapping_items})
 
     def create_join_for_product_to_main_category_map(self, context, sdd_context, framework):
-        '''
+        """
         Create a map from join for products to main categories
-        '''
-        file_location = os.path.join(context.file_directory, "joins_configuration",
-                                     f"join_for_product_main_category_{framework}.csv")
+        """
+        file_location = os.path.join(
+            context.file_directory, "joins_configuration", f"join_for_product_main_category_{framework}.csv"
+        )
         join_for_products_to_main_category_map = (
-            context.join_for_products_to_main_category_map_finrep if framework == "FINREP_REF"
+            context.join_for_products_to_main_category_map_finrep
+            if framework == "FINREP_REF"
             else context.join_for_products_to_main_category_map_ae
         )
 
-        with open(file_location, encoding='utf-8') as csvfile:
-            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
+        with open(file_location, encoding="utf-8") as csvfile:
+            filereader = csv.reader(csvfile, delimiter=",", quotechar='"')
             next(filereader)  # Skip header
             for main_category, _, join_for_product in filereader:
                 join_for_products_to_main_category_map[join_for_product] = main_category
 
-    def create_report_to_main_category_map(self, context, sdd_context,
-                                                       full_framework_name,
-                                                       reporting_framework_version):
-        '''
+    def create_report_to_main_category_map(
+        self, context, sdd_context, full_framework_name, reporting_framework_version
+    ):
+        """
         Look through the generated report and create a map of reports to main categories
-        '''
+        """
 
         main_categories_in_scope = (
-            context.main_categories_in_scope_finrep if full_framework_name == "FINREP_REF"
+            context.main_categories_in_scope_finrep
+            if full_framework_name == "FINREP_REF"
             else context.main_categories_in_scope_ae
         )
         for cube_name, combination_list in sdd_context.combination_to_rol_cube_map.items():
             for combination in combination_list:
-                self._process_combination(context, sdd_context, combination,
-                                          cube_name, main_categories_in_scope)
-
-    def _process_combination(self, context, sdd_context, combination,
-                             cube_name, main_categories_in_scope):
+                self._process_combination(context, sdd_context, combination, cube_name, main_categories_in_scope)
+
+    def _process_combination(self, context, sdd_context, combination, cube_name, main_categories_in_scope):
         """
         Process a single combination and update main categories.
 
         Args:
             context: The context object.
             sdd_context: The SDD context object.
             combination: The combination to process.
             cube_name (str): The name of the cube.
             main_categories_in_scope (list): List of main categories in scope.
         """
-        combination_items = sdd_context.combination_item_dictionary.get(
-            combination.combination_id.combination_id, []
-        )
+        combination_items = sdd_context.combination_item_dictionary.get(combination.combination_id.combination_id, [])
 
         cell_instrmnt_ids_list = self._get_cell_instrmnt_ids(combination_items)
         cell_scrty_derivative_list = self._get_cell_scrty_derivative_ids(combination_items)
         if len(cell_instrmnt_ids_list) > 0:
-            self._update_categories(context, cube_name, cell_instrmnt_ids_list,
-                                    main_categories_in_scope, "TYP_INSTRMNT")
-        elif len(cell_scrty_derivative_list)>0:
-            self._update_categories(context, cube_name, cell_scrty_derivative_list,
-                                    main_categories_in_scope, "SCRTY_EXCHNG_TRDBL_DRVTV_TYP")
+            self._update_categories(
+                context, cube_name, cell_instrmnt_ids_list, main_categories_in_scope, "TYP_INSTRMNT"
+            )
+        elif len(cell_scrty_derivative_list) > 0:
+            self._update_categories(
+                context, cube_name, cell_scrty_derivative_list, main_categories_in_scope, "SCRTY_EXCHNG_TRDBL_DRVTV_TYP"
+            )
         else:
-            self._process_accounting_items(context, combination_items,
-                                           cube_name, main_categories_in_scope)
-
+            self._process_accounting_items(context, combination_items, cube_name, main_categories_in_scope)
 
     def _get_cell_instrmnt_ids(self, combination_items):
         """
         Get cell instrument IDs from combination items.
 
@@ -156,12 +154,12 @@
             list: List of cell instrument IDs.
         """
         cell_instrmnt_ids_list = []
         for combination_item in combination_items:
             if combination_item.variable_id and combination_item.variable_id.variable_id == "TYP_INSTRMNT":
-                #ignore the member TYP_INSTRMNT_-1
-                if not (combination_item.member_id.member_id == 'TYP_INSTRMNT_-1'):
+                # ignore the member TYP_INSTRMNT_-1
+                if not (combination_item.member_id.member_id == "TYP_INSTRMNT_-1"):
                     if combination_item.member_id not in cell_instrmnt_ids_list:
                         cell_instrmnt_ids_list.append(combination_item.member_id)
                 else:
                     print("ignoring TYP_INSTRMNT_-1")
         return cell_instrmnt_ids_list
@@ -176,13 +174,16 @@
         Returns:
             list: List of cell instrument IDs.
         """
         cell_scrty_derivative_list = []
         for combination_item in combination_items:
-            if combination_item.variable_id and combination_item.variable_id.variable_id == "SCRTY_EXCHNG_TRDBL_DRVTV_TYP":
-                #ignore the member SCRTY_EXCHNG_TRDBL_DRVTV_TYP_-1
-                if not (combination_item.member_id.member_id == 'SCRTY_EXCHNG_TRDBL_DRVTV_TYP_-1'):
+            if (
+                combination_item.variable_id
+                and combination_item.variable_id.variable_id == "SCRTY_EXCHNG_TRDBL_DRVTV_TYP"
+            ):
+                # ignore the member SCRTY_EXCHNG_TRDBL_DRVTV_TYP_-1
+                if not (combination_item.member_id.member_id == "SCRTY_EXCHNG_TRDBL_DRVTV_TYP_-1"):
                     if combination_item.member_id not in cell_scrty_derivative_list:
                         cell_scrty_derivative_list.append(combination_item.member_id)
         return cell_scrty_derivative_list
 
     def _get_cell_scrty_derivative_ids(self, combination_items):
@@ -195,13 +196,16 @@
         Returns:
             list: List of cell instrument IDs.
         """
         cell_scrty_derivative_list = []
         for combination_item in combination_items:
-            if combination_item.variable_id and combination_item.variable_id.variable_id == "SCRTY_EXCHNG_TRDBL_DRVTV_TYP":
-                #ignore the member SCRTY_EXCHNG_TRDBL_DRVTV_TYP_-1
-                if not (combination_item.member_id.member_id == 'SCRTY_EXCHNG_TRDBL_DRVTV_TYP_-1'):
+            if (
+                combination_item.variable_id
+                and combination_item.variable_id.variable_id == "SCRTY_EXCHNG_TRDBL_DRVTV_TYP"
+            ):
+                # ignore the member SCRTY_EXCHNG_TRDBL_DRVTV_TYP_-1
+                if not (combination_item.member_id.member_id == "SCRTY_EXCHNG_TRDBL_DRVTV_TYP_-1"):
                     if combination_item.member_id not in cell_scrty_derivative_list:
                         cell_scrty_derivative_list.append(combination_item.member_id)
         return cell_scrty_derivative_list
 
     def _update_categories(self, context, cube_name, ids_list, main_categories_in_scope, prefix):
@@ -240,13 +244,15 @@
         for combination_item in combination_items:
             if combination_item.variable_id and combination_item.variable_id.variable_id == "TYP_ACCNTNG_ITM":
                 if combination_item.member_id not in cell_accntng_itm_ids_list:
                     cell_accntng_itm_ids_list.append(combination_item.member_id)
 
-        self._update_categories(context, cube_name, cell_accntng_itm_ids_list, main_categories_in_scope, "TYP_ACCNTNG_ITM")
-
-    #def create_draft_join_for_product_file(self, context, sdd_context, framework):
+        self._update_categories(
+            context, cube_name, cell_accntng_itm_ids_list, main_categories_in_scope, "TYP_ACCNTNG_ITM"
+        )
+
+    # def create_draft_join_for_product_file(self, context, sdd_context, framework):
     #    '''
     #    Create a draft of the join for product file, this should be reviewed and edited
     #    and the edited version used as an input for processing
     #    '''
     #    main_categories_in_scope = (
@@ -293,78 +299,88 @@
     #                f.write(definition + ",NOTHIN_FOUND,," + mc +'\n')
 
     #    f.close()
 
     def create_join_for_product_to_main_category_map(self, context, sdd_context, framework):
-        '''
+        """
         Create a map from join for products to main categories
-        '''
-        file_location = os.path.join(context.file_directory, "joins_configuration",
-                                     f"join_for_product_to_reference_category_{framework}.csv")
+        """
+        file_location = os.path.join(
+            context.file_directory, "joins_configuration", f"join_for_product_to_reference_category_{framework}.csv"
+        )
         join_for_products_to_main_category_map = (
-            context.join_for_products_to_main_category_map_finrep if framework == "FINREP_REF"
+            context.join_for_products_to_main_category_map_finrep
+            if framework == "FINREP_REF"
             else context.join_for_products_to_main_category_map_ae
         )
 
-        with open(file_location, encoding='utf-8') as csvfile:
-            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
+        with open(file_location, encoding="utf-8") as csvfile:
+            filereader = csv.reader(csvfile, delimiter=",", quotechar='"')
             next(filereader)  # Skip header
             for main_category, _, join_for_product in filereader:
                 join_for_products_to_main_category_map[join_for_product] = main_category
 
     def create_il_tables_for_main_category_map(self, context, sdd_context, framework):
-        '''
+        """
         Create a map from main categories such as loans and advances
         to the related input layer such as instrument
-        '''
-        file_location = os.path.join(context.file_directory, "joins_configuration",
-                                     f"join_for_product_ldm_definitions_{framework}.csv")
+        """
+        file_location = os.path.join(
+            context.file_directory, "joins_configuration", f"join_for_product_ldm_definitions_{framework}.csv"
+        )
         if not (context.ldm_or_il == "ldm"):
-            file_location = os.path.join(context.file_directory, "joins_configuration",
-                                     f"join_for_product_il_definitions_{framework}.csv")
+            file_location = os.path.join(
+                context.file_directory, "joins_configuration", f"join_for_product_il_definitions_{framework}.csv"
+            )
         tables_for_main_category_map = (
-            context.tables_for_main_category_map_finrep if framework == "FINREP_REF"
+            context.tables_for_main_category_map_finrep
+            if framework == "FINREP_REF"
             else context.tables_for_main_category_map_ae
         )
 
-        with open(file_location, encoding='utf-8') as csvfile:
-            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
+        with open(file_location, encoding="utf-8") as csvfile:
+            filereader = csv.reader(csvfile, delimiter=",", quotechar='"')
             next(filereader)  # Skip header
             for join_for_product_name, il_table, *_ in filereader:
                 try:
                     main_category = context.join_for_products_to_main_category_map_finrep[join_for_product_name]
                     tables_for_main_category_map.setdefault(main_category, []).append(il_table)
                 except KeyError:
                     # print(f"Could not find main category for join for product {join_for_product_name}")
                     pass
 
     def create_join_for_products_for_main_category_map(self, context, sdd_context, framework):
-        '''
+        """
         Create a map from main categories such as loans and advances
         to the related join for products, where join for product is a combination
         of an input layer and main category description
-        '''
-        file_location = os.path.join(context.file_directory, "joins_configuration",
-                                     f"join_for_product_ldm_definitions_{framework}.csv")
+        """
+        file_location = os.path.join(
+            context.file_directory, "joins_configuration", f"join_for_product_ldm_definitions_{framework}.csv"
+        )
         if not (context.ldm_or_il == "ldm"):
-            file_location = os.path.join(context.file_directory, "joins_configuration",
-                                     f"join_for_product_il_definitions_{framework}.csv")
+            file_location = os.path.join(
+                context.file_directory, "joins_configuration", f"join_for_product_il_definitions_{framework}.csv"
+            )
         join_for_products_to_linked_tables_map = (
-            context.join_for_products_to_linked_tables_map_finrep if framework == "FINREP_REF"
+            context.join_for_products_to_linked_tables_map_finrep
+            if framework == "FINREP_REF"
             else context.join_for_products_to_linked_tables_map_ae
         )
         join_for_products_to_to_filter_map = (
-            context.join_for_products_to_to_filter_map_finrep if framework == "FINREP_REF"
+            context.join_for_products_to_to_filter_map_finrep
+            if framework == "FINREP_REF"
             else context.join_for_products_to_to_filter_map_ae
         )
         table_and_part_tuple_map = (
-            context.table_and_part_tuple_map_finrep if framework == "FINREP_REF"
+            context.table_and_part_tuple_map_finrep
+            if framework == "FINREP_REF"
             else context.table_and_part_tuple_map_ae
         )
 
-        with open(file_location, encoding='utf-8') as csvfile:
-            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
+        with open(file_location, encoding="utf-8") as csvfile:
+            filereader = csv.reader(csvfile, delimiter=",", quotechar='"')
             next(filereader)  # Skip header
             for join_for_product_name, il_table, the_filter, linked_table_list, comments in filereader:
                 try:
                     main_category = context.join_for_products_to_main_category_map_finrep[join_for_product_name]
                     table_and_part_tuple = (il_table, join_for_product_name)
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/joins_meta_data/main_category_finder.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/generate_test_data/ldm_utils.py	2025-09-18 09:25:30.006052+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/generate_test_data/ldm_utils.py	2025-09-21 17:07:36.643490+00:00
@@ -9,36 +9,36 @@
 #
 # Contributors:
 #    Neil Mackenzie - initial API and implementation
 #
 
-'''
+"""
 Created on 22 Jan 2022
 
 @author: Neil
-'''
+"""
 
 import unicodedata
 from pyecore.resources import ResourceSet, URI
 
 from pybirdai.regdna import ELReference
 
 
 class Utils(object):
-    '''
+    """
     Documentation for Utils
-    '''
+    """
 
     @classmethod
     def unique_value(cls, the_enum, adapted_value):
-        '''
-            if the adapted value already exists in the enum then
-            append it with _x2
-            if the that string appended with _x2 already exists,
-            then append with_x3 instead
-            if that exists then _x4 etc.
-        '''
+        """
+        if the adapted value already exists in the enum then
+        append it with _x2
+        if the that string appended with _x2 already exists,
+        then append with_x3 instead
+        if that exists then _x4 etc.
+        """
         new_adapted_value = adapted_value
         if Utils.contains_literal(the_enum.eLiterals, adapted_value):
             new_adapted_value = adapted_value + "_x2"
         counter = 1
         finished = False
@@ -49,71 +49,71 @@
         # it would be better if BIRD addressed this repetition.
         # it is particularly noticeable in NUTS and NACE codes.
         # this high limit increases the processing time from under 1 minute
         # to a few minutes for the full BIRD data model.
         limit = 32
-        while ((counter < limit) and not (finished)):
+        while (counter < limit) and not (finished):
             counter = counter + 1
             if Utils.contains_literal(the_enum.eLiterals, adapted_value + "_x" + str(counter)):
-                new_adapted_value = adapted_value + "_x" + str(counter+1)
+                new_adapted_value = adapted_value + "_x" + str(counter + 1)
             else:
                 finished = True
 
         return new_adapted_value
 
     @classmethod
     def unique_name(cls, the_enum, enum_used_name):
-        '''
+        """
         if the adapted name already exists in the enum then append it with _x2
         if the that string appended with _x2 already exists, then append with_x3 instead
         if that exists then _x4 etc.
-        '''
+        """
         new_adapted_name = enum_used_name
         counter = 1
         finished = False
         limit = 32
         if Utils.contains_name(the_enum.eLiterals, enum_used_name):
             new_adapted_name = enum_used_name + "_x2"
 
         while (counter < limit) and not finished:
             counter = counter + 1
             if Utils.contains_name(the_enum.eLiterals, enum_used_name + "_x" + str(counter)):
-                new_adapted_name = enum_used_name + "_x" + str(counter+1)
+                new_adapted_name = enum_used_name + "_x" + str(counter + 1)
             else:
                 finished = True
 
         return new_adapted_name
 
     @classmethod
-    def get_members_of_the_domain(cls,  the_domain, member_id_to_domain_map):
-        '''
+    def get_members_of_the_domain(cls, the_domain, member_id_to_domain_map):
+        """
         return a list of members that belong to the domain
-        '''
+        """
         return_list = []
         for key, value in member_id_to_domain_map.items():
             if value == the_domain:
                 return_list.append(key)
         return return_list
 
     @classmethod
     def superclass_contains_feature(cls, the_superclass, attribute):
-        '''
+        """
         Checks if a superclass contains the attribute
-        '''
+        """
         attributes = the_superclass.eStructuralFeatures
         contains = False
         for attribute2 in attributes:
             if attribute2.name == attribute.name:
                 contains = True
 
         return contains
 
     @classmethod
     def has_member_called(cls, the_class, member_name):
-        '''
+        """
         Checks if the class has a member with the name memberName
-        '''
+        """
 
         members = the_class.members
         contains = False
         for member in members:
             if member.name == member_name:
@@ -121,218 +121,377 @@
 
         return contains
 
     @classmethod
     def number_of_relationships_to_this_class(cls, source_class, target_class):
-        '''
+        """
         Checks how many relationships there are between 2 classes
         It is possible that one class might have 2 different relationships
         to the same class.
-        '''
+        """
         features = source_class.eStructuralFeatures
         counter = 0
         # do this for relationship attributes only.
         for feature in features:
             if isinstance(feature, ELReference):
                 feature_type = feature.eType
                 if feature_type == target_class:
-                    counter = counter+1
+                    counter = counter + 1
 
         return counter
 
     @classmethod
     def make_valid_id_for_literal(cls, input_string):
-        '''
+        """
         Tranlate text to be a valid id, without special characters, and following
         the rules for valid id's in regdna
-        '''
-
-        amended_input_string = input_string.replace('  ', ' ').replace(' ', '_').replace(')', '_').replace('(', '_') \
-            .replace(',', '_').replace('\'', '_').replace('\n', '_').replace('\r', '_').replace('\'t', '_').replace('new', 'New') \
-            .replace('\\', '_').replace('/', '_').replace(':', '_') \
-            .replace('+', '_').replace('.', '_').replace('?', '_').replace('\'', '_').replace('>', '_gt') \
-            .replace('<', '_lt').replace('\"', '_').replace(';', '_').replace('$', '_').replace('=', '_eq').replace('#', '_') \
-            .replace('&', '_').replace('%', '_').replace('[', '_').replace(']', '_').replace('?', '_').replace('–', '_').replace('__', '_').replace('__', '_') \
-            .replace(chr(0x2019), '_').replace(chr(65533), '_').replace(chr(0x00A8), '_').replace(chr(0x00A9), '_')  \
-            .replace(chr(0x00A4), '_').replace(chr(0x00B6), '_').replace(chr(0x00D0), '_').replace(chr(0x00BA), '_') \
-            .replace(chr(0x2020), '_').replace(chr(0x00B5), '_').replace(chr(0x20AC), '_').replace(chr(0x00B4), '_') \
-            .replace(chr(0x0192), '_').replace(chr(0x00B2), '_').replace(chr(0x00BF), '_').replace(chr(0x00B0), '_') \
-            .replace(chr(0x00A6), '_').replace(chr(0x203A), '_').replace(chr(0x00A2), '_').replace(chr(0x2122), '_') \
-            .replace(chr(0x00B1), '_').replace(chr(0x00B9), '_').replace(chr(0x00AE), '_').replace(chr(0x2014), '_') \
-            .replace(chr(0x02DC), '_').replace(chr(0x201E), '_').replace(chr(0x2026), '_').replace(chr(0x00BF), '_') \
-            .replace(chr(0x00BB), '_').replace(chr(0x00AB), '_').replace(chr(0x2022), '_').replace(chr(0x00AC), '_') \
-            .replace(chr(0x2021), '_').replace(chr(0x00A5), '_').replace(chr(0x201E), '_').replace(chr(0x201C), '_') \
-            .replace(chr(0x00AF), '_').replace(chr(0x201D), '_').replace(chr(0x00A3), '_').replace(chr(0x2030), '_') \
-            .replace(chr(0x00BD), '_').replace(chr(0x00BC), '_').replace(chr(0x00BE), '_').replace(chr(0x00A1), '_') \
-            .replace(chr(0x2018), '_').replace(chr(0x0060), '_').replace(chr(0x00B4), '_').replace(chr(0x2026), '_') \
-            .replace(chr(0x200B), '_').replace(chr(0x202F), '_').replace(chr(0x205F), '_').replace(chr(0x3000), '_') \
-            .replace(chr(0x2000), '_').replace(chr(0x2001), '_').replace(chr(0x2002), '_').replace(chr(0x2003), '_') \
-            .replace(chr(0x2004), '_').replace(chr(0x2005), '_').replace(chr(0x2006), '_').replace(chr(0x2007), '_') \
-            .replace(chr(0x2008), '_').replace(chr(0x2009), '_').replace(chr(0x200A), '_').replace(chr(0x00A0), '_') \
-            .replace(chr(0x0027), '_').replace(chr(0x2019), '_').replace(chr(0x2018), '_').replace(chr(0x201A), '_').replace(chr(0x00B7), '_')
-
-
-        return_string = Utils.replace_acutes_graves_and_circumflexes(
-            amended_input_string).replace('\'', '_')
+        """
+
+        amended_input_string = (
+            input_string.replace("  ", " ")
+            .replace(" ", "_")
+            .replace(")", "_")
+            .replace("(", "_")
+            .replace(",", "_")
+            .replace("'", "_")
+            .replace("\n", "_")
+            .replace("\r", "_")
+            .replace("'t", "_")
+            .replace("new", "New")
+            .replace("\\", "_")
+            .replace("/", "_")
+            .replace(":", "_")
+            .replace("+", "_")
+            .replace(".", "_")
+            .replace("?", "_")
+            .replace("'", "_")
+            .replace(">", "_gt")
+            .replace("<", "_lt")
+            .replace('"', "_")
+            .replace(";", "_")
+            .replace("$", "_")
+            .replace("=", "_eq")
+            .replace("#", "_")
+            .replace("&", "_")
+            .replace("%", "_")
+            .replace("[", "_")
+            .replace("]", "_")
+            .replace("?", "_")
+            .replace("–", "_")
+            .replace("__", "_")
+            .replace("__", "_")
+            .replace(chr(0x2019), "_")
+            .replace(chr(65533), "_")
+            .replace(chr(0x00A8), "_")
+            .replace(chr(0x00A9), "_")
+            .replace(chr(0x00A4), "_")
+            .replace(chr(0x00B6), "_")
+            .replace(chr(0x00D0), "_")
+            .replace(chr(0x00BA), "_")
+            .replace(chr(0x2020), "_")
+            .replace(chr(0x00B5), "_")
+            .replace(chr(0x20AC), "_")
+            .replace(chr(0x00B4), "_")
+            .replace(chr(0x0192), "_")
+            .replace(chr(0x00B2), "_")
+            .replace(chr(0x00BF), "_")
+            .replace(chr(0x00B0), "_")
+            .replace(chr(0x00A6), "_")
+            .replace(chr(0x203A), "_")
+            .replace(chr(0x00A2), "_")
+            .replace(chr(0x2122), "_")
+            .replace(chr(0x00B1), "_")
+            .replace(chr(0x00B9), "_")
+            .replace(chr(0x00AE), "_")
+            .replace(chr(0x2014), "_")
+            .replace(chr(0x02DC), "_")
+            .replace(chr(0x201E), "_")
+            .replace(chr(0x2026), "_")
+            .replace(chr(0x00BF), "_")
+            .replace(chr(0x00BB), "_")
+            .replace(chr(0x00AB), "_")
+            .replace(chr(0x2022), "_")
+            .replace(chr(0x00AC), "_")
+            .replace(chr(0x2021), "_")
+            .replace(chr(0x00A5), "_")
+            .replace(chr(0x201E), "_")
+            .replace(chr(0x201C), "_")
+            .replace(chr(0x00AF), "_")
+            .replace(chr(0x201D), "_")
+            .replace(chr(0x00A3), "_")
+            .replace(chr(0x2030), "_")
+            .replace(chr(0x00BD), "_")
+            .replace(chr(0x00BC), "_")
+            .replace(chr(0x00BE), "_")
+            .replace(chr(0x00A1), "_")
+            .replace(chr(0x2018), "_")
+            .replace(chr(0x0060), "_")
+            .replace(chr(0x00B4), "_")
+            .replace(chr(0x2026), "_")
+            .replace(chr(0x200B), "_")
+            .replace(chr(0x202F), "_")
+            .replace(chr(0x205F), "_")
+            .replace(chr(0x3000), "_")
+            .replace(chr(0x2000), "_")
+            .replace(chr(0x2001), "_")
+            .replace(chr(0x2002), "_")
+            .replace(chr(0x2003), "_")
+            .replace(chr(0x2004), "_")
+            .replace(chr(0x2005), "_")
+            .replace(chr(0x2006), "_")
+            .replace(chr(0x2007), "_")
+            .replace(chr(0x2008), "_")
+            .replace(chr(0x2009), "_")
+            .replace(chr(0x200A), "_")
+            .replace(chr(0x00A0), "_")
+            .replace(chr(0x0027), "_")
+            .replace(chr(0x2019), "_")
+            .replace(chr(0x2018), "_")
+            .replace(chr(0x201A), "_")
+            .replace(chr(0x00B7), "_")
+        )
+
+        return_string = Utils.replace_acutes_graves_and_circumflexes(amended_input_string).replace("'", "_")
 
         if return_string == "op":
             return_string = "_op"
         return return_string
 
     @classmethod
     def make_valid_id(cls, input_string):
-        '''
+        """
         Tranlate text to be a valid id, without special characters, and following
         the rules for valid id's in regdna
-        '''
+        """
 
         # we do not allow id's to start with  number, if it does then we prepend with an underscore
         if len(input_string) > 0:
-            if ((input_string[0] >= '0') and (input_string[0] <= '9')):
+            if (input_string[0] >= "0") and (input_string[0] <= "9"):
                 input_string = "_" + input_string
         # we replace special characters not allowed in id's with an underscore
-        amended_input_string = input_string.replace('  ', ' ').replace(' ', '_').replace(')', '_').replace('(', '_') \
-            .replace(',', '_').replace('\'', '_').replace('\n', '_').replace('\r', '_').replace('\'t', '_').replace('new', 'New') \
-            .replace('\\', '_').replace('/', '_').replace('-', '_').replace(':', '_') \
-            .replace('+', '_').replace('.', '_').replace('?', '_').replace('\'', '_').replace('>', '_gt') \
-            .replace('<', '_lt').replace('\"', '_').replace(';', '_').replace('$', '_').replace('=', '_eq').replace('#', '_') \
-            .replace('&', '_').replace('%', '_').replace('[', '_').replace(']', '_').replace('?', '_').replace('–', '_').replace('__', '_').replace('__', '_') \
-            .replace(chr(0x2019), '_').replace(chr(65533), '_').replace(chr(0x00A8), '_').replace(chr(0x00A9), '_')  \
-            .replace(chr(0x00A4), '_').replace(chr(0x00B6), '_').replace(chr(0x00D0), '_').replace(chr(0x00BA), '_') \
-            .replace(chr(0x2020), '_').replace(chr(0x00B5), '_').replace(chr(0x20AC), '_').replace(chr(0x00B4), '_') \
-            .replace(chr(0x0192), '_').replace(chr(0x00B2), '_').replace(chr(0x00BF), '_').replace(chr(0x00B0), '_') \
-            .replace(chr(0x00A6), '_').replace(chr(0x203A), '_').replace(chr(0x00A2), '_').replace(chr(0x2122), '_') \
-            .replace(chr(0x00B1), '_').replace(chr(0x00B9), '_').replace(chr(0x00AE), '_').replace(chr(0x2014), '_') \
-            .replace(chr(0x02DC), '_').replace(chr(0x201E), '_').replace(chr(0x2026), '_').replace(chr(0x00BF), '_') \
-            .replace(chr(0x00BB), '_').replace(chr(0x00AB), '_').replace(chr(0x2022), '_').replace(chr(0x00AC), '_') \
-            .replace(chr(0x2021), '_').replace(chr(0x00A5), '_').replace(chr(0x201E), '_').replace(chr(0x201C), '_') \
-            .replace(chr(0x00AF), '_').replace(chr(0x201D), '_').replace(chr(0x00A3), '_').replace(chr(0x2030), '_') \
-            .replace(chr(0x00BD), '_').replace(chr(0x00BC), '_').replace(chr(0x00BE), '_').replace(chr(0x00A1), '_') \
-            .replace(chr(0x2018), '_').replace(chr(0x0060), '_').replace(chr(0x00B4), '_').replace(chr(0x2026), '_') \
-            .replace(chr(0x200B), '_').replace(chr(0x202F), '_').replace(chr(0x205F), '_').replace(chr(0x3000), '_') \
-            .replace(chr(0x2000), '_').replace(chr(0x2001), '_').replace(chr(0x2002), '_').replace(chr(0x2003), '_') \
-            .replace(chr(0x2004), '_').replace(chr(0x2005), '_').replace(chr(0x2006), '_').replace(chr(0x2007), '_') \
-            .replace(chr(0x2008), '_').replace(chr(0x2009), '_').replace(chr(0x200A), '_').replace(chr(0x00A0), '_') \
-            .replace(chr(0x0027), '_').replace(chr(0x2019), '_').replace(chr(0x2018), '_').replace(chr(0x201A), '_').replace(chr(0x00B7), '_')
-
-
-        return_string = Utils.replace_acutes_graves_and_circumflexes(
-            amended_input_string).replace('\'', '_')
+        amended_input_string = (
+            input_string.replace("  ", " ")
+            .replace(" ", "_")
+            .replace(")", "_")
+            .replace("(", "_")
+            .replace(",", "_")
+            .replace("'", "_")
+            .replace("\n", "_")
+            .replace("\r", "_")
+            .replace("'t", "_")
+            .replace("new", "New")
+            .replace("\\", "_")
+            .replace("/", "_")
+            .replace("-", "_")
+            .replace(":", "_")
+            .replace("+", "_")
+            .replace(".", "_")
+            .replace("?", "_")
+            .replace("'", "_")
+            .replace(">", "_gt")
+            .replace("<", "_lt")
+            .replace('"', "_")
+            .replace(";", "_")
+            .replace("$", "_")
+            .replace("=", "_eq")
+            .replace("#", "_")
+            .replace("&", "_")
+            .replace("%", "_")
+            .replace("[", "_")
+            .replace("]", "_")
+            .replace("?", "_")
+            .replace("–", "_")
+            .replace("__", "_")
+            .replace("__", "_")
+            .replace(chr(0x2019), "_")
+            .replace(chr(65533), "_")
+            .replace(chr(0x00A8), "_")
+            .replace(chr(0x00A9), "_")
+            .replace(chr(0x00A4), "_")
+            .replace(chr(0x00B6), "_")
+            .replace(chr(0x00D0), "_")
+            .replace(chr(0x00BA), "_")
+            .replace(chr(0x2020), "_")
+            .replace(chr(0x00B5), "_")
+            .replace(chr(0x20AC), "_")
+            .replace(chr(0x00B4), "_")
+            .replace(chr(0x0192), "_")
+            .replace(chr(0x00B2), "_")
+            .replace(chr(0x00BF), "_")
+            .replace(chr(0x00B0), "_")
+            .replace(chr(0x00A6), "_")
+            .replace(chr(0x203A), "_")
+            .replace(chr(0x00A2), "_")
+            .replace(chr(0x2122), "_")
+            .replace(chr(0x00B1), "_")
+            .replace(chr(0x00B9), "_")
+            .replace(chr(0x00AE), "_")
+            .replace(chr(0x2014), "_")
+            .replace(chr(0x02DC), "_")
+            .replace(chr(0x201E), "_")
+            .replace(chr(0x2026), "_")
+            .replace(chr(0x00BF), "_")
+            .replace(chr(0x00BB), "_")
+            .replace(chr(0x00AB), "_")
+            .replace(chr(0x2022), "_")
+            .replace(chr(0x00AC), "_")
+            .replace(chr(0x2021), "_")
+            .replace(chr(0x00A5), "_")
+            .replace(chr(0x201E), "_")
+            .replace(chr(0x201C), "_")
+            .replace(chr(0x00AF), "_")
+            .replace(chr(0x201D), "_")
+            .replace(chr(0x00A3), "_")
+            .replace(chr(0x2030), "_")
+            .replace(chr(0x00BD), "_")
+            .replace(chr(0x00BC), "_")
+            .replace(chr(0x00BE), "_")
+            .replace(chr(0x00A1), "_")
+            .replace(chr(0x2018), "_")
+            .replace(chr(0x0060), "_")
+            .replace(chr(0x00B4), "_")
+            .replace(chr(0x2026), "_")
+            .replace(chr(0x200B), "_")
+            .replace(chr(0x202F), "_")
+            .replace(chr(0x205F), "_")
+            .replace(chr(0x3000), "_")
+            .replace(chr(0x2000), "_")
+            .replace(chr(0x2001), "_")
+            .replace(chr(0x2002), "_")
+            .replace(chr(0x2003), "_")
+            .replace(chr(0x2004), "_")
+            .replace(chr(0x2005), "_")
+            .replace(chr(0x2006), "_")
+            .replace(chr(0x2007), "_")
+            .replace(chr(0x2008), "_")
+            .replace(chr(0x2009), "_")
+            .replace(chr(0x200A), "_")
+            .replace(chr(0x00A0), "_")
+            .replace(chr(0x0027), "_")
+            .replace(chr(0x2019), "_")
+            .replace(chr(0x2018), "_")
+            .replace(chr(0x201A), "_")
+            .replace(chr(0x00B7), "_")
+        )
+
+        return_string = Utils.replace_acutes_graves_and_circumflexes(amended_input_string).replace("'", "_")
 
         if return_string == "op":
             return_string = "_op"
         return return_string
 
     @classmethod
     def replace_acutes_graves_and_circumflexes(cls, the_input_string):
-        '''
+        """
         We replace letters with acutes , graves, and circumflexes, with the basic letter.
         So for example "a acute" is replaced with "a"
-        '''
-        return unicodedata.normalize('NFD', the_input_string).encode('ascii', 'ignore').decode('ascii')
+        """
+        return unicodedata.normalize("NFD", the_input_string).encode("ascii", "ignore").decode("ascii")
 
     @classmethod
     def in_enum_excluded_list(cls, adapted_enum_name):
-        '''
+        """
         TODO not sure if we still need this, it was introduces to deal with
         problematic domains in the past.
-        '''
-        if ((adapted_enum_name == "All_last_days_of_months___YYYY_MM") or
-            (adapted_enum_name == "All_last_days_of_quarters___YYYY_MM") or
-                (adapted_enum_name == "All_possible_dates_YYYY_MM_DD")):
+        """
+        if (
+            (adapted_enum_name == "All_last_days_of_months___YYYY_MM")
+            or (adapted_enum_name == "All_last_days_of_quarters___YYYY_MM")
+            or (adapted_enum_name == "All_possible_dates_YYYY_MM_DD")
+        ):
 
             print(" field in excludedlist: " + adapted_enum_name)
             return True
         else:
             return False
 
     @classmethod
     def contains_literal(cls, members, adapted_value):
-        '''
+        """
         checks if an enum contains a particular literal
-        '''
+        """
         contains = False
         for e_enum_literal in members:
             if e_enum_literal.name.lower() == adapted_value.lower():
                 contains = True
 
         return contains
 
     @classmethod
     def contains_name(cls, members, adapted_name):
-        '''
+        """
         checks if an enum contains a particular name
-        '''
+        """
         contains = False
         for e_enum_literal in members:
             if e_enum_literal.literal.lower() == adapted_name.lower():
                 contains = True
 
         return contains
 
     @classmethod
     def get_literals_for_enumeration(cls, domain, members_module):
-        '''
+        """
         returns the list of literals for an enumerations
-        '''
+        """
         return_members_list = []
         for member in members_module.members:
             if member.domain_id == domain:
                 return_members_list.append(member)
         return return_members_list
 
     @classmethod
     def get_ecore_datatype_for_datatype(cls, context):
-        '''
+        """
         returns the ecore data type for a data type
-        '''
+        """
         return context.e_string
 
     @classmethod
     def find_enum(cls, enum_name, enum_map):
-        '''
+        """
         returns the enum for a reference output layer
-        '''
+        """
 
         return_val = None
         for key, value in enum_map.items():
             if value.name.lower() == enum_name.lower():
                 return_val = value
 
         return return_val
 
     @classmethod
     def special_cases(cls, new_adapted_value, counter):
-        '''
+        """
         Deals with special cases where we need to adapt the name of the enum
-        '''
+        """
 
         return_val = new_adapted_value
         if new_adapted_value == "A_S":
             return_val = "A_S_dup"
         if new_adapted_value == "s_p_":
             return_val = "s_p_dup" + str(counter)
         return return_val
 
     @classmethod
-    def get_annotation_with_source(cls,element, source):
-        '''
+    def get_annotation_with_source(cls, element, source):
+        """
         returns the annotation with the source
-        '''
+        """
         return_annotation = None
         for annotation in element.eAnnotations:
             if annotation.source is not None:
                 if annotation.source.name == source:
                     return_annotation = annotation
 
         return return_annotation
 
     @classmethod
-    def get_annotation_directive(cls,package, name):
-        '''
+    def get_annotation_directive(cls, package, name):
+        """
         returns the annotation directive with the name
-        '''
+        """
         return_annotation_directive = None
         for annotation_directive in package.annotationDirectives:
             if annotation_directive.name == name:
                 return_annotation_directive = annotation_directive
         return return_annotation_directive
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/generate_test_data/ldm_utils.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/input_model/import_input_model.py	2025-09-15 13:18:11.383207+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/input_model/import_input_model.py	2025-09-21 17:07:36.649060+00:00
@@ -15,20 +15,21 @@
 from django.apps import apps
 from django.db import models
 import os
 import csv
 from pybirdai.context.csv_column_index_context import ColumnIndexes
-from django.db.models.fields import CharField,DateTimeField,BooleanField,FloatField,BigIntegerField
+from django.db.models.fields import CharField, DateTimeField, BooleanField, FloatField, BigIntegerField
 from django.db import transaction
 from uuid import uuid4
 from pybirdai.views import load_variables_from_csv_file
 import logging
 from django.conf import settings
 import copy
 
 logging.basicConfig(level=logging.INFO)
 
+
 class ImportInputModel(object):
     """
     A class for creating reference domains, variables, and cubes in the SDD model.
     """
 
@@ -45,99 +46,82 @@
         context.fields = dict()
         context.derived_properties = dict()
         ImportInputModel._fetch_derived_fields_from_model(context)
         ImportInputModel._create_maintenance_agency(sdd_context)
         ImportInputModel._create_primitive_domains(sdd_context)
-        if hasattr(context, 'alternative_folder_for_subdomains'):
-            ImportInputModel._create_subdomain_to_domain_map(sdd_context,alternative_folder=context.alternative_folder_for_subdomains)
+        if hasattr(context, "alternative_folder_for_subdomains"):
+            ImportInputModel._create_subdomain_to_domain_map(
+                sdd_context, alternative_folder=context.alternative_folder_for_subdomains
+            )
         else:
             ImportInputModel._create_subdomain_to_domain_map(sdd_context)
         ImportInputModel._process_models(sdd_context, context)
-         # Load extra variables from CSV file
+        # Load extra variables from CSV file
         from django.conf import settings
+
         base_dir = settings.BASE_DIR
-        extra_variables_path = os.path.join(base_dir, 'resources', 'extra_variables', 'extra_variables.csv')
+        extra_variables_path = os.path.join(base_dir, "resources", "extra_variables", "extra_variables.csv")
         variables_loaded = load_variables_from_csv_file(extra_variables_path)
         if variables_loaded > 0:
             print(f"Loaded {variables_loaded} extra variables from CSV file.")
         else:
             print("No extra variables loaded (file not found or empty).")
 
-
-
     def _create_maintenance_agency(sdd_context):
         """
         Create a maintenance agency named 'REF' and add it to the SDD context.
 
         Args:
             sdd_context: The SDD context object to store the created
                          maintenance agency.
         """
         # Prepare all agencies at once
         agencies = [
-            MAINTENANCE_AGENCY(
-                name="REF",
-                code="REF",
-                maintenance_agency_id="REF"
-            ),
-            MAINTENANCE_AGENCY(
-                name="NODE",
-                code="NODE",
-                maintenance_agency_id="NODE"
-            ),
-            MAINTENANCE_AGENCY(
-                name="SDD_DOMAIN",
-                code="SDD_DOMAIN",
-                maintenance_agency_id="SDD_DOMAIN"
-            ),
-            MAINTENANCE_AGENCY(
-                name="ECB",
-                code="ECB",
-                maintenance_agency_id="ECB"
-            )
+            MAINTENANCE_AGENCY(name="REF", code="REF", maintenance_agency_id="REF"),
+            MAINTENANCE_AGENCY(name="NODE", code="NODE", maintenance_agency_id="NODE"),
+            MAINTENANCE_AGENCY(name="SDD_DOMAIN", code="SDD_DOMAIN", maintenance_agency_id="SDD_DOMAIN"),
+            MAINTENANCE_AGENCY(name="ECB", code="ECB", maintenance_agency_id="ECB"),
         ]
 
         # Bulk create all agencies
         created_agencies = MAINTENANCE_AGENCY.objects.bulk_create(agencies, ignore_conflicts=True)
 
-
         # Update dictionary with created instances
-        sdd_context.agency_dictionary.update({
-            agency.code: agency for agency in created_agencies
-        })
-
+        sdd_context.agency_dictionary.update({agency.code: agency for agency in created_agencies})
 
     def _create_primitive_domains(sdd_context):
         """
         Create a 'String' domain and add it to the SDD context.
 
         Args:
             sdd_context: The SDD context object to store the created string
                          domain.
         """
         domains = []
-        for domain_type in ['String', 'Integer', 'Date', 'Float', 'Boolean']:
+        for domain_type in ["String", "Integer", "Date", "Float", "Boolean"]:
             domain = DOMAIN(
                 domain_id=domain_type,
                 name=domain_type,
                 description=domain_type,
                 data_type=domain_type,
-                maintenance_agency_id=sdd_context.agency_dictionary["ECB"]
+                maintenance_agency_id=sdd_context.agency_dictionary["ECB"],
             )
             domains.append(domain)
             sdd_context.domain_dictionary[domain_type] = domain
 
         # Bulk create all domains
         DOMAIN.objects.bulk_create(domains, ignore_conflicts=True)
 
-    def _create_subdomain_to_domain_map(sdd_context, alternative_folder:str=""):
-        file_location = sdd_context.file_directory + os.sep + (alternative_folder or "technical_export") + os.sep + "subdomain.csv"
+    def _create_subdomain_to_domain_map(sdd_context, alternative_folder: str = ""):
+        file_location = (
+            sdd_context.file_directory + os.sep + (alternative_folder or "technical_export") + os.sep + "subdomain.csv"
+        )
 
         header_skipped = False
 
-        with open(file_location, encoding='utf-8') as csvfile:
-            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
+        with open(file_location, encoding="utf-8") as csvfile:
+            filereader = csv.reader(csvfile, delimiter=",", quotechar='"')
             for row in filereader:
                 if not header_skipped:
                     header_skipped = True
                 else:
                     domain_id = row[ColumnIndexes().subdomain_domain_id_index]
@@ -178,44 +162,47 @@
         bird_cube.name = model.__name__
         bird_cube_cube_structure.cube_structure_id = model.__name__
         bird_cube_cube_structure.name = model.__name__
         bird_cube.cube_structure_id = bird_cube_cube_structure
 
-        sdd_context.bird_cube_structure_dictionary[
-            bird_cube_cube_structure.name] = bird_cube_cube_structure
+        sdd_context.bird_cube_structure_dictionary[bird_cube_cube_structure.name] = bird_cube_cube_structure
         sdd_context.bird_cube_dictionary[bird_cube.name] = bird_cube
         if context.save_derived_sdd_items:
             bird_cube_cube_structure.save()
             bird_cube.save()
 
     def _fetch_derived_fields_from_model(context):
         import ast
 
         # Read bird_data_model.py and find functions with lineage decorator
-        bird_data_model_path = os.path.join(
-            settings.BASE_DIR,
-            "pybirdai",
-            "models",
-            'bird_data_model.py')
-
-
-        with open(bird_data_model_path, 'r', encoding='utf-8') as file:
+        bird_data_model_path = os.path.join(settings.BASE_DIR, "pybirdai", "models", "bird_data_model.py")
+
+        with open(bird_data_model_path, "r", encoding="utf-8") as file:
             tree = ast.parse(file.read())
-
 
             for node in [_node for _node in tree.body if isinstance(_node, ast.ClassDef)]:
                 property_nodes = []
 
-                for function_node in [function_node for function_node in node.body if isinstance(function_node, ast.FunctionDef)]:
+                for function_node in [
+                    function_node for function_node in node.body if isinstance(function_node, ast.FunctionDef)
+                ]:
                     for decorator in function_node.decorator_list:
-                        if isinstance(decorator, ast.Call) and isinstance(decorator.func, ast.Name) and decorator.func.id == 'lineage':
+                        if (
+                            isinstance(decorator, ast.Call)
+                            and isinstance(decorator.func, ast.Name)
+                            and decorator.func.id == "lineage"
+                        ):
                             property_nodes.append(function_node.name)
 
                 if not property_nodes:
                     continue
 
-                for meta_node in [meta_node for meta_node in node.body if isinstance(meta_node, ast.ClassDef) and meta_node.name == 'Meta']:
+                for meta_node in [
+                    meta_node
+                    for meta_node in node.body
+                    if isinstance(meta_node, ast.ClassDef) and meta_node.name == "Meta"
+                ]:
                     for field in meta_node.body:
                         if isinstance(field, ast.Assign) and field.targets[0].id == "verbose_name":
                             verbose_name = field.value.s
                             break
 
@@ -224,11 +211,11 @@
 
                 context.derived_properties[verbose_name] = property_nodes
                 logging.info(f"Processed derived properties for {verbose_name} : {property_nodes}")
                 property_nodes = []
 
-    def _provide_fields(context,verbose_name,model_name ):
+    def _provide_fields(context, verbose_name, model_name):
         """
         Provide type information for a list of variable IDs.
 
         Args:
             list_of_variable_ids: A list of variable IDs to provide type information for.
@@ -240,11 +227,11 @@
             if variable_id in context.fields:
                 field = copy.copy(context.fields[variable_id])
                 field.model.__name__ = model_name
                 fields.append(field)
                 continue
-            fields.append(models.CharField(variable_id,max_length=1000))
+            fields.append(models.CharField(variable_id, max_length=1000))
         return fields
 
     def _process_fields(model, sdd_context, context):
         """
         Process all fields of the given model.
@@ -254,19 +241,16 @@
             sdd_context: The SDD context object to store created elements.
             context: The context object containing configuration settings.
         """
         relevant_field_types = (CharField, DateTimeField, BigIntegerField, BooleanField, FloatField)
 
-        fields = [
-            field for field in model._meta.get_fields()
-            if isinstance(field, relevant_field_types)
-        ]
+        fields = [field for field in model._meta.get_fields() if isinstance(field, relevant_field_types)]
 
         context.fields.update({field.name: field for field in fields})
 
         if model._meta.verbose_name in context.derived_properties:
-            fields += ImportInputModel._provide_fields(context,model._meta.verbose_name,model.__name__)
+            fields += ImportInputModel._provide_fields(context, model._meta.verbose_name, model.__name__)
             # logging.info(f"Processing derived properties for model {model._meta.verbose_name} :: fields : {fields}")
 
         variables_to_create = []
         cube_structure_items_to_create = []
 
@@ -278,64 +262,72 @@
             if variable_id not in sdd_context.variable_dictionary:
                 variable = VARIABLE(
                     maintenance_agency_id=sdd_context.agency_dictionary["REF"],
                     variable_id=variable_id,
                     code=variable_id,
-                    name=getattr(field, 'verbose_name', variable_id),
-                    domain_id=domain or ImportInputModel._get_default_domain(field, sdd_context)
+                    name=getattr(field, "verbose_name", variable_id),
+                    domain_id=domain or ImportInputModel._get_default_domain(field, sdd_context),
                 )
                 variables_to_create.append(variable)
                 sdd_context.variable_dictionary[variable_id] = variable
 
             # Create cube structure item
 
             if context.save_derived_sdd_items:
                 csid = sdd_context.bird_cube_structure_dictionary[model.__name__]
-                if (csid,variable_id) not in sdd_context.csi_counter:
-                    sdd_context.csi_counter[(csid,variable_id)] = 0
+                if (csid, variable_id) not in sdd_context.csi_counter:
+                    sdd_context.csi_counter[(csid, variable_id)] = 0
 
                 variable_id_ = sdd_context.variable_dictionary[variable_id]
 
                 csi = CUBE_STRUCTURE_ITEM(
                     cube_structure_id=csid,
                     variable_id=variable_id_,
                     subdomain_id=subdomain,
-                    cube_variable_code = "__".join([csid.cube_structure_id,variable_id_.variable_id,str(sdd_context.csi_counter[(csid,variable_id)])])
+                    cube_variable_code="__".join(
+                        [
+                            csid.cube_structure_id,
+                            variable_id_.variable_id,
+                            str(sdd_context.csi_counter[(csid, variable_id)]),
+                        ]
+                    ),
                 )
 
                 cube_structure_items_to_create.append(csi)
-                #key = f"{csi.cube_structure_id.cube_structure_id}:{csi.variable_id.variable_id}"
-                if csi.cube_structure_id.cube_structure_id not in sdd_context.bird_cube_structure_item_dictionary.keys():
+                # key = f"{csi.cube_structure_id.cube_structure_id}:{csi.variable_id.variable_id}"
+                if (
+                    csi.cube_structure_id.cube_structure_id
+                    not in sdd_context.bird_cube_structure_item_dictionary.keys()
+                ):
                     sdd_context.bird_cube_structure_item_dictionary[csi.cube_structure_id.cube_structure_id] = []
                 sdd_context.bird_cube_structure_item_dictionary[csi.cube_structure_id.cube_structure_id].append(csi)
-                sdd_context.csi_counter[(csid,variable_id)] += 1
+                sdd_context.csi_counter[(csid, variable_id)] += 1
 
         # Bulk create all objects
         if variables_to_create and sdd_context.save_sdd_to_db:
             VARIABLE.objects.bulk_create(variables_to_create, ignore_conflicts=True)
 
         if cube_structure_items_to_create and context.save_derived_sdd_items:
             if model._meta.verbose_name == "Party_role":
                 logging.info("Party_role cube structure items to be created :: %s", len(cube_structure_items_to_create))
             for item in cube_structure_items_to_create:
                 item.save()
-            #CUBE_STRUCTURE_ITEM.objects.bulk_create(cube_structure_items_to_create)
+            # CUBE_STRUCTURE_ITEM.objects.bulk_create(cube_structure_items_to_create)
 
     @staticmethod
     def _get_default_domain(field, sdd_context):
         if isinstance(field, CharField):
-            return sdd_context.domain_dictionary['String']
+            return sdd_context.domain_dictionary["String"]
         elif isinstance(field, DateTimeField):
-            return sdd_context.domain_dictionary['Date']
+            return sdd_context.domain_dictionary["Date"]
         elif isinstance(field, BigIntegerField):
-            return sdd_context.domain_dictionary['Integer']
+            return sdd_context.domain_dictionary["Integer"]
         elif isinstance(field, BooleanField):
-            return sdd_context.domain_dictionary['Boolean']
+            return sdd_context.domain_dictionary["Boolean"]
         elif isinstance(field, FloatField):
-            return sdd_context.domain_dictionary['Float']
+            return sdd_context.domain_dictionary["Float"]
         return None
-
 
     def _create_domain_and_subdomain_if_needed(field, sdd_context):
         """
         Create a domain for the field if it doesn't exist and add it to the
         SDD context.
@@ -352,11 +344,11 @@
             if not subdomain_id:
                 return None, None
 
             domain_id = None
             try:
-                domain_id = sdd_context.subdomain_to_domain_map[subdomain_id[0:len(subdomain_id)-7]]
+                domain_id = sdd_context.subdomain_to_domain_map[subdomain_id[0 : len(subdomain_id) - 7]]
             except KeyError:
                 pass
 
             domains_to_create = []
             subdomains_to_create = []
@@ -364,13 +356,11 @@
             subdomain_enums_to_create = []
 
             # Create domain if needed
             if domain_id and domain_id not in sdd_context.domain_dictionary:
                 domain = DOMAIN(
-                    domain_id=domain_id,
-                    name=domain_id,
-                    maintenance_agency_id=sdd_context.agency_dictionary["ECB"]
+                    domain_id=domain_id, name=domain_id, maintenance_agency_id=sdd_context.agency_dictionary["ECB"]
                 )
                 domains_to_create.append(domain)
                 sdd_context.domain_dictionary[domain_id] = domain
                 print(f"Adding domain: {domain_id}")
 
@@ -378,11 +368,11 @@
             subdomain = None
             if subdomain_id and subdomain_id not in sdd_context.subdomain_dictionary:
                 subdomain = SUBDOMAIN(
                     subdomain_id=subdomain_id,
                     domain_id=sdd_context.domain_dictionary.get(domain_id),
-                    maintenance_agency_id=sdd_context.agency_dictionary["ECB"]
+                    maintenance_agency_id=sdd_context.agency_dictionary["ECB"],
                 )
                 subdomains_to_create.append(subdomain)
                 sdd_context.subdomain_dictionary[subdomain_id] = subdomain
                 print(f"Adding subdomain: {subdomain_id}")
 
@@ -393,20 +383,17 @@
                     if member_id not in sdd_context.member_dictionary:
                         member = MEMBER(
                             member_id=member_id,
                             name=choice[1],
                             code=choice[0],
-                            domain_id=sdd_context.domain_dictionary.get(domain_id)
+                            domain_id=sdd_context.domain_dictionary.get(domain_id),
                         )
                         members_to_create.append(member)
                         sdd_context.member_dictionary[member_id] = member
 
                         if subdomain:
-                            subdomain_enum = SUBDOMAIN_ENUMERATION(
-                                subdomain_id=subdomain,
-                                member_id=member
-                            )
+                            subdomain_enum = SUBDOMAIN_ENUMERATION(subdomain_id=subdomain, member_id=member)
                             subdomain_enums_to_create.append(subdomain_enum)
                             enum_key = f"{subdomain_id}:{member_id}"
                             sdd_context.subdomain_enumeration_dictionary[enum_key] = subdomain_enum
 
             # Bulk create all objects
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/input_model/import_input_model.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/joins_meta_data/create_joins_meta_data_combinations.py	2025-09-15 13:18:11.384229+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/joins_meta_data/create_joins_meta_data_combinations.py	2025-09-21 17:07:36.657671+00:00
@@ -46,13 +46,11 @@
 class JoinsMetaDataCreator:
     """
     A class for creating generation rules for reports and tables.
     """
 
-    def generate_joins_meta_data(
-        self, context: Any, sdd_context: Any, framework: str
-    ) -> None:
+    def generate_joins_meta_data(self, context: Any, sdd_context: Any, framework: str) -> None:
         """
         Generate generation rules for the given context and framework.
 
         Args:
             context (Any): The context object containing necessary data.
@@ -62,45 +60,38 @@
         self.add_reports(context, sdd_context, framework)
 
     def do_stuff_and_prepare_context(self, context: Any, sdd_context: Any):
 
         self.member_hierarchy_service = MemberHierarchyService()
-        sdd_context = self.member_hierarchy_service.prepare_node_dictionaries_and_lists(
-            sdd_context
+        sdd_context = self.member_hierarchy_service.prepare_node_dictionaries_and_lists(sdd_context)
+
+        all_main_categories = set(
+            map(lambda pk: MEMBER.objects.get(pk=pk), sum(context.report_to_main_category_map.values(), []))
         )
-
-        all_main_categories = set(map(lambda pk: MEMBER.objects.get(pk=pk),
-            sum(context.report_to_main_category_map.values(), [])))
 
         context.category_to_ci = {}
         context.domain_to_member = {}
 
         for combination in COMBINATION.objects.prefetch_related(
-            "combination_item_set__variable_id",
-            "combination_item_set__member_id"):
+            "combination_item_set__variable_id", "combination_item_set__member_id"
+        ):
             if any(item.member_id in all_main_categories for item in combination.combination_item_set.all()):
-                result = {
-                    item.variable_id: item.member_id
-                    for item in combination.combination_item_set.all()
-                }
+                result = {item.variable_id: item.member_id for item in combination.combination_item_set.all()}
                 category = set(result.values()).intersection(all_main_categories).pop()
                 if category.member_id not in context.category_to_ci:
                     context.category_to_ci[category.member_id] = {}
-                context.category_to_ci[category.member_id].update({
-                    combination: result
-                })
+                context.category_to_ci[category.member_id].update({combination: result})
 
         for member in MEMBER.objects.prefetch_related("domain_id"):
             if member.domain_id not in context.domain_to_member:
                 context.domain_to_member[member.domain_id] = set()
             context.domain_to_member[member.domain_id].add(member)
 
         context.facetted_items = {
             output_item.variable_id
             for output_item in CUBE_STRUCTURE_ITEM.objects.all()
-            if output_item.variable_id.domain_id.domain_id
-            in ["String", "Date", "Integer", "Boolean", "Float"]
+            if output_item.variable_id.domain_id.domain_id in ["String", "Date", "Integer", "Boolean", "Float"]
         }
 
         return context, sdd_context
 
     def add_reports(self, context: Any, sdd_context: Any, framework: str) -> None:
@@ -124,44 +115,34 @@
         with open(file_location, encoding="utf-8") as csvfile:
             filereader = csv.reader(csvfile, delimiter=",", quotechar='"')
             next(filereader)  # Skip header
             for row in filereader:
                 report_template = row[0]
-                generated_output_layer = self.find_output_layer_cube(
-                    sdd_context, report_template, framework
-                )
+                generated_output_layer = self.find_output_layer_cube(sdd_context, report_template, framework)
                 if generated_output_layer:
-                    self.add_join_for_products_il(
-                        context, sdd_context, generated_output_layer, framework
-                    )
-
-    def create_ldm_entity_to_linked_entities_map(
-        self, context: Any, sdd_context: Any
-    ) -> None:
+                    self.add_join_for_products_il(context, sdd_context, generated_output_layer, framework)
+
+    def create_ldm_entity_to_linked_entities_map(self, context: Any, sdd_context: Any) -> None:
         """
         Create a mapping of LDM entities to their linked entities.
 
         Args:
             context (Any): The context object containing necessary data.
             sdd_context (Any): The SDD context object.
         """
-        output_file = os.path.join(
-            context.output_directory, "csv", "ldm_entity_related_entities.csv"
-        )
+        output_file = os.path.join(context.output_directory, "csv", "ldm_entity_related_entities.csv")
         memoization_parents_from_disjoint_subtyping_eldm_search = {}
         with open(output_file, "w", encoding="utf-8") as f:
             f.write("ldm_entity,related_entities\n")
             for model in apps.get_models():
                 if model._meta.app_label == "pybirdai":
-                    entities = ELDMSearch.get_all_related_entities(self, context, model, memoization_parents_from_disjoint_subtyping_eldm_search)
-                    related_entities_string = ":".join(
-                        entity.__name__ for entity in entities
+                    entities = ELDMSearch.get_all_related_entities(
+                        self, context, model, memoization_parents_from_disjoint_subtyping_eldm_search
                     )
+                    related_entities_string = ":".join(entity.__name__ for entity in entities)
                     f.write(f"{model.__name__},{related_entities_string}\n")
-                    context.ldm_entity_to_linked_tables_map[model.__name__] = (
-                        related_entities_string
-                    )
+                    context.ldm_entity_to_linked_tables_map[model.__name__] = related_entities_string
 
     def add_join_for_products_il(
         self,
         context: Any,
         sdd_context: Any,
@@ -201,46 +182,30 @@
             main_categories = context.report_to_main_category_map[report_template]
             for mc in main_categories:
                 try:
                     tables = tables_for_main_category_map[mc]
                     for table in tables:
-                        inputLayerTable = self.find_input_layer_cube(
-                            sdd_context, table, framework
-                        )
+                        inputLayerTable = self.find_input_layer_cube(sdd_context, table, framework)
                         join_for_products = table_and_part_tuple_map[mc]
 
                         for join_for_product in join_for_products:
                             # print(f"join_for_product:{join_for_product}")
                             # print(inputLayerTable)
                             input_entity_list = [inputLayerTable]
-                            linked_tables = join_for_products_to_linked_tables_map[
-                                join_for_product
-                            ]
+                            linked_tables = join_for_products_to_linked_tables_map[join_for_product]
                             linked_tables_list = linked_tables.split(":")
-                            if (
-                                inputLayerTable
-                                and inputLayerTable.cube_structure_id
-                                not in linked_tables_list
-                            ):
-                                linked_tables_list.append(
-                                    inputLayerTable.cube_structure_id
-                                )
+                            if inputLayerTable and inputLayerTable.cube_structure_id not in linked_tables_list:
+                                linked_tables_list.append(inputLayerTable.cube_structure_id)
                             extra_tables = []
                             for the_table in linked_tables_list:
                                 extra_linked_tables = []
                                 try:
                                     # if the_table.endswith("_ELDM"):
-                                    extra_linked_tables_string = (
-                                        context.ldm_entity_to_linked_tables_map[
-                                            the_table
-                                        ]
-                                    )
+                                    extra_linked_tables_string = context.ldm_entity_to_linked_tables_map[the_table]
                                     # else:
                                     #    extra_linked_tables_string = context.ldm_entity_to_linked_tables_map[the_table + "_ELDM"]
-                                    extra_linked_tables = (
-                                        extra_linked_tables_string.split(":")
-                                    )
+                                    extra_linked_tables = extra_linked_tables_string.split(":")
                                 except KeyError:
                                     pass
 
                                 for extra_table in extra_linked_tables:
                                     if extra_table not in extra_tables:
@@ -249,101 +214,74 @@
                             for extra_table in extra_tables:
                                 if extra_table not in linked_tables_list:
                                     linked_tables_list.append(extra_table)
 
                             for the_table in linked_tables_list:
-                                the_input_table = self.find_input_layer_cube(
-                                    sdd_context, the_table, framework
-                                )
+                                the_input_table = self.find_input_layer_cube(sdd_context, the_table, framework)
                                 if the_input_table:
                                     input_entity_list.append(the_input_table)
 
                             if join_for_product[0] == table:
                                 for input_entity in input_entity_list:
                                     # print(f"input_entity:{input_entity}")
                                     cube_link = CUBE_LINK()
                                     cube_link.description = f"{join_for_product[0]}:{mc}:{join_for_product[1]}:{input_entity.cube_structure_id}"
-                                    cube_link.name = f"{join_for_product[0]}:{join_for_product[1]}:{input_entity.cube_structure_id}"
+                                    cube_link.name = (
+                                        f"{join_for_product[0]}:{join_for_product[1]}:{input_entity.cube_structure_id}"
+                                    )
                                     cube_link.join_identifier = join_for_product[1]
-                                    primary_cube = sdd_context.bird_cube_dictionary.get(
-                                        input_entity.cube_structure_id
-                                    )
+                                    primary_cube = sdd_context.bird_cube_dictionary.get(input_entity.cube_structure_id)
                                     if primary_cube:
                                         cube_link.primary_cube_id = primary_cube
                                         cube_link.cube_link_id = (
                                             f"{report_template}:"
                                             f"{input_entity.cube_structure_id}:{join_for_product[1]}"
                                         )
                                     else:
-                                        cube_link.cube_link_id = f"{input_entity.cube_structure_id}:{join_for_product[1]}"
+                                        cube_link.cube_link_id = (
+                                            f"{input_entity.cube_structure_id}:{join_for_product[1]}"
+                                        )
                                         # print(f"cube_link.primary_cube_id not found for {table}")
                                     cube_link.foreign_cube_id = generated_output_layer
 
-                                    if (
-                                        cube_link.cube_link_id
-                                        not in sdd_context.cube_link_dictionary
-                                    ):
-                                        sdd_context.cube_link_dictionary[
-                                            cube_link.cube_link_id
-                                        ] = cube_link
+                                    if cube_link.cube_link_id not in sdd_context.cube_link_dictionary:
+                                        sdd_context.cube_link_dictionary[cube_link.cube_link_id] = cube_link
                                         foreign_cube = cube_link.foreign_cube_id
                                         join_identifier = cube_link.join_identifier
-                                        join_for_report_id = (
-                                            foreign_cube.cube_id
-                                            + ":"
-                                            + cube_link.join_identifier
+                                        join_for_report_id = foreign_cube.cube_id + ":" + cube_link.join_identifier
+
+                                        if foreign_cube.cube_id not in sdd_context.cube_link_to_foreign_cube_map:
+                                            sdd_context.cube_link_to_foreign_cube_map[foreign_cube.cube_id] = []
+                                        sdd_context.cube_link_to_foreign_cube_map[foreign_cube.cube_id].append(
+                                            cube_link
                                         )
 
-                                        if (
-                                            foreign_cube.cube_id
-                                            not in sdd_context.cube_link_to_foreign_cube_map
-                                        ):
-                                            sdd_context.cube_link_to_foreign_cube_map[
-                                                foreign_cube.cube_id
-                                            ] = []
-                                        sdd_context.cube_link_to_foreign_cube_map[
-                                            foreign_cube.cube_id
-                                        ].append(cube_link)
-
-                                        if (
-                                            join_identifier
-                                            not in sdd_context.cube_link_to_join_identifier_map
-                                        ):
-                                            sdd_context.cube_link_to_join_identifier_map[
-                                                join_identifier
-                                            ] = []
-                                        sdd_context.cube_link_to_join_identifier_map[
-                                            join_identifier
-                                        ].append(cube_link)
-
-                                        if (
-                                            join_for_report_id
-                                            not in sdd_context.cube_link_to_join_for_report_id_map
-                                        ):
-                                            sdd_context.cube_link_to_join_for_report_id_map[
-                                                join_for_report_id
-                                            ] = []
-                                        sdd_context.cube_link_to_join_for_report_id_map[
-                                            join_for_report_id
-                                        ].append(cube_link)
-
-                                        num_of_cube_link_items = self.add_field_to_field_lineage_to_rules_for_join_for_product(
-                                            context,
-                                            sdd_context,
-                                            generated_output_layer,
-                                            input_entity,
-                                            mc,
-                                            report_template,
-                                            framework,
-                                            cube_link,
-                                            cube_structure_item_links_to_create,
+                                        if join_identifier not in sdd_context.cube_link_to_join_identifier_map:
+                                            sdd_context.cube_link_to_join_identifier_map[join_identifier] = []
+                                        sdd_context.cube_link_to_join_identifier_map[join_identifier].append(cube_link)
+
+                                        if join_for_report_id not in sdd_context.cube_link_to_join_for_report_id_map:
+                                            sdd_context.cube_link_to_join_for_report_id_map[join_for_report_id] = []
+                                        sdd_context.cube_link_to_join_for_report_id_map[join_for_report_id].append(
+                                            cube_link
                                         )
 
-                                        if (
-                                            context.save_derived_sdd_items
-                                            and num_of_cube_link_items > 0
-                                        ):
+                                        num_of_cube_link_items = (
+                                            self.add_field_to_field_lineage_to_rules_for_join_for_product(
+                                                context,
+                                                sdd_context,
+                                                generated_output_layer,
+                                                input_entity,
+                                                mc,
+                                                report_template,
+                                                framework,
+                                                cube_link,
+                                                cube_structure_item_links_to_create,
+                                            )
+                                        )
+
+                                        if context.save_derived_sdd_items and num_of_cube_link_items > 0:
                                             cube_links_to_create.append(cube_link)
 
                 except KeyError:
                     # traceback.print_exc()
                     logging.warning(f"no tables for main category:{mc}")
@@ -388,13 +326,11 @@
             report_template (str): The report template name.
             framework (str): The framework being used (e.g., "FINREP_REF").
             cube_link (Any): The cube link object.
         """
         num_of_cube_link_items = 0
-        for output_item in sdd_context.bird_cube_structure_item_dictionary[
-            output_entity.cube_id + "_cube_structure"
-        ]:
+        for output_item in sdd_context.bird_cube_structure_item_dictionary[output_entity.cube_id + "_cube_structure"]:
             operation_exists = self.operation_exists_in_cell_for_report_with_category(
                 context, sdd_context, output_item, category, report_template
             )
             in_facetted_items = output_item.variable_id in context.facetted_items
 
@@ -402,13 +338,11 @@
                 input_columns = self.find_variables_with_same_members_then_same_name(
                     context, sdd_context, output_item, input_entity, in_facetted_items
                 )
                 if input_columns:
                     for input_column in input_columns:
-                        csil, sdd_context = self.provide_csilink(
-                            output_item, input_column, cube_link, sdd_context
-                        )
+                        csil, sdd_context = self.provide_csilink(output_item, input_column, cube_link, sdd_context)
                         if context.save_derived_sdd_items:
                             cube_structure_item_links_to_create.append(csil)
                             num_of_cube_link_items = num_of_cube_link_items + 1
         return num_of_cube_link_items
 
@@ -424,24 +358,15 @@
                 f"{csil.foreign_cube_variable_code.variable_id.variable_id}",
                 f"{csil.primary_cube_variable_code.variable_id.variable_id}",
             ]
         )
 
-        sdd_context.cube_structure_item_links_dictionary[
-            csil.cube_structure_item_link_id
-        ] = csil
-
-        if (
-            cube_link.cube_link_id
-            not in sdd_context.cube_structure_item_link_to_cube_link_map
-        ):
-            sdd_context.cube_structure_item_link_to_cube_link_map[
-                cube_link.cube_link_id
-            ] = []
-        sdd_context.cube_structure_item_link_to_cube_link_map[
-            cube_link.cube_link_id
-        ].append(csil)
+        sdd_context.cube_structure_item_links_dictionary[csil.cube_structure_item_link_id] = csil
+
+        if cube_link.cube_link_id not in sdd_context.cube_structure_item_link_to_cube_link_map:
+            sdd_context.cube_structure_item_link_to_cube_link_map[cube_link.cube_link_id] = []
+        sdd_context.cube_structure_item_link_to_cube_link_map[cube_link.cube_link_id].append(csil)
         return csil, sdd_context
 
     def valid_operation(
         self,
         context: Any,
@@ -487,23 +412,18 @@
 
         Returns:
             bool: True if the operation exists, False otherwise.
         """
         report_combinations = {
-            el.combination_id
-            for el in sdd_context.combination_to_rol_cube_map.get(report_template, [])
+            el.combination_id for el in sdd_context.combination_to_rol_cube_map.get(report_template, [])
         }
         concerned_combinations_by_category_with_item = {
             combination
             for combination, items in context.category_to_ci[category].items()
             if output_item.variable_id in items
         }
-        concerned_combinations = (
-            concerned_combinations_by_category_with_item.intersection(
-                report_combinations
-            )
-        )
+        concerned_combinations = concerned_combinations_by_category_with_item.intersection(report_combinations)
         if concerned_combinations:
             context.variable_members_in_combinations = dict()
             for combination in concerned_combinations:
                 cis = context.category_to_ci[category][combination]
                 for var_, mem_ in cis.items():
@@ -534,44 +454,30 @@
             List[Any]: A list of matching variables.
         """
 
         related_variables = []
 
-        target_domain = (
-            output_item.variable_id.domain_id if output_item.variable_id else None
-        )
-
-        field_list = sdd_context.bird_cube_structure_item_dictionary.get(
-            input_entity.cube_structure_id, []
-        )
+        target_domain = output_item.variable_id.domain_id if output_item.variable_id else None
+
+        field_list = sdd_context.bird_cube_structure_item_dictionary.get(input_entity.cube_structure_id, [])
 
         if not in_facetted_items:
             # Same members / combination comparison
 
-            output_members = context.variable_members_in_combinations.get(
-                output_item.variable_id, set()
-            )
-            hierarchies = sdd_context.domain_to_hierarchy_dictionary.get(
-                output_item.variable_id.domain_id, []
-            )
+            output_members = context.variable_members_in_combinations.get(output_item.variable_id, set())
+            hierarchies = sdd_context.domain_to_hierarchy_dictionary.get(output_item.variable_id.domain_id, [])
             all_output_members = output_members.copy()
             if hierarchies:
-                for output_member, hierarchy in itertools.product(
-                    output_members, hierarchies
-                ):
+                for output_member, hierarchy in itertools.product(output_members, hierarchies):
                     all_output_members.update(
                         self.member_hierarchy_service.get_member_list_considering_hierarchies(
                             sdd_context, output_member, hierarchy
                         )
                     )
             IGNORED_DOMAINS = ["String", "Date", "Integer", "Boolean", "Float"]
 
-            if (
-                target_domain
-                and target_domain.domain_id
-                and target_domain.domain_id not in IGNORED_DOMAINS
-            ):
+            if target_domain and target_domain.domain_id and target_domain.domain_id not in IGNORED_DOMAINS:
                 # Early exit if no output members to compare
                 if not all_output_members:
                     return related_variables
 
                 # Initialize subdomain enumeration cache if not exists
@@ -579,15 +485,11 @@
                     sdd_context.subdomain_enumeration_cache = {}
 
                 # Collect all unique subdomains from field_list to batch load
                 subdomains_to_load = set()
                 for csi in field_list:
-                    if (
-                        csi.variable_id
-                        and csi.variable_id.domain_id
-                        and csi.subdomain_id
-                    ):
+                    if csi.variable_id and csi.variable_id.domain_id and csi.subdomain_id:
                         subdomain_id = csi.subdomain_id.subdomain_id
                         if subdomain_id not in sdd_context.subdomain_enumeration_cache:
                             subdomains_to_load.add(subdomain_id)
 
                 # Batch load subdomain enumerations for all subdomains at once
@@ -600,23 +502,17 @@
 
                     # Group by subdomain_id for caching
                     for enum in subdomain_enums:
                         subdomain_id = enum.subdomain_id.subdomain_id
                         if subdomain_id not in sdd_context.subdomain_enumeration_cache:
-                            sdd_context.subdomain_enumeration_cache[subdomain_id] = (
-                                set()
-                            )
-                        sdd_context.subdomain_enumeration_cache[subdomain_id].add(
-                            enum.member_id
-                        )
+                            sdd_context.subdomain_enumeration_cache[subdomain_id] = set()
+                        sdd_context.subdomain_enumeration_cache[subdomain_id].add(enum.member_id)
 
                     # Mark empty subdomains to avoid future queries
                     for subdomain_id in subdomains_to_load:
                         if subdomain_id not in sdd_context.subdomain_enumeration_cache:
-                            sdd_context.subdomain_enumeration_cache[subdomain_id] = (
-                                set()
-                            )
+                            sdd_context.subdomain_enumeration_cache[subdomain_id] = set()
 
                 # Now process field_list using cached data
                 for csi in field_list:
                     bool_1 = csi.variable_id and csi.variable_id.domain_id
                     if not bool_1:
@@ -625,16 +521,12 @@
                     subdomain = csi.subdomain_id
                     bool_2 = False
                     if subdomain:
                         subdomain_id = subdomain.subdomain_id
                         # Use cached subdomain enumeration data
-                        subdomain_members = sdd_context.subdomain_enumeration_cache.get(
-                            subdomain_id, set()
-                        )
-                        bool_2 = bool(
-                            subdomain_members.intersection(all_output_members)
-                        )
+                        subdomain_members = sdd_context.subdomain_enumeration_cache.get(subdomain_id, set())
+                        bool_2 = bool(subdomain_members.intersection(all_output_members))
                     else:
                         # print(f"no subdomain for {csi}:{csi.variable_id}:{csi.cube_structure_id.cube_structure_id}")
                         bool_2 = False
 
                     if bool_1 and bool_2:
@@ -644,26 +536,20 @@
                 return related_variables
 
         # Same name comparison
 
         # logging.warning(f"CHECKING OUTPUT VARIABLE NAME FOR {output_item}")
-        output_variable_name = (
-            output_item.variable_id.variable_id if output_item.variable_id else None
-        )
+        output_variable_name = output_item.variable_id.variable_id if output_item.variable_id else None
         if output_variable_name:
             related_variables = [
-                csi
-                for csi in field_list
-                if csi.variable_id and csi.variable_id.name == output_variable_name
+                csi for csi in field_list if csi.variable_id and csi.variable_id.name == output_variable_name
             ]
         # if not related_variables:
         #     logging.warning(f"No related variables found for {output_item}")
         return related_variables
 
-    def find_output_layer_cube(
-        self, sdd_context: Any, output_layer_name: str, framework: str
-    ) -> Any:
+    def find_output_layer_cube(self, sdd_context: Any, output_layer_name: str, framework: str) -> Any:
         """
         Find the output layer cube for a given output layer name and framework.
 
         Args:
             sdd_context (Any): The SDD context object.
@@ -671,20 +557,14 @@
             framework (str): The framework being used (e.g., "FINREP_REF").
 
         Returns:
             Any: The output layer cube if found, None otherwise.
         """
-        output_layer_name = (
-            f"{output_layer_name}_REF_FINREP_3_0"
-            if framework == "FINREP_REF"
-            else output_layer_name
-        )
+        output_layer_name = f"{output_layer_name}_REF_FINREP_3_0" if framework == "FINREP_REF" else output_layer_name
         return sdd_context.bird_cube_dictionary.get(output_layer_name)
 
-    def find_input_layer_cube(
-        self, sdd_context: Any, input_layer_name: str, framework: str
-    ) -> Any:
+    def find_input_layer_cube(self, sdd_context: Any, input_layer_name: str, framework: str) -> Any:
         """
         Find the input layer cube for a given input layer name and framework.
 
         Args:
             sdd_context (Any): The SDD context object.
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/joins_meta_data/create_joins_meta_data_combinations.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/pybird/orchestration_original.py	2025-08-02 18:37:08.456506+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/pybird/orchestration_original.py	2025-09-21 17:07:36.681073+00:00
@@ -12,102 +12,104 @@
 
 
 from pybirdai.process_steps.pybird.csv_converter import CSVConverter
 
 import importlib
-class Orchestration:
-	# Class variable to track initialized objects
-	_initialized_objects = set()
-	
-	def init(self,theObject):
-		# Check if this object has already been initialized
-		object_id = id(theObject)
-		if object_id in Orchestration._initialized_objects:
-			print(f"Object of type {theObject.__class__.__name__} already initialized, skipping.")
-			# Even if we're skipping full initialization, we still need to ensure references are set
-			self._ensure_references_set(theObject)
-			return
-		
-		# Mark this object as initialized
-		Orchestration._initialized_objects.add(object_id)
-		
-		# Set up references for the object
-		self._ensure_references_set(theObject)
-
-	def _ensure_references_set(self, theObject):
-		"""
-		Ensure that all table references are properly set for the object.
-		This is called both during full initialization and when initialization is skipped.
-		"""
-		references = [method for method in dir(theObject.__class__) if not callable(
-		getattr(theObject.__class__, method)) and not method.startswith('__')]
-		for eReference in references:
-			if eReference.endswith("Table"):
-				# Only set the reference if it's currently None
-				if getattr(theObject, eReference) is None:
-					from django.apps import apps
-					table_name = eReference.split('_Table')[0]
-					relevant_model = None
-					try:
-						relevant_model = apps.get_model('pybirdai',table_name)
-					except LookupError:
-						print("LookupError: " + table_name)
-
-					if relevant_model:
-						print("relevant_model: " + str(relevant_model))
-						newObject = relevant_model.objects.all()
-						print("newObject: " + str(newObject))
-						if newObject:
-							setattr(theObject,eReference,newObject)
-							CSVConverter.persist_object_as_csv(newObject,True);						
-						
-					else:
-						newObject = Orchestration.createObjectFromReferenceType(eReference);
-						
-						operations = [method for method in dir(newObject.__class__) if callable(
-							getattr(newObject.__class__, method)) and not method.startswith('__')]
-						
-						for operation in operations:
-							if operation == "init":
-								try:
-									getattr(newObject, operation)()
-								except:
-									print (" could not call function called " + operation)
-
-						setattr(theObject,eReference,newObject)
-
-	@classmethod
-	def reset_initialization(cls):
-		"""
-		Reset the initialization tracking.
-		This can be useful for testing or when re-initialization is required.
-		"""
-		cls._initialized_objects.clear()
-		print("Initialization tracking has been reset.")
-		
-	@classmethod
-	def is_initialized(cls, obj):
-		"""
-		Check if an object has been initialized.
-		
-		Args:
-			obj: The object to check
-			
-		Returns:
-			bool: True if the object has been initialized, False otherwise
-		"""
-		return id(obj) in cls._initialized_objects
-
-	def createObjectFromReferenceType(eReference):
-		try:
-			cls = getattr(importlib.import_module('pybirdai.process_steps.filter_code.output_tables'), eReference)
-			new_object = cls()		
-			return new_object;	
-		except:
-			print("Error: " + eReference)
-		
-		
 
 
+class Orchestration:
+    # Class variable to track initialized objects
+    _initialized_objects = set()
 
+    def init(self, theObject):
+        # Check if this object has already been initialized
+        object_id = id(theObject)
+        if object_id in Orchestration._initialized_objects:
+            print(f"Object of type {theObject.__class__.__name__} already initialized, skipping.")
+            # Even if we're skipping full initialization, we still need to ensure references are set
+            self._ensure_references_set(theObject)
+            return
 
+        # Mark this object as initialized
+        Orchestration._initialized_objects.add(object_id)
 
+        # Set up references for the object
+        self._ensure_references_set(theObject)
+
+    def _ensure_references_set(self, theObject):
+        """
+        Ensure that all table references are properly set for the object.
+        This is called both during full initialization and when initialization is skipped.
+        """
+        references = [
+            method
+            for method in dir(theObject.__class__)
+            if not callable(getattr(theObject.__class__, method)) and not method.startswith("__")
+        ]
+        for eReference in references:
+            if eReference.endswith("Table"):
+                # Only set the reference if it's currently None
+                if getattr(theObject, eReference) is None:
+                    from django.apps import apps
+
+                    table_name = eReference.split("_Table")[0]
+                    relevant_model = None
+                    try:
+                        relevant_model = apps.get_model("pybirdai", table_name)
+                    except LookupError:
+                        print("LookupError: " + table_name)
+
+                    if relevant_model:
+                        print("relevant_model: " + str(relevant_model))
+                        newObject = relevant_model.objects.all()
+                        print("newObject: " + str(newObject))
+                        if newObject:
+                            setattr(theObject, eReference, newObject)
+                            CSVConverter.persist_object_as_csv(newObject, True)
+
+                    else:
+                        newObject = Orchestration.createObjectFromReferenceType(eReference)
+
+                        operations = [
+                            method
+                            for method in dir(newObject.__class__)
+                            if callable(getattr(newObject.__class__, method)) and not method.startswith("__")
+                        ]
+
+                        for operation in operations:
+                            if operation == "init":
+                                try:
+                                    getattr(newObject, operation)()
+                                except:
+                                    print(" could not call function called " + operation)
+
+                        setattr(theObject, eReference, newObject)
+
+    @classmethod
+    def reset_initialization(cls):
+        """
+        Reset the initialization tracking.
+        This can be useful for testing or when re-initialization is required.
+        """
+        cls._initialized_objects.clear()
+        print("Initialization tracking has been reset.")
+
+    @classmethod
+    def is_initialized(cls, obj):
+        """
+        Check if an object has been initialized.
+
+        Args:
+                obj: The object to check
+
+        Returns:
+                bool: True if the object has been initialized, False otherwise
+        """
+        return id(obj) in cls._initialized_objects
+
+    def createObjectFromReferenceType(eReference):
+        try:
+            cls = getattr(importlib.import_module("pybirdai.process_steps.filter_code.output_tables"), eReference)
+            new_object = cls()
+            return new_object
+        except:
+            print("Error: " + eReference)
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/pybird/orchestration_original.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/pybird/test_orchestration.py	2025-08-02 18:37:08.456644+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/pybird/test_orchestration.py	2025-09-21 17:07:36.682076+00:00
@@ -10,91 +10,98 @@
 # Contributors:
 #    Neil Mackenzie - initial API and implementation
 
 from pybirdai.process_steps.pybird.orchestration import Orchestration
 
+
 class ReferenceTable:
     """A simple reference table class."""
+
     def init(self):
         print("ReferenceTable initialized!")
         return None
 
+
 class TestTable:
     """A simple test class with an init method and a reference."""
+
     reference_Table = None
-    
+
     def init(self):
         print("TestTable initialized!")
         return None
-    
+
     def calc_referenced_items(self):
         """Test method that uses the reference."""
         if self.reference_Table is None:
             raise AttributeError("'NoneType' object has no attribute 'items'")
         return "Referenced items accessed successfully"
 
+
 def test_initialization_tracking():
     """Test that objects are only initialized once."""
     # Reset initialization tracking to start fresh
     Orchestration.reset_initialization()
-    
+
     # Create a test object
     test_obj = TestTable()
-    
+
     # Check that it's not initialized yet
     assert not Orchestration.is_initialized(test_obj), "Object should not be initialized yet"
-    
+
     # Initialize it
     Orchestration().init(test_obj)
-    
+
     # Check that it's now initialized
     assert Orchestration.is_initialized(test_obj), "Object should be initialized now"
-    
+
     # Check that the reference was set
     assert test_obj.reference_Table is not None, "Reference should be set after initialization"
-    
+
     # Try to initialize it again - should be skipped
     Orchestration().init(test_obj)
-    
+
     # Reset initialization tracking
     Orchestration.reset_initialization()
-    
+
     # Check that it's no longer considered initialized
     assert not Orchestration.is_initialized(test_obj), "Object should not be initialized after reset"
-    
+
     print("All tests passed!")
+
 
 def test_reference_handling():
     """Test that references are properly set even when initialization is skipped."""
     # Reset initialization tracking to start fresh
     Orchestration.reset_initialization()
-    
+
     # Create a test object
     test_obj = TestTable()
-    
+
     # Initialize it
     Orchestration().init(test_obj)
-    
+
     # Check that the reference was set
     assert test_obj.reference_Table is not None, "Reference should be set after initialization"
-    
+
     # Clear the reference to simulate it being lost
     test_obj.reference_Table = None
-    
+
     # Try to initialize it again - should be skipped but references should be set
     Orchestration().init(test_obj)
-    
+
     # Check that the reference was set again
     assert test_obj.reference_Table is not None, "Reference should be set even when initialization is skipped"
-    
+
     # Try to access referenced items
     try:
         result = test_obj.calc_referenced_items()
         print(f"Reference access result: {result}")
     except AttributeError as e:
         assert False, f"Reference access failed: {e}"
-    
+
     print("Reference handling test passed!")
+
 
 if __name__ == "__main__":
     test_initialization_tracking()
-    test_reference_handling() 
\ No newline at end of file
+    test_reference_handling()
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/pybird/test_orchestration.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/pybird/execute_datapoint.py	2025-09-02 15:09:38.717386+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/pybird/execute_datapoint.py	2025-09-21 17:07:36.708363+00:00
@@ -13,110 +13,116 @@
 import importlib
 import os
 from datetime import datetime
 from django.conf import settings
 
+
 class ExecuteDataPoint:
     def execute_data_point(data_point_id):
         ExecuteDataPoint.delete_lineage_data()
         print(f"Executing data point with ID: {data_point_id}")
-        
+
         # Set up AORTA lineage tracking
         from pybirdai.process_steps.pybird.orchestration import Orchestration, OrchestrationWithLineage
         from pybirdai.annotations.decorators import set_lineage_orchestration
         from pybirdai.context.context import Context
-        
+
         # Create orchestration based on configuration
         orchestration = Orchestration()
         execution_name = f"DataPoint_{data_point_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
-        
+
         # Only set up lineage if using the lineage-enhanced orchestrator and lineage is enabled
         if isinstance(orchestration, OrchestrationWithLineage) and orchestration.lineage_enabled:
             # Initialize the trail and metadata without dummy objects
             orchestration.trail = None
             orchestration.metadata_trail = None
             orchestration.current_populated_tables = {}
             orchestration.current_rows = {}
-            
+
             # Create trail directly
             from pybirdai.models import MetaDataTrail, Trail
+
             orchestration.metadata_trail = MetaDataTrail.objects.create()
-            orchestration.trail = Trail.objects.create(
-                name=execution_name,
-                metadata_trail=orchestration.metadata_trail
-            )
+            orchestration.trail = Trail.objects.create(name=execution_name, metadata_trail=orchestration.metadata_trail)
             print(f"Created AORTA Trail: {orchestration.trail.name}")
-            
+
             # Set the global lineage context
             set_lineage_orchestration(orchestration)
         elif isinstance(orchestration, OrchestrationWithLineage):
             print(f"Using lineage orchestrator but lineage tracking is disabled in config")
             # Clear the global lineage context since lineage is disabled
             set_lineage_orchestration(None)
         else:
             print(f"Using original orchestrator - lineage tracking disabled")
             # Clear the global lineage context since we're using original orchestrator
             set_lineage_orchestration(None)
-        
+
         # Initialize with lineage tracking
-        klass = globals()['Cell_' + str(data_point_id)]
+        klass = globals()["Cell_" + str(data_point_id)]
         datapoint = klass()
-        
+
         # Set calculation context early if lineage is enabled
         if isinstance(orchestration, OrchestrationWithLineage) and orchestration.lineage_enabled:
             calculation_name = datapoint.__class__.__name__
             orchestration.current_calculation = calculation_name
             print(f"Set calculation context: {calculation_name}")
-            
+
             # Add debugging to orchestration
             from pybirdai.debug_tracking import add_debug_to_orchestration
+
             add_debug_to_orchestration(orchestration)
-            
+
             # Set calculation context BEFORE initialization to capture all function calls
             orchestration.current_calculation = calculation_name
             print(f"Set orchestration context to: {calculation_name}")
-            
+
             # CRITICAL FIX: Apply wrapper BEFORE init() so calc_referenced_items is wrapped when called
             from pybirdai.process_steps.filter_code.automatic_tracking_wrapper import create_smart_tracking_wrapper
+
             datapoint = create_smart_tracking_wrapper(datapoint, orchestration)
             print(f"Added automatic tracking wrapper to {calculation_name}")
-        
+
         # Execute the datapoint (now init() will call the wrapped calc_referenced_items)
         datapoint.init()
         metric_value = str(datapoint.metric_value())
-        
+
         # Print lineage summary
         if isinstance(orchestration, OrchestrationWithLineage) and orchestration.lineage_enabled:
             trail = orchestration.get_lineage_trail()
             if trail:
                 print(f"AORTA Trail created: {trail.name} (ID: {trail.id})")
                 from pybirdai.models import (
-                    DatabaseTable, PopulatedDataBaseTable, DatabaseField, DatabaseRow,
-                    CalculationUsedRow, CalculationUsedField
+                    DatabaseTable,
+                    PopulatedDataBaseTable,
+                    DatabaseField,
+                    DatabaseRow,
+                    CalculationUsedRow,
+                    CalculationUsedField,
                 )
+
                 print(f"  DatabaseTables: {DatabaseTable.objects.count()}")
                 print(f"  PopulatedTables: {PopulatedDataBaseTable.objects.count()}")
                 print(f"  DatabaseFields: {DatabaseField.objects.count()}")
                 print(f"  DatabaseRows: {DatabaseRow.objects.count()}")
-                
+
                 # Print tracking information
                 used_rows = CalculationUsedRow.objects.filter(trail=trail)
                 used_fields = CalculationUsedField.objects.filter(trail=trail)
                 print(f"  Tracked Used Rows: {used_rows.count()}")
                 print(f"  Tracked Used Fields: {used_fields.count()}")
-                
+
                 if used_rows.exists():
-                    calculation_names = used_rows.values_list('calculation_name', flat=True).distinct()
+                    calculation_names = used_rows.values_list("calculation_name", flat=True).distinct()
                     for calc_name in calculation_names:
                         row_count = used_rows.filter(calculation_name=calc_name).count()
                         field_count = used_fields.filter(calculation_name=calc_name).count()
                         print(f"    {calc_name}: {row_count} rows, {field_count} fields")
-        
+
         del datapoint
         return metric_value
 
     def delete_lineage_data():
         base_dir = settings.BASE_DIR
-        lineage_dir = os.path.join(base_dir, 'results', 'lineage')
+        lineage_dir = os.path.join(base_dir, "results", "lineage")
         for file in os.listdir(lineage_dir):
             if file != "__init__.py":
                 os.remove(os.path.join(lineage_dir, file))
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/pybird/execute_datapoint.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/mapping_join_metadata_eil_ldm/mapping_join_eil_ldm.py	2025-08-02 18:37:08.454901+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/mapping_join_metadata_eil_ldm/mapping_join_eil_ldm.py	2025-09-21 17:07:36.709944+00:00
@@ -12,16 +12,20 @@
 #
 
 import csv
 import os
 
+
 class LinkProcessor:
     """
     Processes link files based on mappings from a source file,
     replacing IDs and codes using standard CSV handling.
     """
-    def __init__(self, mapping_file_path, cube_link_file_path, csi_link_file_path, output_cube_link_path, output_csi_link_path):
+
+    def __init__(
+        self, mapping_file_path, cube_link_file_path, csi_link_file_path, output_cube_link_path, output_csi_link_path
+    ):
         """
         Initializes the LinkProcessor with input and output file paths.
 
         Args:
             mapping_file_path (str): Path to the mapping CSV file.
@@ -56,13 +60,13 @@
                         Returns an empty list if the file is empty or not found.
         """
         data = []
         try:
             # Use utf-8 encoding as it's common. newline='' is crucial for csv module.
-            with open(file_path, mode='r', newline='', encoding='utf-8') as infile:
+            with open(file_path, mode="r", newline="", encoding="utf-8") as infile:
                 reader = csv.DictReader(infile)
-                data = list(reader) # Read all rows into a list
+                data = list(reader)  # Read all rows into a list
         except FileNotFoundError:
             print(f"Error: Input file not found at {file_path}")
         except Exception as e:
             print(f"Error reading file {file_path}: {e}")
         return data
@@ -78,18 +82,18 @@
             list[str]: A list of header strings. Returns an empty list if the file
                        is empty or not found, or an error occurs.
         """
         headers = []
         try:
-            with open(file_path, mode='r', newline='', encoding='utf-8') as infile:
+            with open(file_path, mode="r", newline="", encoding="utf-8") as infile:
                 reader = csv.reader(infile)
                 try:
                     headers = next(reader)
                 except StopIteration:
-                    pass # File is empty
+                    pass  # File is empty
         except FileNotFoundError:
-             print(f"Error: Header file not found at {file_path}")
+            print(f"Error: Header file not found at {file_path}")
         except Exception as e:
             print(f"Error reading headers from {file_path}: {e}")
         return headers
 
     def write_csv_data(self, file_path, data, fieldnames):
@@ -114,27 +118,26 @@
             except OSError as e:
                 print(f"Error creating output directory {output_dir}: {e}")
                 return
 
         try:
-            with open(file_path, mode='w', newline='', encoding='utf-8') as outfile:
+            with open(file_path, mode="w", newline="", encoding="utf-8") as outfile:
                 writer = csv.DictWriter(outfile, fieldnames=fieldnames)
                 writer.writeheader()
                 writer.writerows(data)
         except Exception as e:
             print(f"Error writing data to {file_path}: {e}")
 
-
     def build_mappings(self):
         """
         Builds mapping dictionaries (mapping_dict, mapping_dict_tuple, mapping_cube)
         from the loaded mapping data.
         Skips rows where required mapping columns are missing or empty.
         """
         if not self.mapping_data:
-             print("Warning: Mapping data not loaded. Cannot build mappings.")
-             return
+            print("Warning: Mapping data not loaded. Cannot build mappings.")
+            return
 
         # Expected columns in mapping file
         required_cols = ["Logical_Object_Name", "Entity_Name", "Relational_Object_Name", "Table_Name"]
 
         for row in self.mapping_data:
@@ -142,11 +145,11 @@
             values = [row.get(col) for col in required_cols]
 
             # Check if all required columns exist and have non-empty string values
             # Original pandas used .dropna() on selected columns. DictReader gives empty strings.
             # Treat None or empty string as missing data.
-            if not all(values) or any(value == '' for value in values):
+            if not all(values) or any(value == "" for value in values):
                 # Skip row if any required mapping column is missing or empty
                 # print(f"Skipping mapping row due to missing/empty data: {row}") # Optional debug
                 continue
 
             logical_object_name = values[0]
@@ -159,16 +162,21 @@
             entity_name_processed = entity_name.replace(" ", "_")
 
             # Build mapping_dict
             # Key: Table_Name__Relational_Object_Name__
             # Value: Entity_Name__Logical_Object_Name
-            self.mapping_dict[f"{table_name}__{relational_object_name}__"] = f"{entity_name_processed}__{logical_object_name_processed}"
+            self.mapping_dict[f"{table_name}__{relational_object_name}__"] = (
+                f"{entity_name_processed}__{logical_object_name_processed}"
+            )
 
             # Build mapping_dict_tuple
             # Key: (Table_Name, Relational_Object_Name)
             # Value: (Entity_Name, Logical_Object_Name)
-            self.mapping_dict_tuple[(table_name, relational_object_name)] = (entity_name_processed, logical_object_name_processed)
+            self.mapping_dict_tuple[(table_name, relational_object_name)] = (
+                entity_name_processed,
+                logical_object_name_processed,
+            )
 
             # Build mapping_cube
             # Key: Table_Name
             # Value: Entity_Name (processed)
             self.mapping_cube[table_name] = entity_name_processed
@@ -179,34 +187,34 @@
         """
         if not self.cube_link_data:
             print("Warning: Cube link data not loaded. Cannot process cube links.")
             return []
         if not self.mapping_cube:
-             print("Warning: Mapping cube data not built. Cannot process cube links.")
-             return []
+            print("Warning: Mapping cube data not built. Cannot process cube links.")
+            return []
 
         new_cube_links = []
         # Assuming required column is 'PRIMARY_CUBE_ID'
-        required_col = 'PRIMARY_CUBE_ID'
-        replace_cols = ['CUBE_LINK_ID',"NAME","DESCRIPTION"]
+        required_col = "PRIMARY_CUBE_ID"
+        replace_cols = ["CUBE_LINK_ID", "NAME", "DESCRIPTION"]
 
         for row in self.cube_link_data:
-            primary_cube_id = row.get(required_col,"")
-
-            eil_cube = self.mapping_cube.get(primary_cube_id,"")
+            primary_cube_id = row.get(required_col, "")
+
+            eil_cube = self.mapping_cube.get(primary_cube_id, "")
 
             if eil_cube:
                 # Create a new row (dictionary) for modification to avoid changing original data
                 new_row = row.copy()
                 # Replace the value in the 'PRIMARY_CUBE_ID' column with the mapped value
                 new_row[required_col] = eil_cube
                 for col in replace_cols:
                     new_row[col] = new_row[col].replace(primary_cube_id, eil_cube)
                 new_cube_links.append(new_row)
             # else:
-                 # Skip the row if a mapping for PRIMARY_CUBE_ID is not found in mapping_cube
-                 # (This matches the original code's implicit behaviour)
+            # Skip the row if a mapping for PRIMARY_CUBE_ID is not found in mapping_cube
+            # (This matches the original code's implicit behaviour)
 
         return new_cube_links
 
     def process_csi_links(self):
         """
@@ -216,42 +224,42 @@
         """
         if not self.csi_link_data:
             print("Warning: CSI link data not loaded. Cannot process CSI links.")
             return []
         if not self.mapping_dict or not self.mapping_dict_tuple:
-             print("Warning: Mapping dictionary or tuple mapping not built. Cannot process CSI links.")
-             return []
+            print("Warning: Mapping dictionary or tuple mapping not built. Cannot process CSI links.")
+            return []
 
         new_csi_links = []
         # Assuming required headers are 'PRIMARY_CUBE_VARIABLE_CODE', 'CUBE_STRUCTURE_ITEM_LINK_ID', 'CUBE_LINK_ID'
-        required_cols = ['PRIMARY_CUBE_VARIABLE_CODE', 'CUBE_STRUCTURE_ITEM_LINK_ID', 'CUBE_LINK_ID']
+        required_cols = ["PRIMARY_CUBE_VARIABLE_CODE", "CUBE_STRUCTURE_ITEM_LINK_ID", "CUBE_LINK_ID"]
 
         for row in self.csi_link_data:
             # Check if all required columns exist and have non-empty values
             values = [row.get(col) for col in required_cols]
-            if not all(values) or any(value == '' for value in values):
+            if not all(values) or any(value == "" for value in values):
                 # Skip row if essential columns are missing or empty
                 # print(f"Skipping CSI link row due to missing/empty required data: {row}") # Optional debug
                 continue
 
             primary_cube_variable_code = values[0]
             csi_link_id = values[1]
             cube_link_id = values[2]
 
             # --- Process PRIMARY_CUBE_VARIABLE_CODE mapping ---
             row_code_parts = primary_cube_variable_code.rsplit("__", 1)
-            row_code_prefix = row_code_parts[0] + "__" # Original adds "__" back
+            row_code_prefix = row_code_parts[0] + "__"  # Original adds "__" back
 
             eil_csi = self.mapping_dict.get(row_code_prefix)
 
             # --- Process CUBE_STRUCTURE_ITEM_LINK_ID and CUBE_LINK_ID mapping ---
             csil_split = csi_link_id.split(":")
             # Expecting format like "prefix:old_cube:mid1:mid2:variable" (5 parts)
             if len(csil_split) != 5:
-                 # Skip if CUBE_STRUCTURE_ITEM_LINK_ID format is unexpected
-                 # print(f"Skipping CSI link row due to unexpected CUBE_STRUCTURE_ITEM_LINK_ID format: {csi_link_id}") # Optional debug
-                 continue
+                # Skip if CUBE_STRUCTURE_ITEM_LINK_ID format is unexpected
+                # print(f"Skipping CSI link row due to unexpected CUBE_STRUCTURE_ITEM_LINK_ID format: {csi_link_id}") # Optional debug
+                continue
 
             # Extract old_cube and variable from the split string
             # Use try-except just in case, though len check should prevent IndexError
             old_cube = csil_split[1]
             variable = csil_split[4]
@@ -261,36 +269,35 @@
             # --- Apply mappings if both parts are found ---
             # Original logic implicitly requires both mappings to exist (it checks eil_csi,
             # then accesses the result of mapping_dict_tuple.get which caused a None error).
             # We explicitly check for both here.
             if eil_csi and mapping_tuple_result:
-                new_row = row.copy() # Create a new row dict for modification
+                new_row = row.copy()  # Create a new row dict for modification
 
                 # Apply mapping for PRIMARY_CUBE_VARIABLE_CODE
                 # Replace only the prefix part with the new mapped value
-                new_row['PRIMARY_CUBE_VARIABLE_CODE'] = primary_cube_variable_code.replace(row_code_prefix, eil_csi)
+                new_row["PRIMARY_CUBE_VARIABLE_CODE"] = primary_cube_variable_code.replace(row_code_prefix, eil_csi)
 
                 # Apply mapping for CUBE_STRUCTURE_ITEM_LINK_ID
                 (new_cube, new_variable) = mapping_tuple_result
-                new_row['CUBE_STRUCTURE_ITEM_LINK_ID'] = ":".join([
-                    csil_split[0], new_cube, csil_split[2], csil_split[3], new_variable
-                ])
+                new_row["CUBE_STRUCTURE_ITEM_LINK_ID"] = ":".join(
+                    [csil_split[0], new_cube, csil_split[2], csil_split[3], new_variable]
+                )
 
                 # Apply mapping for CUBE_LINK_ID
                 # Replace the old_cube part with new_cube
                 # Original code did a simple replace. Assuming old_cube substring exists in CUBE_LINK_ID.
                 # A safer approach might involve checking if old_cube is a part of CUBE_LINK_ID first.
                 # Sticking to original logic's direct replace.
-                new_row['CUBE_LINK_ID'] = cube_link_id.replace(old_cube, new_cube)
+                new_row["CUBE_LINK_ID"] = cube_link_id.replace(old_cube, new_cube)
 
                 new_csi_links.append(new_row)
             # else:
-                 # Skip the row if either the variable code mapping (eil_csi)
-                 # or the cube/variable tuple mapping (mapping_tuple_result) is not found.
+            # Skip the row if either the variable code mapping (eil_csi)
+            # or the cube/variable tuple mapping (mapping_tuple_result) is not found.
 
         return new_csi_links
-
 
     def process_all_files(self):
         """
         Orchestrates the loading of data, building mappings, processing links,
         and writing output files.
@@ -301,14 +308,17 @@
         # This ensures output files have correct headers even if processing results in no rows
         cube_link_headers = self.read_csv_headers(self.cube_link_file_path)
         csi_link_headers = self.read_csv_headers(self.csi_link_file_path)
 
         if not cube_link_headers:
-            print(f"Warning: Could not read headers from {self.cube_link_file_path}. Output for cube links might be skipped or incorrect.")
+            print(
+                f"Warning: Could not read headers from {self.cube_link_file_path}. Output for cube links might be skipped or incorrect."
+            )
         if not csi_link_headers:
-            print(f"Warning: Could not read headers from {self.csi_link_file_path}. Output for CSI links might be skipped or incorrect.")
-
+            print(
+                f"Warning: Could not read headers from {self.csi_link_file_path}. Output for CSI links might be skipped or incorrect."
+            )
 
         # Load input data using DictReader
         print(f"Loading mapping data from {self.mapping_file_path}...")
         self.mapping_data = self.read_csv_data(self.mapping_file_path)
         print(f"Loading cube link data from {self.cube_link_file_path}...")
@@ -321,11 +331,10 @@
         self.build_mappings()
         print(f"Built {len(self.mapping_dict)} variable code mappings.")
         print(f"Built {len(self.mapping_dict_tuple)} cube/variable mappings.")
         print(f"Built {len(self.mapping_cube)} cube mappings.")
 
-
         # Process link data using the built mappings
         print("Processing cube links...")
         processed_cube_links = self.process_cube_links()
         print(f"Processed {len(processed_cube_links)} cube link rows.")
 
@@ -348,19 +357,19 @@
 
         # Define file paths based on the original script
         MAPPING_FILE = "resources/ldm/mappings.csv"
         CUBE_LINK_INPUT_FILE = "resources/joins_export/export_filecube_link.csv"
         CSI_LINK_INPUT_FILE = "resources/joins_export/export_filecube_structure_item_link.csv"
-        CUBE_LINK_OUTPUT_FILE = "resources/joins_export/new_cube_links_df.csv" # Naming matches original output
-        CSI_LINK_OUTPUT_FILE = "resources/joins_export/new_csi_links_df.csv"   # Naming matches original output
+        CUBE_LINK_OUTPUT_FILE = "resources/joins_export/new_cube_links_df.csv"  # Naming matches original output
+        CSI_LINK_OUTPUT_FILE = "resources/joins_export/new_csi_links_df.csv"  # Naming matches original output
 
         # Instantiate the processor
         processor = cls(
             mapping_file_path=MAPPING_FILE,
             cube_link_file_path=CUBE_LINK_INPUT_FILE,
             csi_link_file_path=CSI_LINK_INPUT_FILE,
             output_cube_link_path=CUBE_LINK_OUTPUT_FILE,
-            output_csi_link_path=CSI_LINK_OUTPUT_FILE
+            output_csi_link_path=CSI_LINK_OUTPUT_FILE,
         )
 
         # Run the processing method
         processor.process_all_files()
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/mapping_join_metadata_eil_ldm/mapping_join_eil_ldm.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/report_filters/create_output_layers.py	2025-09-15 13:18:11.385887+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/report_filters/create_output_layers.py	2025-09-21 17:07:36.717950+00:00
@@ -14,29 +14,26 @@
 from pybirdai.models.bird_meta_data_model import *
 
 import os
 import csv
 
+
 class CreateOutputLayers:
     def create_filters(self, context, sdd_context, framework, version):
         """
         Create output layers for each cube mapping based on variable mappings
         and expanded variable set mappings.
         """
-        file_location = os.path.join(
-            context.file_directory, "joins_configuration", f"in_scope_reports_{framework}.csv"
-        )
-        in_scope_reports = self._get_in_scope_reports(
-            file_location, framework, version
-        )
+        file_location = os.path.join(context.file_directory, "joins_configuration", f"in_scope_reports_{framework}.csv")
+        in_scope_reports = self._get_in_scope_reports(file_location, framework, version)
 
         # Lists to collect objects for bulk creation
         cubes_to_create = []
         structures_to_create = []
 
         for destination_cube in sdd_context.mapping_to_cube_dictionary.keys():
-            if destination_cube.replace('.', '_') in in_scope_reports:
+            if destination_cube.replace(".", "_") in in_scope_reports:
                 cube, structure = self.create_output_layer_for_cube_mapping(
                     context, sdd_context, destination_cube, framework
                 )
                 if cube and structure:  # Only add if objects were created
                     cubes_to_create.append(cube)
@@ -57,14 +54,14 @@
             version (str): The version of the framework.
 
         Returns:
             list: A list of in-scope report names.
         """
-        with open(file_location, encoding='utf-8') as csvfile:
+        with open(file_location, encoding="utf-8") as csvfile:
             return [
                 self._generate_report_name(row[0], framework, version)
-                for row in csv.reader(csvfile, delimiter=',', quotechar='"')
+                for row in csv.reader(csvfile, delimiter=",", quotechar='"')
             ][1:]
 
     def _generate_report_name(self, report_template, framework, version):
         """
         Generate a report name based on the template, framework, and version.
@@ -75,14 +72,14 @@
             version (str): The version of the framework.
 
         Returns:
             str: The generated report name.
         """
-        version_str = version.replace('.', '_')
+        version_str = version.replace(".", "_")
         templates = {
-            'FINREP_REF': f'M_{report_template}_REF_FINREP {version_str}',
-            'AE_REF': f'M_{report_template}_REF_AE{framework} {version_str}'
+            "FINREP_REF": f"M_{report_template}_REF_FINREP {version_str}",
+            "AE_REF": f"M_{report_template}_REF_AE{framework} {version_str}",
         }
         return templates[framework]
 
     def create_output_layer_for_cube_mapping(self, context, sdd_context, destination_cube, framework):
         """
@@ -90,18 +87,18 @@
         Returns the created cube and structure instead of saving them.
         """
         output_layer_cube, output_layer_cube_structure = self._create_cube_and_structure(destination_cube)
 
         structures_and_cubes = {
-            'structure': (sdd_context.bird_cube_structure_dictionary, output_layer_cube_structure),
-            'cube': (sdd_context.bird_cube_dictionary, output_layer_cube),
-            'FINREP_REF': (sdd_context.finrep_output_cubes, output_layer_cube),
-            'AE_REF': (sdd_context.ae_output_cubes, output_layer_cube)
+            "structure": (sdd_context.bird_cube_structure_dictionary, output_layer_cube_structure),
+            "cube": (sdd_context.bird_cube_dictionary, output_layer_cube),
+            "FINREP_REF": (sdd_context.finrep_output_cubes, output_layer_cube),
+            "AE_REF": (sdd_context.ae_output_cubes, output_layer_cube),
         }
 
-        structures_and_cubes['structure'][0][output_layer_cube_structure.name] = output_layer_cube_structure
-        structures_and_cubes['cube'][0][output_layer_cube.name] = output_layer_cube
+        structures_and_cubes["structure"][0][output_layer_cube_structure.name] = output_layer_cube_structure
+        structures_and_cubes["cube"][0][output_layer_cube.name] = output_layer_cube
 
         if framework in structures_and_cubes:
             structures_and_cubes[framework][0][output_layer_cube.name] = output_layer_cube
 
         return output_layer_cube, output_layer_cube_structure
@@ -119,11 +116,11 @@
         cube_name = self._generate_cube_name(destination_cube)
 
         output_layer_cube = CUBE()
         output_layer_cube.cube_id = cube_name
         output_layer_cube.name = cube_name
-        output_layer_cube.cube_type = 'RC'
+        output_layer_cube.cube_type = "RC"
 
         output_layer_cube_structure = CUBE_STRUCTURE()
         output_layer_cube_structure.cube_structure_id = f"{cube_name}_cube_structure"
         output_layer_cube_structure.name = f"{cube_name}_cube_structure"
 
@@ -139,6 +136,6 @@
             destination_cube (str): The destination cube name.
 
         Returns:
             str: The generated cube name.
         """
-        return destination_cube.replace('.', '_').replace(' ', '_')[2:]
+        return destination_cube.replace(".", "_").replace(" ", "_")[2:]
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/report_filters/create_output_layers.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/pybird/csv_converter.py	2025-09-02 15:09:38.717254+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/pybird/csv_converter.py	2025-09-21 17:07:36.760334+00:00
@@ -15,296 +15,312 @@
 from django.conf import settings
 from django.apps import apps
 from django.db.models import QuerySet
 from django.db.models.fields.related import ReverseOneToOneDescriptor
 
+
 class CSVConverter:
 
-	def persist_object_as_csv(theObject,useLongNames):
-		print("persist_object_as_csv theObject: " + str(theObject))
-		#if 'FNNCL_ASST_INSTRMNT_DRVD_DT' in str(theObject):
-		#	import pdb;pdb.set_trace()
-		fileName = ""
-		base_dir = settings.BASE_DIR 
-		output_directory = os.path.join(base_dir, 'results','lineage')
-		table_name = CSVConverter.get_table_name(theObject)
-		csvString = CSVConverter.createCSVStringForTable(theObject,useLongNames,table_name)
-		try:
-			if (useLongNames):
-				fileName = table_name + "_longnames.csv"
-				file = open(output_directory + os.sep + fileName, "w",  encoding='utf-8') 
-				file.write(csvString)
-			else:
-				fileName = table_name + ".csv"
-				file = open(output_directory + os.sep + fileName, "w",  encoding='utf-8') 
-				file.write(csvString)
-
-		except Exception as e: 
-			print("Exception  " + str(e)  )
-			print("File " + fileName  + " already exists" )
-
-		print("persist_object_as_csv succesfully written: " + str(theObject))
-
-	def get_table_name(theObject):
-		table_name = None
-		if isinstance(theObject, QuerySet):
-			table_name = theObject.model.__name__
-		else:
-			class_name = theObject.__class__.__name__
-			table_name = class_name.split('_Table')[0]
-		return table_name
-	
-	def createCSVStringForTable( theObject,  useLongNames, table_name):
-		object_list = []
-		headerCreated = False
-		csvString = ""
-		django_model = False
-		if isinstance(theObject, QuerySet):
-			relevant_model = apps.get_model('pybirdai',table_name)
-			print(f"relevant_model: {relevant_model}")
-			object_list = relevant_model.objects.all()
-			print(f"newObject: {object_list}")
-			django_model = True
-			
-			# Note: Removed broad Django model access tracking as it pollutes lineage with unused fields
-			# Lineage is now tracked through improved @lineage decorators on actual calculations
-			# CSVConverter._track_django_model_access(table_name, object_list)
-		else:
-			object_list = CSVConverter.get_contained_objects(theObject)
-
-		for o in object_list:
-			if not headerCreated:
-				csvString = csvString + CSVConverter.createCSVHeaderStringForRow(o,django_model)
-				headerCreated = True
-				csvString = csvString + CSVConverter.createCSVStringForRow(o, useLongNames,django_model)
-
-
-
-		return csvString + "\n"
-		
-	def get_contained_objects(theObject):
-		'''
-		Get all contianed/composed objects
-		Q.) How do we recognise composed objects?
-		A.) 1.) If it is a djangomodel get the list of object/rows
-			2.) Look at the instance members of the object, if it is a list, and does not have a name that ends in Table then get the list. 
-
-		'''
-		rows = []
-		try:
-			rows = theObject.objects.all()
-			return rows
-		except:
-			instance_members = [member for member in dir(theObject.__class__) if not callable(
-            getattr(theObject.__class__, member)) and not member.startswith('__')]
-		
-			for member in instance_members:
-				if not (member.endswith("Table")) and isinstance(getattr(theObject.__class__, member), list):
-					rows = getattr(theObject, member)
-					return rows
-
-		return rows
-
-					
-
-	def createCSVStringForRow(theObject,useLongNames,django_model):
-		clazz = None
-		csvString = ""
-		eClass = theObject.__class__
-		firstItem = True
-		if django_model:
-			references = [method for method in dir(theObject.__class__) if not callable(
-				getattr(theObject.__class__, method)) and not method.startswith('__')]
-			for relationship in references:
-				if not(relationship == "objects") and not(relationship == "_meta") and\
-					  not(relationship.endswith("_domain")) and\
-						not isinstance(getattr(theObject.__class__,relationship),ReverseOneToOneDescriptor):
-					cardinality = 1
-					if not (relationship is None):
-						cardinality = 1
-						
-					if not(cardinality == -1):
-						if firstItem:
-							referencedItem = getattr(theObject,relationship)
-							referencedItemString = str(referencedItem)
-							if referencedItemString.endswith(".None"):
-								referencedItemString = "None"
-							csvString = csvString + str(referencedItemString)
-							firstItem = False
-
-						else:
-							#change next line
-							referencedItem = getattr(theObject, relationship)
-							#referencedItemString = CSVConverter.getReferencedItemString(relationship, referencedItem,useLongNames)
-							referencedItemString = str(referencedItem)
-							if referencedItemString.endswith(".None"):
-								referencedItemString = "None"
-							csvString = csvString + "," + str(referencedItemString)
-		else:
-			# Don't automatically call all methods - this causes unwanted function evaluations
-			# Instead, just serialize the object representation
-			if (firstItem):
-				csvString = csvString + str(theObject)
-				firstItem = False
-			else:
-				csvString = csvString + "," + str(theObject)
-
-		return csvString + "\n"
-
-	def createCSVHeaderStringForRow(theObject,django_model):
-		clazz = None
-		csvString = ""
-		eClass = theObject.__class__
-		
-		if django_model:
-			sfs = [method for method in dir(theObject.__class__) if not callable(
-					getattr(theObject.__class__, method)) and not method.startswith('__')]
-			firstItem = True
-			for  eStructuralFeature in  sfs:
-				if not(eStructuralFeature == "objects") and not(eStructuralFeature == "_meta") and\
-					  not(eStructuralFeature.endswith("_domain")) and\
-						not isinstance(getattr(theObject.__class__,eStructuralFeature),ReverseOneToOneDescriptor):
-				#boolean relationship = (eStructuralFeature instanceof EReference)
-					relationship = True
-					cardinality = 1
-					if relationship:
-						#cardinality = ((EReference) eStructuralFeature).getUpperBound()
-						cardinality = 1
-
-					#dont show any items in the inout data that have  cardinality	of -1
-					if(cardinality != -1):
-						if (firstItem):
-							csvString = csvString + eStructuralFeature
-							firstItem = False
-						else:
-							csvString = csvString + "," + eStructuralFeature
-		else:
-			firstItem =True
-			operations = [method for method in dir(theObject.__class__) if callable(
-				getattr(theObject.__class__, method)) and not method.startswith('__')]
-
-			for eOperation in operations:
-				if firstItem:
-					csvString = csvString + eOperation
-					firstItem = False
-
-				else:
-					csvString = csvString + "," + eOperation
-
-		return csvString + "\n"
-
-	
-	def getReferencedItemString(eStructuralFeature, referencedItem,useLongNames):
-		returnString  = None
-		 #temporary vaiable
-		is_reference = True
-		is_attribute = False
-		is_enum = False
-		is_date = False
-		if ( referencedItem is None):
-			returnString = "null"
-		#else if (eStructuralFeature instanceof EReference)
-		if is_reference:
-
-			upperbound = 1
-			if upperbound == 1:
-				# somehow get the return type of the method
-				eClass = referencedItem.__class__
+    def persist_object_as_csv(theObject, useLongNames):
+        print("persist_object_as_csv theObject: " + str(theObject))
+        # if 'FNNCL_ASST_INSTRMNT_DRVD_DT' in str(theObject):
+        # 	import pdb;pdb.set_trace()
+        fileName = ""
+        base_dir = settings.BASE_DIR
+        output_directory = os.path.join(base_dir, "results", "lineage")
+        table_name = CSVConverter.get_table_name(theObject)
+        csvString = CSVConverter.createCSVStringForTable(theObject, useLongNames, table_name)
+        try:
+            if useLongNames:
+                fileName = table_name + "_longnames.csv"
+                file = open(output_directory + os.sep + fileName, "w", encoding="utf-8")
+                file.write(csvString)
+            else:
+                fileName = table_name + ".csv"
+                file = open(output_directory + os.sep + fileName, "w", encoding="utf-8")
+                file.write(csvString)
+
+        except Exception as e:
+            print("Exception  " + str(e))
+            print("File " + fileName + " already exists")
+
+        print("persist_object_as_csv succesfully written: " + str(theObject))
+
+    def get_table_name(theObject):
+        table_name = None
+        if isinstance(theObject, QuerySet):
+            table_name = theObject.model.__name__
+        else:
+            class_name = theObject.__class__.__name__
+            table_name = class_name.split("_Table")[0]
+        return table_name
+
+    def createCSVStringForTable(theObject, useLongNames, table_name):
+        object_list = []
+        headerCreated = False
+        csvString = ""
+        django_model = False
+        if isinstance(theObject, QuerySet):
+            relevant_model = apps.get_model("pybirdai", table_name)
+            print(f"relevant_model: {relevant_model}")
+            object_list = relevant_model.objects.all()
+            print(f"newObject: {object_list}")
+            django_model = True
+
+            # Note: Removed broad Django model access tracking as it pollutes lineage with unused fields
+            # Lineage is now tracked through improved @lineage decorators on actual calculations
+            # CSVConverter._track_django_model_access(table_name, object_list)
+        else:
+            object_list = CSVConverter.get_contained_objects(theObject)
+
+        for o in object_list:
+            if not headerCreated:
+                csvString = csvString + CSVConverter.createCSVHeaderStringForRow(o, django_model)
+                headerCreated = True
+                csvString = csvString + CSVConverter.createCSVStringForRow(o, useLongNames, django_model)
+
+        return csvString + "\n"
+
+    def get_contained_objects(theObject):
+        """
+        Get all contianed/composed objects
+        Q.) How do we recognise composed objects?
+        A.) 1.) If it is a djangomodel get the list of object/rows
+                2.) Look at the instance members of the object, if it is a list, and does not have a name that ends in Table then get the list.
+
+        """
+        rows = []
+        try:
+            rows = theObject.objects.all()
+            return rows
+        except:
+            instance_members = [
+                member
+                for member in dir(theObject.__class__)
+                if not callable(getattr(theObject.__class__, member)) and not member.startswith("__")
+            ]
+
+            for member in instance_members:
+                if not (member.endswith("Table")) and isinstance(getattr(theObject.__class__, member), list):
+                    rows = getattr(theObject, member)
+                    return rows
+
+        return rows
+
+    def createCSVStringForRow(theObject, useLongNames, django_model):
+        clazz = None
+        csvString = ""
+        eClass = theObject.__class__
+        firstItem = True
+        if django_model:
+            references = [
+                method
+                for method in dir(theObject.__class__)
+                if not callable(getattr(theObject.__class__, method)) and not method.startswith("__")
+            ]
+            for relationship in references:
+                if (
+                    not (relationship == "objects")
+                    and not (relationship == "_meta")
+                    and not (relationship.endswith("_domain"))
+                    and not isinstance(getattr(theObject.__class__, relationship), ReverseOneToOneDescriptor)
+                ):
+                    cardinality = 1
+                    if not (relationship is None):
+                        cardinality = 1
+
+                    if not (cardinality == -1):
+                        if firstItem:
+                            referencedItem = getattr(theObject, relationship)
+                            referencedItemString = str(referencedItem)
+                            if referencedItemString.endswith(".None"):
+                                referencedItemString = "None"
+                            csvString = csvString + str(referencedItemString)
+                            firstItem = False
+
+                        else:
+                            # change next line
+                            referencedItem = getattr(theObject, relationship)
+                            # referencedItemString = CSVConverter.getReferencedItemString(relationship, referencedItem,useLongNames)
+                            referencedItemString = str(referencedItem)
+                            if referencedItemString.endswith(".None"):
+                                referencedItemString = "None"
+                            csvString = csvString + "," + str(referencedItemString)
+        else:
+            # Don't automatically call all methods - this causes unwanted function evaluations
+            # Instead, just serialize the object representation
+            if firstItem:
+                csvString = csvString + str(theObject)
+                firstItem = False
+            else:
+                csvString = csvString + "," + str(theObject)
+
+        return csvString + "\n"
+
+    def createCSVHeaderStringForRow(theObject, django_model):
+        clazz = None
+        csvString = ""
+        eClass = theObject.__class__
+
+        if django_model:
+            sfs = [
+                method
+                for method in dir(theObject.__class__)
+                if not callable(getattr(theObject.__class__, method)) and not method.startswith("__")
+            ]
+            firstItem = True
+            for eStructuralFeature in sfs:
+                if (
+                    not (eStructuralFeature == "objects")
+                    and not (eStructuralFeature == "_meta")
+                    and not (eStructuralFeature.endswith("_domain"))
+                    and not isinstance(getattr(theObject.__class__, eStructuralFeature), ReverseOneToOneDescriptor)
+                ):
+                    # boolean relationship = (eStructuralFeature instanceof EReference)
+                    relationship = True
+                    cardinality = 1
+                    if relationship:
+                        # cardinality = ((EReference) eStructuralFeature).getUpperBound()
+                        cardinality = 1
+
+                    # dont show any items in the inout data that have  cardinality	of -1
+                    if cardinality != -1:
+                        if firstItem:
+                            csvString = csvString + eStructuralFeature
+                            firstItem = False
+                        else:
+                            csvString = csvString + "," + eStructuralFeature
+        else:
+            firstItem = True
+            operations = [
+                method
+                for method in dir(theObject.__class__)
+                if callable(getattr(theObject.__class__, method)) and not method.startswith("__")
+            ]
+
+            for eOperation in operations:
+                if firstItem:
+                    csvString = csvString + eOperation
+                    firstItem = False
+
+                else:
+                    csvString = csvString + "," + eOperation
+
+        return csvString + "\n"
+
+    def getReferencedItemString(eStructuralFeature, referencedItem, useLongNames):
+        returnString = None
+        # temporary vaiable
+        is_reference = True
+        is_attribute = False
+        is_enum = False
+        is_date = False
+        if referencedItem is None:
+            returnString = "null"
+        # else if (eStructuralFeature instanceof EReference)
+        if is_reference:
+
+            upperbound = 1
+            if upperbound == 1:
+                # somehow get the return type of the method
+                eClass = referencedItem.__class__
                 # somehow get the identifying attribute of the class
-				# idattr = eClass.getEIDAttribute()
-				idattr = None
-				references = [method for method in dir(eStructuralFeature.__class__) if not callable(
-            		getattr(eStructuralFeature.__class__, method)) and not method.startswith('__')]
-            
-				for eStructuralFeature2 in references:
-				
-					if eStructuralFeature2 == idattr:
-						attributeValue = getattr(referencedItem.__class__, eStructuralFeature2)
-						if (attributeValue):
-							returnString = str(attributeValue)
-
-					if (returnString is None):
-						returnString = eStructuralFeature.__name__
-
-			else:
-				returnString = "multiple_"+ eStructuralFeature.getName()
-		elif is_attribute and  is_enum:
-
-			if (useLongNames):
-				pass#returnString = "\"" + ((Enumerator) referencedItem).getName().replace('_',' ') + " (" + ((Enumerator) referencedItem).getLiteral() + ")\"" 
-			else:
-				pass# returnString = ((Enumerator) referencedItem).getLiteral()
-
-		else:
-			if is_date:
-				pattern = "MM/dd/yyyy"
-				#SimpleDateFormat simpleDateFormat = new SimpleDateFormat(pattern)
-				#returnString =  simpleDateFormat.format((java.util.Date) referencedItem)
-				returnString = str(referencedItem)
-			else:
-				returnString = str(referencedItem)
-		return returnString
-	
-	@staticmethod
-	def _track_django_model_access(table_name, queryset):
-		"""Track Django model access through orchestration for lineage"""
-		try:
-			# Import here to avoid circular imports
-			from pybirdai.annotations.decorators import _lineage_context
-			
-			# Get the orchestration instance from the global context
-			orchestration = _lineage_context.get('orchestration')
-			if not orchestration or not orchestration.lineage_enabled:
-				return
-			
-			# Create a mock table wrapper object for the Django model
-			class DjangoModelTableWrapper:
-				def __init__(self, model_name):
-					self.__class__.__name__ = f"{model_name}_Table"
-					self.model_name = model_name
-					
-				def __str__(self):
-					return f"DjangoModelTableWrapper({self.model_name})"
-			
-			# Create the wrapper and track it
-			wrapper = DjangoModelTableWrapper(table_name)
-			
-			# Check if orchestration supports lineage tracking
-			if hasattr(orchestration, 'init_with_lineage'):
-				# Initialize through orchestration to create PopulatedDataBaseTable
-				orchestration.init_with_lineage(wrapper, f"Django Model Access: {table_name}")
-				
-				# Track the data if there are rows
-				if queryset.exists():
-					# Convert QuerySet to list of dictionaries for tracking
-					data_items = []
-					django_model_objects = []  # Keep track of original Django model objects
-					for obj in queryset:
-						row_data = {}
-						# Get model fields to extract data
-						for field in obj._meta.fields:
-							try:
-								value = getattr(obj, field.name)
-								if value is not None:
-									row_data[field.name] = value
-							except:
-								pass
-						if row_data:
-							data_items.append(row_data)
-							django_model_objects.append(obj)  # Store the original Django object
-					
-					# Track the data processing - use the original table name, not "_data" suffix
-					# This ensures Django model data is associated with PopulatedDataBaseTable, not EvaluatedDerivedTable
-					if data_items:
-						# Pass both the dictionaries (for CSV) and Django objects (for tracking)
-						orchestration.track_data_processing(table_name, data_items, django_model_objects)
-			else:
-				# Original orchestrator - no lineage tracking
-				pass
-			
-			print(f"Tracked Django model access: {table_name} ({queryset.count()} rows)")
-			
-		except Exception as e:
-			print(f"Error tracking Django model access for {table_name}: {e}")
-			# Don't let tracking errors break the actual processing
-			pass
-
+                # idattr = eClass.getEIDAttribute()
+                idattr = None
+                references = [
+                    method
+                    for method in dir(eStructuralFeature.__class__)
+                    if not callable(getattr(eStructuralFeature.__class__, method)) and not method.startswith("__")
+                ]
+
+                for eStructuralFeature2 in references:
+
+                    if eStructuralFeature2 == idattr:
+                        attributeValue = getattr(referencedItem.__class__, eStructuralFeature2)
+                        if attributeValue:
+                            returnString = str(attributeValue)
+
+                    if returnString is None:
+                        returnString = eStructuralFeature.__name__
+
+            else:
+                returnString = "multiple_" + eStructuralFeature.getName()
+        elif is_attribute and is_enum:
+
+            if useLongNames:
+                pass  # returnString = "\"" + ((Enumerator) referencedItem).getName().replace('_',' ') + " (" + ((Enumerator) referencedItem).getLiteral() + ")\""
+            else:
+                pass  # returnString = ((Enumerator) referencedItem).getLiteral()
+
+        else:
+            if is_date:
+                pattern = "MM/dd/yyyy"
+                # SimpleDateFormat simpleDateFormat = new SimpleDateFormat(pattern)
+                # returnString =  simpleDateFormat.format((java.util.Date) referencedItem)
+                returnString = str(referencedItem)
+            else:
+                returnString = str(referencedItem)
+        return returnString
+
+    @staticmethod
+    def _track_django_model_access(table_name, queryset):
+        """Track Django model access through orchestration for lineage"""
+        try:
+            # Import here to avoid circular imports
+            from pybirdai.annotations.decorators import _lineage_context
+
+            # Get the orchestration instance from the global context
+            orchestration = _lineage_context.get("orchestration")
+            if not orchestration or not orchestration.lineage_enabled:
+                return
+
+            # Create a mock table wrapper object for the Django model
+            class DjangoModelTableWrapper:
+                def __init__(self, model_name):
+                    self.__class__.__name__ = f"{model_name}_Table"
+                    self.model_name = model_name
+
+                def __str__(self):
+                    return f"DjangoModelTableWrapper({self.model_name})"
+
+            # Create the wrapper and track it
+            wrapper = DjangoModelTableWrapper(table_name)
+
+            # Check if orchestration supports lineage tracking
+            if hasattr(orchestration, "init_with_lineage"):
+                # Initialize through orchestration to create PopulatedDataBaseTable
+                orchestration.init_with_lineage(wrapper, f"Django Model Access: {table_name}")
+
+                # Track the data if there are rows
+                if queryset.exists():
+                    # Convert QuerySet to list of dictionaries for tracking
+                    data_items = []
+                    django_model_objects = []  # Keep track of original Django model objects
+                    for obj in queryset:
+                        row_data = {}
+                        # Get model fields to extract data
+                        for field in obj._meta.fields:
+                            try:
+                                value = getattr(obj, field.name)
+                                if value is not None:
+                                    row_data[field.name] = value
+                            except:
+                                pass
+                        if row_data:
+                            data_items.append(row_data)
+                            django_model_objects.append(obj)  # Store the original Django object
+
+                    # Track the data processing - use the original table name, not "_data" suffix
+                    # This ensures Django model data is associated with PopulatedDataBaseTable, not EvaluatedDerivedTable
+                    if data_items:
+                        # Pass both the dictionaries (for CSV) and Django objects (for tracking)
+                        orchestration.track_data_processing(table_name, data_items, django_model_objects)
+            else:
+                # Original orchestrator - no lineage tracking
+                pass
+
+            print(f"Tracked Django model access: {table_name} ({queryset.count()} rows)")
+
+        except Exception as e:
+            print(f"Error tracking Django model access for {table_name}: {e}")
+            # Don't let tracking errors break the actual processing
+            pass
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/pybird/csv_converter.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/pybird/create_python_django_transformations.py	2025-08-02 18:37:08.455259+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/pybird/create_python_django_transformations.py	2025-09-21 17:07:36.768755+00:00
@@ -21,130 +21,139 @@
 
 class CreatePythonTransformations:
 
     @staticmethod
     def create_python_joins(context, sdd_context):
-        '''
+        """
         Read in the transformation meta data and create python classes
-        '''
+        """
 
         # Initialize AORTA tracking
         orchestration = Orchestration()
-        if hasattr(context, 'enable_lineage_tracking') and context.enable_lineage_tracking:
-            orchestration.init_with_lineage(None, f"Transformation_Generation_{datetime.now().strftime('%Y%m%d_%H%M%S')}")
+        if hasattr(context, "enable_lineage_tracking") and context.enable_lineage_tracking:
+            orchestration.init_with_lineage(
+                None, f"Transformation_Generation_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
+            )
             print("AORTA lineage tracking enabled for transformation generation")
 
         CreatePythonTransformations.delete_generated_python_join_files(context)
-        CreatePythonTransformations.create_output_classes( sdd_context)
+        CreatePythonTransformations.create_output_classes(sdd_context)
         CreatePythonTransformations.create_slice_classes(sdd_context)
         # get all the cube_links for a report
 
-    def create_output_classes(  sdd_context):
-
-         #get all the cubes_structure_items for that cube and make a related Python class.
-        file = open(sdd_context.output_directory + os.sep + 'generated_python_joins' + os.sep +  'output_tables.py', "a",  encoding='utf-8')
+    def create_output_classes(sdd_context):
+
+        # get all the cubes_structure_items for that cube and make a related Python class.
+        file = open(
+            sdd_context.output_directory + os.sep + "generated_python_joins" + os.sep + "output_tables.py",
+            "a",
+            encoding="utf-8",
+        )
         file.write("from pybirdai.process_steps.pybird.orchestration import Orchestration\n")
         file.write("from datetime import datetime\n")
         file.write("from pybirdai.annotations.decorators import lineage, track_table_init\n")
         for report_id, cube_links in sdd_context.cube_link_to_foreign_cube_map.items():
             print(f"report_id: {report_id}")
-            file.write("from ." + report_id  + "_logic import *\n")
+            file.write("from ." + report_id + "_logic import *\n")
             file.write("\nclass " + report_id + ":\n")
             file.write("\tunionOfLayers = None #  " + report_id + "_UnionItem  unionOfLayers\n")
             cube_structure_items = []
             try:
-                cube_structure_items = sdd_context.bird_cube_structure_item_dictionary[report_id+ '_cube_structure']
+                cube_structure_items = sdd_context.bird_cube_structure_item_dictionary[report_id + "_cube_structure"]
             except KeyError:
                 print(f"No cube structure items for report_id: {report_id}")
             for cube_structure_item in cube_structure_items:
                 print(f"cube_structure_item: {cube_structure_item}")
                 variable = cube_structure_item.variable_id
 
                 domain = variable.domain_id.domain_id
-                file.write('\t@lineage(dependencies={"unionOfLayers.'+ variable.variable_id +'"})\n')
-                if domain == 'String':
-                    file.write('\tdef ' + variable.variable_id + '(self) -> str:\n')
-                elif domain == 'Integer':
-                    file.write('\tdef ' + variable.variable_id + '(self) -> int:\n')
-                elif domain == 'Date':
-                    file.write('\tdef ' + variable.variable_id + '(self) -> datetime:\n')
-                elif domain == 'Float':
-                    file.write('\tdef ' + variable.variable_id + '(self) -> float:\n')
-                elif domain == 'Boolean':
-                    file.write('\tdef ' + variable.variable_id + '(self) -> bool:\n')
+                file.write('\t@lineage(dependencies={"unionOfLayers.' + variable.variable_id + '"})\n')
+                if domain == "String":
+                    file.write("\tdef " + variable.variable_id + "(self) -> str:\n")
+                elif domain == "Integer":
+                    file.write("\tdef " + variable.variable_id + "(self) -> int:\n")
+                elif domain == "Date":
+                    file.write("\tdef " + variable.variable_id + "(self) -> datetime:\n")
+                elif domain == "Float":
+                    file.write("\tdef " + variable.variable_id + "(self) -> float:\n")
+                elif domain == "Boolean":
+                    file.write("\tdef " + variable.variable_id + "(self) -> bool:\n")
                 else:
-                    file.write('\tdef ' + variable.variable_id + '(self) -> str:\n')
-                    file.write('\t\t\'\'\' return string from ' + domain + ' enumeration \'\'\'\n')
-
-                file.write('\t\treturn self.unionOfLayers.' + variable.variable_id + '()\n')
-                file.write('\n')
-            file.write('\n')
-            file.write("class " +report_id + "_Table :\n" )
-            #file.write("\tunionOfLayersTable = None # " + report_id + "_UnionTable\n" )
-            file.write("\t" + report_id + "_UnionTable = None # unionOfLayersTable\n" )
-            file.write("\t" + report_id + "s = [] #" + report_id + "[]\n" )
-            file.write("\tdef  calc_" + report_id + "s(self) -> list[" + report_id + "] :\n" )
-            file.write("\t\titems = [] # " + report_id + "[]\n" )
-            file.write("\t\tfor item in self." +report_id + "_UnionTable." + report_id + "_UnionItems:\n" )
-            file.write("\t\t\tnewItem = " + report_id + "()\n" )
-            file.write("\t\t\tnewItem.unionOfLayers = item\n" )
-            file.write("\t\t\titems.append(newItem)\n" )
-            file.write("\t\treturn items\n" )
-            file.write("\t@track_table_init\n" )
-            file.write("\tdef init(self):\n" )
-            file.write("\t\tOrchestration().init(self)\n" )
-            file.write("\t\tself." + report_id + "s = []\n" )
-            file.write("\t\tself." + report_id + "s.extend(self.calc_" + report_id + "s())\n" )
+                    file.write("\tdef " + variable.variable_id + "(self) -> str:\n")
+                    file.write("\t\t''' return string from " + domain + " enumeration '''\n")
+
+                file.write("\t\treturn self.unionOfLayers." + variable.variable_id + "()\n")
+                file.write("\n")
+            file.write("\n")
+            file.write("class " + report_id + "_Table :\n")
+            # file.write("\tunionOfLayersTable = None # " + report_id + "_UnionTable\n" )
+            file.write("\t" + report_id + "_UnionTable = None # unionOfLayersTable\n")
+            file.write("\t" + report_id + "s = [] #" + report_id + "[]\n")
+            file.write("\tdef  calc_" + report_id + "s(self) -> list[" + report_id + "] :\n")
+            file.write("\t\titems = [] # " + report_id + "[]\n")
+            file.write("\t\tfor item in self." + report_id + "_UnionTable." + report_id + "_UnionItems:\n")
+            file.write("\t\t\tnewItem = " + report_id + "()\n")
+            file.write("\t\t\tnewItem.unionOfLayers = item\n")
+            file.write("\t\t\titems.append(newItem)\n")
+            file.write("\t\treturn items\n")
+            file.write("\t@track_table_init\n")
+            file.write("\tdef init(self):\n")
+            file.write("\t\tOrchestration().init(self)\n")
+            file.write("\t\tself." + report_id + "s = []\n")
+            file.write("\t\tself." + report_id + "s.extend(self.calc_" + report_id + "s())\n")
             file.write("\t\tfrom pybirdai.process_steps.pybird.csv_converter import CSVConverter\n")
             file.write("\t\tCSVConverter.persist_object_as_csv(self,True)\n")
-            file.write("\t\treturn None\n" )
-            file.write('\n')
-
-    def create_slice_classes( sdd_context):
+            file.write("\t\treturn None\n")
+            file.write("\n")
+
+    def create_slice_classes(sdd_context):
         for report_id, cube_links in sdd_context.cube_link_to_foreign_cube_map.items():
-            file = open(sdd_context.output_directory + os.sep + 'generated_python_joins' + os.sep +  report_id + '_logic.py', "a",  encoding='utf-8')
+            file = open(
+                sdd_context.output_directory + os.sep + "generated_python_joins" + os.sep + report_id + "_logic.py",
+                "a",
+                encoding="utf-8",
+            )
             file.write("from pybirdai.models.bird_data_model import *\n")
             file.write("from pybirdai.process_steps.pybird.orchestration import Orchestration\n")
             file.write("from pybirdai.process_steps.pybird.csv_converter import CSVConverter\n")
             file.write("from datetime import datetime\n")
             file.write("from pybirdai.annotations.decorators import lineage\n")
 
             file.write("\nclass " + report_id + "_UnionItem:\n")
             file.write("\tbase = None #" + report_id + "_Base\n")
             cube_structure_items = []
             try:
-                cube_structure_items = sdd_context.bird_cube_structure_item_dictionary[report_id+ '_cube_structure']
+                cube_structure_items = sdd_context.bird_cube_structure_item_dictionary[report_id + "_cube_structure"]
             except KeyError:
                 print(f"No cube structure items for report_id: {report_id}")
             for cube_structure_item in cube_structure_items:
                 print(f"cube_structure_item: {cube_structure_item}")
                 variable = cube_structure_item.variable_id
 
                 domain = variable.domain_id.domain_id
-                file.write('\t@lineage(dependencies={"base.'+ variable.variable_id +'"})\n')
-                if domain == 'String':
-                    file.write('\tdef ' + variable.variable_id + '(self) -> str:\n')
-                elif domain == 'Integer':
-                    file.write('\tdef ' + variable.variable_id + '(self) -> int:\n')
-                elif domain == 'Date':
-                    file.write('\tdef ' + variable.variable_id + '(self) -> datetime:\n')
-                elif domain == 'Float':
-                    file.write('\tdef ' + variable.variable_id + '(self) -> float:\n')
-                elif domain == 'Boolean':
-                    file.write('\tdef ' + variable.variable_id + '(self) -> bool:\n')
+                file.write('\t@lineage(dependencies={"base.' + variable.variable_id + '"})\n')
+                if domain == "String":
+                    file.write("\tdef " + variable.variable_id + "(self) -> str:\n")
+                elif domain == "Integer":
+                    file.write("\tdef " + variable.variable_id + "(self) -> int:\n")
+                elif domain == "Date":
+                    file.write("\tdef " + variable.variable_id + "(self) -> datetime:\n")
+                elif domain == "Float":
+                    file.write("\tdef " + variable.variable_id + "(self) -> float:\n")
+                elif domain == "Boolean":
+                    file.write("\tdef " + variable.variable_id + "(self) -> bool:\n")
                 else:
-                    file.write('\tdef ' + variable.variable_id + '(self) -> str:\n')
-                    file.write('\t\t\'\'\' return string from ' + domain + ' enumeration \'\'\'\n')
-
-                file.write('\t\treturn self.base.' + variable.variable_id + '()')
-                file.write('\n')
-
+                    file.write("\tdef " + variable.variable_id + "(self) -> str:\n")
+                    file.write("\t\t''' return string from " + domain + " enumeration '''\n")
+
+                file.write("\t\treturn self.base." + variable.variable_id + "()")
+                file.write("\n")
 
             file.write("\nclass " + report_id + "_Base:\n")
             cube_structure_items = []
             try:
-                cube_structure_items = sdd_context.bird_cube_structure_item_dictionary[report_id+ '_cube_structure']
+                cube_structure_items = sdd_context.bird_cube_structure_item_dictionary[report_id + "_cube_structure"]
             except KeyError:
                 print(f"No cube structure items for report_id: {report_id}")
 
             if len(cube_structure_items) == 0:
                 file.write("\tpass\n")
@@ -152,48 +161,63 @@
             for cube_structure_item in cube_structure_items:
                 print(f"cube_structure_item: {cube_structure_item}")
                 variable = cube_structure_item.variable_id
 
                 domain = variable.domain_id.domain_id
-                if domain == 'String':
-                    file.write('\tdef ' + variable.variable_id + '() -> str:\n')
-                elif domain == 'Integer':
-                    file.write('\tdef ' + variable.variable_id + '() -> int:\n')
-                elif domain == 'Date':
-                    file.write('\tdef ' + variable.variable_id + '() -> datetime:\n')
-                elif domain == 'Float':
-                    file.write('\tdef ' + variable.variable_id + '() -> float:\n')
-                elif domain == 'Boolean':
-                    file.write('\tdef ' + variable.variable_id + '() -> bool:\n')
+                if domain == "String":
+                    file.write("\tdef " + variable.variable_id + "() -> str:\n")
+                elif domain == "Integer":
+                    file.write("\tdef " + variable.variable_id + "() -> int:\n")
+                elif domain == "Date":
+                    file.write("\tdef " + variable.variable_id + "() -> datetime:\n")
+                elif domain == "Float":
+                    file.write("\tdef " + variable.variable_id + "() -> float:\n")
+                elif domain == "Boolean":
+                    file.write("\tdef " + variable.variable_id + "() -> bool:\n")
                 else:
-                    file.write('\tdef ' + variable.variable_id + '() -> str:\n')
-                    file.write('\t\t\'\'\' return string from ' + domain + ' enumeration \'\'\'\n')
-
-                file.write('\t\tpass')
-                file.write('\n')
-
+                    file.write("\tdef " + variable.variable_id + "() -> str:\n")
+                    file.write("\t\t''' return string from " + domain + " enumeration '''\n")
+
+                file.write("\t\tpass")
+                file.write("\n")
 
             file.write("\nclass " + report_id + "_UnionTable :\n")
-            file.write("\t" + report_id + "_UnionItems = [] # " +  report_id + "_UnionItem []\n" )
+            file.write("\t" + report_id + "_UnionItems = [] # " + report_id + "_UnionItem []\n")
             join_ids_added = []
             for join_for_report_id, cube_links in sdd_context.cube_link_to_join_for_report_id_map.items():
                 for cube_link in cube_links:
                     the_report_id = cube_link.foreign_cube_id.cube_id
                     if the_report_id == report_id:
                         if cube_link.join_identifier not in join_ids_added:
-                            file.write("\t" + report_id + "_" + cube_link.join_identifier.replace(' ','_') + "_Table = None # " +  cube_link.join_identifier.replace(' ','_') + "\n")
+                            file.write(
+                                "\t"
+                                + report_id
+                                + "_"
+                                + cube_link.join_identifier.replace(" ", "_")
+                                + "_Table = None # "
+                                + cube_link.join_identifier.replace(" ", "_")
+                                + "\n"
+                            )
                             join_ids_added.append(cube_link.join_identifier)
             file.write("\tdef calc_" + report_id + "_UnionItems(self) -> list[" + report_id + "_UnionItem] :\n")
             file.write("\t\titems = [] # " + report_id + "_UnionItem []\n")
 
             join_ids_added = []
             for join_for_report_id, cube_links in sdd_context.cube_link_to_join_for_report_id_map.items():
                 for cube_link in cube_links:
                     the_report_id = cube_link.foreign_cube_id.cube_id
                     if the_report_id == report_id:
                         if cube_link.join_identifier not in join_ids_added:
-                            file.write("\t\tfor item in self." + report_id + "_" + cube_link.join_identifier.replace(' ','_') + "_Table." + cube_link.join_identifier.replace(' ','_') + "s:\n")
+                            file.write(
+                                "\t\tfor item in self."
+                                + report_id
+                                + "_"
+                                + cube_link.join_identifier.replace(" ", "_")
+                                + "_Table."
+                                + cube_link.join_identifier.replace(" ", "_")
+                                + "s:\n"
+                            )
                             file.write("\t\t\tnewItem = " + report_id + "_UnionItem()\n")
                             file.write("\t\t\tnewItem.base = item\n")
                             file.write("\t\t\titems.append(newItem)\n")
                             join_ids_added.append(cube_link.join_identifier)
             file.write("\t\treturn items\n")
@@ -212,69 +236,113 @@
                 for cube_link in cube_links:
                     the_report_id = cube_link.foreign_cube_id.cube_id
                     if the_report_id == report_id:
                         # only write the class header once
                         if not class_header_is_written:
-                            file.write("\nclass " + cube_link.join_identifier.replace(' ','_') + "(" + report_id + "_Base):\n")
+                            file.write(
+                                "\nclass " + cube_link.join_identifier.replace(" ", "_") + "(" + report_id + "_Base):\n"
+                            )
                             class_header_is_written = True
 
                         cube_structure_item_links = []
                         try:
-                            cube_structure_item_links = sdd_context.cube_structure_item_link_to_cube_link_map[cube_link.cube_link_id]
+                            cube_structure_item_links = sdd_context.cube_structure_item_link_to_cube_link_map[
+                                cube_link.cube_link_id
+                            ]
                         except KeyError:
                             print(f"No cube structure item links for cube_link: {cube_link.cube_link_id}")
                         primary_cubes_added = []
                         if len(cube_structure_item_links) == 0:
                             file.write("\tpass\n")
                         for cube_structure_item_link in cube_structure_item_links:
                             if cube_structure_item_link.cube_link_id.primary_cube_id.cube_id not in primary_cubes_added:
-                                file.write("\t" + cube_structure_item_link.cube_link_id.primary_cube_id.cube_id  + " = None # " + cube_structure_item_link.cube_link_id.primary_cube_id.cube_id + "\n")
-                                primary_cubes_added.append(cube_structure_item_link.cube_link_id.primary_cube_id.cube_id)
+                                file.write(
+                                    "\t"
+                                    + cube_structure_item_link.cube_link_id.primary_cube_id.cube_id
+                                    + " = None # "
+                                    + cube_structure_item_link.cube_link_id.primary_cube_id.cube_id
+                                    + "\n"
+                                )
+                                primary_cubes_added.append(
+                                    cube_structure_item_link.cube_link_id.primary_cube_id.cube_id
+                                )
                         for cube_structure_item_link in cube_structure_item_links:
-                            file.write('\t@lineage(dependencies={"'+ cube_structure_item_link.cube_link_id.primary_cube_id.cube_id + '.' + cube_structure_item_link.primary_cube_variable_code.variable_id.variable_id +'"})\n')
-                            file.write("\tdef " + cube_structure_item_link.foreign_cube_variable_code.variable_id.variable_id + "(self):\n")
-                            file.write("\t\treturn self." +  cube_structure_item_link.cube_link_id.primary_cube_id.cube_id + "." + cube_structure_item_link.primary_cube_variable_code.variable_id.variable_id + "\n")
-
+                            file.write(
+                                '\t@lineage(dependencies={"'
+                                + cube_structure_item_link.cube_link_id.primary_cube_id.cube_id
+                                + "."
+                                + cube_structure_item_link.primary_cube_variable_code.variable_id.variable_id
+                                + '"})\n'
+                            )
+                            file.write(
+                                "\tdef "
+                                + cube_structure_item_link.foreign_cube_variable_code.variable_id.variable_id
+                                + "(self):\n"
+                            )
+                            file.write(
+                                "\t\treturn self."
+                                + cube_structure_item_link.cube_link_id.primary_cube_id.cube_id
+                                + "."
+                                + cube_structure_item_link.primary_cube_variable_code.variable_id.variable_id
+                                + "\n"
+                            )
 
             for join_for_report_id, cube_links in sdd_context.cube_link_to_join_for_report_id_map.items():
 
-                report_and_join =   join_for_report_id.split(':')
+                report_and_join = join_for_report_id.split(":")
                 join_id = report_and_join[1]
                 if report_and_join[0] == report_id:
-                    file.write("\nclass " + report_id + "_" + join_id.replace(' ','_') + "_Table:\n" )
+                    file.write("\nclass " + report_id + "_" + join_id.replace(" ", "_") + "_Table:\n")
                     for cube_link in cube_links:
                         cube_structure_item_links = []
                         try:
-                            cube_structure_item_links = sdd_context.cube_structure_item_link_to_cube_link_map[cube_link.cube_link_id]
+                            cube_structure_item_links = sdd_context.cube_structure_item_link_to_cube_link_map[
+                                cube_link.cube_link_id
+                            ]
                         except KeyError:
                             print(f"No cube structure item links for cube_link: {cube_link.cube_link_id}")
 
                         primary_cubes_added = []
                         for cube_structure_item_link in cube_structure_item_links:
                             if cube_structure_item_link.cube_link_id.primary_cube_id.cube_id not in primary_cubes_added:
-                                file.write("\t" + cube_structure_item_link.cube_link_id.primary_cube_id.cube_id  + "_Table = None # " + cube_structure_item_link.cube_link_id.primary_cube_id.cube_id + "\n")
-                                primary_cubes_added.append(cube_structure_item_link.cube_link_id.primary_cube_id.cube_id)
-
+                                file.write(
+                                    "\t"
+                                    + cube_structure_item_link.cube_link_id.primary_cube_id.cube_id
+                                    + "_Table = None # "
+                                    + cube_structure_item_link.cube_link_id.primary_cube_id.cube_id
+                                    + "\n"
+                                )
+                                primary_cubes_added.append(
+                                    cube_structure_item_link.cube_link_id.primary_cube_id.cube_id
+                                )
 
                 if report_and_join[0] == report_id:
                     join_id = report_and_join[1]
-                    file.write("\t" + join_id.replace(' ','_') + "s = []# " + join_id.replace(' ','_') + "[]\n")
-                    file.write("\tdef calc_" + join_id.replace(' ','_') + "s(self) :\n")
-                    file.write("\t\titems = [] # " + join_id.replace(' ','_') + "[\n")
+                    file.write("\t" + join_id.replace(" ", "_") + "s = []# " + join_id.replace(" ", "_") + "[]\n")
+                    file.write("\tdef calc_" + join_id.replace(" ", "_") + "s(self) :\n")
+                    file.write("\t\titems = [] # " + join_id.replace(" ", "_") + "[\n")
                     file.write("\t\t# Join up any refered tables that you need to join\n")
                     file.write("\t\t# loop through the main table\n")
-                    file.write("\t\t# set any references you want to on the new Item so that it can refer to themin operations\n")
+                    file.write(
+                        "\t\t# set any references you want to on the new Item so that it can refer to themin operations\n"
+                    )
                     file.write("\t\treturn items\n")
                     file.write("\tdef init(self):\n")
                     file.write("\t\tOrchestration().init(self)\n")
-                    file.write("\t\tself." + join_id.replace(' ','_') + "s = []\n")
-                    file.write("\t\tself." + join_id.replace(' ','_') + "s.extend(self.calc_" + join_id.replace(' ','_') + "s())\n")
+                    file.write("\t\tself." + join_id.replace(" ", "_") + "s = []\n")
+                    file.write(
+                        "\t\tself."
+                        + join_id.replace(" ", "_")
+                        + "s.extend(self.calc_"
+                        + join_id.replace(" ", "_")
+                        + "s())\n"
+                    )
                     file.write("\t\tCSVConverter.persist_object_as_csv(self,True)\n")
 
                     file.write("\t\treturn None\n")
                     file.write("\n")
 
     def delete_generated_python_join_files(context):
         base_dir = settings.BASE_DIR
-        python_dir = os.path.join(base_dir, 'results',  'generated_python_joins')
+        python_dir = os.path.join(base_dir, "results", "generated_python_joins")
         for file in os.listdir(python_dir):
             os.remove(os.path.join(python_dir, file))
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/pybird/create_python_django_transformations.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/pybird/create_executable_filters.py	2025-09-02 15:09:38.717102+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/pybird/create_executable_filters.py	2025-09-21 17:07:36.781576+00:00
@@ -30,33 +30,43 @@
         # Keep the member node caching
         if member not in self._node_cache:
             self._node_cache[member] = member in sdd_context.members_that_are_nodes
         return self._node_cache[member]
 
-    def create_executable_filters(self,context, sdd_context):
+    def create_executable_filters(self, context, sdd_context):
         CreateExecutableFilters.delete_generated_python_filter_files(self, context)
         CreateExecutableFilters.delete_generated_html_filter_files(self, context)
-        CreateExecutableFilters.prepare_node_dictionaries_and_lists(self,sdd_context)
+        CreateExecutableFilters.prepare_node_dictionaries_and_lists(self, sdd_context)
 
         # Initialize AORTA tracking
         orchestration = Orchestration()
-        if hasattr(context, 'enable_lineage_tracking') and context.enable_lineage_tracking:
+        if hasattr(context, "enable_lineage_tracking") and context.enable_lineage_tracking:
             orchestration.init_with_lineage(self, f"Filter_Generation_{datetime.now().strftime('%Y%m%d_%H%M%S')}")
             print("AORTA lineage tracking enabled for filter generation")
-        file = open(sdd_context.output_directory + os.sep + 'generated_python_filters' + os.sep +  'report_cells.py', "a",  encoding='utf-8')
-        report_html_file = open(sdd_context.output_directory + os.sep + 'generated_html' + os.sep +  'report_templates.html', "a",  encoding='utf-8')
+        file = open(
+            sdd_context.output_directory + os.sep + "generated_python_filters" + os.sep + "report_cells.py",
+            "a",
+            encoding="utf-8",
+        )
+        report_html_file = open(
+            sdd_context.output_directory + os.sep + "generated_html" + os.sep + "report_templates.html",
+            "a",
+            encoding="utf-8",
+        )
         report_html_file.write("{% extends 'base.html' %}\n")
         report_html_file.write("{% block content %}\n")
         report_html_file.write("<!DOCTYPE html>\n")
         report_html_file.write("<html>\n")
         report_html_file.write("<head>\n")
         report_html_file.write("<title>Report Templates</title>\n")
         report_html_file.write("</head>\n")
         report_html_file.write("<body>\n")
         report_html_file.write("<h1>Report Templates</h1>\n")
-        report_html_file.write("<table border=\"1\">\n")
-        report_html_file.write("<a href=\"{% url 'pybirdai:step_by_step_mode'%}\">Back to the PyBIRD AI Home Page</a>\n")
+        report_html_file.write('<table border="1">\n')
+        report_html_file.write(
+            "<a href=\"{% url 'pybirdai:step_by_step_mode'%}\">Back to the PyBIRD AI Home Page</a>\n"
+        )
 
         file.write("from pybirdai.models.bird_data_model import *\n")
         file.write("from .output_tables import *\n")
         file.write("from pybirdai.process_steps.pybird.orchestration import Orchestration\n")
         file.write("from pybirdai.annotations.decorators import lineage\n")
@@ -64,94 +74,126 @@
         # create a copy of combination_to_rol_cube_map which is ordered by cube_id
         cube_ids = sorted(sdd_context.combination_to_rol_cube_map.keys())
         for cube_id in cube_ids:
             combination_list = sdd_context.combination_to_rol_cube_map[cube_id]
 
-            report_html_file.write("<tr><td><a href=\"{% url 'pybirdai:show_report' '" + cube_id +'.html' + "'%}\">" +cube_id + "</a></td></tr>\n")
-            filter_html_file = open(sdd_context.output_directory + os.sep + 'generated_html' + os.sep +  cube_id +'.html', "a",  encoding='utf-8')
+            report_html_file.write(
+                "<tr><td><a href=\"{% url 'pybirdai:show_report' '"
+                + cube_id
+                + ".html"
+                + "'%}\">"
+                + cube_id
+                + "</a></td></tr>\n"
+            )
+            filter_html_file = open(
+                sdd_context.output_directory + os.sep + "generated_html" + os.sep + cube_id + ".html",
+                "a",
+                encoding="utf-8",
+            )
 
             filter_html_file.write("{% extends 'base.html' %}\n")
             filter_html_file.write("{% block content %}\n")
             filter_html_file.write("<!DOCTYPE html>\n")
             filter_html_file.write("<html>\n")
             filter_html_file.write("<head>\n")
             filter_html_file.write("<title>Execute Datapoints</title>\n")
             filter_html_file.write("</head>\n")
             filter_html_file.write("<body>\n")
             filter_html_file.write("<h1>" + cube_id + "</h1>\n")
-            filter_html_file.write("<table border=\"1\">\n")
-            filter_html_file.write("<a href=\"{% url 'pybirdai:report_templates'%}\">Back to the PyBIRD Reports Templates Page</a>\n")
-
+            filter_html_file.write('<table border="1">\n')
+            filter_html_file.write(
+                "<a href=\"{% url 'pybirdai:report_templates'%}\">Back to the PyBIRD Reports Templates Page</a>\n"
+            )
 
             for combination in combination_list:
                 if combination.combination_id.metric:
-                    filter_html_file.write("<tr><td><a href=\"{% url 'pybirdai:execute_data_point' '" + combination.combination_id.combination_id + "'%}\">" + cube_id + "_" + combination.combination_id.combination_id + "</a></td></tr>\n")
-
+                    filter_html_file.write(
+                        "<tr><td><a href=\"{% url 'pybirdai:execute_data_point' '"
+                        + combination.combination_id.combination_id
+                        + "'%}\">"
+                        + cube_id
+                        + "_"
+                        + combination.combination_id.combination_id
+                        + "</a></td></tr>\n"
+                    )
 
                     file.write("class Cell_" + combination.combination_id.combination_id + ":\n")
                     file.write("\t" + cube_id + "_Table = None\n")
                     file.write("\t" + cube_id + "s = []\n")
-                    file.write("\t@lineage(dependencies={\"" + cube_id + "." +combination.combination_id.metric.name + "\"})\n")
-                    file.write("\tdef metric_value(self):\n"  )
+                    file.write(
+                        '\t@lineage(dependencies={"' + cube_id + "." + combination.combination_id.metric.name + '"})\n'
+                    )
+                    file.write("\tdef metric_value(self):\n")
                     file.write("\t\ttotal = 0\n")
                     file.write("\t\tfor item in self." + cube_id + "s:\n")
                     file.write("\t\t\ttotal += item." + combination.combination_id.metric.name + "()\n")
                     file.write("\t\treturn total\n")
-                    calc_string = ''
-                    calc_lineage_string = '\t@lineage(dependencies={'
-                    calc_string +="\tdef calc_referenced_items(self):\n"
-                    calc_string +="\t\titems = self." + cube_id + "_Table." + cube_id + "s\n"
-                    calc_string +="\t\tfor item in items:\n"
-                    calc_string +="\t\t\tfilter_passed = True\n"
+                    calc_string = ""
+                    calc_lineage_string = "\t@lineage(dependencies={"
+                    calc_string += "\tdef calc_referenced_items(self):\n"
+                    calc_string += "\t\titems = self." + cube_id + "_Table." + cube_id + "s\n"
+                    calc_string += "\t\tfor item in items:\n"
+                    calc_string += "\t\t\tfilter_passed = True\n"
                     combination_item_list = []
                     try:
-                        combination_item_list =  sdd_context.combination_item_dictionary[combination.combination_id.combination_id]
+                        combination_item_list = sdd_context.combination_item_dictionary[
+                            combination.combination_id.combination_id
+                        ]
                     except:
                         pass
                     item_counter = 0
                     for combination_item in combination_item_list:
-                        leaf_node_members = CreateExecutableFilters.get_leaf_node_codes(self,
-                                                                                      sdd_context,
-                                                                                      combination_item.member_id,
-                                                                                      combination_item.member_hierarchy)
-    
+                        leaf_node_members = CreateExecutableFilters.get_leaf_node_codes(
+                            self, sdd_context, combination_item.member_id, combination_item.member_hierarchy
+                        )
+
                         if len(leaf_node_members) > 0:
-                            if (len(leaf_node_members) == 1) and (str(leaf_node_members[0].code) == '0'):
+                            if (len(leaf_node_members) == 1) and (str(leaf_node_members[0].code) == "0"):
                                 pass
                             else:
-                                calc_string +="\t\t\tif "
+                                calc_string += "\t\t\tif "
                                 for leaf_node_member in leaf_node_members:
-                                    calc_string += "\t\t\t\t(item." + combination_item.variable_id.name + "() == '" + str(leaf_node_member.code) + "')  or \\\n"
+                                    calc_string += (
+                                        "\t\t\t\t(item."
+                                        + combination_item.variable_id.name
+                                        + "() == '"
+                                        + str(leaf_node_member.code)
+                                        + "')  or \\\n"
+                                    )
                                 calc_string += "\t\t\t\tFalse:\n"
                                 calc_string += "\t\t\t\tpass\n"
                                 calc_string += "\t\t\telse:\n"
                                 calc_string += "\t\t\t\tfilter_passed = False\n"
                                 if item_counter > 0:
-                                    calc_lineage_string += ','
-                                    calc_lineage_string += '\n\t\t\t'
+                                    calc_lineage_string += ","
+                                    calc_lineage_string += "\n\t\t\t"
                                 calc_lineage_string += '"'
-                                calc_lineage_string += cube_id 
-                                calc_lineage_string += '.'
-                                calc_lineage_string += combination_item.variable_id.name 
+                                calc_lineage_string += cube_id
+                                calc_lineage_string += "."
+                                calc_lineage_string += combination_item.variable_id.name
                                 calc_lineage_string += '"'
-                            
+
                                 item_counter += 1
                         else:
-                            print("No leaf node members for " + combination_item.variable_id.name +":" + combination_item.member_id.member_id)
-
-                    calc_lineage_string += '})\n'
+                            print(
+                                "No leaf node members for "
+                                + combination_item.variable_id.name
+                                + ":"
+                                + combination_item.member_id.member_id
+                            )
+
+                    calc_lineage_string += "})\n"
 
                     file.write(calc_lineage_string)
-                    file.write(calc_string + '\n')
+                    file.write(calc_string + "\n")
                     file.write("\t\t\tif filter_passed:\n")
                     file.write("\t\t\t\tself." + cube_id + "s.append(item)\n")
                     file.write("\tdef init(self):\n")
                     file.write("\t\tOrchestration().init(self)\n")
                     file.write("\t\tself." + cube_id + "s = []\n")
                     file.write("\t\tself.calc_referenced_items()\n")
                     file.write("\t\treturn None\n")
-
 
             filter_html_file.write("</table>\n")
             filter_html_file.write("</body>\n")
             filter_html_file.write("</html>\n")
             filter_html_file.write("</table>\n")
@@ -164,71 +206,111 @@
         report_html_file.write("</table>\n")
         report_html_file.write("</body>\n")
         report_html_file.write("</html>\n")
         report_html_file.write("{% endblock %}\n")
 
-
     def get_leaf_node_codes(self, sdd_context, member, member_hierarchy):
         return_list = []
         if member is not None:
             members = self.get_member_list_considering_hierarchies(sdd_context, member, member_hierarchy)
             return_list = members  # Keep original order
         return return_list
 
-    def get_literal_list_considering_hierarchies(self, context, sdd_context, literal, member, domain_id, warning_list, template_code, combination_id, variable_id, framework, cube_type, input_cube_type):
+    def get_literal_list_considering_hierarchies(
+        self,
+        context,
+        sdd_context,
+        literal,
+        member,
+        domain_id,
+        warning_list,
+        template_code,
+        combination_id,
+        variable_id,
+        framework,
+        cube_type,
+        input_cube_type,
+    ):
         cache_key = (literal, member, domain_id) if literal and member else None
         if cache_key in self._literal_list_cache:
             return self._literal_list_cache[cache_key].copy()
 
         return_list = []
         is_node = self.is_member_a_node(sdd_context, member)
 
         if literal is None:
             if not is_node:
-                warning_list.append(("error", "member does not exist in input layer and is not a node", template_code, combination_id, variable_id, member.member_id, None, domain_id))
+                warning_list.append(
+                    (
+                        "error",
+                        "member does not exist in input layer and is not a node",
+                        template_code,
+                        combination_id,
+                        variable_id,
+                        member.member_id,
+                        None,
+                        domain_id,
+                    )
+                )
         else:
             if not is_node:
                 return_list = [literal]
 
         for domain, hierarchy_list in sdd_context.domain_to_hierarchy_dictionary.items():
             if domain.domain_id == domain_id:
                 for hierarchy in hierarchy_list:
                     hierarchy_id = hierarchy.member_hierarchy_id
                     literal_list = []
-                    self.get_literal_list_considering_hierarchy(context, sdd_context, member, hierarchy_id, literal_list, framework, cube_type, input_cube_type)
+                    self.get_literal_list_considering_hierarchy(
+                        context, sdd_context, member, hierarchy_id, literal_list, framework, cube_type, input_cube_type
+                    )
                     return_list.extend(literal_list)
 
         if len(return_list) == 0:
-            warning_list.append(("error", "could not find any input layer members or sub members for member", template_code, combination_id, variable_id, member.member_id, None, domain_id))
+            warning_list.append(
+                (
+                    "error",
+                    "could not find any input layer members or sub members for member",
+                    template_code,
+                    combination_id,
+                    variable_id,
+                    member.member_id,
+                    None,
+                    domain_id,
+                )
+            )
 
         self._literal_list_cache[cache_key] = return_list
         return return_list.copy()
 
-    def get_literal_list_considering_hierarchy(self, context, sdd_context, member, hierarchy, literal_list, framework, cube_type, input_cube_type):
+    def get_literal_list_considering_hierarchy(
+        self, context, sdd_context, member, hierarchy, literal_list, framework, cube_type, input_cube_type
+    ):
         key = member.member_id + ":" + hierarchy
         child_members = []
         try:
             child_members = sdd_context.member_plus_hierarchy_to_child_literals[key]
             for item in child_members:
                 if item.domain_id is None:
                     print("domain_id is None for " + item.member_id)
                 if item is None:
                     print("item is None for " + item.member_id)
                 literal = item
-                if not(literal is None):
-                    if not(literal in literal_list):
-                        is_node = CreateExecutableFilters.is_member_a_node(self,sdd_context,literal)
+                if not (literal is None):
+                    if not (literal in literal_list):
+                        is_node = CreateExecutableFilters.is_member_a_node(self, sdd_context, literal)
                         if not (is_node):
                             literal_list.append(literal)
 
             for item in child_members:
-                CreateExecutableFilters.get_literal_list_considering_hierarchy(self,context,sdd_context,item,hierarchy, literal_list,framework,cube_type,input_cube_type)
+                CreateExecutableFilters.get_literal_list_considering_hierarchy(
+                    self, context, sdd_context, item, hierarchy, literal_list, framework, cube_type, input_cube_type
+                )
         except KeyError:
             pass
 
-
-    def find_member_node(self,sdd_context,member_id,hierarchy):
+    def find_member_node(self, sdd_context, member_id, hierarchy):
         try:
             return sdd_context.member_hierarchy_node_dictionary[hierarchy + ":" + member_id.member_id]
         except:
             pass
 
@@ -238,19 +320,23 @@
         sdd_context.member_plus_hierarchy_to_child_literals = {}
         sdd_context.domain_to_hierarchy_dictionary = {}
 
         # Pre-process hierarchy nodes
         for node in sdd_context.member_hierarchy_node_dictionary.values():
-            if node.parent_member_id and node.parent_member_id != '':
+            if node.parent_member_id and node.parent_member_id != "":
                 sdd_context.members_that_are_nodes.add(node.parent_member_id)
-                member_plus_hierarchy = f"{node.parent_member_id.member_id}:{node.member_hierarchy_id.member_hierarchy_id}"
+                member_plus_hierarchy = (
+                    f"{node.parent_member_id.member_id}:{node.member_hierarchy_id.member_hierarchy_id}"
+                )
 
                 if member_plus_hierarchy not in sdd_context.member_plus_hierarchy_to_child_literals:
                     sdd_context.member_plus_hierarchy_to_child_literals[member_plus_hierarchy] = [node.member_id]
                 else:
                     if node.member_id not in sdd_context.member_plus_hierarchy_to_child_literals[member_plus_hierarchy]:
-                        sdd_context.member_plus_hierarchy_to_child_literals[member_plus_hierarchy].append(node.member_id)
+                        sdd_context.member_plus_hierarchy_to_child_literals[member_plus_hierarchy].append(
+                            node.member_id
+                        )
 
         # Build domain hierarchy mapping
         for hierarchy in sdd_context.member_hierarchy_dictionary.values():
             domain_id = hierarchy.domain_id
             if domain_id not in sdd_context.domain_to_hierarchy_dictionary:
@@ -299,14 +385,14 @@
         except KeyError:
             pass
 
     def delete_generated_python_filter_files(self, context):
         base_dir = settings.BASE_DIR
-        python_dir = os.path.join(base_dir, 'results', 'generated_python_filters')
+        python_dir = os.path.join(base_dir, "results", "generated_python_filters")
         for file in os.listdir(python_dir):
             os.remove(os.path.join(python_dir, file))
 
     def delete_generated_html_filter_files(self, context):
         base_dir = settings.BASE_DIR
-        html_dir = os.path.join(base_dir, 'results', 'generated_html')
+        html_dir = os.path.join(base_dir, "results", "generated_html")
         for file in os.listdir(html_dir):
             os.remove(os.path.join(html_dir, file))
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/pybird/create_executable_filters.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py	2025-09-02 15:09:38.716866+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py	2025-09-21 17:07:36.792747+00:00
@@ -27,478 +27,464 @@
 lineage processor but storing results in BPMN_lite format.
 """
 
 from django.db import transaction
 from pybirdai.models.bpmn_lite_models import (
-    UserTask, ServiceTask, SequenceFlow, SubProcess, 
-    SubProcessFlowElement, WorkflowModule
+    UserTask,
+    ServiceTask,
+    SequenceFlow,
+    SubProcess,
+    SubProcessFlowElement,
+    WorkflowModule,
 )
 
 
 class BPMNMetadataLineageProcessor:
     """
     Processes metadata lineage and stores it using BPMN_lite models.
-    
+
     Creates BPMN workflows where:
     - UserTask = Input data items (tables/columns that are consumed)
     - ServiceTask = Output data items (tables/columns that are produced)
     - SequenceFlow = Transformation processes connecting input to output
     - SubProcess = Container for the complete datapoint workflow
     """
-    
+
     def __init__(self, sdd_context):
         self.sdd_context = sdd_context
         self.created_tasks = {}  # Cache for created tasks
         self.created_flows = {}  # Cache for created flows
-        
+
     def process_datapoint_metadata_lineage(self, datapoint):
         """
         Create BPMN workflow for a specific datapoint's metadata lineage.
-        
+
         Args:
             datapoint: COMBINATION object representing the datapoint
         """
         print(f"Creating BPMN workflow for datapoint: {datapoint.combination_id}")
-        
+
         # Create main SubProcess to contain the workflow
         workflow_subprocess = self._create_or_get_subprocess(
             id=f"workflow_{datapoint.combination_id}",
             name=f"Metadata Lineage Workflow: {datapoint.combination_id}",
-            description=f"Complete metadata lineage workflow for datapoint {datapoint.combination_id}"
-        )
-        
+            description=f"Complete metadata lineage workflow for datapoint {datapoint.combination_id}",
+        )
+
         # Create workflow module to organize the workflow
         workflow_module = self._create_or_get_workflow_module(
             module_id=f"module_{datapoint.combination_id}",
-            module_name=f"Datapoint {datapoint.combination_id} Lineage Module"
-        )
-        
+            module_name=f"Datapoint {datapoint.combination_id} Lineage Module",
+        )
+
         # Process the complete lineage chain similar to the original processor
         self._process_complete_bpmn_lineage(workflow_subprocess, datapoint)
-        
+
         return workflow_subprocess
-    
+
     def _create_or_get_subprocess(self, id, name, description):
         """Create or get existing SubProcess."""
         subprocess, created = SubProcess.objects.get_or_create(
-            id=id,
-            defaults={
-                'name': name,
-                'description': description
-            }
+            id=id, defaults={"name": name, "description": description}
         )
         if created:
             print(f"Created SubProcess: {name}")
         return subprocess
-    
+
     def _create_or_get_workflow_module(self, module_id, module_name):
         """Create or get existing WorkflowModule."""
         module, created = WorkflowModule.objects.get_or_create(
-            module_id=module_id,
-            defaults={
-                'module_name': module_name
-            }
+            module_id=module_id, defaults={"module_name": module_name}
         )
         if created:
             print(f"Created WorkflowModule: {module_name}")
         return module
-    
+
     def _create_or_get_user_task(self, id, name, description, entity_reference=None):
         """
         Create or get UserTask (represents input/consumed data items).
-        
+
         Args:
             id: Unique identifier for the task
             name: Task name
             description: Task description
             entity_reference: Reference to the data entity (table/column location)
         """
         cache_key = f"user_{id}"
         if cache_key in self.created_tasks:
             return self.created_tasks[cache_key]
-        
+
         task, created = UserTask.objects.get_or_create(
-            id=id,
-            defaults={
-                'name': name,
-                'description': description,
-                'entity_reference': entity_reference
-            }
-        )
-        
+            id=id, defaults={"name": name, "description": description, "entity_reference": entity_reference}
+        )
+
         if created:
             print(f"Created UserTask (input): {name}")
-        
+
         self.created_tasks[cache_key] = task
         return task
-    
+
     def _create_or_get_service_task(self, id, name, description, enriched_attribute_reference=None):
         """
         Create or get ServiceTask (represents output/produced data items).
-        
+
         Args:
             id: Unique identifier for the task
             name: Task name
             description: Task description
             enriched_attribute_reference: Reference to the produced data entity
         """
         cache_key = f"service_{id}"
         if cache_key in self.created_tasks:
             return self.created_tasks[cache_key]
-        
+
         task, created = ServiceTask.objects.get_or_create(
             id=id,
             defaults={
-                'name': name,
-                'description': description,
-                'enriched_attribute_reference': enriched_attribute_reference
-            }
-        )
-        
+                "name": name,
+                "description": description,
+                "enriched_attribute_reference": enriched_attribute_reference,
+            },
+        )
+
         if created:
             print(f"Created ServiceTask (output): {name}")
-        
+
         self.created_tasks[cache_key] = task
         return task
-    
+
     def _create_or_get_sequence_flow(self, id, name, source_task, target_task, description=None):
         """
         Create or get SequenceFlow (represents transformation process).
-        
+
         Args:
             id: Unique identifier for the flow
             name: Flow name
             source_task: Task that produces the data (UserTask or ServiceTask)
             target_task: Task that consumes the data (UserTask or ServiceTask)
             description: Flow description
         """
         cache_key = f"flow_{id}"
         if cache_key in self.created_flows:
             return self.created_flows[cache_key]
-        
+
         flow, created = SequenceFlow.objects.get_or_create(
             id=id,
-            defaults={
-                'name': name,
-                'description': description,
-                'source_ref': source_task,
-                'target_ref': target_task
-            }
-        )
-        
+            defaults={"name": name, "description": description, "source_ref": source_task, "target_ref": target_task},
+        )
+
         if created:
             print(f"Created SequenceFlow: {source_task.name} -> {target_task.name}")
-        
+
         self.created_flows[cache_key] = flow
         return flow
-    
+
     def _add_element_to_subprocess(self, subprocess, element, element_type):
         """Add a flow element to a subprocess."""
         SubProcessFlowElement.objects.get_or_create(
-            sub_process=subprocess,
-            flow_element_type=element_type,
-            flow_element_id=element.id
-        )
-    
+            sub_process=subprocess, flow_element_type=element_type, flow_element_id=element.id
+        )
+
     def _process_complete_bpmn_lineage(self, workflow_subprocess, datapoint):
         """
         Process complete BPMN lineage similar to the original processor.
-        
+
         This follows the same logic as _trace_complete_lineage but creates BPMN elements.
         """
         from pybirdai.models.bird_meta_data_model import (
-            COMBINATION_ITEM, CUBE, CUBE_STRUCTURE_ITEM, CUBE_LINK, 
-            CUBE_STRUCTURE_ITEM_LINK
-        )
-        
+            COMBINATION_ITEM,
+            CUBE,
+            CUBE_STRUCTURE_ITEM,
+            CUBE_LINK,
+            CUBE_STRUCTURE_ITEM_LINK,
+        )
+
         # Step 1: Create main datapoint ServiceTask (output)
         datapoint_service_task = self._create_or_get_service_task(
             id=f"datapoint_{datapoint.combination_id}",
             name=f"Datapoint: {datapoint.combination_id}",
             description=f"Final datapoint result for {datapoint.combination_id}",
-            enriched_attribute_reference=datapoint.combination_id
-        )
-        
-        self._add_element_to_subprocess(workflow_subprocess, datapoint_service_task, 'ServiceTask')
-        
+            enriched_attribute_reference=datapoint.combination_id,
+        )
+
+        self._add_element_to_subprocess(workflow_subprocess, datapoint_service_task, "ServiceTask")
+
         # Step 2: Create metric column ServiceTask if exists
         if datapoint.metric:
             metric_service_task = self._create_or_get_service_task(
                 id=f"metric_{datapoint.combination_id}_{datapoint.metric.variable_id}",
                 name=f"Metric: {datapoint.metric.variable_id}",
                 description=f"Metric column {datapoint.metric.variable_id} for datapoint",
-                enriched_attribute_reference=f"{datapoint.combination_id}.{datapoint.metric.variable_id}"
+                enriched_attribute_reference=f"{datapoint.combination_id}.{datapoint.metric.variable_id}",
             )
-            
-            self._add_element_to_subprocess(workflow_subprocess, metric_service_task, 'ServiceTask')
-            
+
+            self._add_element_to_subprocess(workflow_subprocess, metric_service_task, "ServiceTask")
+
             # Create flow from datapoint to metric
             metric_flow = self._create_or_get_sequence_flow(
                 id=f"flow_datapoint_to_metric_{datapoint.combination_id}",
                 name="Datapoint to Metric",
                 source_task=datapoint_service_task,
                 target_task=metric_service_task,
-                description="Flow from datapoint to metric column"
+                description="Flow from datapoint to metric column",
             )
-            
-            self._add_element_to_subprocess(workflow_subprocess, metric_flow, 'SequenceFlow')
-        
+
+            self._add_element_to_subprocess(workflow_subprocess, metric_flow, "SequenceFlow")
+
         # Step 3: Process output tables and their lineage
         self._trace_bpmn_output_table(workflow_subprocess, datapoint)
-    
+
     def _trace_bpmn_output_table(self, workflow_subprocess, datapoint):
         """
         Trace BPMN lineage for output tables, similar to _trace_single_output_table.
         """
         from pybirdai.models.bird_meta_data_model import (
-            COMBINATION_ITEM, CUBE, CUBE_STRUCTURE_ITEM, CUBE_LINK, 
-            CUBE_STRUCTURE_ITEM_LINK
-        )
-        
+            COMBINATION_ITEM,
+            CUBE,
+            CUBE_STRUCTURE_ITEM,
+            CUBE_LINK,
+            CUBE_STRUCTURE_ITEM_LINK,
+        )
+
         # Find the main output table for this datapoint
-        parts = datapoint.combination_id.split('_')
-        if len(parts) >= 2 and parts[-1] == 'REF':
-            base_table_name = '_'.join(parts[:-2])
+        parts = datapoint.combination_id.split("_")
+        if len(parts) >= 2 and parts[-1] == "REF":
+            base_table_name = "_".join(parts[:-2])
         else:
             base_table_name = datapoint.combination_id
-        
+
         try:
             output_cube = CUBE.objects.get(cube_id=base_table_name)
         except CUBE.DoesNotExist:
             print(f"Output cube not found: {base_table_name}")
             return
-        
+
         print(f"Processing BPMN lineage for output table: {output_cube.cube_id}")
-        
+
         # Create ServiceTask for output table
         output_table_service_task = self._create_or_get_service_task(
             id=f"output_table_{output_cube.cube_id}",
             name=f"Output Table: {output_cube.cube_id}",
             description=f"Output table {output_cube.cube_id}",
-            enriched_attribute_reference=output_cube.cube_id
-        )
-        
-        self._add_element_to_subprocess(workflow_subprocess, output_table_service_task, 'ServiceTask')
-        
+            enriched_attribute_reference=output_cube.cube_id,
+        )
+
+        self._add_element_to_subprocess(workflow_subprocess, output_table_service_task, "ServiceTask")
+
         # Get columns used in this combination
-        combination_items = COMBINATION_ITEM.objects.filter(
-            combination_id=datapoint
-        ).select_related('variable_id')
-        
+        combination_items = COMBINATION_ITEM.objects.filter(combination_id=datapoint).select_related("variable_id")
+
         used_columns = set()
         for combo_item in combination_items:
             if combo_item.variable_id:
                 used_columns.add(combo_item.variable_id.variable_id)
-        
+
         print(f"Processing {len(used_columns)} columns for BPMN lineage")
-        
+
         # Process columns in output table
         if output_cube.cube_structure_id:
             output_columns = CUBE_STRUCTURE_ITEM.objects.filter(
-                cube_structure_id=output_cube.cube_structure_id,
-                variable_id__variable_id__in=used_columns
-            ).select_related('variable_id')
-            
+                cube_structure_id=output_cube.cube_structure_id, variable_id__variable_id__in=used_columns
+            ).select_related("variable_id")
+
             for col_item in output_columns:
                 if not col_item.variable_id:
                     continue
-                
+
                 # Create ServiceTask for output column
                 output_col_service_task = self._create_or_get_service_task(
                     id=f"output_col_{output_cube.cube_id}_{col_item.variable_id.variable_id}",
                     name=f"Output Column: {col_item.variable_id.variable_id}",
                     description=f"Output column {col_item.variable_id.variable_id} in {output_cube.cube_id}",
-                    enriched_attribute_reference=f"{output_cube.cube_id}.{col_item.variable_id.variable_id}"
-                )
-                
-                self._add_element_to_subprocess(workflow_subprocess, output_col_service_task, 'ServiceTask')
-        
+                    enriched_attribute_reference=f"{output_cube.cube_id}.{col_item.variable_id.variable_id}",
+                )
+
+                self._add_element_to_subprocess(workflow_subprocess, output_col_service_task, "ServiceTask")
+
         # Process cube links (product-specific joins)
         cube_links = CUBE_LINK.objects.filter(foreign_cube_id=output_cube)
-        
+
         if cube_links.exists():
             print(f"Processing {cube_links.count()} cube links for BPMN workflows")
-            
+
             # Group by product name
             product_groups = {}
             for link in cube_links:
-                cube_link_parts = link.cube_link_id.split(':')
+                cube_link_parts = link.cube_link_id.split(":")
                 if len(cube_link_parts) >= 3:
                     product_name = cube_link_parts[2]
                 else:
-                    product_name = 'DEFAULT'
-                
+                    product_name = "DEFAULT"
+
                 # Determine source cube
                 source_cube = None
-                if hasattr(link, 'primary_cube_id') and link.primary_cube_id and link.primary_cube_id != output_cube:
+                if hasattr(link, "primary_cube_id") and link.primary_cube_id and link.primary_cube_id != output_cube:
                     source_cube = link.primary_cube_id
-                
-                if source_cube and (not source_cube.cube_type or source_cube.cube_type != 'RC'):
+
+                if source_cube and (not source_cube.cube_type or source_cube.cube_type != "RC"):
                     if product_name not in product_groups:
                         product_groups[product_name] = []
                     product_groups[product_name].append((link, source_cube))
-            
+
             # Process each product group
             for product_name, link_source_pairs in product_groups.items():
                 self._process_bpmn_product_group(
-                    workflow_subprocess, output_cube, product_name, 
-                    link_source_pairs, used_columns
-                )
-    
-    def _process_bpmn_product_group(self, workflow_subprocess, output_cube, product_name, 
-                                   link_source_pairs, used_columns):
+                    workflow_subprocess, output_cube, product_name, link_source_pairs, used_columns
+                )
+
+    def _process_bpmn_product_group(
+        self, workflow_subprocess, output_cube, product_name, link_source_pairs, used_columns
+    ):
         """
         Process a product-specific group and create corresponding BPMN elements.
         """
         from pybirdai.models.bird_meta_data_model import CUBE_STRUCTURE_ITEM
-        
+
         print(f"Creating BPMN workflow for product group: {product_name}")
-        
+
         # Create ServiceTask for product-specific join table
         join_table_name = f"{output_cube.cube_id}.{product_name}"
         join_table_service_task = self._create_or_get_service_task(
             id=f"join_table_{output_cube.cube_id}_{product_name.replace(' ', '_')}",
             name=f"Join Table: {product_name}",
             description=f"Product-specific join table for {product_name}",
-            enriched_attribute_reference=join_table_name
-        )
-        
-        self._add_element_to_subprocess(workflow_subprocess, join_table_service_task, 'ServiceTask')
-        
+            enriched_attribute_reference=join_table_name,
+        )
+
+        self._add_element_to_subprocess(workflow_subprocess, join_table_service_task, "ServiceTask")
+
         # Process source tables and create UserTasks for input data
         source_tables = set(source_cube for _, source_cube in link_source_pairs)
-        
+
         for source_cube in source_tables:
             # Create UserTask for input table (consumed data)
             source_table_user_task = self._create_or_get_user_task(
                 id=f"input_table_{source_cube.cube_id}",
                 name=f"Input Table: {source_cube.cube_id}",
                 description=f"Input table {source_cube.cube_id}",
-                entity_reference=source_cube.cube_id
+                entity_reference=source_cube.cube_id,
             )
-            
-            self._add_element_to_subprocess(workflow_subprocess, source_table_user_task, 'UserTask')
-            
+
+            self._add_element_to_subprocess(workflow_subprocess, source_table_user_task, "UserTask")
+
             # Create SequenceFlow from input table to join table
             table_join_flow = self._create_or_get_sequence_flow(
                 id=f"flow_{source_cube.cube_id}_to_join_{product_name.replace(' ', '_')}",
                 name=f"Join {source_cube.cube_id} for {product_name}",
                 source_task=source_table_user_task,
                 target_task=join_table_service_task,
-                description=f"Join process from {source_cube.cube_id} for product {product_name}"
+                description=f"Join process from {source_cube.cube_id} for product {product_name}",
             )
-            
-            self._add_element_to_subprocess(workflow_subprocess, table_join_flow, 'SequenceFlow')
-            
+
+            self._add_element_to_subprocess(workflow_subprocess, table_join_flow, "SequenceFlow")
+
             # Create UserTasks for input columns and flows for column transformations
             if source_cube.cube_structure_id:
                 input_columns = CUBE_STRUCTURE_ITEM.objects.filter(
-                    cube_structure_id=source_cube.cube_structure_id,
-                    variable_id__variable_id__in=used_columns
-                ).select_related('variable_id')
-                
+                    cube_structure_id=source_cube.cube_structure_id, variable_id__variable_id__in=used_columns
+                ).select_related("variable_id")
+
                 for input_col in input_columns:
                     if input_col.variable_id:
                         # Create UserTask for input column
                         input_col_user_task = self._create_or_get_user_task(
                             id=f"input_col_{source_cube.cube_id}_{input_col.variable_id.variable_id}",
                             name=f"Input Column: {input_col.variable_id.variable_id}",
                             description=f"Input column {input_col.variable_id.variable_id} from {source_cube.cube_id}",
-                            entity_reference=f"{source_cube.cube_id}.{input_col.variable_id.variable_id}"
+                            entity_reference=f"{source_cube.cube_id}.{input_col.variable_id.variable_id}",
                         )
-                        
-                        self._add_element_to_subprocess(workflow_subprocess, input_col_user_task, 'UserTask')
-                        
+
+                        self._add_element_to_subprocess(workflow_subprocess, input_col_user_task, "UserTask")
+
                         # Create ServiceTask for join table column
                         join_col_service_task = self._create_or_get_service_task(
                             id=f"join_col_{product_name.replace(' ', '_')}_{input_col.variable_id.variable_id}",
                             name=f"Join Column: {input_col.variable_id.variable_id}",
                             description=f"Join table column {input_col.variable_id.variable_id} for {product_name}",
-                            enriched_attribute_reference=f"{join_table_name}.{input_col.variable_id.variable_id}"
+                            enriched_attribute_reference=f"{join_table_name}.{input_col.variable_id.variable_id}",
                         )
-                        
-                        self._add_element_to_subprocess(workflow_subprocess, join_col_service_task, 'ServiceTask')
-                        
+
+                        self._add_element_to_subprocess(workflow_subprocess, join_col_service_task, "ServiceTask")
+
                         # Create SequenceFlow for column transformation
                         col_transform_flow = self._create_or_get_sequence_flow(
                             id=f"flow_transform_{source_cube.cube_id}_{input_col.variable_id.variable_id}_to_{product_name.replace(' ', '_')}",
                             name=f"Transform {input_col.variable_id.variable_id}",
                             source_task=input_col_user_task,
                             target_task=join_col_service_task,
-                            description=f"Transform column {input_col.variable_id.variable_id} for {product_name}"
+                            description=f"Transform column {input_col.variable_id.variable_id} for {product_name}",
                         )
-                        
-                        self._add_element_to_subprocess(workflow_subprocess, col_transform_flow, 'SequenceFlow')
-        
+
+                        self._add_element_to_subprocess(workflow_subprocess, col_transform_flow, "SequenceFlow")
+
         # Create SequenceFlow from join table to output table
         join_to_output_flow = self._create_or_get_sequence_flow(
             id=f"flow_join_{product_name.replace(' ', '_')}_to_output",
             name=f"Copy {product_name} to Output",
             source_task=join_table_service_task,
             target_task=self._get_or_create_output_table_task(output_cube),
-            description=f"Copy from {product_name} join table to output table"
-        )
-        
-        self._add_element_to_subprocess(workflow_subprocess, join_to_output_flow, 'SequenceFlow')
-        
+            description=f"Copy from {product_name} join table to output table",
+        )
+
+        self._add_element_to_subprocess(workflow_subprocess, join_to_output_flow, "SequenceFlow")
+
         # Create column-to-column flows from join to output
-        self._create_bpmn_column_copy_flows(
-            workflow_subprocess, output_cube, product_name, used_columns
-        )
-    
+        self._create_bpmn_column_copy_flows(workflow_subprocess, output_cube, product_name, used_columns)
+
     def _get_or_create_output_table_task(self, output_cube):
         """Get or create ServiceTask for output table."""
         return self._create_or_get_service_task(
             id=f"output_table_{output_cube.cube_id}",
             name=f"Output Table: {output_cube.cube_id}",
             description=f"Output table {output_cube.cube_id}",
-            enriched_attribute_reference=output_cube.cube_id
-        )
-    
+            enriched_attribute_reference=output_cube.cube_id,
+        )
+
     def _create_bpmn_column_copy_flows(self, workflow_subprocess, output_cube, product_name, used_columns):
         """
         Create column-to-column copy flows from join table columns to output table columns.
         """
         from pybirdai.models.bird_meta_data_model import CUBE_STRUCTURE_ITEM
-        
+
         if not output_cube.cube_structure_id:
             return
-        
+
         join_table_name = f"{output_cube.cube_id}.{product_name}"
-        
+
         output_columns = CUBE_STRUCTURE_ITEM.objects.filter(
-            cube_structure_id=output_cube.cube_structure_id,
-            variable_id__variable_id__in=used_columns
-        ).select_related('variable_id')
-        
+            cube_structure_id=output_cube.cube_structure_id, variable_id__variable_id__in=used_columns
+        ).select_related("variable_id")
+
         for output_col in output_columns:
             if output_col.variable_id:
                 # Create/get ServiceTask for join table column
                 join_col_service_task = self._create_or_get_service_task(
                     id=f"join_col_{product_name.replace(' ', '_')}_{output_col.variable_id.variable_id}",
                     name=f"Join Column: {output_col.variable_id.variable_id}",
                     description=f"Join table column {output_col.variable_id.variable_id} for {product_name}",
-                    enriched_attribute_reference=f"{join_table_name}.{output_col.variable_id.variable_id}"
-                )
-                
+                    enriched_attribute_reference=f"{join_table_name}.{output_col.variable_id.variable_id}",
+                )
+
                 # Create ServiceTask for output column
                 output_col_service_task = self._create_or_get_service_task(
                     id=f"output_col_{output_cube.cube_id}_{output_col.variable_id.variable_id}",
                     name=f"Output Column: {output_col.variable_id.variable_id}",
                     description=f"Output column {output_col.variable_id.variable_id}",
-                    enriched_attribute_reference=f"{output_cube.cube_id}.{output_col.variable_id.variable_id}"
-                )
-                
-                self._add_element_to_subprocess(workflow_subprocess, output_col_service_task, 'ServiceTask')
-                
+                    enriched_attribute_reference=f"{output_cube.cube_id}.{output_col.variable_id.variable_id}",
+                )
+
+                self._add_element_to_subprocess(workflow_subprocess, output_col_service_task, "ServiceTask")
+
                 # Create SequenceFlow for column copy
                 col_copy_flow = self._create_or_get_sequence_flow(
                     id=f"flow_copy_{product_name.replace(' ', '_')}_{output_col.variable_id.variable_id}_to_output",
                     name=f"Copy {output_col.variable_id.variable_id} to Output",
                     source_task=join_col_service_task,
                     target_task=output_col_service_task,
-                    description=f"Copy column {output_col.variable_id.variable_id} from {product_name} join table to output table"
-                )
-                
-                self._add_element_to_subprocess(workflow_subprocess, col_copy_flow, 'SequenceFlow')
-                
-                print(f"Created BPMN column copy flow: {join_table_name}.{output_col.variable_id.variable_id} -> {output_cube.cube_id}.{output_col.variable_id.variable_id}")
\ No newline at end of file
+                    description=f"Copy column {output_col.variable_id.variable_id} from {product_name} join table to output table",
+                )
+
+                self._add_element_to_subprocess(workflow_subprocess, col_copy_flow, "SequenceFlow")
+
+                print(
+                    f"Created BPMN column copy flow: {join_table_name}.{output_col.variable_id.variable_id} -> {output_cube.cube_id}.{output_col.variable_id.variable_id}"
+                )
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/sqldeveloper_import/convert_regdna_to_xcore.py	2025-09-15 13:18:11.386740+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/sqldeveloper_import/convert_regdna_to_xcore.py	2025-09-21 17:07:36.810671+00:00
@@ -45,11 +45,11 @@
     """Convert ELAttribute to XCore format."""
     lines = []
 
     # Add annotations
     for annotation in attribute.eAnnotations:
-        lines.append(f'\t{convert_annotation_details(annotation)}')
+        lines.append(f"\t{convert_annotation_details(annotation)}")
 
     # Build attribute line
     attr_line = "\t"
     if attribute.iD:
         attr_line += "id "
@@ -67,16 +67,15 @@
     lines = []
 
     is_identifying_realtionship = False
     # Add annotations
     for annotation in reference.eAnnotations:
-        lines.append(f'\t{convert_annotation_details(annotation)}')
+        lines.append(f"\t{convert_annotation_details(annotation)}")
         details = []
         for detail in annotation.details:
-            if detail.key  == 'is_identifying_relationship' and detail.value == 'true':
+            if detail.key == "is_identifying_relationship" and detail.value == "true":
                 is_identifying_realtionship = True
-
 
     # Build reference line
     ref_line = "\t"
     if reference.containment:
         ref_line += "contains "
@@ -97,11 +96,11 @@
     """Convert ELOperation to XCore format."""
     lines = []
 
     # Add annotations
     for annotation in operation.eAnnotations:
-        lines.append(f'\t{convert_annotation_details(annotation)}')
+        lines.append(f"\t{convert_annotation_details(annotation)}")
 
     # Build operation line
     op_line = "\top "
     type_name = operation.eType.name if operation.eType else "void"
     cardinality = get_cardinality_string(operation)
@@ -247,11 +246,11 @@
     # Ensure output directory exists
     os.makedirs(output_directory, exist_ok=True)
 
     # Write XCore file
     output_path = os.path.join(output_directory, f"{elpackage.name}.xcore")
-    with open(output_path, 'w', encoding='utf-8') as f:
+    with open(output_path, "w", encoding="utf-8") as f:
         f.write(xcore_content)
 
     print(f"Generated XCore file: {output_path}")
 
 
@@ -266,11 +265,11 @@
     packages = [
         context.types_package,
         context.ldm_domains_package,
         context.ldm_entities_package,
         context.il_domains_package,
-        context.il_tables_package
+        context.il_tables_package,
     ]
 
     for package in packages:
         if package and package.eClassifiers:  # Only process if package has content
             save_xcore_file(package, output_directory)
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/sqldeveloper_import/convert_regdna_to_xcore.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/report_filters/create_report_filters.py	2025-09-15 13:18:11.386257+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/report_filters/create_report_filters.py	2025-09-21 17:07:36.834848+00:00
@@ -14,10 +14,11 @@
 from pybirdai.models.bird_meta_data_model import *
 import os
 import csv
 from uuid import uuid4
 
+
 class CreateReportFilters:
     def create_report_filters(self, context, sdd_context, framework, version):
         """
         Create report filters based on the given context, SDD context, framework, and version.
 
@@ -37,20 +38,24 @@
         self.combination_items_to_create = []
         self.cube_structure_items_to_create = []
         self.cube_to_combinations_to_create = []
 
         for cell_id, tuples in cell_to_variable_member_tuple_map.items():
-            CreateReportFilters.process_cell(self,cell_id, tuples, sdd_context, context, framework, version)
+            CreateReportFilters.process_cell(self, cell_id, tuples, sdd_context, context, framework, version)
 
         # Bulk create all collected objects at the end
         if context.save_derived_sdd_items:
             COMBINATION.objects.bulk_create(self.combinations_to_create, batch_size=5000, ignore_conflicts=True)
-            COMBINATION_ITEM.objects.bulk_create(self.combination_items_to_create, batch_size=5000, ignore_conflicts=True)
+            COMBINATION_ITEM.objects.bulk_create(
+                self.combination_items_to_create, batch_size=5000, ignore_conflicts=True
+            )
             for item in self.cube_structure_items_to_create:
-               item.save()
-            #CUBE_STRUCTURE_ITEM.objects.bulk_create(self.cube_structure_items_to_create, batch_size=5000, ignore_conflicts=True)
-            CUBE_TO_COMBINATION.objects.bulk_create(self.cube_to_combinations_to_create, batch_size=5000, ignore_conflicts=True)
+                item.save()
+            # CUBE_STRUCTURE_ITEM.objects.bulk_create(self.cube_structure_items_to_create, batch_size=5000, ignore_conflicts=True)
+            CUBE_TO_COMBINATION.objects.bulk_create(
+                self.cube_to_combinations_to_create, batch_size=5000, ignore_conflicts=True
+            )
 
     def read_in_scope_reports(file_location):
         """
         Read in-scope reports from a CSV file.
 
@@ -58,11 +63,11 @@
             file_location (str): The path to the CSV file.
 
         Returns:
             list: A list of report templates from the CSV file.
         """
-        with open(file_location, encoding='utf-8') as csvfile:
+        with open(file_location, encoding="utf-8") as csvfile:
             return [row[0] for row in csv.reader(csvfile) if row and row[0] != "report_template"]
 
     def create_cell_to_variable_member_map(sdd_context):
         """
         Create a mapping of cell IDs to variable-member tuples.
@@ -98,11 +103,11 @@
             if tuples:
                 cell_to_variable_member_tuple_map[cell_id] = tuples
 
         return cell_to_variable_member_tuple_map
 
-    def process_cell(self,cell_id, tuples, sdd_context, context, framework, version):
+    def process_cell(self, cell_id, tuples, sdd_context, context, framework, version):
         """
         Process a single cell, creating combinations and filters.
 
         Args:
             cell_id: The ID of the cell being processed.
@@ -119,20 +124,26 @@
             return
 
         cube_mapping_id = CreateReportFilters.get_report_cube_mapping_id_for_table_id(cell.table_id.table_id, framework)
         relevant_mappings = sdd_context.mapping_to_cube_dictionary.get(cube_mapping_id, [])
 
-        report_rol_cube = CreateReportFilters.get_rol_cube_for_table_id(Utils.make_valid_id(cell.table_id.table_id), sdd_context, framework, version)
+        report_rol_cube = CreateReportFilters.get_rol_cube_for_table_id(
+            Utils.make_valid_id(cell.table_id.table_id), sdd_context, framework, version
+        )
         if not report_rol_cube:
-            #print(f"Could not find report for {cell.table_id.table_id}")
+            # print(f"Could not find report for {cell.table_id.table_id}")
             pass
             return
         combination_id = cell.table_cell_combination_id
-        if combination_id and not(combination_id == ''):
-            CreateReportFilters.create_combination_and_filters(self,combination_id, tuples, relevant_mappings, report_rol_cube, sdd_context, context)
-
-    def create_combination_and_filters(self,table_cell_combination_id, tuples, relevant_mappings, report_rol_cube, sdd_context, context):
+        if combination_id and not (combination_id == ""):
+            CreateReportFilters.create_combination_and_filters(
+                self, combination_id, tuples, relevant_mappings, report_rol_cube, sdd_context, context
+            )
+
+    def create_combination_and_filters(
+        self, table_cell_combination_id, tuples, relevant_mappings, report_rol_cube, sdd_context, context
+    ):
         """
         Create a combination and associated filters for a given cell.
 
         Args:
             cell_id: The ID of the cell.
@@ -141,26 +152,27 @@
             report_rol_cube: The ROL cube for the report.
             sdd_context: The SDD context object.
             context: The context object.
         """
 
-
         qualified_combination_id = report_rol_cube.cube_id + "_" + table_cell_combination_id
         report_cell = COMBINATION(combination_id=qualified_combination_id)
         metric = CreateReportFilters.get_metric(sdd_context, tuples, relevant_mappings)
         if metric:
-            CreateReportFilters.add_variable_to_rol_cube(self,context, sdd_context, report_rol_cube, metric)
+            CreateReportFilters.add_variable_to_rol_cube(self, context, sdd_context, report_rol_cube, metric)
         report_cell.metric = metric
-        if not(qualified_combination_id in sdd_context.combination_dictionary.keys()):
+        if not (qualified_combination_id in sdd_context.combination_dictionary.keys()):
             sdd_context.combination_dictionary[qualified_combination_id] = report_cell
             if context.save_derived_sdd_items:
                 self.combinations_to_create.append(report_cell)  # Changed from save() to append
 
-        CreateReportFilters.create_cube_to_combination(self,report_cell, report_rol_cube, sdd_context, context)
-        CreateReportFilters.create_filters(self, report_cell, tuples, relevant_mappings, report_rol_cube, sdd_context, context)
-
-    def add_variable_to_rol_cube(self,context, sdd_context, report_rol_cube, metric):
+        CreateReportFilters.create_cube_to_combination(self, report_cell, report_rol_cube, sdd_context, context)
+        CreateReportFilters.create_filters(
+            self, report_cell, tuples, relevant_mappings, report_rol_cube, sdd_context, context
+        )
+
+    def add_variable_to_rol_cube(self, context, sdd_context, report_rol_cube, metric):
         """
         Add a variable to the ROL cube if it doesn't already exist.
 
         Args:
             sdd_context: The SDD context object.
@@ -184,11 +196,11 @@
                 self.cube_structure_items_to_create.append(csi)  # Changed from save() to append
             sdd_context.bird_cube_structure_item_dictionary.setdefault(
                 report_rol_cube.cube_structure_id.cube_structure_id, []
             ).append(csi)
 
-    def get_metric( sdd_context, tuples, relevant_mappings):
+    def get_metric(sdd_context, tuples, relevant_mappings):
         """
         Get the metric (variable) based on the given tuples and mappings.
 
         Args:
             sdd_context: The SDD context object.
@@ -202,21 +214,26 @@
 
         for var_id, member_id in tuples:
             if member_id is None:
                 if var_id is not None:
                     try:
-                        dpm_var_id = var_id.variable_id.replace('EBA_', 'DPM_')
+                        dpm_var_id = var_id.variable_id.replace("EBA_", "DPM_")
                         variable_mapping_items = var_mapping_dict[dpm_var_id]
                         # Use next() with generator expression for early exit
-                        return next((item.variable_id for item in variable_mapping_items
-
-                                if ((item.is_source == 'false') or (item.is_source == 'False'))), None)
+                        return next(
+                            (
+                                item.variable_id
+                                for item in variable_mapping_items
+                                if ((item.is_source == "false") or (item.is_source == "False"))
+                            ),
+                            None,
+                        )
                     except KeyError:
                         print(f"Could not find variable mapping for {var_id.variable_id}")
         return None
 
-    def create_filters( self, report_cell, tuples, relevant_mappings, report_rol_cube, sdd_context, context):
+    def create_filters(self, report_cell, tuples, relevant_mappings, report_rol_cube, sdd_context, context):
         """
         Create filters for a given report cell.
 
         Args:
             report_cell: The report cell object.
@@ -234,16 +251,18 @@
                 the_filter = COMBINATION_ITEM()
                 the_filter.combination_id = report_cell
                 the_filter.variable_id = ref_variable
                 the_filter.member_id = ref_member
                 the_filter.member_hierarchy = ref_member_hierarchy
-                var_string = 'None'
+                var_string = "None"
                 if ref_variable:
                     var_string = ref_variable.variable_id
-                    CreateReportFilters.add_variable_to_rol_cube(self,context, sdd_context, report_rol_cube, ref_variable)
-
-                member_string = 'None'
+                    CreateReportFilters.add_variable_to_rol_cube(
+                        self, context, sdd_context, report_rol_cube, ref_variable
+                    )
+
+                member_string = "None"
                 if ref_member:
                     member_string = ref_member.member_id
 
                 sdd_context.combination_item_dictionary.setdefault(report_cell.combination_id, []).append(the_filter)
                 if context.save_derived_sdd_items:
@@ -268,23 +287,27 @@
             member_mapping = mapping.mapping_id.member_mapping_id
             if not member_mapping:
                 continue
 
             member_mapping_item_row_dict = CreateReportFilters.create_member_mapping_item_row_dict(
-                sdd_context, member_mapping)
+                sdd_context, member_mapping
+            )
 
             for member_mapping_items in member_mapping_item_row_dict.values():
                 # Group items by is_source for faster processing
 
-                source_items = [(item.variable_id, item.member_id) for item in member_mapping_items
-                              if ((item.is_source.lower() == 'true') )]
+                source_items = [
+                    (item.variable_id, item.member_id)
+                    for item in member_mapping_items
+                    if ((item.is_source.lower() == "true"))
+                ]
                 # Check if all source items are in non_ref_tuple_list
                 if all(item in non_ref_set for item in source_items):
                     ref_tuple_list.extend(
                         (item.variable_id, item.member_id, item.member_hierarchy)
                         for item in member_mapping_items
-                        if ((item.is_source.lower() != 'true'))
+                        if ((item.is_source.lower() != "true"))
                     )
 
         return ref_tuple_list
 
     def create_member_mapping_item_row_dict(sdd_context, member_mapping):
@@ -300,11 +323,13 @@
         """
         member_mapping_item_row_dict = {}
         member_mapping_items = sdd_context.member_mapping_items_dictionary[member_mapping.member_mapping_id]
 
         for member_mapping_item in member_mapping_items:
-            member_mapping_item_row_dict.setdefault(member_mapping_item.member_mapping_row, []).append(member_mapping_item)
+            member_mapping_item_row_dict.setdefault(member_mapping_item.member_mapping_row, []).append(
+                member_mapping_item
+            )
 
         return member_mapping_item_row_dict
 
     def create_variable_mapping_row_dict(sdd_context, variable_mapping):
         """
@@ -323,24 +348,24 @@
         for variable_mapping_item in variable_mapping_items:
             variable_mapping_item_row_dict.setdefault(0, []).append(variable_mapping_item)
 
         return variable_mapping_item_row_dict
 
-    def get_report_cube_mapping_id_for_table_id( table_id, framework):
+    def get_report_cube_mapping_id_for_table_id(table_id, framework):
         """
         Get the report cube mapping ID for a given table ID and framework.
 
         Args:
             table_id (str): The ID of the table.
             framework (str): The framework being used.
 
         Returns:
             str: The report cube mapping ID.
         """
-        return 'M_' + table_id.replace(framework + '_', '')
-
-    def get_rol_cube_for_table_id( table_id, sdd_context, framework, version):
+        return "M_" + table_id.replace(framework + "_", "")
+
+    def get_rol_cube_for_table_id(table_id, sdd_context, framework, version):
         """
         Get the ROL cube for a given table ID.
 
         Args:
             table_id (str): The ID of the table.
@@ -350,16 +375,16 @@
 
         Returns:
             The ROL cube object or None if not found.
         """
         try:
-            key = table_id[11:len(table_id)]
+            key = table_id[11 : len(table_id)]
             return sdd_context.bird_cube_dictionary[key]
         except KeyError:
             return None
 
-    def create_cube_to_combination(self,report_cell, report_rol_cube, sdd_context, context):
+    def create_cube_to_combination(self, report_cell, report_rol_cube, sdd_context, context):
         """
         Create a cube-to-combination mapping.
 
         Args:
             report_cell: The report cell object.
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/report_filters/create_report_filters.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/report_filters/create_non_reference_output_layers.py	2025-08-02 18:37:08.457010+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/report_filters/create_non_reference_output_layers.py	2025-09-21 17:07:36.886374+00:00
@@ -16,10 +16,11 @@
 import re
 import os
 import csv
 from datetime import datetime
 import uuid
+
 
 class CreateNROutputLayers:
     def __init__(self):
         self.memoization = {}
         # Lists to collect objects for bulk creation
@@ -41,21 +42,21 @@
         Extract framework identifier from table ID.
         Table IDs have format: FRAMEWORK_VERSION_TABLECODE
         e.g., EBA_FINREP_3.0.0_F01.01 -> EBA_FINREP
         """
         # Split by underscore and take all parts except the last two (version and table code)
-        parts = table_id.split('_')[:2]
-        return '_'.join(parts)
+        parts = table_id.split("_")[:2]
+        return "_".join(parts)
 
     def extract_version_from_table_id(self, table_id):
         """
         Extract version from table ID.
         e.g., EBA_FINREP_3.0.0_F01.01 -> 3.0.0
         """
-        parts = table_id.split('_')
+        parts = table_id.split("_")
         for part in parts:
-            if '.' in part and any(c.isdigit() for c in part):
+            if "." in part and any(c.isdigit() for c in part):
                 return part
         return None
 
     def get_framework_object(self, framework_id):
         """
@@ -80,13 +81,13 @@
         """
         Query tables by framework name.
         """
         framework_upper = framework.upper()
         # Query using various patterns
-        tables = TABLE.objects.filter(
-            table_id__contains=framework_upper).filter(
-            table_id__contains="_"+version.replace(".","_"))
+        tables = TABLE.objects.filter(table_id__contains=framework_upper).filter(
+            table_id__contains="_" + version.replace(".", "_")
+        )
         return tables
 
     def get_tables_by_code(self, table_code):
         """
         Get a specific table by its code.
@@ -119,25 +120,23 @@
         """
         # Get all cells for the table
         cells = TABLE_CELL.objects.filter(table_id=table)
 
         # Get all cell positions for these cells
-        cell_positions = CELL_POSITION.objects.filter(
-            cell_id__in=cells
+        cell_positions = CELL_POSITION.objects.filter(cell_id__in=cells)
+
+        # Get unique axis ordinates from cell positions
+        axis_ordinate_ids = cell_positions.values_list("axis_ordinate_id", flat=True).distinct()
+
+        # Get all ordinate items for these axis ordinates
+        ordinate_items = ORDINATE_ITEM.objects.filter(axis_ordinate_id__in=axis_ordinate_ids).select_related(
+            "axis_ordinate_id", "variable_id", "member_id"
         )
 
-        # Get unique axis ordinates from cell positions
-        axis_ordinate_ids = cell_positions.values_list('axis_ordinate_id', flat=True).distinct()
-
-        # Get all ordinate items for these axis ordinates
-        ordinate_items = ORDINATE_ITEM.objects.filter(
-            axis_ordinate_id__in=axis_ordinate_ids
-        ).select_related('axis_ordinate_id', 'variable_id', 'member_id')
-
         return cells, cell_positions, ordinate_items
 
-    def create_output_layers(self, table: TABLE,framework_object:FRAMEWORK, save_to_db=True):
+    def create_output_layers(self, table: TABLE, framework_object: FRAMEWORK, save_to_db=True):
         """
         Main method to create output layers for a given table.
         """
         self.save_to_db = save_to_db
 
@@ -156,21 +155,17 @@
 
         # Fetch all necessary data
         cells, cell_positions, ordinate_items = self._fetch_objects_for_creation(table)
 
         # Create cube and cube structure
-        cube, cube_structure = self.create_cube_and_structure_from_rendering(table,framework_object)
+        cube, cube_structure = self.create_cube_and_structure_from_rendering(table, framework_object)
 
         # Create combinations and items from cells
-        self.create_combination_and_items_from_rendering(
-            cells, cell_positions, ordinate_items, cube
-        )
+        self.create_combination_and_items_from_rendering(cells, cell_positions, ordinate_items, cube)
 
         # Create cube structure items from all variables in the table
-        self.create_cube_structure_items_from_rendering(
-            ordinate_items, cube_structure
-        )
+        self.create_cube_structure_items_from_rendering(ordinate_items, cube_structure)
 
         # Bulk save all created objects if requested
         if save_to_db:
             self._bulk_save_objects()
 
@@ -188,11 +183,11 @@
         cube_name = table.table_id
         cube = CUBE()
         cube.cube_id = cube_name
         cube.name = table.name or cube_name
         cube.description = table.description
-        cube.cube_type = 'RC'  # Report Cube
+        cube.cube_type = "RC"  # Report Cube
         cube.framework_id = framework_object
 
         # Create cube structure
         cube_structure = CUBE_STRUCTURE()
         cube_structure.cube_structure_id = table.table_id
@@ -247,13 +242,11 @@
 
             # Collect all ordinate items for this cell
             cell_ordinate_items = []
             for position in positions:
                 if position.axis_ordinate_id_id in ordinate_to_items:
-                    cell_ordinate_items.extend(
-                        ordinate_to_items[position.axis_ordinate_id_id]
-                    )
+                    cell_ordinate_items.extend(ordinate_to_items[position.axis_ordinate_id_id])
 
             # Find the metric (variable without member)
             metric = None
             for item in cell_ordinate_items:
                 if item.variable_id and not item.member_id:
@@ -289,33 +282,25 @@
 
         for item in ordinate_items:
             if item.variable_id:
                 var_id = item.variable_id.variable_id
                 if var_id not in variable_to_members:
-                    variable_to_members[var_id] = {
-                        'variable': item.variable_id,
-                        'members': set(),
-                        'hierarchies': set()
-                    }
+                    variable_to_members[var_id] = {"variable": item.variable_id, "members": set(), "hierarchies": set()}
 
                 if item.member_id:
-                    variable_to_members[var_id]['members'].add(item.member_id)
+                    variable_to_members[var_id]["members"].add(item.member_id)
 
         # Create cube structure item for each unique variable
         for var_id, var_data in variable_to_members.items():
             csi = CUBE_STRUCTURE_ITEM()
             csi.cube_structure_id = cube_structure
-            csi.variable_id = var_data['variable']
+            csi.variable_id = var_data["variable"]
             csi.cube_variable_code = f"{cube_structure.cube_structure_id}__{var_id}"
 
             # Create subdomain from members if any exist
-            if var_data['members']:
-                subdomain = self._get_or_create_subdomain(
-                    var_data['variable'],
-                    var_data['members'],
-                    cube_structure
-                )
+            if var_data["members"]:
+                subdomain = self._get_or_create_subdomain(var_data["variable"], var_data["members"], cube_structure)
                 csi.subdomain_id = subdomain
                 csi.description = f"Variable with {len(var_data['members'])} members in subdomain"
             else:
                 csi.description = f"Variable without member restrictions"
 
@@ -327,14 +312,11 @@
         """
         # Create a unique subdomain ID based on variable and cube structure
         subdomain_id = f"{variable.variable_id}_OUTPUT_SD_{cube_structure.cube_structure_id}"
 
         # Check if subdomain already exists in our creation list
-        existing_subdomain = next(
-            (sd for sd in self.subdomains_to_create if sd.subdomain_id == subdomain_id),
-            None
-        )
+        existing_subdomain = next((sd for sd in self.subdomains_to_create if sd.subdomain_id == subdomain_id), None)
 
         if existing_subdomain:
             return existing_subdomain
 
         # Check if subdomain exists in database
@@ -351,11 +333,11 @@
         subdomain.code = subdomain_id
         subdomain.is_listed = True
         subdomain.description = f"Generated subdomain for output layer"
 
         # Get the variable's domain if it exists
-        if hasattr(variable, 'domain_id') and variable.domain_id:
+        if hasattr(variable, "domain_id") and variable.domain_id:
             subdomain.domain_id = variable.domain_id
 
         self.subdomains_to_create.append(subdomain)
 
         # Create subdomain enumeration entries for each member
@@ -374,55 +356,37 @@
         """
         Bulk save all created objects to the database.
         """
 
         if self.cube_structures_to_create:
-            CUBE_STRUCTURE.objects.bulk_create(
-                self.cube_structures_to_create, batch_size=1000, ignore_conflicts=True
-            )
+            CUBE_STRUCTURE.objects.bulk_create(self.cube_structures_to_create, batch_size=1000, ignore_conflicts=True)
 
         if self.cubes_to_create:
-            CUBE.objects.bulk_create(
-                self.cubes_to_create, batch_size=1000, ignore_conflicts=True
-            )
+            CUBE.objects.bulk_create(self.cubes_to_create, batch_size=1000, ignore_conflicts=True)
 
         if self.combinations_to_create:
-            COMBINATION.objects.bulk_create(
-                self.combinations_to_create, batch_size=5000
-            )
+            COMBINATION.objects.bulk_create(self.combinations_to_create, batch_size=5000)
 
         # Save subdomains first as they are referenced by other objects
         if self.subdomains_to_create:
-            SUBDOMAIN.objects.bulk_create(
-                self.subdomains_to_create, batch_size=1000
-            )
+            SUBDOMAIN.objects.bulk_create(self.subdomains_to_create, batch_size=1000)
 
         if self.subdomain_enumerations_to_create:
-            SUBDOMAIN_ENUMERATION.objects.bulk_create(
-                self.subdomain_enumerations_to_create, batch_size=5000
-            )
+            SUBDOMAIN_ENUMERATION.objects.bulk_create(self.subdomain_enumerations_to_create, batch_size=5000)
 
         if self.combination_items_to_create:
-            COMBINATION_ITEM.objects.bulk_create(
-                self.combination_items_to_create, batch_size=5000
-            )
+            COMBINATION_ITEM.objects.bulk_create(self.combination_items_to_create, batch_size=5000)
 
         if self.cube_structure_items_to_create:
-            CUBE_STRUCTURE_ITEM.objects.bulk_create(
-                self.cube_structure_items_to_create, batch_size=5000
-            )
+            CUBE_STRUCTURE_ITEM.objects.bulk_create(self.cube_structure_items_to_create, batch_size=5000)
 
         if self.cube_to_combinations_to_create:
-            CUBE_TO_COMBINATION.objects.bulk_create(
-                self.cube_to_combinations_to_create, batch_size=5000
-            )
+            CUBE_TO_COMBINATION.objects.bulk_create(self.cube_to_combinations_to_create, batch_size=5000)
 
         # Update cells with new combination IDs
         if self.cells_to_update:
-            TABLE_CELL.objects.bulk_update(
-                self.cells_to_update, ['table_cell_combination_id'], batch_size=5000
-            )
+            TABLE_CELL.objects.bulk_update(self.cells_to_update, ["table_cell_combination_id"], batch_size=5000)
 
     def process_by_framework_version(self, framework, version, save_to_db=True):
         """
         Process all tables for a specific framework version.
 
@@ -432,58 +396,51 @@
             save_to_db: Whether to save results to database
 
         Returns:
             dict: Processing results with status, processed tables, and errors
         """
-        results = {
-            'status': 'success',
-            'framework': framework,
-            'version': version,
-            'processed': [],
-            'errors': []
-        }
+        results = {"status": "success", "framework": framework, "version": version, "processed": [], "errors": []}
 
         try:
             # Get framework object
             framework_object = self.get_framework_object(framework)
             if not framework_object:
-                results['status'] = 'error'
-                results['message'] = f"Framework '{framework}' not found in database"
+                results["status"] = "error"
+                results["message"] = f"Framework '{framework}' not found in database"
                 return results
 
             # Get tables for this framework version
             tables = self.get_tables_by_framework_version(framework, version)
 
             if not tables:
-                results['status'] = 'warning'
-                results['message'] = f"No tables found for framework '{framework}' version '{version}'"
+                results["status"] = "warning"
+                results["message"] = f"No tables found for framework '{framework}' version '{version}'"
                 return results
 
             # Process each table
             for table in tables:
                 try:
                     cube, cube_structure = self.create_output_layers(table, framework_object, save_to_db)
-                    results['processed'].append({
-                        'table_id': table.table_id,
-                        'table_name': table.name,
-                        'cube_id': cube.cube_id,
-                        'cube_structure_id': cube_structure.cube_structure_id
-                    })
+                    results["processed"].append(
+                        {
+                            "table_id": table.table_id,
+                            "table_name": table.name,
+                            "cube_id": cube.cube_id,
+                            "cube_structure_id": cube_structure.cube_structure_id,
+                        }
+                    )
                 except Exception as e:
-                    results['errors'].append({
-                        'table_id': table.table_id,
-                        'error': str(e)
-                    })
-
-            if results['errors'] and not results['processed']:
-                results['status'] = 'error'
-            elif results['errors']:
-                results['status'] = 'partial'
+                    results["errors"].append({"table_id": table.table_id, "error": str(e)})
+
+            if results["errors"] and not results["processed"]:
+                results["status"] = "error"
+            elif results["errors"]:
+                results["status"] = "partial"
 
         except Exception as e:
-            results['status'] = 'error'
-            results['message'] = f"Error processing framework version: {str(e)}"
+            results["status"] = "error"
+            results["message"] = f"Error processing framework version: {str(e)}"
 
         return results
 
     def process_by_framework(self, framework, save_to_db=True):
         """
@@ -494,58 +451,52 @@
             save_to_db: Whether to save results to database
 
         Returns:
             dict: Processing results with status, processed tables, and errors
         """
-        results = {
-            'status': 'success',
-            'framework': framework,
-            'processed': [],
-            'errors': []
-        }
+        results = {"status": "success", "framework": framework, "processed": [], "errors": []}
 
         try:
             # Get framework object
             framework_object = self.get_framework_object(framework)
             if not framework_object:
-                results['status'] = 'error'
-                results['message'] = f"Framework '{framework}' not found in database"
+                results["status"] = "error"
+                results["message"] = f"Framework '{framework}' not found in database"
                 return results
 
             # Get all tables for this framework
             tables = self.get_tables_by_framework(framework)
 
             if not tables:
-                results['status'] = 'warning'
-                results['message'] = f"No tables found for framework '{framework}'"
+                results["status"] = "warning"
+                results["message"] = f"No tables found for framework '{framework}'"
                 return results
 
             # Process each table
             for table in tables:
                 try:
                     cube, cube_structure = self.create_output_layers(table, framework_object, save_to_db)
-                    results['processed'].append({
-                        'table_id': table.table_id,
-                        'table_name': table.name,
-                        'cube_id': cube.cube_id,
-                        'cube_structure_id': cube_structure.cube_structure_id,
-                        'version': self.extract_version_from_table_id(table.table_id)
-                    })
+                    results["processed"].append(
+                        {
+                            "table_id": table.table_id,
+                            "table_name": table.name,
+                            "cube_id": cube.cube_id,
+                            "cube_structure_id": cube_structure.cube_structure_id,
+                            "version": self.extract_version_from_table_id(table.table_id),
+                        }
+                    )
                 except Exception as e:
-                    results['errors'].append({
-                        'table_id': table.table_id,
-                        'error': str(e)
-                    })
-
-            if results['errors'] and not results['processed']:
-                results['status'] = 'error'
-            elif results['errors']:
-                results['status'] = 'partial'
+                    results["errors"].append({"table_id": table.table_id, "error": str(e)})
+
+            if results["errors"] and not results["processed"]:
+                results["status"] = "error"
+            elif results["errors"]:
+                results["status"] = "partial"
 
         except Exception as e:
-            results['status'] = 'error'
-            results['message'] = f"Error processing framework: {str(e)}"
+            results["status"] = "error"
+            results["message"] = f"Error processing framework: {str(e)}"
 
         return results
 
     def process_by_table_code_version(self, table_code, version, save_to_db=True):
         """
@@ -557,49 +508,43 @@
             save_to_db: Whether to save results to database
 
         Returns:
             dict: Processing results with status and table details
         """
-        results = {
-            'status': 'success',
-            'table_code': table_code,
-            'version': version,
-            'processed': None,
-            'error': None
-        }
+        results = {"status": "success", "table_code": table_code, "version": version, "processed": None, "error": None}
 
         try:
             # Get table by code and version
             table = self.get_table_by_code_version(table_code, version)
 
             if not table:
-                results['status'] = 'error'
-                results['error'] = f"Table with code '{table_code}' and version '{version}' not found"
+                results["status"] = "error"
+                results["error"] = f"Table with code '{table_code}' and version '{version}' not found"
                 return results
 
             # Extract framework from table ID
             framework_id = self.extract_framework_from_table_id(table.table_id)
             framework_object = self.get_framework_object(framework_id) if framework_id else None
 
             if not framework_object:
-                results['status'] = 'warning'
-                results['error'] = f"Framework not found for table '{table.table_id}', proceeding without framework"
+                results["status"] = "warning"
+                results["error"] = f"Framework not found for table '{table.table_id}', proceeding without framework"
 
             # Process the table
             cube, cube_structure = self.create_output_layers(table, framework_object, save_to_db)
 
-            results['processed'] = {
-                'table_id': table.table_id,
-                'table_name': table.name,
-                'cube_id': cube.cube_id,
-                'cube_structure_id': cube_structure.cube_structure_id,
-                'framework': framework_id
+            results["processed"] = {
+                "table_id": table.table_id,
+                "table_name": table.name,
+                "cube_id": cube.cube_id,
+                "cube_structure_id": cube_structure.cube_structure_id,
+                "framework": framework_id,
             }
 
         except Exception as e:
-            results['status'] = 'error'
-            results['error'] = str(e)
+            results["status"] = "error"
+            results["error"] = str(e)
 
         return results
 
     def process_by_table_code(self, table_code, save_to_db=True):
         """
@@ -610,65 +555,55 @@
             save_to_db: Whether to save results to database
 
         Returns:
             dict: Processing results with status, processed tables, and errors
         """
-        results = {
-            'status': 'success',
-            'table_code': table_code,
-            'processed': [],
-            'errors': []
-        }
+        results = {"status": "success", "table_code": table_code, "processed": [], "errors": []}
 
         try:
             # Get all tables with this code
-            tables = TABLE.objects.filter(
-                Q(table_id__endswith=f"_{table_code}") |
-                Q(name=table_code)
-            )
+            tables = TABLE.objects.filter(Q(table_id__endswith=f"_{table_code}") | Q(name=table_code))
 
             if not tables:
-                results['status'] = 'error'
-                results['message'] = f"No tables found with code '{table_code}'"
+                results["status"] = "error"
+                results["message"] = f"No tables found with code '{table_code}'"
                 return results
 
             # Process each table
             for table in tables:
                 try:
                     # Extract framework from table ID
                     framework_id = self.extract_framework_from_table_id(table.table_id)
                     framework_object = self.get_framework_object(framework_id) if framework_id else None
 
                     if not framework_object:
-                        results['errors'].append({
-                            'table_id': table.table_id,
-                            'error': f"Framework not found for table, skipping"
-                        })
+                        results["errors"].append(
+                            {"table_id": table.table_id, "error": f"Framework not found for table, skipping"}
+                        )
                         continue
 
                     cube, cube_structure = self.create_output_layers(table, framework_object, save_to_db)
 
-                    results['processed'].append({
-                        'table_id': table.table_id,
-                        'table_name': table.name,
-                        'cube_id': cube.cube_id,
-                        'cube_structure_id': cube_structure.cube_structure_id,
-                        'framework': framework_id,
-                        'version': self.extract_version_from_table_id(table.table_id)
-                    })
+                    results["processed"].append(
+                        {
+                            "table_id": table.table_id,
+                            "table_name": table.name,
+                            "cube_id": cube.cube_id,
+                            "cube_structure_id": cube_structure.cube_structure_id,
+                            "framework": framework_id,
+                            "version": self.extract_version_from_table_id(table.table_id),
+                        }
+                    )
 
                 except Exception as e:
-                    results['errors'].append({
-                        'table_id': table.table_id,
-                        'error': str(e)
-                    })
-
-            if results['errors'] and not results['processed']:
-                results['status'] = 'error'
-            elif results['errors']:
-                results['status'] = 'partial'
+                    results["errors"].append({"table_id": table.table_id, "error": str(e)})
+
+            if results["errors"] and not results["processed"]:
+                results["status"] = "error"
+            elif results["errors"]:
+                results["status"] = "partial"
 
         except Exception as e:
-            results['status'] = 'error'
-            results['message'] = f"Error processing table code: {str(e)}"
+            results["status"] = "error"
+            results["message"] = f"Error processing table code: {str(e)}"
 
         return results
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/report_filters/create_non_reference_output_layers.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/sqldeveloper_import/import_sqldev_ldm_to_django.py	2025-09-15 13:18:11.387462+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/sqldeveloper_import/import_sqldev_ldm_to_django.py	2025-09-21 17:07:36.921152+00:00
@@ -13,21 +13,23 @@
 import os
 
 from pybirdai.regdna import ELAttribute, ELClass, ELEnum
 from pybirdai.regdna import ELReference
 
+
 class RegDNAToDJango(object):
-    '''
+    """
     Documentation for SQLDevLDMImport
-    '''
-    def convert(self,context):
-        '''
+    """
+
+    def convert(self, context):
+        """
         Documentation for the method.
-        '''
-        #ensure the existing files are properly removed and recreated
-        models_path = context.output_directory + os.sep + 'database_configuration_files' + os.sep + 'models.py'
-        admin_path = context.output_directory + os.sep + 'database_configuration_files' + os.sep + 'admin.py'
+        """
+        # ensure the existing files are properly removed and recreated
+        models_path = context.output_directory + os.sep + "database_configuration_files" + os.sep + "models.py"
+        admin_path = context.output_directory + os.sep + "database_configuration_files" + os.sep + "admin.py"
 
         # Force deletion and recreation to avoid append-related duplicates
         try:
             os.remove(models_path)
         except (FileNotFoundError, PermissionError):
@@ -37,42 +39,44 @@
             os.remove(admin_path)
         except (FileNotFoundError, PermissionError):
             pass
 
         # Use write mode instead of append to ensure clean files
-        models_file = open(models_path, "w",  encoding='utf-8')
-        admin_file = open(admin_path, "w",  encoding='utf-8')
-        if context.ldm_or_il == 'ldm':
-            RegDNAToDJango.createDjangoForPackage(self,context.ldm_entities_package,models_file,context)
-            RegDNAToDJango.createDjangoAdminForPackage(self,context.ldm_entities_package,admin_file,context)
+        models_file = open(models_path, "w", encoding="utf-8")
+        admin_file = open(admin_path, "w", encoding="utf-8")
+        if context.ldm_or_il == "ldm":
+            RegDNAToDJango.createDjangoForPackage(self, context.ldm_entities_package, models_file, context)
+            RegDNAToDJango.createDjangoAdminForPackage(self, context.ldm_entities_package, admin_file, context)
         else:
-            RegDNAToDJango.createDjangoForPackage(self,context.il_tables_package,models_file,context)
-            RegDNAToDJango.createDjangoAdminForPackage(self,context.il_tables_package,admin_file,context)
+            RegDNAToDJango.createDjangoForPackage(self, context.il_tables_package, models_file, context)
+            RegDNAToDJango.createDjangoAdminForPackage(self, context.il_tables_package, admin_file, context)
 
     def djangoChoices(self, theEnum):
 
-        returnString =  theEnum.name + " = {"
-
-        for literal in  theEnum.eLiterals:
-            returnString  = returnString  + '\t\t' +"\""+ literal.literal + "\":\""+literal.name + "\",\n"
-
-        returnString  = returnString  + "}"
+        returnString = theEnum.name + " = {"
+
+        for literal in theEnum.eLiterals:
+            returnString = returnString + "\t\t" + '"' + literal.literal + '":"' + literal.name + '",\n'
+
+        returnString = returnString + "}"
         return returnString
 
     def createDjangoForPackage(self, elpackage, output_file, context):
-        '''
+        """
         Documentation for the method.
-        '''
-        output_file.write('from django.db import models\r\n')
+        """
+        output_file.write("from django.db import models\r\n")
 
         for theImport in elpackage.imports:
-            if not(theImport.importedNamespace.trim == "types.*"):
-                output_file.write('from ' + theImport.importedNamespace + ' import *\r\n')
+            if not (theImport.importedNamespace.trim == "types.*"):
+                output_file.write("from " + theImport.importedNamespace + " import *\r\n")
         class_names_written = []
         for elclass in elpackage.eClassifiers:
-            if  isinstance(elclass ,ELClass):
-                RegDNAToDJango.write_class_and_superclasses_in_correct_order(self, elclass, output_file, class_names_written)
+            if isinstance(elclass, ELClass):
+                RegDNAToDJango.write_class_and_superclasses_in_correct_order(
+                    self, elclass, output_file, class_names_written
+                )
 
         output_file.close()
 
     def write_class_and_superclasses_in_correct_order(self, elclass, output_file, classes_written):
         print(elclass.name)
@@ -83,67 +87,129 @@
                 try:
                     print(elclass.eSuperTypes[0].name)
                 except:
                     print("no superclass name")
                 if elclass.eSuperTypes[0].name not in classes_written:
-                    RegDNAToDJango.write_class_and_superclasses_in_correct_order(self, elclass.eSuperTypes[0], output_file, classes_written)
-                output_file.write('class ' + elclass.name + '(' + elclass.eSuperTypes[0].name + '):\r\n')
+                    RegDNAToDJango.write_class_and_superclasses_in_correct_order(
+                        self, elclass.eSuperTypes[0], output_file, classes_written
+                    )
+                output_file.write("class " + elclass.name + "(" + elclass.eSuperTypes[0].name + "):\r\n")
             else:
-                output_file.write('class ' + elclass.name + '(models.Model):\r\n')
-                output_file.write('\ttest_id = models.CharField("test_id",max_length=1000,default=None, blank=True, null=True)\r\n')
+                output_file.write("class " + elclass.name + "(models.Model):\r\n")
+                output_file.write(
+                    '\ttest_id = models.CharField("test_id",max_length=1000,default=None, blank=True, null=True)\r\n'
+                )
             for elmember in elclass.eStructuralFeatures:
-                if  isinstance(elmember ,ELAttribute):
+                if isinstance(elmember, ELAttribute):
                     if isinstance(elmember.eAttributeType, ELEnum):
-                        output_file.write('\t' + RegDNAToDJango.djangoChoices(self,elmember.eAttributeType) + '\r\n')
-                        output_file.write('\t' + elmember.name + ' = models.CharField("' + elmember.name + '",max_length=1000, choices=' + elmember.eAttributeType.name +',default=None, blank=True, null=True, db_comment="' + elmember.eAttributeType.name +'")\r\n')
+                        output_file.write("\t" + RegDNAToDJango.djangoChoices(self, elmember.eAttributeType) + "\r\n")
+                        output_file.write(
+                            "\t"
+                            + elmember.name
+                            + ' = models.CharField("'
+                            + elmember.name
+                            + '",max_length=1000, choices='
+                            + elmember.eAttributeType.name
+                            + ',default=None, blank=True, null=True, db_comment="'
+                            + elmember.eAttributeType.name
+                            + '")\r\n'
+                        )
                     elif (elmember.eAttributeType.name == "String") and elmember.iD:
-                        output_file.write('\t' + elmember.name + ' = models.CharField("' + elmember.name + '",max_length=1000, primary_key=True)\r\n')
+                        output_file.write(
+                            "\t"
+                            + elmember.name
+                            + ' = models.CharField("'
+                            + elmember.name
+                            + '",max_length=1000, primary_key=True)\r\n'
+                        )
                     elif elmember.eAttributeType.name == "String":
-                        output_file.write('\t' + elmember.name + ' = models.CharField("' + elmember.name + '",max_length=1000,default=None, blank=True, null=True)\r\n')
+                        output_file.write(
+                            "\t"
+                            + elmember.name
+                            + ' = models.CharField("'
+                            + elmember.name
+                            + '",max_length=1000,default=None, blank=True, null=True)\r\n'
+                        )
                     elif elmember.eAttributeType.name == "double":
-                        output_file.write('\t' + elmember.name + ' = models.FloatField("' + elmember.name + '",default=None, blank=True, null=True)\r\n')
+                        output_file.write(
+                            "\t"
+                            + elmember.name
+                            + ' = models.FloatField("'
+                            + elmember.name
+                            + '",default=None, blank=True, null=True)\r\n'
+                        )
                     elif elmember.eAttributeType.name == "int":
-                        output_file.write('\t' + elmember.name + ' = models.BigIntegerField("' + elmember.name + '",default=None, blank=True, null=True)\r\n')
+                        output_file.write(
+                            "\t"
+                            + elmember.name
+                            + ' = models.BigIntegerField("'
+                            + elmember.name
+                            + '",default=None, blank=True, null=True)\r\n'
+                        )
                     elif elmember.eAttributeType.name == "Date":
-                        output_file.write('\t' + elmember.name + ' = models.DateTimeField("' + elmember.name + '",default=None, blank=True, null=True)\r\n')
+                        output_file.write(
+                            "\t"
+                            + elmember.name
+                            + ' = models.DateTimeField("'
+                            + elmember.name
+                            + '",default=None, blank=True, null=True)\r\n'
+                        )
                     elif elmember.eAttributeType.name == "boolean":
-                        output_file.write('\t' + elmember.name + ' = models.BooleanField("' + elmember.name + '",default=None, blank=True, null=True)\r\n')
+                        output_file.write(
+                            "\t"
+                            + elmember.name
+                            + ' = models.BooleanField("'
+                            + elmember.name
+                            + '",default=None, blank=True, null=True)\r\n'
+                        )
                 if isinstance(elmember, ELReference):
                     # only create a foreign key if the upper bound is 1, not that n to 1 relationships have
                     # a refernce on both sides of the relationship, we only show the one with cardiantlity of 1.
                     if elmember.upperBound == 1:
-                        output_file.write('\t' + elmember.name + ' = models.ForeignKey("' + elmember.eType.name + '", models.SET_NULL,blank=True,null=True,related_name="' + elclass.name + '_to_' + elmember.name + 's")\r\n')
+                        output_file.write(
+                            "\t"
+                            + elmember.name
+                            + ' = models.ForeignKey("'
+                            + elmember.eType.name
+                            + '", models.SET_NULL,blank=True,null=True,related_name="'
+                            + elclass.name
+                            + "_to_"
+                            + elmember.name
+                            + 's")\r\n'
+                        )
                     else:
                         if elmember.eOpposite is not None:
                             pass
                         else:
-                            print("asssociation with cardinality of N does not have an opposite relationship:" + elmember.name)
+                            print(
+                                "asssociation with cardinality of N does not have an opposite relationship:"
+                                + elmember.name
+                            )
 
             long_name_exists = False
             for annotion in elclass.eAnnotations:
                 if annotion.source is not None:
                     if annotion.source.name == "long_name":
-                        output_file.write('\t' + 'class Meta:\r\n')
-                        output_file.write('\t\t' + 'verbose_name = \'' + annotion.details[0].value + '\'\r\n')
-                        output_file.write('\t\t' + 'verbose_name_plural = \'' + annotion.details[0].value + 's\'\r\n')
+                        output_file.write("\t" + "class Meta:\r\n")
+                        output_file.write("\t\t" + "verbose_name = '" + annotion.details[0].value + "'\r\n")
+                        output_file.write("\t\t" + "verbose_name_plural = '" + annotion.details[0].value + "s'\r\n")
                         long_name_exists = True
                 else:
                     print("no source for annotation" + elclass.name)
 
-
             if not long_name_exists:
-                output_file.write('\t' + 'class Meta:\r\n')
-                output_file.write('\t\t' + 'verbose_name = \'' + elclass.name + '\'\r\n')
-                output_file.write('\t\t' + 'verbose_name_plural = \'' + elclass.name + 's\'\r\n')
+                output_file.write("\t" + "class Meta:\r\n")
+                output_file.write("\t\t" + "verbose_name = '" + elclass.name + "'\r\n")
+                output_file.write("\t\t" + "verbose_name_plural = '" + elclass.name + "s'\r\n")
 
             classes_written.append(elclass.name)
 
     def createDjangoAdminForPackage(self, elpackage, output_file, context):
-        '''
+        """
         Documentation for the method.
-        '''
-        output_file.write('from django.contrib import admin\r\n')
+        """
+        output_file.write("from django.contrib import admin\r\n")
         for elclass in elpackage.eClassifiers:
-            if  isinstance(elclass ,ELClass):
-                output_file.write('from .models.bird_data_model import ' + elclass.name + '\n')
-                output_file.write('admin.site.register(' + elclass.name + ')\n')
+            if isinstance(elclass, ELClass):
+                output_file.write("from .models.bird_data_model import " + elclass.name + "\n")
+                output_file.write("admin.site.register(" + elclass.name + ")\n")
         output_file.close()
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/sqldeveloper_import/import_sqldev_ldm_to_django.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/upload_files/file_uploader.py	2025-09-16 19:34:24.821473+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/upload_files/file_uploader.py	2025-09-21 17:07:36.985077+00:00
@@ -14,19 +14,18 @@
 import os
 import re
 import uuid
 from pathlib import Path
 
+
 class FileUploader:
     """
     A class for secure file upload handling with path traversal protection.
     """
 
     # Define allowed file extensions for security
-    ALLOWED_EXTENSIONS = {
-        '.csv', '.xml', '.xsd', '.txt', '.json', '.dmd'
-    }
+    ALLOWED_EXTENSIONS = {".csv", ".xml", ".xsd", ".txt", ".json", ".dmd"}
 
     # Maximum file size (50MB)
     MAX_FILE_SIZE = 50 * 1024 * 1024
 
     def _sanitize_filename(self, filename):
@@ -44,14 +43,14 @@
 
         # Get just the filename part (remove any path components)
         safe_name = os.path.basename(filename)
 
         # Remove path traversal sequences
-        safe_name = safe_name.replace('..', '').replace('/', '').replace('\\', '')
+        safe_name = safe_name.replace("..", "").replace("/", "").replace("\\", "")
 
         # Remove or replace dangerous characters
-        safe_name = re.sub(r'[<>:"|?*\x00-\x1f]', '', safe_name)
+        safe_name = re.sub(r'[<>:"|?*\x00-\x1f]', "", safe_name)
 
         # Ensure filename is not empty and has reasonable length
         if not safe_name or len(safe_name) > 255:
             return None
 
@@ -94,13 +93,11 @@
             return abs_directory
         except (OSError, ValueError):
             return None
 
     # Define allowed file extensions for security
-    ALLOWED_EXTENSIONS = {
-        '.csv', '.xml', '.xsd', '.txt', '.json', '.dmd'
-    }
+    ALLOWED_EXTENSIONS = {".csv", ".xml", ".xsd", ".txt", ".json", ".dmd"}
 
     # Maximum file size (50MB)
     MAX_FILE_SIZE = 50 * 1024 * 1024
 
     def _sanitize_filename(self, filename):
@@ -118,14 +115,14 @@
 
         # Get just the filename part (remove any path components)
         safe_name = os.path.basename(filename)
 
         # Remove path traversal sequences
-        safe_name = safe_name.replace('..', '').replace('/', '').replace('\\', '')
+        safe_name = safe_name.replace("..", "").replace("/", "").replace("\\", "")
 
         # Remove or replace dangerous characters
-        safe_name = re.sub(r'[<>:"|?*\x00-\x1f]', '', safe_name)
+        safe_name = re.sub(r'[<>:"|?*\x00-\x1f]', "", safe_name)
 
         # Ensure filename is not empty and has reasonable length
         if not safe_name or len(safe_name) > 255:
             return None
 
@@ -180,206 +177,156 @@
         Returns:
             dict: Status of the upload operation
         """
         print("Uploading SQLDeveloper EIL files")
 
-
         if not request or not request.FILES:
-            return {
-                'status': 'error',
-                'message': 'No files were uploaded'
-            }
+            return {"status": "error", "message": "No files were uploaded"}
 
         uploaded_files = []
         resource_directory = sdd_context.file_directory
-        eil_directory = os.path.join(resource_directory, 'il')
+        eil_directory = os.path.join(resource_directory, "il")
         # delete all files in the directory
         for file in os.listdir(eil_directory):
             os.remove(os.path.join(eil_directory, file))
 
-        for file in request.FILES.getlist('eil_files'):
+        for file in request.FILES.getlist("eil_files"):
             try:
                 # Secure file upload with validation
                 file_info = self._save_file(file, eil_directory)
-                uploaded_files.append({
-                    'original_name': file_info['original_name'],
-                    'safe_name': file_info['safe_name'],
-                    'path': file_info['path'],
-                    'size': file_info['size']
-                })
-
-            except ValueError as e:
-                return {
-                    'status': 'error',
-                    'message': f'Security error uploading file {file.name}: {str(e)}'
-                }
+                uploaded_files.append(
+                    {
+                        "original_name": file_info["original_name"],
+                        "safe_name": file_info["safe_name"],
+                        "path": file_info["path"],
+                        "size": file_info["size"],
+                    }
+                )
+
+            except ValueError as e:
+                return {"status": "error", "message": f"Security error uploading file {file.name}: {str(e)}"}
 
             except Exception as e:
-                return {
-                    'status': 'error',
-                    'message': f'Error uploading file {file.name}: {str(e)}'
-                }
-
-        return {
-            'status': 'success',
-            'files': uploaded_files
-        }
+                return {"status": "error", "message": f"Error uploading file {file.name}: {str(e)}"}
+
+        return {"status": "success", "files": uploaded_files}
 
     def upload_sqldev_eldm_files(self, sdd_context, request=None):
         """
         Handle the upload of SQLDeveloper ELDM files.
         """
         print("Uploading SQLDeveloper ELDM files")
 
-
         if not request or not request.FILES:
-            return {
-                'status': 'error',
-                'message': 'No files were uploaded'
-            }
+            return {"status": "error", "message": "No files were uploaded"}
 
         uploaded_files = []
         resource_directory = sdd_context.file_directory
-        eldm_directory = os.path.join(resource_directory, 'ldm')
+        eldm_directory = os.path.join(resource_directory, "ldm")
         # delete all files in the directory
         for file in os.listdir(eldm_directory):
             os.remove(os.path.join(eldm_directory, file))
 
-        for file in request.FILES.getlist('eldm_files'):
+        for file in request.FILES.getlist("eldm_files"):
             try:
                 # Secure file upload with validation
                 file_info = self._save_file(file, eldm_directory)
-                uploaded_files.append({
-                    'original_name': file_info['original_name'],
-                    'safe_name': file_info['safe_name'],
-                    'path': file_info['path'],
-                    'size': file_info['size']
-                })
-
-            except ValueError as e:
-                return {
-                    'status': 'error',
-                    'message': f'Security error uploading file {file.name}: {str(e)}'
-                }
-
-            except ValueError as e:
-                return {
-                    'status': 'error',
-                    'message': f'Security error uploading file {file.name}: {str(e)}'
-                }
+                uploaded_files.append(
+                    {
+                        "original_name": file_info["original_name"],
+                        "safe_name": file_info["safe_name"],
+                        "path": file_info["path"],
+                        "size": file_info["size"],
+                    }
+                )
+
+            except ValueError as e:
+                return {"status": "error", "message": f"Security error uploading file {file.name}: {str(e)}"}
+
+            except ValueError as e:
+                return {"status": "error", "message": f"Security error uploading file {file.name}: {str(e)}"}
             except Exception as e:
-                return {
-                    'status': 'error',
-                    'message': f'Error uploading file {file.name}: {str(e)}'
-                }
-
-        return {
-            'status': 'success',
-            'files': uploaded_files
-        }
+                return {"status": "error", "message": f"Error uploading file {file.name}: {str(e)}"}
+
+        return {"status": "success", "files": uploaded_files}
 
     def upload_technical_export_files(self, sdd_context, request=None):
         """
         Handle the upload of Technical Export files.
         """
         print("Uploading Technical Export files")
 
-
         if not request or not request.FILES:
-            return {
-                'status': 'error',
-                'message': 'No files were uploaded'
-            }
+            return {"status": "error", "message": "No files were uploaded"}
 
         uploaded_files = []
         resource_directory = sdd_context.file_directory
-        technical_export_directory = os.path.join(resource_directory, 'technical_export')
+        technical_export_directory = os.path.join(resource_directory, "technical_export")
 
         # delete all files in the directory
         for file in os.listdir(technical_export_directory):
             os.remove(os.path.join(technical_export_directory, file))
 
-        for file in request.FILES.getlist('technical_export_files'):
+        for file in request.FILES.getlist("technical_export_files"):
             try:
                 # Secure file upload with validation
                 file_info = self._save_file(file, technical_export_directory)
-                uploaded_files.append({
-                    'original_name': file_info['original_name'],
-                    'safe_name': file_info['safe_name'],
-                    'path': file_info['path'],
-                    'size': file_info['size']
-                })
-
-            except ValueError as e:
-                return {
-                    'status': 'error',
-                    'message': f'Security error uploading file {file.name}: {str(e)}'
-                }
-
-            except ValueError as e:
-                return {
-                    'status': 'error',
-                    'message': f'Security error uploading file {file.name}: {str(e)}'
-                }
+                uploaded_files.append(
+                    {
+                        "original_name": file_info["original_name"],
+                        "safe_name": file_info["safe_name"],
+                        "path": file_info["path"],
+                        "size": file_info["size"],
+                    }
+                )
+
+            except ValueError as e:
+                return {"status": "error", "message": f"Security error uploading file {file.name}: {str(e)}"}
+
+            except ValueError as e:
+                return {"status": "error", "message": f"Security error uploading file {file.name}: {str(e)}"}
             except Exception as e:
-                return {
-                    'status': 'error',
-                    'message': f'Error uploading file {file.name}: {str(e)}'
-                }
-
-        return {
-            'status': 'success',
-            'files': uploaded_files
-        }
+                return {"status": "error", "message": f"Error uploading file {file.name}: {str(e)}"}
+
+        return {"status": "success", "files": uploaded_files}
 
     def upload_joins_configuration(self, sdd_context, request=None):
         """
         Handle the upload of Joins Configuration files.
         """
         print("Uploading Joins Configuration files")
 
-
         if not request or not request.FILES:
-            return {
-                'status': 'error',
-                'message': 'No files were uploaded'
-            }
+            return {"status": "error", "message": "No files were uploaded"}
 
         uploaded_files = []
         resource_directory = sdd_context.file_directory
-        joins_configuration_directory = os.path.join(resource_directory, 'joins_configuration')
+        joins_configuration_directory = os.path.join(resource_directory, "joins_configuration")
 
         # delete all files in the directory
         for file in os.listdir(joins_configuration_directory):
             os.remove(os.path.join(joins_configuration_directory, file))
 
-        for file in request.FILES.getlist('joins_configuration_files'):
+        for file in request.FILES.getlist("joins_configuration_files"):
             try:
                 # Secure file upload with validation
                 file_info = self._save_file(file, joins_configuration_directory)
-                uploaded_files.append({
-                    'original_name': file_info['original_name'],
-                    'safe_name': file_info['safe_name'],
-                    'path': file_info['path'],
-                    'size': file_info['size']
-                })
-
-            except ValueError as e:
-                return {
-                    'status': 'error',
-                    'message': f'Security error uploading file {file.name}: {str(e)}'
-                }
+                uploaded_files.append(
+                    {
+                        "original_name": file_info["original_name"],
+                        "safe_name": file_info["safe_name"],
+                        "path": file_info["path"],
+                        "size": file_info["size"],
+                    }
+                )
+
+            except ValueError as e:
+                return {"status": "error", "message": f"Security error uploading file {file.name}: {str(e)}"}
             except Exception as e:
-                return {
-                    'status': 'error',
-                    'message': f'Error uploading file {file.name}: {str(e)}'
-                }
-
-        return {
-            'status': 'success',
-            'files': uploaded_files
-        }
+                return {"status": "error", "message": f"Error uploading file {file.name}: {str(e)}"}
+
+        return {"status": "success", "files": uploaded_files}
 
     def _save_file(self, file, directory):
         """
         Secure method to save the uploaded file with path traversal protection.
 
@@ -443,17 +390,12 @@
         if not os.path.abspath(file_path).startswith(os.path.abspath(safe_directory)):
             raise ValueError("Path traversal attempt detected")
 
         # Save the file using chunks to handle large files efficiently
         try:
-            with open(file_path, 'wb+') as destination:
+            with open(file_path, "wb+") as destination:
                 for chunk in file.chunks():
                     destination.write(chunk)
 
-            return {
-                'original_name': file.name,
-                'safe_name': safe_filename,
-                'path': file_path,
-                'size': file.size
-            }
+            return {"original_name": file.name, "safe_name": safe_filename, "path": file_path, "size": file.size}
         except IOError as e:
             raise ValueError(f"Failed to save file: {str(e)}")
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/upload_files/file_uploader.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/sqldeveloper_import/import_sqldev_il_to_regdna.py	2025-09-15 13:18:11.387108+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/sqldeveloper_import/import_sqldev_il_to_regdna.py	2025-09-21 17:07:37.041272+00:00
@@ -21,58 +21,57 @@
 # SPDE-License-Identifier: EPL-2.0
 #
 # Contributors:
 #    Neil Mackenzie - initial API and implementation
 #
-'''
+"""
 Created on 22 Jan 2022
 
 @author: Neil
-'''
+"""
 
 import os
 import csv
 from pybirdai.utils.utils import Utils
 
 from pybirdai.regdna import ELAttribute, ELClass, ELEnum
 from pybirdai.regdna import ELEnumLiteral, ELReference
 
 
-
 class SQLDeveloperILImport(object):
-    '''
+    """
     Documentation for SQLDeveloperILImport
-    '''
+    """
 
     def do_import(self, context):
-        '''
+        """
         import the items from the BIRD LDM csv files
-        '''
+        """
         SQLDeveloperILImport.add_il_classes_to_package(self, context)
         SQLDeveloperILImport.add_il_enums_to_package(self, context)
         SQLDeveloperILImport.add_il_literals_to_enums(self, context)
         SQLDeveloperILImport.create_il_types_map(self, context)
         SQLDeveloperILImport.add_il_pk_attributes_to_classes(self, context)
         SQLDeveloperILImport.add_il_attributes_to_classes(self, context)
         SQLDeveloperILImport.create_fk_to_column_map(self, context)
         SQLDeveloperILImport.add_il_relationships_between_classes(self, context)
 
     def add_il_classes_to_package(self, context):
-        '''
+        """
         for each entity in the IL, create a class and add it to the package
-        '''
+        """
 
         file_location = context.file_directory + os.sep + "il" + os.sep + "DM_Tables.csv"
 
         header_skipped = False
         # Load all the entities from the csv file, make an ELClass per entity,
         # and add the ELClass to the package
-        with open(file_location,  encoding='utf-8') as csvfile:
-            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
+        with open(file_location, encoding="utf-8") as csvfile:
+            filereader = csv.reader(csvfile, delimiter=",", quotechar='"')
             for row in filereader:
                 # skip the first line which is the header.
-                if (not header_skipped):
+                if not header_skipped:
                     header_skipped = True
                 else:
 
                     class_name = row[0]
                     object_id = row[1]
@@ -88,84 +87,76 @@
                     # this means that this class is a root
                     # of a type heirarchy....we will set such classes
                     # to be abstract.
 
                     containment_reference = ELReference()
-                    containment_reference.name = eclass.name+"s"
+                    containment_reference.name = eclass.name + "s"
                     containment_reference.eType = eclass
                     containment_reference.upperBound = -1
                     containment_reference.lowerBound = 0
                     containment_reference.containment = True
 
-
-
-                    context.il_tables_package.eClassifiers.extend([
-                                                                        eclass])
-
+                    context.il_tables_package.eClassifiers.extend([eclass])
 
                     # maintain a map a objectIDs to ELClasses, we add to
                     # the map even in input layters come from the websiute,
                     # this is because we want to find the table rrealtionship
                     # info that is not currently on the website, and use that to
                     # enrich the website content
                     context.classes_map[object_id] = eclass
 
-
     def add_il_enums_to_package(self, context):
-        '''
+        """
         for each domain in the IL add an enum to the package
-        '''
+        """
         file_location = context.file_directory + os.sep + "il" + os.sep + "DM_Domains.csv"
         header_skipped = False
         counter = 0
         # Create an ELEnum for each domain, and add it to the ELPackage
-        with open(file_location,  encoding='utf-8') as csvfile:
-            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
-            for row in filereader:
-                if (not header_skipped):
-                    header_skipped = True
-                else:
-                    counter = counter+1
+        with open(file_location, encoding="utf-8") as csvfile:
+            filereader = csv.reader(csvfile, delimiter=",", quotechar='"')
+            for row in filereader:
+                if not header_skipped:
+                    header_skipped = True
+                else:
+                    counter = counter + 1
                     enum_id = row[0]
                     enum_name = row[1]
                     synonym = row[3]
                     adapted_enum_name = Utils.make_valid_id(synonym)
                     the_enum = ELEnum()
                     the_enum.name = adapted_enum_name + "_domain"
                     # maintain a map of enum IDS to ELEnum objects
                     context.enum_map[enum_id] = the_enum
-                    context.il_domains_package.eClassifiers.extend([
-                                                                        the_enum])
+                    context.il_domains_package.eClassifiers.extend([the_enum])
 
     def add_il_literals_to_enums(self, context):
-        '''
+        """
         for each memebr of a domain the IL, add a literal to the corresponding enum
-        '''
+        """
         file_location = context.file_directory + os.sep + "il" + os.sep + "DM_Domain_AVT.csv"
         header_skipped = False
         counter = 0
         # Add the members of a domain as literals of the related Enum
-        with open(file_location,  encoding='utf-8') as csvfile:
-            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
-            for row in filereader:
-                if (not header_skipped):
+        with open(file_location, encoding="utf-8") as csvfile:
+            filereader = csv.reader(csvfile, delimiter=",", quotechar='"')
+            for row in filereader:
+                if not header_skipped:
                     header_skipped = True
                 else:
                     try:
-                        counter = counter+1
+                        counter = counter + 1
                         enum_id = row[0]
-                        adapted_enum_name = Utils.make_valid_id_for_literal( row[3])
+                        adapted_enum_name = Utils.make_valid_id_for_literal(row[3])
                         value = row[4]
-                        adapted_value = Utils.make_valid_id( value)
+                        adapted_value = Utils.make_valid_id(value)
                         try:
                             the_enum = context.enum_map[enum_id]
-                            new_adapted_value = Utils.unique_value(
-                                the_enum, adapted_value)
-                            #new_adapted_value = Utils.special_cases(
+                            new_adapted_value = Utils.unique_value(the_enum, adapted_value)
+                            # new_adapted_value = Utils.special_cases(
                             #    new_adapted_value, counter)
-                            new_adapted_name = Utils.unique_name(
-                                the_enum, adapted_enum_name)
+                            new_adapted_name = Utils.unique_name(the_enum, adapted_enum_name)
 
                             enum_literal = ELEnumLiteral()
                             enum_literal.name = new_adapted_value
                             enum_literal.literal = new_adapted_name
                             enum_literal.value = counter
@@ -173,71 +164,69 @@
 
                         except KeyError:
                             print("missing domain: " + enum_id)
 
                     except IndexError:
-                        print(
-                            "row in DM_Domain_AVT.csv skipped  due to improper formatting at row number")
+                        print("row in DM_Domain_AVT.csv skipped  due to improper formatting at row number")
                         print(counter)
 
     def create_il_types_map(self, context):
-        '''
+        """
         for each type in the IL, create a map of typeID to type name
-        '''
+        """
         # for each logicalDatatype for orcle 12c, make a Datatype if we have an
         # equivalent
 
         file_location = context.file_directory + os.sep + "il" + os.sep + "DM_Logical_To_Native.csv"
         header_skipped = False
-        with open(file_location,  encoding='utf-8') as csvfile:
-            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
-            for row in filereader:
-                if (not header_skipped):
+        with open(file_location, encoding="utf-8") as csvfile:
+            filereader = csv.reader(csvfile, delimiter=",", quotechar='"')
+            for row in filereader:
+                if not header_skipped:
                     header_skipped = True
                 else:
                     rdbms_type = row[3]
                     rdbms_version = row[4]
                     data_type_id = row[0]
-                    if ((rdbms_type.strip() == "Oracle Database")
-                        and (rdbms_version.strip() == "12cR2")):
+                    if (rdbms_type.strip() == "Oracle Database") and (rdbms_version.strip() == "12cR2"):
                         native_type = row[2]
 
-                        if (native_type.strip() == "VARCHAR"):
+                        if native_type.strip() == "VARCHAR":
 
                             context.datatype_map[data_type_id] = context.types.e_string
 
-                        if (native_type.strip() == "VARCHAR2"):
+                        if native_type.strip() == "VARCHAR2":
 
                             context.datatype_map[data_type_id] = context.types.e_string
 
-                        if (native_type.strip() == "INTEGER"):
+                        if native_type.strip() == "INTEGER":
 
                             context.datatype_map[data_type_id] = context.types.e_int
 
-                        if (native_type.strip() == "DATE"):
+                        if native_type.strip() == "DATE":
 
                             context.datatype_map[data_type_id] = context.types.e_date
 
-                        if (native_type.strip() == "NUMBER"):
+                        if native_type.strip() == "NUMBER":
 
                             context.datatype_map[data_type_id] = context.types.e_double
 
-                        if (native_type.strip() == "UNKNOWN"):
+                        if native_type.strip() == "UNKNOWN":
 
                             context.datatype_map[data_type_id] = context.types.e_string
 
     def add_il_pk_attributes_to_classes(self, context):
         file_location = context.file_directory + os.sep + "il" + os.sep + "DM_Columns.csv"
         header_skipped = False
         # For each attribute add an ELAttribute to the correct ELClass representing the Entity
         # the attribute should have the correct type, which may be a specific
         # enumeration
 
-        with open(file_location,  encoding='utf-8') as csvfile:
-            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
-            for row in filereader:
-                if (not header_skipped):
+        with open(file_location, encoding="utf-8") as csvfile:
+            filereader = csv.reader(csvfile, delimiter=",", quotechar='"')
+            for row in filereader:
+                if not header_skipped:
                     header_skipped = True
                 else:
                     attribute_name = row[0]
                     attribute_id = row[1]
                     amended_attribute_name = Utils.make_valid_id(attribute_name)
@@ -248,52 +237,52 @@
                     relationID = row[35]
                     primary_key_or_not = row[34]
                     the_class = context.classes_map[class_id]
 
                     class_is_derived = False
-                    if (the_class.name.endswith("_derived")):
+                    if the_class.name.endswith("_derived"):
                         class_is_derived = True
 
                     the_attribute_name = amended_attribute_name
 
                     # we only add attributes here if they are not representing a relationship
                     if primary_key_or_not == "P":
 
-                        if (attribute_kind == "Domain"):
+                        if attribute_kind == "Domain":
                             enum_id = row[13]
                             the_enum = context.enum_map[enum_id]
 
                             attribute = ELAttribute()
 
                             attribute.lowerBound = 0
                             attribute.upperBound = 1
 
-                            if (the_enum.name == "String"):
+                            if the_enum.name == "String":
                                 attribute.name = the_attribute_name
                                 attribute.eType = context.types.e_string
                                 attribute.eAttributeType = context.types.e_string
                                 attribute.upperBound = 1
                                 attribute.lowerBound = 1
-                            elif (the_enum.name.startswith("String_")):
+                            elif the_enum.name.startswith("String_"):
                                 attribute.name = the_attribute_name
                                 attribute.eType = context.types.e_string
                                 attribute.eAttributeType = context.types.e_string
                                 attribute.upperBound = 1
                                 attribute.lowerBound = 1
-                            elif (the_enum.name.startswith("STRNG_")):
+                            elif the_enum.name.startswith("STRNG_"):
                                 attribute.name = the_attribute_name
                                 attribute.eType = context.types.e_string
                                 attribute.eAttributeType = context.types.e_string
                                 attribute.upperBound = 1
                                 attribute.lowerBound = 1
-                            elif (the_enum.name == "Number"):
+                            elif the_enum.name == "Number":
                                 attribute.name = the_attribute_name
                                 attribute.eType = context.types.e_double
                                 attribute.eAttributeType = context.types.e_double
                                 attribute.upperBound = 1
                                 attribute.lowerBound = 1
-                            elif (the_enum.name == "RL_domain"):
+                            elif the_enum.name == "RL_domain":
                                 attribute.name = the_attribute_name
                                 attribute.eType = context.types.e_double
                                 attribute.eAttributeType = context.types.e_double
                                 attribute.upperBound = 1
                                 attribute.lowerBound = 1
@@ -301,73 +290,75 @@
                                 attribute.name = the_attribute_name
                                 attribute.eType = context.types.e_double
                                 attribute.eAttributeType = context.types.e_double
                                 attribute.upperBound = 1
                                 attribute.lowerBound = 1
-                            elif (the_enum.name.startswith("Real_")):
+                            elif the_enum.name.startswith("Real_"):
                                 attribute.name = the_attribute_name
                                 attribute.eType = context.types.e_double
                                 attribute.eAttributeType = context.types.e_double
                                 attribute.upperBound = 1
                                 attribute.lowerBound = 1
-                            elif (the_enum.name.startswith("Monetary")):
-                                attribute.name = the_attribute_name
-                                attribute.eType = context.types.e_int
-                                attribute.eAttributeType = context.types.e_int
-                                attribute.upperBound = 1
-                                attribute.lowerBound = 1
-                            elif (the_enum.name.startswith("MNTRY_")):
-                                attribute.name = the_attribute_name
-                                attribute.eType = context.types.e_int
-                                attribute.eAttributeType = context.types.e_int
-                                attribute.upperBound = 1
-                                attribute.lowerBound = 1
-                            elif (the_enum.name.startswith("INTGR_")):
-                                attribute.name = the_attribute_name
-                                attribute.eType = context.types.e_int
-                                attribute.eAttributeType = context.types.e_int
-                                attribute.upperBound = 1
-                                attribute.lowerBound = 1
-                            elif (the_enum.name.startswith("YR")):
-                                attribute.name = the_attribute_name
-                                attribute.eType = context.types.e_int
-                                attribute.eAttributeType = context.types.e_int
-                                attribute.upperBound = 1
-                                attribute.lowerBound = 1
-                            elif (the_enum.name.startswith("Non_negative_monetary_amounts_with_2_decimals")):
-                                attribute.name = the_attribute_name
-                                attribute.eType = context.types.e_int
-                                attribute.eAttributeType = context.types.e_int
-                                attribute.upperBound = 1
-                                attribute.lowerBound = 1
-                            elif (the_enum.name.startswith("Non_negative_integers")):
-                                attribute.name = the_attribute_name
-                                attribute.eType = context.types.e_int
-                                attribute.eAttributeType = context.types.e_int
-                                attribute.upperBound = 1
-                                attribute.lowerBound = 1
-                            elif (the_enum.name.startswith("Positive_and_negative_monetary_amounts_with_2_decimals_domain")):
-                                attribute.name = the_attribute_name
-                                attribute.eType = context.types.e_int
-                                attribute.eAttributeType = context.types.e_int
-                                attribute.upperBound = 1
-                                attribute.lowerBound = 1
-                            elif (the_enum.name.startswith("Positive_and_negative_monetary_amounts_with_2_decimals")):
-                                attribute.name = the_attribute_name
-                                attribute.eType = context.types.e_int
-                                attribute.eAttributeType = context.types.e_int
-                                attribute.upperBound = 1
-                                attribute.lowerBound = 1
-
-                            elif (the_enum.name.startswith("All_possible_dates")):
+                            elif the_enum.name.startswith("Monetary"):
+                                attribute.name = the_attribute_name
+                                attribute.eType = context.types.e_int
+                                attribute.eAttributeType = context.types.e_int
+                                attribute.upperBound = 1
+                                attribute.lowerBound = 1
+                            elif the_enum.name.startswith("MNTRY_"):
+                                attribute.name = the_attribute_name
+                                attribute.eType = context.types.e_int
+                                attribute.eAttributeType = context.types.e_int
+                                attribute.upperBound = 1
+                                attribute.lowerBound = 1
+                            elif the_enum.name.startswith("INTGR_"):
+                                attribute.name = the_attribute_name
+                                attribute.eType = context.types.e_int
+                                attribute.eAttributeType = context.types.e_int
+                                attribute.upperBound = 1
+                                attribute.lowerBound = 1
+                            elif the_enum.name.startswith("YR"):
+                                attribute.name = the_attribute_name
+                                attribute.eType = context.types.e_int
+                                attribute.eAttributeType = context.types.e_int
+                                attribute.upperBound = 1
+                                attribute.lowerBound = 1
+                            elif the_enum.name.startswith("Non_negative_monetary_amounts_with_2_decimals"):
+                                attribute.name = the_attribute_name
+                                attribute.eType = context.types.e_int
+                                attribute.eAttributeType = context.types.e_int
+                                attribute.upperBound = 1
+                                attribute.lowerBound = 1
+                            elif the_enum.name.startswith("Non_negative_integers"):
+                                attribute.name = the_attribute_name
+                                attribute.eType = context.types.e_int
+                                attribute.eAttributeType = context.types.e_int
+                                attribute.upperBound = 1
+                                attribute.lowerBound = 1
+                            elif the_enum.name.startswith(
+                                "Positive_and_negative_monetary_amounts_with_2_decimals_domain"
+                            ):
+                                attribute.name = the_attribute_name
+                                attribute.eType = context.types.e_int
+                                attribute.eAttributeType = context.types.e_int
+                                attribute.upperBound = 1
+                                attribute.lowerBound = 1
+                            elif the_enum.name.startswith("Positive_and_negative_monetary_amounts_with_2_decimals"):
+                                attribute.name = the_attribute_name
+                                attribute.eType = context.types.e_int
+                                attribute.eAttributeType = context.types.e_int
+                                attribute.upperBound = 1
+                                attribute.lowerBound = 1
+
+                            elif the_enum.name.startswith("All_possible_dates"):
                                 attribute.name = the_attribute_name
                                 attribute.eType = context.types.e_date
                                 attribute.eAttributeType = context.types.e_date
                                 attribute.upperBound = 1
                                 attribute.lowerBound = 1
 
-                            elif (the_enum.name.startswith("DT_FLL")):
+                            elif the_enum.name.startswith("DT_FLL"):
                                 attribute.name = the_attribute_name
                                 attribute.eType = context.types.e_date
                                 attribute.eAttributeType = context.types.e_date
                                 attribute.upperBound = 1
                                 attribute.lowerBound = 1
@@ -379,50 +370,42 @@
                                 attribute.eType = the_enum
                                 attribute.eAttributeType = the_enum
                                 attribute.upperBound = 1
                                 attribute.lowerBound = 1
 
-
-
-                        if (attribute_kind == "Logical Type"):
+                        if attribute_kind == "Logical Type":
                             data_type_id = row[14]
                             try:
                                 datatype = context.datatype_map[data_type_id]
                                 attribute = ELAttribute()
                                 attribute.lowerBound = 0
                                 attribute.upperBound = 1
                                 attribute.name = amended_attribute_name
-                                attribute.eType = Utils.get_ecore_datatype_for_datatype(
-                                    self)
-                                attribute.eAttributeType = Utils.get_ecore_datatype_for_datatype(
-                                    self)
-
-
+                                attribute.eType = Utils.get_ecore_datatype_for_datatype(self)
+                                attribute.eAttributeType = Utils.get_ecore_datatype_for_datatype(self)
 
                             except KeyError:
                                 print("missing datatype: ")
                                 print(data_type_id)
 
                         try:
 
                             the_class = context.classes_map[class_id]
-                            SQLDeveloperILImport.add_composite_pk_if_missing(
-                                self, context, the_class)
+                            SQLDeveloperILImport.add_composite_pk_if_missing(self, context, the_class)
                             the_class.eStructuralFeatures.extend([attribute])
-
 
                         except:
                             print("missing class2: ")
                             print(class_id)
                     else:
                         if mandatory == "Y":
                             context.fk_to_mandatory_map[attribute_id] = "M"
 
     def add_composite_pk_if_missing(self, context, the_class):
-        '''
+        """
         If the class does not have a composite PK, add one
-        '''
+        """
 
         pk_name = the_class.name + "_uniqueID"
         pk_exists = False
         for member in the_class.eStructuralFeatures:
             if member.name == pk_name:
@@ -437,25 +420,25 @@
             attribute.lowerBound = 0
             attribute.upperBound = 1
             the_class.eStructuralFeatures.append(attribute)
 
     def add_il_attributes_to_classes(self, context):
-        '''
+        """
         For each attribute on an entity of the IL, add an attribute
         to the relevant class in the package
-        '''
-
-        file_location = context.file_directory + os.sep +  "il" + os.sep + "DM_Columns.csv"
+        """
+
+        file_location = context.file_directory + os.sep + "il" + os.sep + "DM_Columns.csv"
         header_skipped = False
         # For each attribute add an ELAttribute to the correct ELClass representing the Entity
         # the attribute should have the correct type, which may be a specific
         # enumeration
 
-        with open(file_location,  encoding='utf-8') as csvfile:
-            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
-            for row in filereader:
-                if (not header_skipped):
+        with open(file_location, encoding="utf-8") as csvfile:
+            filereader = csv.reader(csvfile, delimiter=",", quotechar='"')
+            for row in filereader:
+                if not header_skipped:
                     header_skipped = True
                 else:
                     attribute_name = row[0]
                     attribute_id = row[1]
                     amended_attribute_name = Utils.make_valid_id(attribute_name)
@@ -466,22 +449,20 @@
                     preferred_abbreviation = row[0]
                     relation_id = row[35]
                     primary_key_or_not = row[34]
                     the_class = context.classes_map[class_id]
 
-
-
                     if len(preferred_abbreviation) == 0:
                         preferred_abbreviation = attribute_name
                         print("no preferred abbreviation for attribute: " + attribute_name)
 
                     the_attribute_name = Utils.make_valid_id(preferred_abbreviation)
 
                     # we only add attributes here if they are not representing a relationship
                     if (relation_id == "") and not (primary_key_or_not == "P"):
 
-                        if (attribute_kind == "Domain"):
+                        if attribute_kind == "Domain":
                             enum_id = row[13]
                             the_enum = context.enum_map[enum_id]
 
                             attribute = ELAttribute()
 
@@ -556,24 +537,21 @@
 
                         else:
                             attribute.name = the_attribute_name
                             attribute.eType = the_enum
                             attribute.eAttributeType = the_enum
-
 
                         if attribute_kind == "Logical Type":
                             data_type_id = row[14]
                             try:
                                 datatype = context.datatype_map[data_type_id]
                                 attribute = ELAttribute()
                                 attribute.lowerBound = 0
                                 attribute.upperBound = 1
                                 attribute.name = amended_attribute_name
-                                attribute.eType = Utils.get_ecore_datatype_for_datatype(
-                                    self)
-                                attribute.eAttributeType = Utils.get_ecore_datatype_for_datatype(
-                                    self)
+                                attribute.eType = Utils.get_ecore_datatype_for_datatype(self)
+                                attribute.eAttributeType = Utils.get_ecore_datatype_for_datatype(self)
 
                             except KeyError:
                                 print("missing datatype: ")
                                 print(data_type_id)
 
@@ -588,35 +566,35 @@
                     else:
                         if mandatory == "Y":
                             context.fk_to_mandatory_map[attribute_id] = "M"
 
     def create_fk_to_column_map(self, context):
-        '''
+        """
         Create a map of FK to column ID
-        '''
-        file_location = context.file_directory + os.sep +  "il" + os.sep + "DM_Constr_Index_Columns.csv"
-        header_skipped = False
-        with open(file_location,  encoding='utf-8') as csvfile:
-            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
-            for row in filereader:
-                if (not header_skipped):
+        """
+        file_location = context.file_directory + os.sep + "il" + os.sep + "DM_Constr_Index_Columns.csv"
+        header_skipped = False
+        with open(file_location, encoding="utf-8") as csvfile:
+            filereader = csv.reader(csvfile, delimiter=",", quotechar='"')
+            for row in filereader:
+                if not header_skipped:
                     header_skipped = True
                 else:
                     fk_name = row[6]
                     columnID = row[2]
                     context.fk_to_column_map[fk_name] = columnID
 
     def add_il_relationships_between_classes(self, context):
-        '''
+        """
         For each relationship in the IL, add a reference between the relevant classes
-        '''
-        file_location = context.file_directory + os.sep +  "il" + os.sep + "DM_ForeignKeys.csv"
-        header_skipped = False
-        with open(file_location,  encoding='utf-8') as csvfile:
-            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
-            for row in filereader:
-                if (not header_skipped):
+        """
+        file_location = context.file_directory + os.sep + "il" + os.sep + "DM_ForeignKeys.csv"
+        header_skipped = False
+        with open(file_location, encoding="utf-8") as csvfile:
+            filereader = csv.reader(csvfile, delimiter=",", quotechar='"')
+            for row in filereader:
+                if not header_skipped:
                     header_skipped = True
                 else:
                     fk_id = row[0]
                     source_id = row[10]
                     target_id = row[12]
@@ -642,17 +620,16 @@
                     try:
                         target_class = context.classes_map[target_id]
                     except KeyError:
                         print("missing target class: " + target_id)
 
-                    num_of_relations = Utils.number_of_relationships_to_this_class(
-                        the_class, target_class)
-                    if (num_of_relations > 0):
+                    num_of_relations = Utils.number_of_relationships_to_this_class(the_class, target_class)
+                    if num_of_relations > 0:
                         reference_name = reference_name + str(num_of_relations)
                     relational_attribute = None
-                    if (target_optional.strip() == "Y"):
-                        if (source_to_target_cardinality.strip() == "*"):
+                    if target_optional.strip() == "Y":
+                        if source_to_target_cardinality.strip() == "*":
                             reference_name = reference_name + "s"
                             e_reference = ELReference()
                             e_reference.name = reference_name
                             e_reference.eType = target_class
                             # upper bound of -1 means there is no upper bounds, so represents an open list of reference
@@ -667,11 +644,11 @@
                             e_reference.upperBound = 1
                             e_reference.lowerBound = 0
                             e_reference.containment = False
 
                     else:
-                        if (source_to_target_cardinality.strip() == "*"):
+                        if source_to_target_cardinality.strip() == "*":
                             reference_name = reference_name + "s"
                             e_reference = ELReference()
                             e_reference.name = reference_name
                             e_reference.eType = target_class
                             e_reference.upperBound = -1
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/sqldeveloper_import/import_sqldev_il_to_regdna.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py	2025-09-15 13:18:11.377376+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py	2025-09-21 17:07:37.045205+00:00
@@ -18,30 +18,32 @@
 from pathlib import Path
 import traceback
 
 
 class ImportWebsiteToSDDModel(object):
-    '''
+    """
     Class responsible for importing SDD csv files into an instance of the analysis model
-    '''
-    def import_report_templates_from_sdd(self, sdd_context, file_dir : str = "technical_export", ancrdt_include:bool = False):
-        '''
+    """
+
+    def import_report_templates_from_sdd(
+        self, sdd_context, file_dir: str = "technical_export", ancrdt_include: bool = False
+    ):
+        """
         Import SDD csv files into an instance of the analysis model
-        '''
-        self.base_path = os.sep.join([sdd_context.file_directory,file_dir])
-
+        """
+        self.base_path = os.sep.join([sdd_context.file_directory, file_dir])
 
         if os.path.exists(self.base_path + os.sep + "maintenance_agency.csv"):
             ImportWebsiteToSDDModel.create_maintenance_agencies(self, sdd_context)
         if os.path.exists(self.base_path + os.sep + "framework.csv"):
             ImportWebsiteToSDDModel.create_frameworks(self, sdd_context)
         if os.path.exists(self.base_path + os.sep + "domain.csv"):
-            ImportWebsiteToSDDModel.create_all_domains(self, sdd_context,False,ancrdt_include)
+            ImportWebsiteToSDDModel.create_all_domains(self, sdd_context, False, ancrdt_include)
         if os.path.exists(self.base_path + os.sep + "member.csv"):
-            ImportWebsiteToSDDModel.create_all_members(self, sdd_context,False,ancrdt_include)
+            ImportWebsiteToSDDModel.create_all_members(self, sdd_context, False, ancrdt_include)
         if os.path.exists(self.base_path + os.sep + "variable.csv"):
-            ImportWebsiteToSDDModel.create_all_variables(self, sdd_context,False,ancrdt_include)
+            ImportWebsiteToSDDModel.create_all_variables(self, sdd_context, False, ancrdt_include)
 
         if os.path.exists(self.base_path + os.sep + "table.csv"):
             ImportWebsiteToSDDModel.create_report_tables(self, sdd_context)
         if os.path.exists(self.base_path + os.sep + "table_cell.csv"):
             ImportWebsiteToSDDModel.create_table_cells(self, sdd_context)
@@ -58,107 +60,101 @@
             if os.path.exists(self.base_path + os.sep + "subdomain.csv"):
                 ImportWebsiteToSDDModel.create_all_subdomains(self, sdd_context)
             if os.path.exists(self.base_path + os.sep + "cube.csv"):
                 ImportWebsiteToSDDModel.create_all_structures(self, sdd_context)
 
-
-
-
     def import_semantic_integrations_from_sdd(self, sdd_context):
-        '''
+        """
         Import SDD csv files into an instance of the analysis model
-        '''
+        """
         ImportWebsiteToSDDModel.delete_mapping_warnings_files(self, sdd_context)
         ImportWebsiteToSDDModel.create_all_variable_mappings(self, sdd_context)
         ImportWebsiteToSDDModel.create_all_variable_mapping_items(self, sdd_context)
         ImportWebsiteToSDDModel.create_member_mappings(self, sdd_context)
         ImportWebsiteToSDDModel.create_all_member_mappings_items(self, sdd_context)
         ImportWebsiteToSDDModel.create_all_mapping_definitions(self, sdd_context)
         ImportWebsiteToSDDModel.create_all_mapping_to_cubes(self, sdd_context)
 
     def import_hierarchies_from_sdd(self, sdd_context):
-        '''
+        """
         Import hierarchies from CSV file
-        '''
+        """
         ImportWebsiteToSDDModel.delete_hierarchy_warnings_files(self, sdd_context)
         ImportWebsiteToSDDModel.create_all_member_hierarchies(self, sdd_context)
         ImportWebsiteToSDDModel.create_all_parent_members_with_children_locally(self, sdd_context)
         ImportWebsiteToSDDModel.create_all_member_hierarchies_nodes(self, sdd_context)
 
     def create_maintenance_agencies(self, context):
-        '''
+        """
         Import maintenance agencies from CSV file using bulk create
-        '''
+        """
         file_location = self.base_path + os.sep + "maintenance_agency.csv"
         header_skipped = False
         agencies_to_create = []
 
-        with open(file_location, encoding='utf-8') as csvfile:
-            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
+        with open(file_location, encoding="utf-8") as csvfile:
+            filereader = csv.reader(csvfile, delimiter=",", quotechar='"')
             for row in filereader:
                 if not header_skipped:
                     header_skipped = True
                 else:
                     code = row[ColumnIndexes().maintenance_agency_code]
                     description = row[ColumnIndexes().maintenance_agency_description]
                     id = row[ColumnIndexes().maintenance_agency_id]
                     name = row[ColumnIndexes().maintenance_agency_name]
 
-                    maintenance_agency = MAINTENANCE_AGENCY(
-                        name=ImportWebsiteToSDDModel.replace_dots(self, id))
+                    maintenance_agency = MAINTENANCE_AGENCY(name=ImportWebsiteToSDDModel.replace_dots(self, id))
                     maintenance_agency.code = code
                     maintenance_agency.description = description
                     maintenance_agency.maintenance_agency_id = ImportWebsiteToSDDModel.replace_dots(self, id)
 
                     agencies_to_create.append(maintenance_agency)
                     context.agency_dictionary[id] = maintenance_agency
 
-
         if context.save_sdd_to_db and agencies_to_create:
             MAINTENANCE_AGENCY.objects.bulk_create(agencies_to_create, batch_size=1000, ignore_conflicts=True)
 
     def create_frameworks(self, context):
-        '''
+        """
         Import frameworks from CSV file using bulk create
-        '''
+        """
         file_location = self.base_path + os.sep + "framework.csv"
         header_skipped = False
         frameworks_to_create = []
 
-        with open(file_location, encoding='utf-8') as csvfile:
-            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
+        with open(file_location, encoding="utf-8") as csvfile:
+            filereader = csv.reader(csvfile, delimiter=",", quotechar='"')
             for row in filereader:
                 if not header_skipped:
                     header_skipped = True
                 else:
                     code = row[ColumnIndexes().framework_code]
                     description = row[ColumnIndexes().framework_description]
                     id = row[ColumnIndexes().framework_id]
                     name = row[ColumnIndexes().framework_name]
 
-                    framework = FRAMEWORK(
-                        name=ImportWebsiteToSDDModel.replace_dots(self, id))
+                    framework = FRAMEWORK(name=ImportWebsiteToSDDModel.replace_dots(self, id))
                     framework.code = code
                     framework.description = description
                     framework.framework_id = ImportWebsiteToSDDModel.replace_dots(self, id)
 
                     frameworks_to_create.append(framework)
                     context.framework_dictionary[ImportWebsiteToSDDModel.replace_dots(self, id)] = framework
 
         if context.save_sdd_to_db and frameworks_to_create:
             FRAMEWORK.objects.bulk_create(frameworks_to_create, batch_size=1000, ignore_conflicts=True)
 
-    def create_all_domains(self, context, ref, ancrdt_include:bool = False):
-        '''
+    def create_all_domains(self, context, ref, ancrdt_include: bool = False):
+        """
         Import all domains from CSV file using bulk create
-        '''
+        """
         file_location = self.base_path + os.sep + "domain.csv"
         header_skipped = False
         domains_to_create = []
 
-        with open(file_location, encoding='utf-8') as csvfile:
-            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
+        with open(file_location, encoding="utf-8") as csvfile:
+            filereader = csv.reader(csvfile, delimiter=",", quotechar='"')
             for row in filereader:
                 if not header_skipped:
                     header_skipped = True
                 else:
                     maintenance_agency = row[ColumnIndexes().domain_maintenence_agency]
@@ -178,13 +174,17 @@
 
                     try:
                         if include:
                             domain = DOMAIN(name=ImportWebsiteToSDDModel.replace_dots(self, domain_id))
                             if maintenance_agency == "":
-                                maintenance_agency = ImportWebsiteToSDDModel.find_maintenance_agency_with_id(self,context,"SDD_DOMAIN")
+                                maintenance_agency = ImportWebsiteToSDDModel.find_maintenance_agency_with_id(
+                                    self, context, "SDD_DOMAIN"
+                                )
                             else:
-                                maintenance_agency = ImportWebsiteToSDDModel.find_maintenance_agency_with_id(self,context,maintenance_agency)
+                                maintenance_agency = ImportWebsiteToSDDModel.find_maintenance_agency_with_id(
+                                    self, context, maintenance_agency
+                                )
                             domain.maintenance_agency_id = maintenance_agency
                             domain.code = code
                             domain.description = description
                             domain.domain_id = ImportWebsiteToSDDModel.replace_dots(self, domain_id)
                             domain.name = domain_name
@@ -197,24 +197,23 @@
                             else:
                                 context.domain_dictionary[domain.domain_id] = domain
                     except:
                         raise ValueError(f"{row} :: {traceback.format_exc()}")
 
-
         if context.save_sdd_to_db and domains_to_create:
             DOMAIN.objects.bulk_create(domains_to_create, batch_size=1000, ignore_conflicts=True)
 
-    def create_all_members(self, context, ref, ancrdt_include:bool=False):
-        '''
+    def create_all_members(self, context, ref, ancrdt_include: bool = False):
+        """
         Import all members from CSV file using bulk create
-        '''
+        """
         file_location = self.base_path + os.sep + "member.csv"
         header_skipped = False
         members_to_create = []
 
-        with open(file_location, encoding='utf-8') as csvfile:
-            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
+        with open(file_location, encoding="utf-8") as csvfile:
+            filereader = csv.reader(csvfile, delimiter=",", quotechar='"')
             for row in filereader:
                 if not header_skipped:
                     header_skipped = True
                 else:
                     code = row[ColumnIndexes().member_member_code_index]
@@ -236,13 +235,17 @@
                     try:
 
                         if include:
 
                             if ref or maintenance_agency == "":
-                                maintenance_agency = ImportWebsiteToSDDModel.find_maintenance_agency_with_id(self,context,"ECB")
+                                maintenance_agency = ImportWebsiteToSDDModel.find_maintenance_agency_with_id(
+                                    self, context, "ECB"
+                                )
                             else:
-                                maintenance_agency = ImportWebsiteToSDDModel.find_maintenance_agency_with_id(self,context,maintenance_agency)
+                                maintenance_agency = ImportWebsiteToSDDModel.find_maintenance_agency_with_id(
+                                    self, context, maintenance_agency
+                                )
 
                             member = MEMBER(name=ImportWebsiteToSDDModel.replace_dots(self, member_id))
                             member.member_id = ImportWebsiteToSDDModel.replace_dots(self, member_id)
                             member.code = code
                             member.description = description
@@ -259,24 +262,23 @@
                                 context.member_id_to_member_code_map[member.member_id] = code
 
                     except:
                         raise ValueError(f"{row} :: {traceback.format_exc()}")
 
-
         if context.save_sdd_to_db and members_to_create:
             MEMBER.objects.bulk_create(members_to_create, batch_size=1000, ignore_conflicts=True)
 
     def create_all_variables(self, context, ref, ancrdt_include=False):
-        '''
+        """
         Import all variables from CSV file using bulk create
-        '''
+        """
         file_location = self.base_path + os.sep + "variable.csv"
         header_skipped = False
         variables_to_create = []
 
-        with open(file_location, encoding='utf-8') as csvfile:
-            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
+        with open(file_location, encoding="utf-8") as csvfile:
+            filereader = csv.reader(csvfile, delimiter=",", quotechar='"')
             for row in filereader:
                 if not header_skipped:
                     header_skipped = True
                 else:
                     maintenence_agency = row[ColumnIndexes().variable_variable_maintenence_agency]
@@ -294,11 +296,13 @@
                     if (not ref) and not (maintenence_agency == "ECB"):
                         include = True
 
                     if include:
                         variable = VARIABLE(name=ImportWebsiteToSDDModel.replace_dots(self, variable_id))
-                        maintenance_agency_id = ImportWebsiteToSDDModel.find_maintenance_agency_with_id(self,context,maintenence_agency)
+                        maintenance_agency_id = ImportWebsiteToSDDModel.find_maintenance_agency_with_id(
+                            self, context, maintenence_agency
+                        )
                         variable.code = code
                         variable.variable_id = ImportWebsiteToSDDModel.replace_dots(self, variable_id)
                         variable.name = name
                         domain = ImportWebsiteToSDDModel.find_domain_with_id(self, context, domain_id)
                         variable.domain_id = domain
@@ -307,11 +311,11 @@
 
                         variables_to_create.append(variable)
                         context.variable_dictionary[variable.variable_id] = variable
                         context.variable_to_domain_map[variable.variable_id] = domain
                         context.variable_to_long_names_map[variable.variable_id] = name
-                        if not((primary_concept == "") or (primary_concept == None)):
+                        if not ((primary_concept == "") or (primary_concept == None)):
                             context.variable_to_primary_concept_map[variable.variable_id] = primary_concept
 
         if context.save_sdd_to_db and variables_to_create:
             VARIABLE.objects.bulk_create(variables_to_create, batch_size=1000, ignore_conflicts=True)
 
@@ -323,92 +327,104 @@
         missing_children = []
 
         # Pre-fetch all hierarchies for faster lookup
         hierarchy_cache = {}
 
-        with open(f"{context.file_directory}/technical_export/member_hierarchy_node.csv", encoding='utf-8') as csvfile:
+        with open(f"{context.file_directory}/technical_export/member_hierarchy_node.csv", encoding="utf-8") as csvfile:
             next(csvfile)  # Skip header more efficiently
             for row in csv.reader(csvfile):
                 parent_member_id = row[ColumnIndexes().member_hierarchy_node_parent_member_id]
                 member_id = row[ColumnIndexes().member_hierarchy_node_member_id]
                 hierarchy_id = row[ColumnIndexes().member_hierarchy_node_hierarchy_id]
 
                 if not parent_member_id:
                     continue
 
                 if hierarchy_id not in hierarchy_cache:
-                    hierarchy_cache[hierarchy_id] = ImportWebsiteToSDDModel.find_member_hierarchy_with_id(self,hierarchy_id,context)
+                    hierarchy_cache[hierarchy_id] = ImportWebsiteToSDDModel.find_member_hierarchy_with_id(
+                        self, hierarchy_id, context
+                    )
 
                 hierarchy = hierarchy_cache[hierarchy_id]
                 if hierarchy:
                     domain = hierarchy.domain_id
-                    parent_members_child_triples.append((parent_member_id,member_id,domain))
+                    parent_members_child_triples.append((parent_member_id, member_id, domain))
                     parent_members.add(parent_member_id)
 
         # Process parent-child relationships in batches
         for parent_member_id, member_id, domain in parent_members_child_triples:
             if member_id in parent_members:
-                if not any(parent_member_id in d for d in (context.members_that_are_nodes,
-                                                         context.member_dictionary,
-                                                         context.member_dictionary)):
+                if not any(
+                    parent_member_id in d
+                    for d in (context.members_that_are_nodes, context.member_dictionary, context.member_dictionary)
+                ):
                     parent_member = MEMBER(
                         name=ImportWebsiteToSDDModel.replace_dots(self, parent_member_id),
                         member_id=ImportWebsiteToSDDModel.replace_dots(self, parent_member_id),
-                        maintenance_agency_id=ImportWebsiteToSDDModel.find_maintenance_agency_with_id(self,context,"NODE"),
-                        domain_id=domain
+                        maintenance_agency_id=ImportWebsiteToSDDModel.find_maintenance_agency_with_id(
+                            self, context, "NODE"
+                        ),
+                        domain_id=domain,
                     )
                     parent_members_to_create.append(parent_member)
                     context.member_dictionary[parent_member.member_id] = parent_member
                     if not (parent_member.domain_id is None) and not (parent_member.domain_id == ""):
                         context.member_id_to_domain_map[parent_member] = domain
                         context.member_id_to_member_code_map[parent_member.member_id] = parent_member.member_id
 
                     context.members_that_are_nodes[parent_member_id] = parent_member
             else:
-                member = ImportWebsiteToSDDModel.find_member_with_id(self,member_id,context)
+                member = ImportWebsiteToSDDModel.find_member_with_id(self, member_id, context)
                 if member is None:
-                    missing_children.append((parent_member_id,member_id))
-                elif not any(parent_member_id in d for d in (context.members_that_are_nodes,
-                                                          context.member_dictionary,
-                                                          context.member_dictionary)):
+                    missing_children.append((parent_member_id, member_id))
+                elif not any(
+                    parent_member_id in d
+                    for d in (context.members_that_are_nodes, context.member_dictionary, context.member_dictionary)
+                ):
                     parent_member = MEMBER(
                         name=ImportWebsiteToSDDModel.replace_dots(self, parent_member_id),
                         member_id=ImportWebsiteToSDDModel.replace_dots(self, parent_member_id),
-                        maintenance_agency_id=ImportWebsiteToSDDModel.find_maintenance_agency_with_id(self,context,"NODE"),
-                        domain_id=domain
+                        maintenance_agency_id=ImportWebsiteToSDDModel.find_maintenance_agency_with_id(
+                            self, context, "NODE"
+                        ),
+                        domain_id=domain,
                     )
                     parent_members_to_create.append(parent_member)
                     context.members_that_are_nodes[parent_member_id] = parent_member
 
                     context.member_dictionary[parent_member.member_id] = parent_member
                     if not (parent_member.domain_id is None) and not (parent_member.domain_id == ""):
                         context.member_id_to_domain_map[parent_member] = domain
                         context.member_id_to_member_code_map[parent_member.member_id] = parent_member.member_id
 
         if context.save_sdd_to_db and parent_members_to_create:
-            MEMBER.objects.bulk_create(parent_members_to_create, batch_size=5000,ignore_conflicts=True)  # Increased batch size
-
-        ImportWebsiteToSDDModel.save_missing_children_to_csv(context,missing_children)
+            MEMBER.objects.bulk_create(
+                parent_members_to_create, batch_size=5000, ignore_conflicts=True
+            )  # Increased batch size
+
+        ImportWebsiteToSDDModel.save_missing_children_to_csv(context, missing_children)
 
     def create_all_member_hierarchies(self, context):
-        '''
+        """
         Import all member hierarchies with batch processing
-        '''
+        """
         missing_domains = set()  # Using set for faster lookups
         hierarchies_to_create = []
 
-        with open(f"{context.file_directory}/technical_export/member_hierarchy.csv", encoding='utf-8') as csvfile:
+        with open(f"{context.file_directory}/technical_export/member_hierarchy.csv", encoding="utf-8") as csvfile:
             next(csvfile)  # Skip header more efficiently
             for row in csv.reader(csvfile):
                 maintenance_agency_id = row[ColumnIndexes().member_hierarchy_maintenance_agency]
                 code = row[ColumnIndexes().member_hierarchy_code]
                 id = row[ColumnIndexes().member_hierarchy_id]
                 domain_id = row[ColumnIndexes().member_hierarchy_domain_id]
                 description = row[ColumnIndexes().member_hierarchy_description]
 
-                maintenance_agency = ImportWebsiteToSDDModel.find_maintenance_agency_with_id(self,context,maintenance_agency_id)
-                domain = ImportWebsiteToSDDModel.find_domain_with_id(self,context,domain_id)
+                maintenance_agency = ImportWebsiteToSDDModel.find_maintenance_agency_with_id(
+                    self, context, maintenance_agency_id
+                )
+                domain = ImportWebsiteToSDDModel.find_domain_with_id(self, context, domain_id)
 
                 if domain is None:
                     missing_domains.add(domain_id)
                     continue
 
@@ -416,11 +432,11 @@
                     name=ImportWebsiteToSDDModel.replace_dots(self, id),
                     member_hierarchy_id=ImportWebsiteToSDDModel.replace_dots(self, id),
                     code=code,
                     description=description,
                     maintenance_agency_id=maintenance_agency,
-                    domain_id=domain
+                    domain_id=domain,
                 )
 
                 if hierarchy.member_hierarchy_id not in context.member_hierarchy_dictionary:
                     hierarchies_to_create.append(hierarchy)
                     context.member_hierarchy_dictionary[hierarchy.member_hierarchy_id] = hierarchy
@@ -429,49 +445,54 @@
             MEMBER_HIERARCHY.objects.bulk_create(hierarchies_to_create, batch_size=5000)  # Increased batch size
 
         if missing_domains:
             ImportWebsiteToSDDModel.save_missing_domains_to_csv(context, list(missing_domains))
 
-    def save_missing_domains_to_csv(context,missing_domains):
-        filename = context.output_directory + os.sep + "generated_hierarchy_warnings" + os.sep + "missing_domains_ancrdt.csv"
-        with open(filename, 'w', newline='') as csvfile:
+    def save_missing_domains_to_csv(context, missing_domains):
+        filename = (
+            context.output_directory + os.sep + "generated_hierarchy_warnings" + os.sep + "missing_domains_ancrdt.csv"
+        )
+        with open(filename, "w", newline="") as csvfile:
             writer = csv.writer(csvfile)
             writer.writerows(missing_domains)
 
-    def save_missing_members_to_csv(context,missing_members):
-        filename = context.output_directory + os.sep + "generated_hierarchy_warnings" + os.sep + "missing_members_ancrdt.csv"
-        with open(filename, 'w', newline='') as csvfile:
+    def save_missing_members_to_csv(context, missing_members):
+        filename = (
+            context.output_directory + os.sep + "generated_hierarchy_warnings" + os.sep + "missing_members_ancrdt.csv"
+        )
+        with open(filename, "w", newline="") as csvfile:
             writer = csv.writer(csvfile)
             writer.writerows(missing_members)
 
-    def save_missing_variables_to_csv(context,missing_variables):
-        filename = context.output_directory + os.sep + "generated_hierarchy_warnings" + os.sep + "missing_variables_ancrdt.csv"
-        with open(filename, 'w', newline='') as csvfile:
+    def save_missing_variables_to_csv(context, missing_variables):
+        filename = (
+            context.output_directory + os.sep + "generated_hierarchy_warnings" + os.sep + "missing_variables_ancrdt.csv"
+        )
+        with open(filename, "w", newline="") as csvfile:
             writer = csv.writer(csvfile)
             writer.writerows(missing_variables)
 
-    def save_missing_children_to_csv(context,missing_children):
-        filename = context.output_directory + os.sep + "generated_hierarchy_warnings" + os.sep + "missing_children_ancrdt.csv"
-        with open(filename, 'w', newline='') as csvfile:
+    def save_missing_children_to_csv(context, missing_children):
+        filename = (
+            context.output_directory + os.sep + "generated_hierarchy_warnings" + os.sep + "missing_children_ancrdt.csv"
+        )
+        with open(filename, "w", newline="") as csvfile:
             writer = csv.writer(csvfile)
             writer.writerows(missing_children)
 
-
-
-
     def create_all_member_hierarchies_nodes(self, context):
-        '''
+        """
         Import all member hierarchy nodes from CSV file
-        '''
+        """
         file_location = self.base_path + os.sep + "member_hierarchy_node.csv"
         header_skipped = False
         missing_members = []
         missing_hierarchies = []
         nodes_to_create = []
 
-        with open(file_location, encoding='utf-8') as csvfile:
-            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
+        with open(file_location, encoding="utf-8") as csvfile:
+            filereader = csv.reader(csvfile, delimiter=",", quotechar='"')
             for row in filereader:
                 if not header_skipped:
                     header_skipped = True
                 else:
                     hierarchy_id = row[ColumnIndexes().member_hierarchy_node_hierarchy_id]
@@ -481,59 +502,69 @@
                     comparator = row[ColumnIndexes().member_hierarchy_node_comparator]
                     operator = row[ColumnIndexes().member_hierarchy_node_operator]
                     valid_from = row[ColumnIndexes().member_hierarchy_node_valid_from]
                     valid_to = row[ColumnIndexes().member_hierarchy_node_valid_to]
 
-                    hierarchy = ImportWebsiteToSDDModel.find_member_hierarchy_with_id(self,hierarchy_id,context)
+                    hierarchy = ImportWebsiteToSDDModel.find_member_hierarchy_with_id(self, hierarchy_id, context)
                     if hierarchy is None:
                         print(f"Hierarchy {hierarchy_id} not found")
                         missing_hierarchies.append(hierarchy_id)
                     else:
-                        member = ImportWebsiteToSDDModel.find_member_with_id_for_hierarchy(self,member_id,hierarchy,context)
+                        member = ImportWebsiteToSDDModel.find_member_with_id_for_hierarchy(
+                            self, member_id, hierarchy, context
+                        )
                         if member is None:
                             print(f"Member {member_id} not found in the database for hierarchy {hierarchy_id}")
-                            missing_members.append((hierarchy_id,member_id))
+                            missing_members.append((hierarchy_id, member_id))
                         else:
-                            parent_member = ImportWebsiteToSDDModel.find_member_with_id(self,parent_member_id,context)
+                            parent_member = ImportWebsiteToSDDModel.find_member_with_id(self, parent_member_id, context)
                             if not (parent_member is None):
                                 hierarchy_node = MEMBER_HIERARCHY_NODE()
                                 hierarchy_node.member_hierarchy_id = hierarchy
                                 hierarchy_node.comparator = comparator
                                 hierarchy_node.operator = operator
                                 hierarchy_node.member_id = member
                                 hierarchy_node.level = int(node_level)
                                 hierarchy_node.parent_member_id = parent_member
                                 nodes_to_create.append(hierarchy_node)
-                                context.member_hierarchy_node_dictionary[hierarchy_id + ":" + member_id] = hierarchy_node
+                                context.member_hierarchy_node_dictionary[hierarchy_id + ":" + member_id] = (
+                                    hierarchy_node
+                                )
 
         if context.save_sdd_to_db and nodes_to_create:
             MEMBER_HIERARCHY_NODE.objects.bulk_create(nodes_to_create, batch_size=1000, ignore_conflicts=True)
 
-        ImportWebsiteToSDDModel.save_missing_members_to_csv(context,missing_members)
-        ImportWebsiteToSDDModel.save_missing_hierarchies_to_csv(context,missing_hierarchies)
-
-    def save_missing_hierarchies_to_csv(context,missing_hierarchies):
-        filename = context.output_directory + os.sep + "generated_hierarchy_warnings" + os.sep + "missing_hierarchies_ancrdt.csv"
-        with open(filename, 'w', newline='') as csvfile:
+        ImportWebsiteToSDDModel.save_missing_members_to_csv(context, missing_members)
+        ImportWebsiteToSDDModel.save_missing_hierarchies_to_csv(context, missing_hierarchies)
+
+    def save_missing_hierarchies_to_csv(context, missing_hierarchies):
+        filename = (
+            context.output_directory
+            + os.sep
+            + "generated_hierarchy_warnings"
+            + os.sep
+            + "missing_hierarchies_ancrdt.csv"
+        )
+        with open(filename, "w", newline="") as csvfile:
             writer = csv.writer(csvfile)
             writer.writerows(missing_hierarchies)
 
-    def find_member_with_id_for_hierarchy(self,member_id,hierarchy,context):
+    def find_member_with_id_for_hierarchy(self, member_id, hierarchy, context):
         domain = hierarchy.domain_id
-        member = MEMBER.objects.filter(domain_id=domain,member_id=member_id).first()
+        member = MEMBER.objects.filter(domain_id=domain, member_id=member_id).first()
         return member
 
     def create_report_tables(self, context):
-        '''
+        """
         Import all tables from the rendering package CSV file using bulk create
-        '''
+        """
         file_location = self.base_path + os.sep + "table.csv"
         header_skipped = False
         tables_to_create = []
 
-        with open(file_location, encoding='utf-8') as csvfile:
-            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
+        with open(file_location, encoding="utf-8") as csvfile:
+            filereader = csv.reader(csvfile, delimiter=",", quotechar='"')
             for row in filereader:
                 if not header_skipped:
                     header_skipped = True
                 else:
                     table_id = row[ColumnIndexes().table_table_id]
@@ -543,36 +574,37 @@
                     maintenance_agency_id = row[ColumnIndexes().table_maintenance_agency_id]
                     version = row[ColumnIndexes().table_version]
                     valid_from = row[ColumnIndexes().table_valid_from]
                     valid_to = row[ColumnIndexes().table_valid_to]
 
-                    table = TABLE(
-                        name=ImportWebsiteToSDDModel.replace_dots(self, table_id))
+                    table = TABLE(name=ImportWebsiteToSDDModel.replace_dots(self, table_id))
                     table.table_id = ImportWebsiteToSDDModel.replace_dots(self, table_id)
                     table.name = display_name
                     table.code = code
                     table.description = description
-                    maintenance_agency = ImportWebsiteToSDDModel.find_maintenance_agency_with_id(self,context,maintenance_agency_id)
+                    maintenance_agency = ImportWebsiteToSDDModel.find_maintenance_agency_with_id(
+                        self, context, maintenance_agency_id
+                    )
                     table.maintenance_agency_id = maintenance_agency
                     table.version = version
 
                     tables_to_create.append(table)
                     context.report_tables_dictionary[table.table_id] = table
 
         if context.save_sdd_to_db and tables_to_create:
             TABLE.objects.bulk_create(tables_to_create, batch_size=1000, ignore_conflicts=True)
 
     def create_axis(self, context):
-        '''
+        """
         Import all axes from the rendering package CSV file using bulk create
-        '''
+        """
         file_location = self.base_path + os.sep + "axis.csv"
         header_skipped = False
         axes_to_create = []
 
-        with open(file_location, encoding='utf-8') as csvfile:
-            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
+        with open(file_location, encoding="utf-8") as csvfile:
+            filereader = csv.reader(csvfile, delimiter=",", quotechar='"')
             for row in filereader:
                 if not header_skipped:
                     header_skipped = True
                 else:
                     axis_id = row[ColumnIndexes().axis_id]
@@ -581,12 +613,11 @@
                     axis_name = row[ColumnIndexes().axis_name]
                     axis_description = row[ColumnIndexes().axis_description]
                     axis_table_id = row[ColumnIndexes().axis_table_id]
                     axis_is_open_axis = row[ColumnIndexes().axis_is_open_axis]
 
-                    axis = AXIS(
-                        name=ImportWebsiteToSDDModel.replace_dots(self, axis_id))
+                    axis = AXIS(name=ImportWebsiteToSDDModel.replace_dots(self, axis_id))
                     axis.axis_id = ImportWebsiteToSDDModel.replace_dots(self, axis_id)
                     axis.orientation = axis_orientation
                     axis.description = axis_description
                     axis.table_id = ImportWebsiteToSDDModel.find_table_with_id(self, context, axis_table_id)
 
@@ -595,19 +626,19 @@
 
         if context.save_sdd_to_db and axes_to_create:
             AXIS.objects.bulk_create(axes_to_create, batch_size=1000, ignore_conflicts=True)
 
     def create_axis_ordinates(self, context):
-        '''
+        """
         Import all axis ordinates from the rendering package CSV file using bulk create
-        '''
+        """
         file_location = self.base_path + os.sep + "axis_ordinate.csv"
         header_skipped = False
         ordinates_to_create = []
 
-        with open(file_location, encoding='utf-8') as csvfile:
-            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
+        with open(file_location, encoding="utf-8") as csvfile:
+            filereader = csv.reader(csvfile, delimiter=",", quotechar='"')
             for row in filereader:
                 if not header_skipped:
                     header_skipped = True
                 else:
                     axis_ordinate_id = row[ColumnIndexes().axis_ordinate_axis_ordinate_id]
@@ -618,35 +649,36 @@
                     axis_ordinate_axis_id = row[ColumnIndexes().axis_ordinate_axis_id]
                     axis_ordinate_parent_axis_ordinate_id = row[ColumnIndexes().axis_ordinate_parent_axis_ordinate_id]
                     axis_ordinate_name = row[ColumnIndexes().axis_ordinate_name]
                     axis_ordinate_description = row[ColumnIndexes().axis_ordinate_description]
 
-                    axis_ordinate = AXIS_ORDINATE(
-                        name=ImportWebsiteToSDDModel.replace_dots(self, axis_ordinate_id))
+                    axis_ordinate = AXIS_ORDINATE(name=ImportWebsiteToSDDModel.replace_dots(self, axis_ordinate_id))
                     axis_ordinate.axis_ordinate_id = ImportWebsiteToSDDModel.replace_dots(self, axis_ordinate_id)
                     axis_ordinate.code = axis_ordinate_code
                     axis_ordinate.path = axis_ordinate_path
-                    axis_ordinate.axis_id = ImportWebsiteToSDDModel.find_axis_with_id(self, context, ImportWebsiteToSDDModel.replace_dots(self,axis_ordinate_axis_id))
+                    axis_ordinate.axis_id = ImportWebsiteToSDDModel.find_axis_with_id(
+                        self, context, ImportWebsiteToSDDModel.replace_dots(self, axis_ordinate_axis_id)
+                    )
                     axis_ordinate.name = axis_ordinate_name
                     axis_ordinate.description = axis_ordinate_description
 
                     ordinates_to_create.append(axis_ordinate)
                     context.axis_ordinate_dictionary[axis_ordinate.axis_ordinate_id] = axis_ordinate
 
         if context.save_sdd_to_db and ordinates_to_create:
             AXIS_ORDINATE.objects.bulk_create(ordinates_to_create, batch_size=1000, ignore_conflicts=True)
 
     def create_ordinate_items(self, context):
-        '''
+        """
         Import all ordinate items from the rendering package CSV file
-        '''
+        """
         file_location = self.base_path + os.sep + "ordinate_item.csv"
         header_skipped = False
         ordinate_items_to_create = []
 
-        with open(file_location, encoding='utf-8') as csvfile:
-            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
+        with open(file_location, encoding="utf-8") as csvfile:
+            filereader = csv.reader(csvfile, delimiter=",", quotechar='"')
             for row in filereader:
                 if not header_skipped:
                     header_skipped = True
                 else:
                     axis_ordinate_id = row[ColumnIndexes().ordinate_item_axis_ordinate_id]
@@ -656,56 +688,65 @@
                     starting_member_id = row[ColumnIndexes().ordinate_item_starting_member_id]
                     is_starting_member_included = row[ColumnIndexes().ordinate_item_is_starting_member_included]
 
                     ordinate_item = ORDINATE_ITEM()
                     ordinate_item.axis_ordinate_id = ImportWebsiteToSDDModel.find_axis_ordinate_with_id(
-                        self, context, ImportWebsiteToSDDModel.replace_dots(self, axis_ordinate_id))
+                        self, context, ImportWebsiteToSDDModel.replace_dots(self, axis_ordinate_id)
+                    )
                     ordinate_item.variable_id = ImportWebsiteToSDDModel.find_variable_with_id(
-                        self, context, ImportWebsiteToSDDModel.replace_dots(self, variable_id))
+                        self, context, ImportWebsiteToSDDModel.replace_dots(self, variable_id)
+                    )
                     ordinate_item.member_id = ImportWebsiteToSDDModel.find_member_with_id(
-                        self, ImportWebsiteToSDDModel.replace_dots(self, member_id), context)
+                        self, ImportWebsiteToSDDModel.replace_dots(self, member_id), context
+                    )
                     ordinate_item.member_hierarchy_id = ImportWebsiteToSDDModel.find_member_hierarchy_with_id(
-                        self, ImportWebsiteToSDDModel.replace_dots(self, member_hierarchy_id), context)
+                        self, ImportWebsiteToSDDModel.replace_dots(self, member_hierarchy_id), context
+                    )
                     ordinate_item.starting_member_id = ImportWebsiteToSDDModel.find_member_with_id(
-                        self, ImportWebsiteToSDDModel.replace_dots(self, starting_member_id), context)
+                        self, ImportWebsiteToSDDModel.replace_dots(self, starting_member_id), context
+                    )
                     ordinate_item.is_starting_member_included = is_starting_member_included
 
                     ordinate_items_to_create.append(ordinate_item)
 
                     try:
-                        ordinate_items = context.axis_ordinate_to_ordinate_items_map[ordinate_item.axis_ordinate_id.axis_ordinate_id]
+                        ordinate_items = context.axis_ordinate_to_ordinate_items_map[
+                            ordinate_item.axis_ordinate_id.axis_ordinate_id
+                        ]
                         ordinate_items.append(ordinate_item)
                     except KeyError:
-                        context.axis_ordinate_to_ordinate_items_map[ordinate_item.axis_ordinate_id.axis_ordinate_id] = [ordinate_item]
+                        context.axis_ordinate_to_ordinate_items_map[ordinate_item.axis_ordinate_id.axis_ordinate_id] = [
+                            ordinate_item
+                        ]
 
         if context.save_sdd_to_db and ordinate_items_to_create:
             ORDINATE_ITEM.objects.bulk_create(ordinate_items_to_create, batch_size=1000, ignore_conflicts=True)
 
     def create_table_cells(self, context):
-        '''
+        """
         Import all table cells from the rendering package CSV file
-        '''
+        """
         file_location = self.base_path + os.sep + "table_cell.csv"
         header_skipped = False
         table_cells_to_create = []
 
-        with open(file_location, encoding='utf-8') as csvfile:
-            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
+        with open(file_location, encoding="utf-8") as csvfile:
+            filereader = csv.reader(csvfile, delimiter=",", quotechar='"')
             for row in filereader:
                 if not header_skipped:
                     header_skipped = True
                 else:
                     table_cell_cell_id = row[ColumnIndexes().table_cell_cell_id]
                     table_cell_combination_id = row[ColumnIndexes().table_cell_combination_id]
                     table_cell_table_id = row[ColumnIndexes().table_cell_table_id]
 
                     if table_cell_cell_id.endswith("_REF"):
-                        table_cell = TABLE_CELL(
-                            name=ImportWebsiteToSDDModel.replace_dots(self, table_cell_cell_id))
+                        table_cell = TABLE_CELL(name=ImportWebsiteToSDDModel.replace_dots(self, table_cell_cell_id))
                         table_cell.cell_id = ImportWebsiteToSDDModel.replace_dots(self, table_cell_cell_id)
                         table_cell.table_id = ImportWebsiteToSDDModel.find_table_with_id(
-                            self, context, ImportWebsiteToSDDModel.replace_dots(self, table_cell_table_id))
+                            self, context, ImportWebsiteToSDDModel.replace_dots(self, table_cell_table_id)
+                        )
                         table_cell.table_cell_combination_id = table_cell_combination_id
 
                         table_cells_to_create.append(table_cell)
                         context.table_cell_dictionary[table_cell.cell_id] = table_cell
 
@@ -714,51 +755,55 @@
 
         if context.save_sdd_to_db and table_cells_to_create:
             TABLE_CELL.objects.bulk_create(table_cells_to_create, batch_size=1000, ignore_conflicts=True)
 
     def create_cell_positions(self, context):
-        '''
+        """
         Import all cell positions from the rendering package CSV file
-        '''
+        """
         file_location = self.base_path + os.sep + "cell_position.csv"
         header_skipped = False
         cell_positions_to_create = []
 
-        with open(file_location, encoding='utf-8') as csvfile:
-            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
+        with open(file_location, encoding="utf-8") as csvfile:
+            filereader = csv.reader(csvfile, delimiter=",", quotechar='"')
             for row in filereader:
                 if not header_skipped:
                     header_skipped = True
                 else:
                     cell_positions_cell_id = row[ColumnIndexes().cell_positions_cell_id]
                     cell_positions_axis_ordinate_id = row[ColumnIndexes().cell_positions_axis_ordinate_id]
 
                     if cell_positions_cell_id.endswith("_REF"):
                         cell_position = CELL_POSITION()
                         cell_position.axis_ordinate_id = ImportWebsiteToSDDModel.find_axis_ordinate_with_id(
-                            self, context, ImportWebsiteToSDDModel.replace_dots(self, cell_positions_axis_ordinate_id))
+                            self, context, ImportWebsiteToSDDModel.replace_dots(self, cell_positions_axis_ordinate_id)
+                        )
                         cell_position.cell_id = ImportWebsiteToSDDModel.find_table_cell_with_id(
-                            self, context, ImportWebsiteToSDDModel.replace_dots(self, cell_positions_cell_id))
+                            self, context, ImportWebsiteToSDDModel.replace_dots(self, cell_positions_cell_id)
+                        )
 
                         cell_positions_to_create.append(cell_position)
 
-                        cell_positions_list = context.cell_positions_dictionary.setdefault(cell_position.cell_id.cell_id, [])
+                        cell_positions_list = context.cell_positions_dictionary.setdefault(
+                            cell_position.cell_id.cell_id, []
+                        )
                         cell_positions_list.append(cell_position)
 
         if context.save_sdd_to_db and cell_positions_to_create:
             CELL_POSITION.objects.bulk_create(cell_positions_to_create, batch_size=1000, ignore_conflicts=True)
 
     def create_member_mappings(self, context):
-        '''
+        """
         Import all member mappings from the rendering package CSV file
-        '''
+        """
         file_location = self.base_path + os.sep + "member_mapping.csv"
         header_skipped = False
         member_mappings_to_create = []
 
-        with open(file_location, encoding='utf-8') as csvfile:
-            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
+        with open(file_location, encoding="utf-8") as csvfile:
+            filereader = csv.reader(csvfile, delimiter=",", quotechar='"')
             for row in filereader:
                 if not header_skipped:
                     header_skipped = True
                 else:
                     maintenance_agency_id = row[ColumnIndexes().member_map_maintenance_agency_id]
@@ -769,179 +814,190 @@
                         member_mapping = MEMBER_MAPPING()
                         member_mapping.member_mapping_id = member_mapping_id
                         member_mapping.name = name
                         member_mapping.code = code
                         member_mapping.maintenance_agency_id = ImportWebsiteToSDDModel.find_maintenance_agency_with_id(
-                            self, context, maintenance_agency_id)
+                            self, context, maintenance_agency_id
+                        )
 
                         member_mappings_to_create.append(member_mapping)
                         context.member_mapping_dictionary[member_mapping_id] = member_mapping
 
         if context.save_sdd_to_db and member_mappings_to_create:
             MEMBER_MAPPING.objects.bulk_create(member_mappings_to_create, batch_size=1000, ignore_conflicts=True)
 
     def create_all_member_mappings_items(self, context):
-        '''
+        """
         Import all member mapping items from the rendering package CSV file using bulk create
-        '''
+        """
 
         file_location = self.base_path + os.sep + "member_mapping_item.csv"
         header_skipped = False
         missing_members = []
         missing_variables = []
         member_mapping_items_to_create = []
-        with open(file_location, encoding='utf-8') as csvfile:
-            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
+        with open(file_location, encoding="utf-8") as csvfile:
+            filereader = csv.reader(csvfile, delimiter=",", quotechar='"')
             for row in filereader:
                 if not header_skipped:
                     header_skipped = True
                 else:
                     member_mapping_id = row[ColumnIndexes().member_mapping_id]
                     row_number = row[ColumnIndexes().member_mapping_row]
                     variable_id = row[ColumnIndexes().member_mapping_variable_id]
                     is_source = row[ColumnIndexes().member_mapping_is_source]
                     member_id = row[ColumnIndexes().member_mapping_member_id]
                     if not member_mapping_id.startswith("SHS_"):
-                        member = ImportWebsiteToSDDModel.find_member_with_id(
-                                                            self,member_id,context)
-                        variable = ImportWebsiteToSDDModel.find_variable_with_id(
-                                                            self,context,variable_id)
+                        member = ImportWebsiteToSDDModel.find_member_with_id(self, member_id, context)
+                        variable = ImportWebsiteToSDDModel.find_variable_with_id(self, context, variable_id)
 
                         if member is None:
                             if member_id not in missing_members:
-                                missing_members.append((member_id,member_mapping_id,row_number,variable_id))
+                                missing_members.append((member_id, member_mapping_id, row_number, variable_id))
                         if variable is None:
                             if variable_id not in missing_variables:
-                                missing_variables.append((variable_id,'',''))
-
+                                missing_variables.append((variable_id, "", ""))
 
                         if member is None or variable is None:
                             pass
                         else:
                             member_mapping_item = MEMBER_MAPPING_ITEM()
                             member_mapping_item.is_source = is_source
                             member_mapping_item.member_id = member
                             member_mapping_item.variable_id = variable
                             member_mapping_item.member_mapping_row = row_number
-                            member_mapping_item.member_mapping_id  = ImportWebsiteToSDDModel.find_member_mapping_with_id(
-                                                self,context,member_mapping_id)
+                            member_mapping_item.member_mapping_id = ImportWebsiteToSDDModel.find_member_mapping_with_id(
+                                self, context, member_mapping_id
+                            )
 
                             if context.save_sdd_to_db:
                                 member_mapping_items_to_create.append(member_mapping_item)
                             try:
                                 member_mapping_items_list = context.member_mapping_items_dictionary[member_mapping_id]
                                 member_mapping_items_list.append(member_mapping_item)
                             except KeyError:
                                 context.member_mapping_items_dictionary[member_mapping_id] = [member_mapping_item]
         if context.save_sdd_to_db and member_mapping_items_to_create:
-            MEMBER_MAPPING_ITEM.objects.bulk_create(member_mapping_items_to_create, batch_size=1000, ignore_conflicts=True)
+            MEMBER_MAPPING_ITEM.objects.bulk_create(
+                member_mapping_items_to_create, batch_size=1000, ignore_conflicts=True
+            )
         for missing_member in missing_members:
             print(f"Missing member {missing_member}")
         for missing_variable in missing_variables:
             print(f"Missing variable {missing_variable}")
-        #ImportWebsiteToSDDModel.save_missing_mapping_variables_to_csv(context,missing_variables)
-        ImportWebsiteToSDDModel.save_missing_mapping_members_to_csv(context,missing_members)
-
-    def save_missing_mapping_variables_to_csv(context,missing_variables):
-        filename = context.output_directory + os.sep + "generated_mapping_warnings" + os.sep + "mappings_missing_variables.csv"
-        with open(filename, 'w', newline='') as csvfile:
+        # ImportWebsiteToSDDModel.save_missing_mapping_variables_to_csv(context,missing_variables)
+        ImportWebsiteToSDDModel.save_missing_mapping_members_to_csv(context, missing_members)
+
+    def save_missing_mapping_variables_to_csv(context, missing_variables):
+        filename = (
+            context.output_directory + os.sep + "generated_mapping_warnings" + os.sep + "mappings_missing_variables.csv"
+        )
+        with open(filename, "w", newline="") as csvfile:
             writer = csv.writer(csvfile)
-            writer.writerow(["Varaible","Mapping","Valid_to"])
+            writer.writerow(["Varaible", "Mapping", "Valid_to"])
             for var in missing_variables:
-                writer.writerow([var[0],var[1],var[2]])
-
-    def save_missing_mapping_members_to_csv(context,missing_members):
-        filename = context.output_directory + os.sep + "generated_mapping_warnings" + os.sep + "mappings_missing_members.csv"
-        with open(filename, 'w', newline='') as csvfile:
+                writer.writerow([var[0], var[1], var[2]])
+
+    def save_missing_mapping_members_to_csv(context, missing_members):
+        filename = (
+            context.output_directory + os.sep + "generated_mapping_warnings" + os.sep + "mappings_missing_members.csv"
+        )
+        with open(filename, "w", newline="") as csvfile:
             writer = csv.writer(csvfile)
-            writer.writerow(["Member","Mapping","Row","Variable"])
+            writer.writerow(["Member", "Mapping", "Row", "Variable"])
             for mem in missing_members:
-                writer.writerow([mem[0],mem[1],mem[2],mem[3]])
-
-        ImportWebsiteToSDDModel.create_mappings_warnings_summary(context,missing_members)
-
-    def create_mappings_warnings_summary(context,missing_members):
-        filename = context.output_directory + os.sep + "generated_mapping_warnings" + os.sep + "mappings_warnings_summary.csv"
-        #create a list of unique missing variable ids
+                writer.writerow([mem[0], mem[1], mem[2], mem[3]])
+
+        ImportWebsiteToSDDModel.create_mappings_warnings_summary(context, missing_members)
+
+    def create_mappings_warnings_summary(context, missing_members):
+        filename = (
+            context.output_directory + os.sep + "generated_mapping_warnings" + os.sep + "mappings_warnings_summary.csv"
+        )
+        # create a list of unique missing variable ids
         # read mappings_missing_variables file into a dictionary
-        missing_variables= []
+        missing_variables = []
         written_members = []
-        varaibles_filename = context.output_directory + os.sep + "generated_mapping_warnings" + os.sep + "mappings_missing_variables.csv"
-        with open(varaibles_filename, 'r', newline='') as csvfile:
+        varaibles_filename = (
+            context.output_directory + os.sep + "generated_mapping_warnings" + os.sep + "mappings_missing_variables.csv"
+        )
+        with open(varaibles_filename, "r", newline="") as csvfile:
             reader = csv.reader(csvfile)
             for row in reader:
                 if row[0] not in missing_variables:
                     missing_variables.append(row[0])
 
-        with open(filename, 'w', newline='') as csvfile:
+        with open(filename, "w", newline="") as csvfile:
             writer = csv.writer(csvfile)
-            writer.writerow(["Variable","Member"])
+            writer.writerow(["Variable", "Member"])
             for var in missing_variables:
-                writer.writerow([var,''])
+                writer.writerow([var, ""])
 
             for mem in missing_members:
                 variable = mem[3]
                 member = mem[0]
                 if member not in written_members:
                     if variable not in missing_variables:
-                        writer.writerow([variable,member])
+                        writer.writerow([variable, member])
                     written_members.append(member)
 
     def create_all_mapping_definitions(self, context):
-        '''
+        """
         Import all mapping definitions from the rendering package CSV file using bulk create
-        '''
+        """
         file_location = self.base_path + os.sep + "mapping_definition.csv"
         mapping_definitions_to_create = []
 
         # Cache lookups
         member_mapping_cache = {}
         variable_mapping_cache = {}
 
-        with open(file_location, encoding='utf-8') as csvfile:
+        with open(file_location, encoding="utf-8") as csvfile:
             rows = list(csv.reader(csvfile))[1:]  # Skip header
 
             for row in rows:
                 mapping_id = row[ColumnIndexes().mapping_definition_mapping_id]
                 if mapping_id.startswith("SHS_"):
                     continue
 
                 member_mapping_id = row[ColumnIndexes().mapping_definition_member_mapping_id]
                 if member_mapping_id not in member_mapping_cache:
                     member_mapping_cache[member_mapping_id] = ImportWebsiteToSDDModel.find_member_mapping_with_id(
-                        self, context, member_mapping_id)
+                        self, context, member_mapping_id
+                    )
 
                 variable_mapping_id = row[ColumnIndexes().mapping_definition_variable_mapping_id]
                 if variable_mapping_id not in variable_mapping_cache:
                     variable_mapping_cache[variable_mapping_id] = ImportWebsiteToSDDModel.find_variable_mapping_with_id(
-                        self, context, variable_mapping_id)
+                        self, context, variable_mapping_id
+                    )
 
                 mapping_definition = MAPPING_DEFINITION(
                     mapping_id=mapping_id,
                     name=row[ColumnIndexes().mapping_definition_name],
                     code=row[ColumnIndexes().mapping_definition_code],
                     mapping_type=row[ColumnIndexes().mapping_definition_mapping_type],
                     member_mapping_id=member_mapping_cache[member_mapping_id],
-                    variable_mapping_id=variable_mapping_cache[variable_mapping_id]
+                    variable_mapping_id=variable_mapping_cache[variable_mapping_id],
                 )
 
                 mapping_definitions_to_create.append(mapping_definition)
                 context.mapping_definition_dictionary[mapping_id] = mapping_definition
 
         if context.save_sdd_to_db and mapping_definitions_to_create:
             MAPPING_DEFINITION.objects.bulk_create(mapping_definitions_to_create, batch_size=5000)
 
     def create_all_mapping_to_cubes(self, context):
-        '''
+        """
         Import all mapping to cubes from the rendering package CSV file
-        '''
+        """
         file_location = self.base_path + os.sep + "mapping_to_cube.csv"
         header_skipped = False
         mapping_to_cubes_to_create = []
 
-        with open(file_location, encoding='utf-8') as csvfile:
-            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
+        with open(file_location, encoding="utf-8") as csvfile:
+            filereader = csv.reader(csvfile, delimiter=",", quotechar='"')
             for row in filereader:
                 if not header_skipped:
                     header_skipped = True
                 else:
                     mapping_to_cube_mapping_id = row[ColumnIndexes().mapping_to_cube_mapping_id]
@@ -949,70 +1005,77 @@
                     mapping_to_cube_valid_from = row[ColumnIndexes().mapping_to_cube_valid_from]
                     mapping_to_cube_valid_to = row[ColumnIndexes().mapping_to_cube_valid_to]
 
                     if not mapping_to_cube_mapping_id.startswith("M_SHS"):
                         mapping_to_cube = MAPPING_TO_CUBE(
-                            mapping_id=ImportWebsiteToSDDModel.find_mapping_definition_with_id(self, context, mapping_to_cube_mapping_id),
+                            mapping_id=ImportWebsiteToSDDModel.find_mapping_definition_with_id(
+                                self, context, mapping_to_cube_mapping_id
+                            ),
                             cube_mapping_id=ImportWebsiteToSDDModel.replace_dots(self, mapping_to_cube_cube_mapping_id),
                             valid_from=mapping_to_cube_valid_from,
-                            valid_to=mapping_to_cube_valid_to
+                            valid_to=mapping_to_cube_valid_to,
                         )
 
                         mapping_to_cubes_to_create.append(mapping_to_cube)
 
                         mapping_to_cube_list = context.mapping_to_cube_dictionary.setdefault(
-                            mapping_to_cube.cube_mapping_id, [])
+                            mapping_to_cube.cube_mapping_id, []
+                        )
                         mapping_to_cube_list.append(mapping_to_cube)
 
         if context.save_sdd_to_db and mapping_to_cubes_to_create:
             MAPPING_TO_CUBE.objects.bulk_create(mapping_to_cubes_to_create, batch_size=1000, ignore_conflicts=True)
 
     def create_all_variable_mappings(self, context):
-        '''
+        """
         Import all variable mappings from the rendering package CSV file
-        '''
+        """
         file_location = self.base_path + os.sep + "variable_mapping.csv"
 
         # Pre-filter SHS_ entries and build batch
         variable_mappings_to_create = []
 
         # Read entire CSV at once instead of line by line
-        with open(file_location, encoding='utf-8') as csvfile:
+        with open(file_location, encoding="utf-8") as csvfile:
             rows = list(csv.reader(csvfile))[1:]  # Skip header
 
             # Process in a single pass
             for row in rows:
                 variable_mapping_id = row[ColumnIndexes().variable_mapping_variable_mapping_id]
 
-                if not variable_mapping_id.startswith("SHS_") and variable_mapping_id not in context.variable_mapping_dictionary:
+                if (
+                    not variable_mapping_id.startswith("SHS_")
+                    and variable_mapping_id not in context.variable_mapping_dictionary
+                ):
                     variable_mapping = VARIABLE_MAPPING(
                         variable_mapping_id=variable_mapping_id,
                         maintenance_agency_id=ImportWebsiteToSDDModel.find_maintenance_agency_with_id(
-                            self, context, row[ColumnIndexes().variable_mapping_maintenance_agency_id]),
+                            self, context, row[ColumnIndexes().variable_mapping_maintenance_agency_id]
+                        ),
                         code=row[ColumnIndexes().variable_mapping_code],
-                        name=row[ColumnIndexes().variable_mapping_name]
+                        name=row[ColumnIndexes().variable_mapping_name],
                     )
 
                     variable_mappings_to_create.append(variable_mapping)
                     context.variable_mapping_dictionary[variable_mapping_id] = variable_mapping
 
         # Single bulk create with larger batch size
         if context.save_sdd_to_db and variable_mappings_to_create:
             VARIABLE_MAPPING.objects.bulk_create(variable_mappings_to_create, batch_size=5000)
 
     def create_all_variable_mapping_items(self, context):
-        '''
+        """
         Import all variable mapping items from the rendering package CSV file
-        '''
+        """
         file_location = self.base_path + os.sep + "variable_mapping_item.csv"
         missing_variables = []
         variable_mapping_items_to_create = []
 
         # Cache variable lookups
         variable_cache = {}
 
-        with open(file_location, encoding='utf-8') as csvfile:
+        with open(file_location, encoding="utf-8") as csvfile:
             rows = list(csv.reader(csvfile))[1:]  # Skip header
 
             for row in rows:
                 mapping_id = row[ColumnIndexes().varaible_mapping_item_variable_mapping_id]
                 if mapping_id.startswith("SHS_"):
@@ -1021,29 +1084,29 @@
                 variable_id = row[ColumnIndexes().variable_mapping_item_variable_id]
 
                 # Use cached variable lookup
                 if variable_id not in variable_cache:
                     variable_cache[variable_id] = ImportWebsiteToSDDModel.find_variable_with_id(
-                        self, context, variable_id)
+                        self, context, variable_id
+                    )
 
                 variable = variable_cache[variable_id]
 
                 if variable is None:
-                    missing_variables.append((
-                        variable_id,
-                        mapping_id,
-                        row[ColumnIndexes().variable_mapping_item_valid_to]
-                    ))
+                    missing_variables.append(
+                        (variable_id, mapping_id, row[ColumnIndexes().variable_mapping_item_valid_to])
+                    )
                     continue
 
                 variable_mapping_item = VARIABLE_MAPPING_ITEM(
                     variable_id=variable,
                     variable_mapping_id=ImportWebsiteToSDDModel.find_variable_mapping_with_id(
-                        self, context, mapping_id),
+                        self, context, mapping_id
+                    ),
                     is_source=row[ColumnIndexes().variable_mapping_item_is_source],
                     valid_from=row[ColumnIndexes().variable_mapping_item_valid_from],
-                    valid_to=row[ColumnIndexes().variable_mapping_item_valid_to]
+                    valid_to=row[ColumnIndexes().variable_mapping_item_valid_to],
                 )
 
                 variable_mapping_items_to_create.append(variable_mapping_item)
 
                 # Build dictionary in a single operation
@@ -1054,160 +1117,162 @@
             VARIABLE_MAPPING_ITEM.objects.bulk_create(variable_mapping_items_to_create, batch_size=5000)
 
         if missing_variables:
             ImportWebsiteToSDDModel.save_missing_mapping_variables_to_csv(context, missing_variables)
 
-    def find_member_mapping_with_id(self,context,member_mapping_id):
-        '''
+    def find_member_mapping_with_id(self, context, member_mapping_id):
+        """
         Find an existing member mapping with this id
-        '''
+        """
         try:
             return context.member_mapping_dictionary[member_mapping_id]
         except Exception:
             return None
 
-    def find_member_with_id(self,element_id,context):
-        '''
+    def find_member_with_id(self, element_id, context):
+        """
         Find an existing member with this id
-        '''
+        """
         try:
             return MEMBER.objects.get(member_id=element_id)
         except Exception:
             return None
 
-
-    def find_member_hierarchy_with_id(self,element_id,context):
-        '''
+    def find_member_hierarchy_with_id(self, element_id, context):
+        """
         Find an existing member hierarchy with this id
-        '''
+        """
         return context.member_hierarchy_dictionary[element_id]
 
-    def find_variable_with_id(self,context, element_id):
-        '''
+    def find_variable_with_id(self, context, element_id):
+        """
         Find an existing variable with this id
-        '''
+        """
         try:
             return VARIABLE.objects.get(variable_id=element_id)
         except Exception:
             return None
 
-    def find_maintenance_agency_with_id(self,context, element_id):
-        '''
+    def find_maintenance_agency_with_id(self, context, element_id):
+        """
         Find an existing maintenance agency with this id
-        '''
+        """
         return context.agency_dictionary[element_id]
 
-    def find_domain_with_id(self,context, element_id):
-        '''
+    def find_domain_with_id(self, context, element_id):
+        """
         Find an existing domain with this id
-        '''
+        """
         try:
-            return DOMAIN.objects.get(domain_id = element_id)
+            return DOMAIN.objects.get(domain_id=element_id)
         except Exception:
             return None
 
     def find_table_with_id(self, context, table_id):
-        '''
+        """
         Get the report table with the given id
-        '''
+        """
         try:
             return context.report_tables_dictionary[table_id]
         except Exception:
             return None
 
     def find_axis_with_id(self, context, axis_id):
-        '''
+        """
         Get the axis with the given id
-        '''
+        """
         try:
             return context.axis_dictionary[axis_id]
         except Exception:
             return None
 
     def find_table_cell_with_id(self, context, table_cell_id):
-        '''
+        """
         Get the table cell with the given id
-        '''
+        """
         try:
             return context.table_cell_dictionary[table_cell_id]
         except Exception:
             return None
 
     def find_axis_ordinate_with_id(self, context, axis_ordinate_id):
-        '''
+        """
         Get the existing ordinate with the given id
-        '''
+        """
         try:
             return context.axis_ordinate_dictionary[axis_ordinate_id]
         except KeyError:
             return None
 
     def replace_dots(self, text):
-        '''
+        """
         Replace dots with underscores in the given text
-        '''
-        return text.replace('.', '_')
+        """
+        return text.replace(".", "_")
 
     def find_variable_mapping_with_id(self, context, variable_mapping_id):
-        '''
+        """
         Get the variable mapping with the given id
-        '''
+        """
         try:
             return context.variable_mapping_dictionary[variable_mapping_id]
         except KeyError:
             return None
 
     def find_mapping_definition_with_id(self, context, mapping_definition_id):
-        '''
+        """
         get the mapping definition with the given id
-        '''
+        """
         try:
             return context.mapping_definition_dictionary[mapping_definition_id]
         except KeyError:
             return None
 
-
     def delete_hierarchy_warnings_files(self, context):
-        '''
+        """
         Delete warning files more efficiently using pathlib
-        '''
-        warnings_dir = Path(settings.BASE_DIR) / 'results' / 'generated_hierarchy_warnings'
-        for file in warnings_dir.glob('*'):
+        """
+        warnings_dir = Path(settings.BASE_DIR) / "results" / "generated_hierarchy_warnings"
+        for file in warnings_dir.glob("*"):
             file.unlink()
 
     def delete_mapping_warnings_files(self, context):
         base_dir = settings.BASE_DIR
-        mapping_warnings_dir = os.path.join(base_dir, 'results', 'generated_mapping_warnings')
+        mapping_warnings_dir = os.path.join(base_dir, "results", "generated_mapping_warnings")
         for file in os.listdir(mapping_warnings_dir):
             os.remove(os.path.join(mapping_warnings_dir, file))
 
     # Do not know if would be useful apart from importing anacredit
     def create_all_structures(self, context):
-        '''
+        """
         Import all cube structures from CSV file using bulk create
-        '''
+        """
         # Import cubes
 
         # Import cube structures
         file_location = self.base_path + os.sep + "cube_structure.csv"
         header_skipped = False
         structures_to_create = []
 
-        with open(file_location, encoding='utf-8') as csvfile:
-            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
+        with open(file_location, encoding="utf-8") as csvfile:
+            filereader = csv.reader(csvfile, delimiter=",", quotechar='"')
             for row in filereader:
                 if not header_skipped:
                     header_skipped = True
                 else:
                     structure_id = row[ColumnIndexes().cube_structure_id_index]
                     maintenence_agency = row[ColumnIndexes().cube_structure_maintenance_agency]
                     name = row[ColumnIndexes().cube_structure_name_index]
                     description = row[ColumnIndexes().cube_structure_description_index]
 
-                    if not CUBE_STRUCTURE.objects.filter(cube_structure_id=ImportWebsiteToSDDModel.replace_dots(self, structure_id)).exists():
+                    if not CUBE_STRUCTURE.objects.filter(
+                        cube_structure_id=ImportWebsiteToSDDModel.replace_dots(self, structure_id)
+                    ).exists():
                         structure = CUBE_STRUCTURE(name=ImportWebsiteToSDDModel.replace_dots(self, structure_id))
-                        maintenance_agency_id = ImportWebsiteToSDDModel.find_maintenance_agency_with_id(self,context,maintenence_agency)
+                        maintenance_agency_id = ImportWebsiteToSDDModel.find_maintenance_agency_with_id(
+                            self, context, maintenence_agency
+                        )
                         structure.cube_structure_id = ImportWebsiteToSDDModel.replace_dots(self, structure_id)
                         structure.name = name
                         structure.description = description
                         structure.maintenance_agency_id = maintenance_agency_id
 
@@ -1223,12 +1288,12 @@
         items_to_create = []
         csi_creation_failed = set()
 
         csi_counter = dict()
 
-        with open(file_location, encoding='utf-8') as csvfile:
-            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
+        with open(file_location, encoding="utf-8") as csvfile:
+            filereader = csv.reader(csvfile, delimiter=",", quotechar='"')
             for row in filereader:
                 if not header_skipped:
                     header_skipped = True
                 else:
                     structure_id = row[ColumnIndexes().sdd_cube_structure_item_cube_structure_id]
@@ -1239,42 +1304,44 @@
                     subdomain_id = row[ColumnIndexes().sdd_cube_structure_item_subdomain_id]
                     cube_variable_code = row[ColumnIndexes().sdd_cube_structure_item_cube_variable_code]
 
                     # check for subdomains instead.
 
-                    #if not CUBE_STRUCTURE_ITEM.objects.filter(cube_variable_code = cube_variable_code).exists():
+                    # if not CUBE_STRUCTURE_ITEM.objects.filter(cube_variable_code = cube_variable_code).exists():
                     item = CUBE_STRUCTURE_ITEM()
-                    item.cube_structure_id = CUBE_STRUCTURE.objects.get(cube_structure_id = structure_id)
-                    item.variable_id = ImportWebsiteToSDDModel.find_variable_with_id(self,context,dimension_id)
-                    item.member_id = ImportWebsiteToSDDModel.find_member_with_id(self,member_id,context)
+                    item.cube_structure_id = CUBE_STRUCTURE.objects.get(cube_structure_id=structure_id)
+                    item.variable_id = ImportWebsiteToSDDModel.find_variable_with_id(self, context, dimension_id)
+                    item.member_id = ImportWebsiteToSDDModel.find_member_with_id(self, member_id, context)
                     item.role = role
                     item.order = order
-                    if (item.cube_structure_id,item.variable_id) not in csi_counter:
-                        csi_counter[(item.cube_structure_id,item.variable_id)] = 0
-                    item.cube_variable_code = cube_variable_code or "__".join([
-                        item.cube_structure_id,
-                        item.variable_id,
-                        str(csi_counter[(item.cube_structure_id,item.variable_id)])
-                    ]
-                    )
-                    csi_counter[(item.cube_structure_id,item.variable_id)] += 1
+                    if (item.cube_structure_id, item.variable_id) not in csi_counter:
+                        csi_counter[(item.cube_structure_id, item.variable_id)] = 0
+                    item.cube_variable_code = cube_variable_code or "__".join(
+                        [
+                            item.cube_structure_id,
+                            item.variable_id,
+                            str(csi_counter[(item.cube_structure_id, item.variable_id)]),
+                        ]
+                    )
+                    csi_counter[(item.cube_structure_id, item.variable_id)] += 1
                     if subdomain_id:
                         item.subdomain_id = SUBDOMAIN.objects.get(subdomain_id=subdomain_id)
 
-                    if not item.variable_id: csi_creation_failed.add(dimension_id)
+                    if not item.variable_id:
+                        csi_creation_failed.add(dimension_id)
                     items_to_create.append(item)
 
         if context.save_sdd_to_db and items_to_create:
             CUBE_STRUCTURE_ITEM.objects.bulk_create(items_to_create, batch_size=1000, ignore_conflicts=True)
 
         # Import cubes
         file_location = self.base_path + os.sep + "cube.csv"
         header_skipped = False
         cubes_to_create = []
 
-        with open(file_location, encoding='utf-8') as csvfile:
-            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
+        with open(file_location, encoding="utf-8") as csvfile:
+            filereader = csv.reader(csvfile, delimiter=",", quotechar='"')
             for row in filereader:
                 if not header_skipped:
                     header_skipped = True
                 else:
                     cube_id = row[ColumnIndexes().cube_object_id_index]
@@ -1285,33 +1352,35 @@
                     description = row[ColumnIndexes().cube_cube_structure_id_index]
                     cube_structure_id = row[ColumnIndexes().cube_cube_structure_id_index]
 
                     if not CUBE.objects.filter(cube_id=ImportWebsiteToSDDModel.replace_dots(self, cube_id)).exists():
                         cube = CUBE(name=ImportWebsiteToSDDModel.replace_dots(self, cube_id))
-                        maintenance_agency_id = ImportWebsiteToSDDModel.find_maintenance_agency_with_id(self,context,maintenence_agency)
+                        maintenance_agency_id = ImportWebsiteToSDDModel.find_maintenance_agency_with_id(
+                            self, context, maintenence_agency
+                        )
                         cube.code = code
                         cube.cube_id = ImportWebsiteToSDDModel.replace_dots(self, cube_id)
                         cube.name = name
                         cube.description = description
                         cube.maintenance_agency_id = maintenance_agency_id
-                        cube.cube_structure_id = CUBE_STRUCTURE.objects.get(cube_structure_id = cube_structure_id)
+                        cube.cube_structure_id = CUBE_STRUCTURE.objects.get(cube_structure_id=cube_structure_id)
 
                         cubes_to_create.append(cube)
                         context.cube_dictionary[cube.cube_id] = cube
 
         if context.save_sdd_to_db and cubes_to_create:
             CUBE.objects.bulk_create(cubes_to_create, batch_size=1000, ignore_conflicts=True)
 
-    def create_all_subdomains(self,sdd_context):
-        '''
+    def create_all_subdomains(self, sdd_context):
+        """
         Import all subdomains and subdomain enumerations from CSV files
-        '''
+        """
         # Import subdomains
         file_location = self.base_path + os.sep + "subdomain.csv"
         subdomains_to_create = []
 
-        with open(file_location, encoding='utf-8') as csvfile:
+        with open(file_location, encoding="utf-8") as csvfile:
             rows = list(csv.reader(csvfile))[1:]  # Skip header
 
             for row in rows:
                 subdomain_id = row[ColumnIndexes().sdd_subdomain_subdomain_id]
                 domain_id = row[ColumnIndexes().sdd_subdomain_domain_id_id]
@@ -1321,12 +1390,14 @@
 
                 if not SUBDOMAIN.objects.filter(subdomain_id=subdomain_id).exists():
                     subdomain = SUBDOMAIN(
                         subdomain_id=subdomain_id,
                         name=name,
-                        domain_id=ImportWebsiteToSDDModel.find_domain_with_id(self,sdd_context,domain_id),
-                        maintenance_agency_id=ImportWebsiteToSDDModel.find_maintenance_agency_with_id(self,sdd_context,maintenance_agency)
+                        domain_id=ImportWebsiteToSDDModel.find_domain_with_id(self, sdd_context, domain_id),
+                        maintenance_agency_id=ImportWebsiteToSDDModel.find_maintenance_agency_with_id(
+                            self, sdd_context, maintenance_agency
+                        ),
                     )
 
                     subdomains_to_create.append(subdomain)
                     sdd_context.subdomain_dictionary[subdomain_id] = subdomain
 
@@ -1335,32 +1406,39 @@
 
         # Import subdomain enumerations
         file_location = self.base_path + os.sep + "subdomain_enumeration.csv"
         enumerations_to_create = []
 
-        with open(file_location, encoding='utf-8') as csvfile:
+        with open(file_location, encoding="utf-8") as csvfile:
             rows = list(csv.reader(csvfile))[1:]  # Skip header
 
             for row in rows:
                 subdomain_id = row[ColumnIndexes().sdd_subdomain_enumeration_subdomain_id_id]
                 member_id = row[ColumnIndexes().sdd_subdomain_enumeration_member_id_id]
                 order = row[ColumnIndexes().sdd_subdomain_enumeration_order]
                 valid_from = row[ColumnIndexes().sdd_subdomain_enumeration_valid_from]
                 valid_to = row[ColumnIndexes().sdd_subdomain_enumeration_valid_to]
 
-                member = ImportWebsiteToSDDModel.find_member_with_id(self,member_id,sdd_context)
-
-                if member and not SUBDOMAIN_ENUMERATION.objects.filter(
-                        subdomain_id=sdd_context.subdomain_dictionary.get(subdomain_id,SUBDOMAIN.objects.get(subdomain_id=subdomain_id)),
-                        member_id=member
-                    ).exists():
+                member = ImportWebsiteToSDDModel.find_member_with_id(self, member_id, sdd_context)
+
+                if (
+                    member
+                    and not SUBDOMAIN_ENUMERATION.objects.filter(
+                        subdomain_id=sdd_context.subdomain_dictionary.get(
+                            subdomain_id, SUBDOMAIN.objects.get(subdomain_id=subdomain_id)
+                        ),
+                        member_id=member,
+                    ).exists()
+                ):
                     enumeration = SUBDOMAIN_ENUMERATION(
-                        subdomain_id=sdd_context.subdomain_dictionary.get(subdomain_id,SUBDOMAIN.objects.get(subdomain_id=subdomain_id)),
+                        subdomain_id=sdd_context.subdomain_dictionary.get(
+                            subdomain_id, SUBDOMAIN.objects.get(subdomain_id=subdomain_id)
+                        ),
                         member_id=member,
                         order=order,
                         valid_from=valid_from,
-                        valid_to=valid_to
+                        valid_to=valid_to,
                     )
 
                     enumerations_to_create.append(enumeration)
 
                     if subdomain_id not in sdd_context.subdomain_enumeration_dictionary:
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/sqldeveloper_import/import_sqldev_ldm_to_regdna.py	2025-09-15 13:18:11.387880+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/sqldeveloper_import/import_sqldev_ldm_to_regdna.py	2025-09-21 17:07:37.168193+00:00
@@ -18,24 +18,25 @@
 from pybirdai.regdna import ELEnumLiteral, ELReference
 from pybirdai.regdna import ELAnnotation, ELStringToStringMapEntry
 from pybirdai.process_steps.generate_test_data.enrich_ldm_with_il_links_from_fe import InputLayerLinkEnricher
 from pybirdai.process_steps.generate_test_data.traverser import SubtypeExploder
 from pybirdai.process_steps.generate_etl.generate_etl import GenerateETL
-from pybirdai.regdna import  ELPackage, ModuleList, GenerationRulesModule, ReportModule, ELAnnotationDirective
+from pybirdai.regdna import ELPackage, ModuleList, GenerationRulesModule, ReportModule, ELAnnotationDirective
 from pybirdai.context.ecore_lite_types import EcoreLiteTypes
 
+
 class SQLDevLDMImport(object):
-    '''
+    """
     Documentation for SQLDevLDMImport
-    '''
+    """
 
     def do_import(self, context):
-        '''
+        """
         import the items from the BIRD LDM csv files
 
 
-    '''
+        """
 
         SQLDevLDMImport.import_classification_types(self, context)
         SQLDevLDMImport.add_ldm_classes_to_package(self, context)
         SQLDevLDMImport.import_disjoint_subtyping_information(self, context)
         SQLDevLDMImport.set_ldm_super_classes(self, context)
@@ -48,230 +49,229 @@
         SQLDevLDMImport.remove_duplicate_attributes_in_subclasses(self, context)
         SQLDevLDMImport.mark_root_class_as_entity_group_annotation(self, context)
         if context.generate_etl:
             SQLDevLDMImport.generate_etl(self, context)
 
-
     def generate_etl(self, context):
         InputLayerLinkEnricher().enrich_with_links_to_input_layer_columns(context)
 
-        csv_dir = context.output_directory + os.sep + 'csv'
+        csv_dir = context.output_directory + os.sep + "csv"
         for file in os.listdir(csv_dir):
             file_path = os.path.join(csv_dir, file)
             if os.path.isfile(file_path):
                 os.remove(file_path)
         traverser = SubtypeExploder()
 
-        traverser.traverse(context,'SCRTY_EXCHNG_TRDBL_DRVTV',False)
-        traverser.traverse(context,'SCRTY_EXCHNG_TRDBL_DRVTV',True)
-        traverser.traverse(context,'CRDT_FCLTY',False)
-        traverser.traverse(context,'CRDT_FCLTY',True)
-        traverser.traverse(context,'CRDT_FCLTY_ENTTY_RL_ASSGNMNT',False)
-        traverser.traverse(context,'CRDT_FCLTY_ENTTY_RL_ASSGNMNT',True)
-        traverser.traverse(context,'INSTRMNT',True)
-        traverser.traverse(context,'INSTRMNT',False)
-        traverser.traverse(context,'INSTRMNT_RL',False)
-        traverser.traverse(context,'INSTRMNT_RL',True)
-        traverser.traverse(context,'NN_FNNCL_ASST_NN_FNNCL_LBLTY',False)
-        traverser.traverse(context,'NN_FNNCL_ASST_NN_FNNCL_LBLTY',True)
-        traverser.traverse(context,'CSH_HND',False)
-        traverser.traverse(context,'CSH_HND',True)
-        traverser.traverse(context,'PRTY',False)
-        traverser.traverse(context,'PRTY',True)
-        traverser.traverse(context,'SCRTY_EXCHNG_TRDBL_DRVTV_PSTN',False)
-        traverser.traverse(context,'SCRTY_EXCHNG_TRDBL_DRVTV_PSTN',True)
-        traverser.traverse(context,'LNG_SCRTY_PSTN_PRDNTL_PRTFL_ASSGNMNT',False)
-        traverser.traverse(context,'LNG_SCRTY_PSTN_PRDNTL_PRTFL_ASSGNMNT',True)
-        traverser.traverse(context,'LNG_SCRTY_PSTN_PRDNTL_PRTFL_ASSGNMNT_ACCNTNG_CLSSFCTN_FNNCL_ASSTS_ASSGNMNT',False)
-        traverser.traverse(context,'LNG_SCRTY_PSTN_PRDNTL_PRTFL_ASSGNMNT_ACCNTNG_CLSSFCTN_FNNCL_ASSTS_ASSGNMNT',True)
-        traverser.traverse(context,'CLLTRL',False)
-        traverser.traverse(context,'CLLTRL',True)
-        traverser.traverse(context,'INSTRMNT_ENTTY_RL_ASSGNMNT',False)
-        traverser.traverse(context,'INSTRMNT_ENTTY_RL_ASSGNMNT',True)
-        traverser.traverse(context,'FNNCL_CNTRCT',False)
-        traverser.traverse(context,'FNNCL_CNTRCT',True)
-        traverser.traverse(context,'ENTTY_RL',False)
-        traverser.traverse(context,'ENTTY_RL',True)
-        traverser.traverse(context,'FNDMNTL_RVW_TRDNG_BK_STNDRD_APPRCH_RSK_MSR',False)
-        traverser.traverse(context,'FNDMNTL_RVW_TRDNG_BK_STNDRD_APPRCH_RSK_MSR',True)
-        traverser.traverse(context,'RSK_FAC_SA',False)
-        traverser.traverse(context,'RSK_FAC_SA',True)
+        traverser.traverse(context, "SCRTY_EXCHNG_TRDBL_DRVTV", False)
+        traverser.traverse(context, "SCRTY_EXCHNG_TRDBL_DRVTV", True)
+        traverser.traverse(context, "CRDT_FCLTY", False)
+        traverser.traverse(context, "CRDT_FCLTY", True)
+        traverser.traverse(context, "CRDT_FCLTY_ENTTY_RL_ASSGNMNT", False)
+        traverser.traverse(context, "CRDT_FCLTY_ENTTY_RL_ASSGNMNT", True)
+        traverser.traverse(context, "INSTRMNT", True)
+        traverser.traverse(context, "INSTRMNT", False)
+        traverser.traverse(context, "INSTRMNT_RL", False)
+        traverser.traverse(context, "INSTRMNT_RL", True)
+        traverser.traverse(context, "NN_FNNCL_ASST_NN_FNNCL_LBLTY", False)
+        traverser.traverse(context, "NN_FNNCL_ASST_NN_FNNCL_LBLTY", True)
+        traverser.traverse(context, "CSH_HND", False)
+        traverser.traverse(context, "CSH_HND", True)
+        traverser.traverse(context, "PRTY", False)
+        traverser.traverse(context, "PRTY", True)
+        traverser.traverse(context, "SCRTY_EXCHNG_TRDBL_DRVTV_PSTN", False)
+        traverser.traverse(context, "SCRTY_EXCHNG_TRDBL_DRVTV_PSTN", True)
+        traverser.traverse(context, "LNG_SCRTY_PSTN_PRDNTL_PRTFL_ASSGNMNT", False)
+        traverser.traverse(context, "LNG_SCRTY_PSTN_PRDNTL_PRTFL_ASSGNMNT", True)
+        traverser.traverse(context, "LNG_SCRTY_PSTN_PRDNTL_PRTFL_ASSGNMNT_ACCNTNG_CLSSFCTN_FNNCL_ASSTS_ASSGNMNT", False)
+        traverser.traverse(context, "LNG_SCRTY_PSTN_PRDNTL_PRTFL_ASSGNMNT_ACCNTNG_CLSSFCTN_FNNCL_ASSTS_ASSGNMNT", True)
+        traverser.traverse(context, "CLLTRL", False)
+        traverser.traverse(context, "CLLTRL", True)
+        traverser.traverse(context, "INSTRMNT_ENTTY_RL_ASSGNMNT", False)
+        traverser.traverse(context, "INSTRMNT_ENTTY_RL_ASSGNMNT", True)
+        traverser.traverse(context, "FNNCL_CNTRCT", False)
+        traverser.traverse(context, "FNNCL_CNTRCT", True)
+        traverser.traverse(context, "ENTTY_RL", False)
+        traverser.traverse(context, "ENTTY_RL", True)
+        traverser.traverse(context, "FNDMNTL_RVW_TRDNG_BK_STNDRD_APPRCH_RSK_MSR", False)
+        traverser.traverse(context, "FNDMNTL_RVW_TRDNG_BK_STNDRD_APPRCH_RSK_MSR", True)
+        traverser.traverse(context, "RSK_FAC_SA", False)
+        traverser.traverse(context, "RSK_FAC_SA", True)
         # Adding traversal for remaining tables
-        traverser.traverse(context,'ASST_PL',False)
-        traverser.traverse(context,'ASST_PL',True)
-        traverser.traverse(context,'ASST_PL_DBT_SCRTY_PSTN_ASSGNMNT',False)
-        traverser.traverse(context,'ASST_PL_DBT_SCRTY_PSTN_ASSGNMNT',True)
-        traverser.traverse(context,'ASST_PL_EQT_INSTRMNT_NT_SCRT_ASSGNMNT',False)
-        traverser.traverse(context,'ASST_PL_EQT_INSTRMNT_NT_SCRT_ASSGNMNT',True)
-        traverser.traverse(context,'ASST_PL_LN_ASSGNMNT',False)
-        traverser.traverse(context,'ASST_PL_LN_ASSGNMNT',True)
-        traverser.traverse(context,'BLNC_SHT_NTTNG',False)
-        traverser.traverse(context,'BLNC_SHT_NTTNG',True)
-        traverser.traverse(context,'CLLTRL_NN_FNNCL_ASST_ASSGNMNT',False)
-        traverser.traverse(context,'CLLTRL_NN_FNNCL_ASST_ASSGNMNT',True)
-        traverser.traverse(context,'CRDT_FCLTY_CLLTRL_ASSGNMNT',False)
-        traverser.traverse(context,'CRDT_FCLTY_CLLTRL_ASSGNMNT',True)
-        traverser.traverse(context,'CRDT_FCLTY_CLLTRL_RCVD_INSTRMNT_ASSGNMNT',False)
-        traverser.traverse(context,'CRDT_FCLTY_CLLTRL_RCVD_INSTRMNT_ASSGNMNT',True)
-        traverser.traverse(context,'CRDT_RSK_MTGTN_ASSGNMNT',False)
-        traverser.traverse(context,'CRDT_RSK_MTGTN_ASSGNMNT',True)
-        traverser.traverse(context,'CRDT_TRNSFR_OTHR_SCRTSTN_CVRD_BND_PRGRM',False)
-        traverser.traverse(context,'CRDT_TRNSFR_OTHR_SCRTSTN_CVRD_BND_PRGRM',True)
-        traverser.traverse(context,'CVRD_BND_ISSNC',False)
-        traverser.traverse(context,'CVRD_BND_ISSNC',True)
-        traverser.traverse(context,'CVRD_BND_PRGRM',False)
-        traverser.traverse(context,'CVRD_BND_PRGRM',True)
-        traverser.traverse(context,'DBT_SCRTY_ISSD',False)
-        traverser.traverse(context,'DBT_SCRTY_ISSD',True)
-        traverser.traverse(context,'DBT_SCRTY_ISSD_TRDTNL_SCRTSTN_ASSGNMNT',False)
-        traverser.traverse(context,'DBT_SCRTY_ISSD_TRDTNL_SCRTSTN_ASSGNMNT',True)
-        traverser.traverse(context,'EQT_INSTRMNT_LG_EQT_INSTRMNT_NT_SCRT_ASSGNMNT',False)
-        traverser.traverse(context,'EQT_INSTRMNT_LG_EQT_INSTRMNT_NT_SCRT_ASSGNMNT',True)
-        traverser.traverse(context,'ETD_LBLTY_PSTN_SNTHTC_SCRTSTN_ASSGNMNT',False)
-        traverser.traverse(context,'ETD_LBLTY_PSTN_SNTHTC_SCRTSTN_ASSGNMNT',True)
-        traverser.traverse(context,'EXCHNG_TRDBL_DRVTV_PSTN',False)
-        traverser.traverse(context,'EXCHNG_TRDBL_DRVTV_PSTN',True)
-        traverser.traverse(context,'FNDMNTL_RVW_TRDNG_BK_STNDRD_APPRCH_RSK_MSR_ETD_PSTNS',False)
-        traverser.traverse(context,'FNDMNTL_RVW_TRDNG_BK_STNDRD_APPRCH_RSK_MSR_ETD_PSTNS',True)
-        traverser.traverse(context,'FNDMNTL_RVW_TRDNG_BK_STNDRD_APPRCH_RSK_MSR_FR_SCRTY_PSTNS',False)
-        traverser.traverse(context,'FNDMNTL_RVW_TRDNG_BK_STNDRD_APPRCH_RSK_MSR_FR_SCRTY_PSTNS',True)
-        traverser.traverse(context,'FNDMNTL_RVW_TRDNG_BK_STNDRD_APPRCH_RSK_MSR_OTC_PSTNS',False)
-        traverser.traverse(context,'FNDMNTL_RVW_TRDNG_BK_STNDRD_APPRCH_RSK_MSR_OTC_PSTNS',True)
-        traverser.traverse(context,'FNNCL_GRNT_INSTRMNT_DBT_SCRT_DBT_SCRTY_ASSGNMNT',False)
-        traverser.traverse(context,'FNNCL_GRNT_INSTRMNT_DBT_SCRT_DBT_SCRTY_ASSGNMNT',True)
-        traverser.traverse(context,'GRP',False)
-        traverser.traverse(context,'GRP',True)
-        traverser.traverse(context,'GRP_CLNTS_KY_MNGMNT_PRSNLL_ASSGNMNT',False)
-        traverser.traverse(context,'GRP_CLNTS_KY_MNGMNT_PRSNLL_ASSGNMNT',True)
-        traverser.traverse(context,'IMMDT_PRNT_ENTRPRS_ASSGNMNT',False)
-        traverser.traverse(context,'IMMDT_PRNT_ENTRPRS_ASSGNMNT',True)
-        traverser.traverse(context,'INSTRMNT_CLLTRL_RCVD_INSTRMNT_ASSGNMNT',False)
-        traverser.traverse(context,'INSTRMNT_CLLTRL_RCVD_INSTRMNT_ASSGNMNT',True)
-        traverser.traverse(context,'INSTRMNT_HDGD_EXCHNG_TRDBL_DRVTV',False)
-        traverser.traverse(context,'INSTRMNT_HDGD_EXCHNG_TRDBL_DRVTV',True)
-        traverser.traverse(context,'INSTRMNT_HDGD_OTC_DRVTV',False)
-        traverser.traverse(context,'INSTRMNT_HDGD_OTC_DRVTV',True)
-        traverser.traverse(context,'INSTRMNT_PRTCN_ARRNGMNT_ASSGNMNT',False)
-        traverser.traverse(context,'INSTRMNT_PRTCN_ARRNGMNT_ASSGNMNT',True)
-        traverser.traverse(context,'INTRNL_GRP_KY_MNGMNT_PRSNLL_ASSGNMNT',False)
-        traverser.traverse(context,'INTRNL_GRP_KY_MNGMNT_PRSNLL_ASSGNMNT',True)
-        traverser.traverse(context,'INTRNL_GRP_RL',False)
-        traverser.traverse(context,'INTRNL_GRP_RL',True)
-        traverser.traverse(context,'INTRST_RT_RSK_HDG_PRTFL',False)
-        traverser.traverse(context,'INTRST_RT_RSK_HDG_PRTFL',True)
-        traverser.traverse(context,'KB_PR_BCKT',False)
-        traverser.traverse(context,'KB_PR_BCKT',True)
-        traverser.traverse(context,'LN_AND_ADVNC_LG_LN_AND_ADVNC_ASSGNMNT',False)
-        traverser.traverse(context,'LN_AND_ADVNC_LG_LN_AND_ADVNC_ASSGNMNT',True)
-        traverser.traverse(context,'LN_EXCLDNG_RPRCHS_AGRMNT_CLLTRL_ASSGNMNT',False)
-        traverser.traverse(context,'LN_EXCLDNG_RPRCHS_AGRMNT_CLLTRL_ASSGNMNT',True)
-        traverser.traverse(context,'LNG_NN_NGTBL_SCRTY_PSTN_CLLTRL_ASSGNMNT',False)
-        traverser.traverse(context,'LNG_NN_NGTBL_SCRTY_PSTN_CLLTRL_ASSGNMNT',True)
-        traverser.traverse(context,'LNG_SCRTY_PSTN_PRDNTL_PRTFL_ASSGNMNT_RSK_DT',False)
-        traverser.traverse(context,'LNG_SCRTY_PSTN_PRDNTL_PRTFL_ASSGNMNT_RSK_DT',True)
-        traverser.traverse(context,'LNKD_ENTRPRS_ASSGNMNT',False)
-        traverser.traverse(context,'LNKD_ENTRPRS_ASSGNMNT',True)
-        traverser.traverse(context,'MSTR_AGRMNT',False)
-        traverser.traverse(context,'MSTR_AGRMNT',True)
-        traverser.traverse(context,'MSTR_AGRMNT_ENTTY_RL_ASSGNMNT',False)
-        traverser.traverse(context,'MSTR_AGRMNT_ENTTY_RL_ASSGNMNT',True)
-        traverser.traverse(context,'MSTR_AGRMNT_FNNCL_CNTRCT_ASSGNMNT',False)
-        traverser.traverse(context,'MSTR_AGRMNT_FNNCL_CNTRCT_ASSGNMNT',True)
-        traverser.traverse(context,'NN_FNNCL_ASST',False)
-        traverser.traverse(context,'NN_FNNCL_ASST',True)
-        traverser.traverse(context,'NN_FNNCL_LBLTY',False)
-        traverser.traverse(context,'NN_FNNCL_LBLTY',True)
-        traverser.traverse(context,'NTRL_PRSN_KY_MNGMNT_PRSNLL_ASSGNMNT',False)
-        traverser.traverse(context,'NTRL_PRSN_KY_MNGMNT_PRSNLL_ASSGNMNT',True)
-        traverser.traverse(context,'OFF_BLNC_INSTRMNT_CLLTRL_ASSGNMNT',False)
-        traverser.traverse(context,'OFF_BLNC_INSTRMNT_CLLTRL_ASSGNMNT',True)
-        traverser.traverse(context,'OTC_DRVTV_INSTRMNT_SNTHTC_SCRTSTN_ASSGNMNT',False)
-        traverser.traverse(context,'OTC_DRVTV_INSTRMNT_SNTHTC_SCRTSTN_ASSGNMNT',True)
-        traverser.traverse(context,'OTHR_PRTY_ID',False)
-        traverser.traverse(context,'OTHR_PRTY_ID',True)
-        traverser.traverse(context,'PRTCTN_ARRNGMNT',False)
-        traverser.traverse(context,'PRTCTN_ARRNGMNT',True)
-        traverser.traverse(context,'PRTCTN_PRTCTN_PRVD_ASSGNMNT',False)
-        traverser.traverse(context,'PRTCTN_PRTCTN_PRVD_ASSGNMNT',True)
-        traverser.traverse(context,'PRTNR_ENTRPRS_ASSGNMNT',False)
-        traverser.traverse(context,'PRTNR_ENTRPRS_ASSGNMNT',True)
-        traverser.traverse(context,'PRTY_CD',False)
-        traverser.traverse(context,'PRTY_CD',True)
-        traverser.traverse(context,'PRTY_PRVS_PRD_DT',False)
-        traverser.traverse(context,'PRTY_PRVS_PRD_DT',True)
-        traverser.traverse(context,'RPRCHS_AGRMNT_CMPNNT',False)
-        traverser.traverse(context,'RPRCHS_AGRMNT_CMPNNT',True)
-        traverser.traverse(context,'RTNG_AGNCY',False)
-        traverser.traverse(context,'RTNG_AGNCY',True)
-        traverser.traverse(context,'RTNG_GRD',False)
-        traverser.traverse(context,'RTNG_GRD',True)
-        traverser.traverse(context,'RTNG_GRD_CNTRY_ASSGNMNT',False)
-        traverser.traverse(context,'RTNG_GRD_CNTRY_ASSGNMNT',True)
-        traverser.traverse(context,'RTNG_GRD_ISS_BSD_RTNG_SSTM_DBT_SCRTY_ASSGNMNT',False)
-        traverser.traverse(context,'RTNG_GRD_ISS_BSD_RTNG_SSTM_DBT_SCRTY_ASSGNMNT',True)
-        traverser.traverse(context,'RTNG_SYSTM',False)
-        traverser.traverse(context,'RTNG_SYSTM',True)
-        traverser.traverse(context,'RTNG_SYSTM_APPLD_LGL_PRSN',False)
-        traverser.traverse(context,'RTNG_SYSTM_APPLD_LGL_PRSN',True)
-        traverser.traverse(context,'SBSDRY_JNT_VNTR_ASSCT_OTHR_ORGNSTN_ASSGNMNT',False)
-        traverser.traverse(context,'SBSDRY_JNT_VNTR_ASSCT_OTHR_ORGNSTN_ASSGNMNT',True)
-        traverser.traverse(context,'SCRTY_ENTTY_RL_ASSGNMNT',False)
-        traverser.traverse(context,'SCRTY_ENTTY_RL_ASSGNMNT',True)
-        traverser.traverse(context,'SCRTY_HDGD_EXCHNG_TRDBL_DRVTV',False)
-        traverser.traverse(context,'SCRTY_HDGD_EXCHNG_TRDBL_DRVTV',True)
-        traverser.traverse(context,'SCRTY_PSTN',False)
-        traverser.traverse(context,'SCRTY_PSTN',True)
-        traverser.traverse(context,'SCRTY_PSTN_HDGD_OTC_DRVTV',False)
-        traverser.traverse(context,'SCRTY_PSTN_HDGD_OTC_DRVTV',True)
-        traverser.traverse(context,'SCRTY_SCRTY_RPRCHS_AGRMNT_CMPNNT_ASSGNMNT',False)
-        traverser.traverse(context,'SCRTY_SCRTY_RPRCHS_AGRMNT_CMPNNT_ASSGNMNT',True)
-        traverser.traverse(context,'SCTRY_BRRWNG_LNDNG_TRNSCTN_INCLDNG_CSH_CLLTRL',False)
-        traverser.traverse(context,'SCTRY_BRRWNG_LNDNG_TRNSCTN_INCLDNG_CSH_CLLTRL',True)
-        traverser.traverse(context,'SHRT_SCRTY_PSTN_PRDNTL_PRTFL_ASSGNMNT',False)
-        traverser.traverse(context,'SHRT_SCRTY_PSTN_PRDNTL_PRTFL_ASSGNMNT',True)
-        traverser.traverse(context,'SNTHTC_SCRTSTN',False)
-        traverser.traverse(context,'SNTHTC_SCRTSTN',True)
-        traverser.traverse(context,'SYNDCTD_CNTRCT',False)
-        traverser.traverse(context,'SYNDCTD_CNTRCT',True)
-        traverser.traverse(context,'TRDTNL_SCRTSTN',False)
-        traverser.traverse(context,'TRDTNL_SCRTSTN',True)
-        traverser.traverse(context,'TRNCH_SYNTHTC_SCRTSTN_WTHT_SSPE_DPST',False)
-        traverser.traverse(context,'TRNCH_SYNTHTC_SCRTSTN_WTHT_SSPE_DPST',True)
-        traverser.traverse(context,'TRNCH_SYNTHTC_SCRTSTN_WTHT_SSPE_FNNCL_GRNT',False)
-        traverser.traverse(context,'TRNCH_SYNTHTC_SCRTSTN_WTHT_SSPE_FNNCL_GRNT',True)
-        traverser.traverse(context,'TRNCH_TRDTNL_SCRTSTN',False)
-        traverser.traverse(context,'TRNCH_TRDTNL_SCRTSTN',True)
+        traverser.traverse(context, "ASST_PL", False)
+        traverser.traverse(context, "ASST_PL", True)
+        traverser.traverse(context, "ASST_PL_DBT_SCRTY_PSTN_ASSGNMNT", False)
+        traverser.traverse(context, "ASST_PL_DBT_SCRTY_PSTN_ASSGNMNT", True)
+        traverser.traverse(context, "ASST_PL_EQT_INSTRMNT_NT_SCRT_ASSGNMNT", False)
+        traverser.traverse(context, "ASST_PL_EQT_INSTRMNT_NT_SCRT_ASSGNMNT", True)
+        traverser.traverse(context, "ASST_PL_LN_ASSGNMNT", False)
+        traverser.traverse(context, "ASST_PL_LN_ASSGNMNT", True)
+        traverser.traverse(context, "BLNC_SHT_NTTNG", False)
+        traverser.traverse(context, "BLNC_SHT_NTTNG", True)
+        traverser.traverse(context, "CLLTRL_NN_FNNCL_ASST_ASSGNMNT", False)
+        traverser.traverse(context, "CLLTRL_NN_FNNCL_ASST_ASSGNMNT", True)
+        traverser.traverse(context, "CRDT_FCLTY_CLLTRL_ASSGNMNT", False)
+        traverser.traverse(context, "CRDT_FCLTY_CLLTRL_ASSGNMNT", True)
+        traverser.traverse(context, "CRDT_FCLTY_CLLTRL_RCVD_INSTRMNT_ASSGNMNT", False)
+        traverser.traverse(context, "CRDT_FCLTY_CLLTRL_RCVD_INSTRMNT_ASSGNMNT", True)
+        traverser.traverse(context, "CRDT_RSK_MTGTN_ASSGNMNT", False)
+        traverser.traverse(context, "CRDT_RSK_MTGTN_ASSGNMNT", True)
+        traverser.traverse(context, "CRDT_TRNSFR_OTHR_SCRTSTN_CVRD_BND_PRGRM", False)
+        traverser.traverse(context, "CRDT_TRNSFR_OTHR_SCRTSTN_CVRD_BND_PRGRM", True)
+        traverser.traverse(context, "CVRD_BND_ISSNC", False)
+        traverser.traverse(context, "CVRD_BND_ISSNC", True)
+        traverser.traverse(context, "CVRD_BND_PRGRM", False)
+        traverser.traverse(context, "CVRD_BND_PRGRM", True)
+        traverser.traverse(context, "DBT_SCRTY_ISSD", False)
+        traverser.traverse(context, "DBT_SCRTY_ISSD", True)
+        traverser.traverse(context, "DBT_SCRTY_ISSD_TRDTNL_SCRTSTN_ASSGNMNT", False)
+        traverser.traverse(context, "DBT_SCRTY_ISSD_TRDTNL_SCRTSTN_ASSGNMNT", True)
+        traverser.traverse(context, "EQT_INSTRMNT_LG_EQT_INSTRMNT_NT_SCRT_ASSGNMNT", False)
+        traverser.traverse(context, "EQT_INSTRMNT_LG_EQT_INSTRMNT_NT_SCRT_ASSGNMNT", True)
+        traverser.traverse(context, "ETD_LBLTY_PSTN_SNTHTC_SCRTSTN_ASSGNMNT", False)
+        traverser.traverse(context, "ETD_LBLTY_PSTN_SNTHTC_SCRTSTN_ASSGNMNT", True)
+        traverser.traverse(context, "EXCHNG_TRDBL_DRVTV_PSTN", False)
+        traverser.traverse(context, "EXCHNG_TRDBL_DRVTV_PSTN", True)
+        traverser.traverse(context, "FNDMNTL_RVW_TRDNG_BK_STNDRD_APPRCH_RSK_MSR_ETD_PSTNS", False)
+        traverser.traverse(context, "FNDMNTL_RVW_TRDNG_BK_STNDRD_APPRCH_RSK_MSR_ETD_PSTNS", True)
+        traverser.traverse(context, "FNDMNTL_RVW_TRDNG_BK_STNDRD_APPRCH_RSK_MSR_FR_SCRTY_PSTNS", False)
+        traverser.traverse(context, "FNDMNTL_RVW_TRDNG_BK_STNDRD_APPRCH_RSK_MSR_FR_SCRTY_PSTNS", True)
+        traverser.traverse(context, "FNDMNTL_RVW_TRDNG_BK_STNDRD_APPRCH_RSK_MSR_OTC_PSTNS", False)
+        traverser.traverse(context, "FNDMNTL_RVW_TRDNG_BK_STNDRD_APPRCH_RSK_MSR_OTC_PSTNS", True)
+        traverser.traverse(context, "FNNCL_GRNT_INSTRMNT_DBT_SCRT_DBT_SCRTY_ASSGNMNT", False)
+        traverser.traverse(context, "FNNCL_GRNT_INSTRMNT_DBT_SCRT_DBT_SCRTY_ASSGNMNT", True)
+        traverser.traverse(context, "GRP", False)
+        traverser.traverse(context, "GRP", True)
+        traverser.traverse(context, "GRP_CLNTS_KY_MNGMNT_PRSNLL_ASSGNMNT", False)
+        traverser.traverse(context, "GRP_CLNTS_KY_MNGMNT_PRSNLL_ASSGNMNT", True)
+        traverser.traverse(context, "IMMDT_PRNT_ENTRPRS_ASSGNMNT", False)
+        traverser.traverse(context, "IMMDT_PRNT_ENTRPRS_ASSGNMNT", True)
+        traverser.traverse(context, "INSTRMNT_CLLTRL_RCVD_INSTRMNT_ASSGNMNT", False)
+        traverser.traverse(context, "INSTRMNT_CLLTRL_RCVD_INSTRMNT_ASSGNMNT", True)
+        traverser.traverse(context, "INSTRMNT_HDGD_EXCHNG_TRDBL_DRVTV", False)
+        traverser.traverse(context, "INSTRMNT_HDGD_EXCHNG_TRDBL_DRVTV", True)
+        traverser.traverse(context, "INSTRMNT_HDGD_OTC_DRVTV", False)
+        traverser.traverse(context, "INSTRMNT_HDGD_OTC_DRVTV", True)
+        traverser.traverse(context, "INSTRMNT_PRTCN_ARRNGMNT_ASSGNMNT", False)
+        traverser.traverse(context, "INSTRMNT_PRTCN_ARRNGMNT_ASSGNMNT", True)
+        traverser.traverse(context, "INTRNL_GRP_KY_MNGMNT_PRSNLL_ASSGNMNT", False)
+        traverser.traverse(context, "INTRNL_GRP_KY_MNGMNT_PRSNLL_ASSGNMNT", True)
+        traverser.traverse(context, "INTRNL_GRP_RL", False)
+        traverser.traverse(context, "INTRNL_GRP_RL", True)
+        traverser.traverse(context, "INTRST_RT_RSK_HDG_PRTFL", False)
+        traverser.traverse(context, "INTRST_RT_RSK_HDG_PRTFL", True)
+        traverser.traverse(context, "KB_PR_BCKT", False)
+        traverser.traverse(context, "KB_PR_BCKT", True)
+        traverser.traverse(context, "LN_AND_ADVNC_LG_LN_AND_ADVNC_ASSGNMNT", False)
+        traverser.traverse(context, "LN_AND_ADVNC_LG_LN_AND_ADVNC_ASSGNMNT", True)
+        traverser.traverse(context, "LN_EXCLDNG_RPRCHS_AGRMNT_CLLTRL_ASSGNMNT", False)
+        traverser.traverse(context, "LN_EXCLDNG_RPRCHS_AGRMNT_CLLTRL_ASSGNMNT", True)
+        traverser.traverse(context, "LNG_NN_NGTBL_SCRTY_PSTN_CLLTRL_ASSGNMNT", False)
+        traverser.traverse(context, "LNG_NN_NGTBL_SCRTY_PSTN_CLLTRL_ASSGNMNT", True)
+        traverser.traverse(context, "LNG_SCRTY_PSTN_PRDNTL_PRTFL_ASSGNMNT_RSK_DT", False)
+        traverser.traverse(context, "LNG_SCRTY_PSTN_PRDNTL_PRTFL_ASSGNMNT_RSK_DT", True)
+        traverser.traverse(context, "LNKD_ENTRPRS_ASSGNMNT", False)
+        traverser.traverse(context, "LNKD_ENTRPRS_ASSGNMNT", True)
+        traverser.traverse(context, "MSTR_AGRMNT", False)
+        traverser.traverse(context, "MSTR_AGRMNT", True)
+        traverser.traverse(context, "MSTR_AGRMNT_ENTTY_RL_ASSGNMNT", False)
+        traverser.traverse(context, "MSTR_AGRMNT_ENTTY_RL_ASSGNMNT", True)
+        traverser.traverse(context, "MSTR_AGRMNT_FNNCL_CNTRCT_ASSGNMNT", False)
+        traverser.traverse(context, "MSTR_AGRMNT_FNNCL_CNTRCT_ASSGNMNT", True)
+        traverser.traverse(context, "NN_FNNCL_ASST", False)
+        traverser.traverse(context, "NN_FNNCL_ASST", True)
+        traverser.traverse(context, "NN_FNNCL_LBLTY", False)
+        traverser.traverse(context, "NN_FNNCL_LBLTY", True)
+        traverser.traverse(context, "NTRL_PRSN_KY_MNGMNT_PRSNLL_ASSGNMNT", False)
+        traverser.traverse(context, "NTRL_PRSN_KY_MNGMNT_PRSNLL_ASSGNMNT", True)
+        traverser.traverse(context, "OFF_BLNC_INSTRMNT_CLLTRL_ASSGNMNT", False)
+        traverser.traverse(context, "OFF_BLNC_INSTRMNT_CLLTRL_ASSGNMNT", True)
+        traverser.traverse(context, "OTC_DRVTV_INSTRMNT_SNTHTC_SCRTSTN_ASSGNMNT", False)
+        traverser.traverse(context, "OTC_DRVTV_INSTRMNT_SNTHTC_SCRTSTN_ASSGNMNT", True)
+        traverser.traverse(context, "OTHR_PRTY_ID", False)
+        traverser.traverse(context, "OTHR_PRTY_ID", True)
+        traverser.traverse(context, "PRTCTN_ARRNGMNT", False)
+        traverser.traverse(context, "PRTCTN_ARRNGMNT", True)
+        traverser.traverse(context, "PRTCTN_PRTCTN_PRVD_ASSGNMNT", False)
+        traverser.traverse(context, "PRTCTN_PRTCTN_PRVD_ASSGNMNT", True)
+        traverser.traverse(context, "PRTNR_ENTRPRS_ASSGNMNT", False)
+        traverser.traverse(context, "PRTNR_ENTRPRS_ASSGNMNT", True)
+        traverser.traverse(context, "PRTY_CD", False)
+        traverser.traverse(context, "PRTY_CD", True)
+        traverser.traverse(context, "PRTY_PRVS_PRD_DT", False)
+        traverser.traverse(context, "PRTY_PRVS_PRD_DT", True)
+        traverser.traverse(context, "RPRCHS_AGRMNT_CMPNNT", False)
+        traverser.traverse(context, "RPRCHS_AGRMNT_CMPNNT", True)
+        traverser.traverse(context, "RTNG_AGNCY", False)
+        traverser.traverse(context, "RTNG_AGNCY", True)
+        traverser.traverse(context, "RTNG_GRD", False)
+        traverser.traverse(context, "RTNG_GRD", True)
+        traverser.traverse(context, "RTNG_GRD_CNTRY_ASSGNMNT", False)
+        traverser.traverse(context, "RTNG_GRD_CNTRY_ASSGNMNT", True)
+        traverser.traverse(context, "RTNG_GRD_ISS_BSD_RTNG_SSTM_DBT_SCRTY_ASSGNMNT", False)
+        traverser.traverse(context, "RTNG_GRD_ISS_BSD_RTNG_SSTM_DBT_SCRTY_ASSGNMNT", True)
+        traverser.traverse(context, "RTNG_SYSTM", False)
+        traverser.traverse(context, "RTNG_SYSTM", True)
+        traverser.traverse(context, "RTNG_SYSTM_APPLD_LGL_PRSN", False)
+        traverser.traverse(context, "RTNG_SYSTM_APPLD_LGL_PRSN", True)
+        traverser.traverse(context, "SBSDRY_JNT_VNTR_ASSCT_OTHR_ORGNSTN_ASSGNMNT", False)
+        traverser.traverse(context, "SBSDRY_JNT_VNTR_ASSCT_OTHR_ORGNSTN_ASSGNMNT", True)
+        traverser.traverse(context, "SCRTY_ENTTY_RL_ASSGNMNT", False)
+        traverser.traverse(context, "SCRTY_ENTTY_RL_ASSGNMNT", True)
+        traverser.traverse(context, "SCRTY_HDGD_EXCHNG_TRDBL_DRVTV", False)
+        traverser.traverse(context, "SCRTY_HDGD_EXCHNG_TRDBL_DRVTV", True)
+        traverser.traverse(context, "SCRTY_PSTN", False)
+        traverser.traverse(context, "SCRTY_PSTN", True)
+        traverser.traverse(context, "SCRTY_PSTN_HDGD_OTC_DRVTV", False)
+        traverser.traverse(context, "SCRTY_PSTN_HDGD_OTC_DRVTV", True)
+        traverser.traverse(context, "SCRTY_SCRTY_RPRCHS_AGRMNT_CMPNNT_ASSGNMNT", False)
+        traverser.traverse(context, "SCRTY_SCRTY_RPRCHS_AGRMNT_CMPNNT_ASSGNMNT", True)
+        traverser.traverse(context, "SCTRY_BRRWNG_LNDNG_TRNSCTN_INCLDNG_CSH_CLLTRL", False)
+        traverser.traverse(context, "SCTRY_BRRWNG_LNDNG_TRNSCTN_INCLDNG_CSH_CLLTRL", True)
+        traverser.traverse(context, "SHRT_SCRTY_PSTN_PRDNTL_PRTFL_ASSGNMNT", False)
+        traverser.traverse(context, "SHRT_SCRTY_PSTN_PRDNTL_PRTFL_ASSGNMNT", True)
+        traverser.traverse(context, "SNTHTC_SCRTSTN", False)
+        traverser.traverse(context, "SNTHTC_SCRTSTN", True)
+        traverser.traverse(context, "SYNDCTD_CNTRCT", False)
+        traverser.traverse(context, "SYNDCTD_CNTRCT", True)
+        traverser.traverse(context, "TRDTNL_SCRTSTN", False)
+        traverser.traverse(context, "TRDTNL_SCRTSTN", True)
+        traverser.traverse(context, "TRNCH_SYNTHTC_SCRTSTN_WTHT_SSPE_DPST", False)
+        traverser.traverse(context, "TRNCH_SYNTHTC_SCRTSTN_WTHT_SSPE_DPST", True)
+        traverser.traverse(context, "TRNCH_SYNTHTC_SCRTSTN_WTHT_SSPE_FNNCL_GRNT", False)
+        traverser.traverse(context, "TRNCH_SYNTHTC_SCRTSTN_WTHT_SSPE_FNNCL_GRNT", True)
+        traverser.traverse(context, "TRNCH_TRDTNL_SCRTSTN", False)
+        traverser.traverse(context, "TRNCH_TRDTNL_SCRTSTN", True)
         dbt_generator = GenerateETL()
-        dbt_generator.create_etl_guide(os.path.join(context.output_directory, 'csv'), 'ldm_sql.sql', context)
+        dbt_generator.create_etl_guide(os.path.join(context.output_directory, "csv"), "ldm_sql.sql", context)
 
     def import_classification_types(self, context):
-        '''
+        """
         for each classification type in the LDM, create a class and add it to the package
-        '''
-        file_location = context.file_directory + os.sep + 'ldm' + os.sep + "DM_Classification_Types.csv"
+        """
+        file_location = context.file_directory + os.sep + "ldm" + os.sep + "DM_Classification_Types.csv"
         header_skipped = False
-        with open(file_location,  encoding='utf-8') as csvfile:
-            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
+        with open(file_location, encoding="utf-8") as csvfile:
+            filereader = csv.reader(csvfile, delimiter=",", quotechar='"')
             for row in filereader:
                 if not header_skipped:
                     header_skipped = True
                 else:
                     classification_type_id = row[0]
                     classification_type_name = row[1]
                     context.classification_types[classification_type_id] = classification_type_name
 
     def add_ldm_classes_to_package(self, context):
-        '''
+        """
         for each entity in the LDM, create a class and add it to the package
-        '''
-        file_location = context.file_directory + os.sep + 'ldm' + os.sep + "DM_Entities.csv"
+        """
+        file_location = context.file_directory + os.sep + "ldm" + os.sep + "DM_Entities.csv"
 
         header_skipped = False
         # Load all the entities from the csv file, make an ELClass per entity,
         # and add the ELClass to the package
-        with open(file_location,  encoding='utf-8') as csvfile:
-            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
+        with open(file_location, encoding="utf-8") as csvfile:
+            filereader = csv.reader(csvfile, delimiter=",", quotechar='"')
             for row in filereader:
                 # skip the first line which is the header.
                 if not header_skipped:
                     header_skipped = True
                 else:
@@ -282,28 +282,27 @@
                     num_supertype_entity_id = row[26]
                     preferred_abbreviation = row[24]
                     class_name = preferred_abbreviation
                     classification_type = row[10]
                     process_class = True
-                    if context.skip_reference_data_in_ldm and \
-                        (context.classification_types.get(classification_type) == "Reference data"):
-                        if not(object_id in context.reference_data_class_list):
+                    if context.skip_reference_data_in_ldm and (
+                        context.classification_types.get(classification_type) == "Reference data"
+                    ):
+                        if not (object_id in context.reference_data_class_list):
                             context.reference_data_class_list.append(object_id)
                         process_class = False
                     if process_class:
                         altered_class_name = Utils.make_valid_id(class_name)
                         eclass = ELClass(name=altered_class_name)
                         eclass.original_name = entity_name
 
-
-                        context.ldm_entities_package.eClassifiers.extend([
-                                                                            eclass])
+                        context.ldm_entities_package.eClassifiers.extend([eclass])
                         the_long_name_annotation = ELAnnotation()
                         the_long_name_directive = Utils.get_annotation_directive(eclass.eContainer(), "long_name")
                         the_long_name_annotation.source = the_long_name_directive
                         details = the_long_name_annotation.details
-                        mapentry  = ELStringToStringMapEntry()
+                        mapentry = ELStringToStringMapEntry()
                         mapentry.key = "long_name"
                         mapentry.value = Utils.make_valid_id(entity_name)
                         details.append(mapentry)
                         eclass.eAnnotations.append(the_long_name_annotation)
 
@@ -322,26 +321,26 @@
 
                         # maintain a map a objectIDs to ELClasses
                         context.classes_map[object_id] = eclass
 
     def import_disjoint_subtyping_information(self, context):
-        '''
+        """
         we first find out which arcs are 'single arcs' , single arcs
         are ones where an entity has only only one arc and not more than 1.
         for each single arc, create a class.
         for each arc store its source in a dictionary
         for each arc target store a link from target to the arcs class
         later we will set supertypes of the targets to be the arcs class
         later we will set the arc to be a contained class of the source
-        '''
+        """
         file_location = context.file_directory + os.sep + "ldm" + os.sep + "arcs.csv"
         header_skipped = False
         # A dictionary from entity to its arcs
-        entity_to_arc_dictionary = SQLDevLDMImport.get_entity_to_arc_dictionary(self,context,file_location)
-
-        with open(file_location,  encoding='utf-8') as csvfile:
-            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
+        entity_to_arc_dictionary = SQLDevLDMImport.get_entity_to_arc_dictionary(self, context, file_location)
+
+        with open(file_location, encoding="utf-8") as csvfile:
+            filereader = csv.reader(csvfile, delimiter=",", quotechar='"')
             for row in filereader:
                 # skip the first line which is the header.
                 if not header_skipped:
                     header_skipped = True
                 else:
@@ -351,11 +350,11 @@
                     target_entity_name = row[3]
 
                     altered_arc_name = Utils.make_valid_id(arc_name)
                     num_of_siblings_arcs = 0
                     try:
-                        num_of_siblings_arcs = len(entity_to_arc_dictionary[entity_name]) -1
+                        num_of_siblings_arcs = len(entity_to_arc_dictionary[entity_name]) - 1
                     except KeyError:
                         pass
 
                     if num_of_siblings_arcs > 0:
                         arc_class = None
@@ -367,11 +366,13 @@
                             # and we create class for the arc
                             print("altered_arc_name")
                             print(altered_arc_name)
                             arc_class = ELClass(name=altered_arc_name)
                             arc_class.eAbstract = True
-                            source_class = SQLDevLDMImport.find_class_with_long_name(self, context, Utils.make_valid_id(entity_name))
+                            source_class = SQLDevLDMImport.find_class_with_long_name(
+                                self, context, Utils.make_valid_id(entity_name)
+                            )
                             # It can be the case that the source class has not yet been created
                             # because it is a reference data class and we are skipping reference data
                             # in the LDM
                             if source_class is not None:
 
@@ -392,47 +393,51 @@
                                 attribute.iD = True
                                 attribute.lowerBound = 0
                                 attribute.upperBound = 1
                                 the_reference_annotation = ELAnnotation()
 
-                                the_reference_annotation_directive = Utils.get_annotation_directive(source_class.eContainer(), "relationship_type")
+                                the_reference_annotation_directive = Utils.get_annotation_directive(
+                                    source_class.eContainer(), "relationship_type"
+                                )
                                 the_reference_annotation.source = the_reference_annotation_directive
                                 details = the_reference_annotation.details
-                                mapentry  = ELStringToStringMapEntry()
+                                mapentry = ELStringToStringMapEntry()
                                 mapentry.key = "is_identifying_relationship"
                                 mapentry.value = "true"
                                 details.append(mapentry)
                                 non_containment_reference.eAnnotations.append(the_reference_annotation)
 
                                 arc_class.eStructuralFeatures.append(attribute)
                                 the_identified_class_annotation = ELAnnotation()
-                                the_identified_class_directive = Utils.get_annotation_directive(source_class.eContainer(), "relationship_type")
+                                the_identified_class_directive = Utils.get_annotation_directive(
+                                    source_class.eContainer(), "relationship_type"
+                                )
                                 the_identified_class_annotation.source = the_identified_class_directive
                                 details = the_identified_class_annotation.details
-                                mapentry  = ELStringToStringMapEntry()
+                                mapentry = ELStringToStringMapEntry()
                                 mapentry.key = "is_identified_by"
                                 mapentry.value = source_class.name + "." + non_containment_reference.name
                                 details.append(mapentry)
                                 arc_class.eAnnotations.append(the_identified_class_annotation)
 
-                                source_class.eStructuralFeatures.append(
-                                    non_containment_reference)
-
-
-                        target_class = SQLDevLDMImport.find_class_with_long_name(self, context,Utils.make_valid_id(target_entity_name))
+                                source_class.eStructuralFeatures.append(non_containment_reference)
+
+                        target_class = SQLDevLDMImport.find_class_with_long_name(
+                            self, context, Utils.make_valid_id(target_entity_name)
+                        )
                         # It can be the case that the target class has not yet been created
                         # because it is a reference data class and we are skipping reference data
                         # in the LDM
                         if target_class is not None:
                             context.arc_target_to_arc_map[Utils.make_valid_id(target_entity_name)] = target_class
                             target_class.eSuperTypes.extend([arc_class])
 
-    def get_entity_to_arc_dictionary(self,context,file_location) :
+    def get_entity_to_arc_dictionary(self, context, file_location):
         entity_to_arc_dictionary = {}
         header_skipped = False
-        with open(file_location,  encoding='utf-8') as csvfile:
-            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
+        with open(file_location, encoding="utf-8") as csvfile:
+            filereader = csv.reader(csvfile, delimiter=",", quotechar='"')
             for row in filereader:
                 # skip the first line which is the header.
                 if not header_skipped:
                     header_skipped = True
                 else:
@@ -454,140 +459,137 @@
 
             context.entity_to_arc_dictionary = entity_to_arc_dictionary
             return entity_to_arc_dictionary
 
     def find_class_with_name(self, context, name):
-        '''
+        """
         get the class with this name from the input tables package
-        '''
+        """
         for eclassifier in context.ldm_entities_package.eClassifiers:
             if isinstance(eclassifier, ELClass):
                 if eclassifier.name == name:
                     return eclassifier
 
     def find_class_with_long_name(self, context, name):
-        '''
+        """
         get the class with this name from the input tables package
-        '''
+        """
 
         for eclassifier in context.ldm_entities_package.eClassifiers:
             if isinstance(eclassifier, ELClass):
-                for annotation in  eclassifier.eAnnotations:
+                for annotation in eclassifier.eAnnotations:
                     for detail in annotation.details:
-                        if detail.key == 'long_name':
+                        if detail.key == "long_name":
                             if detail.value == name:
                                 return eclassifier
         return None
 
     def set_ldm_super_classes(self, context):
-        '''
+        """
         for each entity in the LDM, set the superclass of the class,
         but not if it already has a super class set by the disjoint subtyping
         processing
-        '''
-        file_location = context.file_directory + os.sep + 'ldm' + os.sep + "DM_Entities.csv"
+        """
+        file_location = context.file_directory + os.sep + "ldm" + os.sep + "DM_Entities.csv"
         header_skipped = False
 
         # Where an entity has a superclass, set the superclass on the ELClass
-        with open(file_location,  encoding='utf-8') as csvfile:
-            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
+        with open(file_location, encoding="utf-8") as csvfile:
+            filereader = csv.reader(csvfile, delimiter=",", quotechar='"')
             for row in filereader:
                 # skip the first line which is the header.
                 if not header_skipped:
                     header_skipped = True
                 else:
                     class_id = row[1]
                     superclass_id = row[25]
                     classification_type = row[10]
                     process_class = True
-                    if context.skip_reference_data_in_ldm and \
-                        (context.classification_types.get(classification_type) == "Reference data"):
+                    if context.skip_reference_data_in_ldm and (
+                        context.classification_types.get(classification_type) == "Reference data"
+                    ):
                         process_class = False
                     if process_class:
                         if not (len(superclass_id.strip()) == 0):
                             theclass = context.classes_map[class_id]
                             superclass = context.classes_map[superclass_id]
                             if len(theclass.eSuperTypes) == 0:
                                 theclass.eSuperTypes.extend([superclass])
 
     def add_ldm_enums_to_package(self, context):
-        '''
+        """
         for each domain in the LDM add an enum to the package
-        '''
-        file_location = context.file_directory + os.sep + 'ldm' + os.sep + "DM_Domains.csv"
+        """
+        file_location = context.file_directory + os.sep + "ldm" + os.sep + "DM_Domains.csv"
         header_skipped = False
         counter = 0
         # Create an ELEnum for each domain, and add it to the ELPackage
-        with open(file_location,  encoding='utf-8') as csvfile:
-            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
+        with open(file_location, encoding="utf-8") as csvfile:
+            filereader = csv.reader(csvfile, delimiter=",", quotechar='"')
             for row in filereader:
                 if not header_skipped:
                     header_skipped = True
                 else:
-                    counter = counter+1
+                    counter = counter + 1
                     enum_id = row[0]
                     enum_name = row[1]
                     synonym = row[3]
                     adapted_enum_name = Utils.make_valid_id(synonym)
                     the_enum = ELEnum()
                     the_enum.name = adapted_enum_name + "_domain"
                     # maintain a map of enum IDS to ELEnum objects
                     context.enum_map[enum_id] = the_enum
-                    context.ldm_domains_package.eClassifiers.extend([
-                                                                        the_enum])
+                    context.ldm_domains_package.eClassifiers.extend([the_enum])
 
     def add_ldm_literals_to_enums(self, context):
-        '''
+        """
         for each memebr of a domain the LDM, add a literal to the corresponding enum
-        '''
-        file_location = context.file_directory + os.sep + 'ldm' + os.sep + "DM_Domain_AVT.csv"
+        """
+        file_location = context.file_directory + os.sep + "ldm" + os.sep + "DM_Domain_AVT.csv"
         header_skipped = False
         counter = 0
         # Add the members of a domain as literals of the related Enum
-        with open(file_location,  encoding='utf-8') as csvfile:
-            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
+        with open(file_location, encoding="utf-8") as csvfile:
+            filereader = csv.reader(csvfile, delimiter=",", quotechar='"')
             for row in filereader:
                 if not header_skipped:
                     header_skipped = True
                 else:
                     try:
-                        counter = counter+1
+                        counter = counter + 1
                         enum_id = row[0]
-                        adapted_enum_name = Utils.make_valid_id_for_literal( row[3])
+                        adapted_enum_name = Utils.make_valid_id_for_literal(row[3])
                         value = row[4]
-                        adapted_value = Utils.make_valid_id( value)
+                        adapted_value = Utils.make_valid_id(value)
                         try:
                             the_enum = context.enum_map[enum_id]
-                            new_adapted_value = Utils.unique_value(
-                                 the_enum, adapted_value)
-                            new_adapted_name = Utils.unique_name(
-                                 the_enum, adapted_enum_name)
+                            new_adapted_value = Utils.unique_value(the_enum, adapted_value)
+                            new_adapted_name = Utils.unique_name(the_enum, adapted_enum_name)
                             enum_literal = ELEnumLiteral()
                             enum_literal.name = new_adapted_value
                             enum_literal.literal = new_adapted_name
                             enum_literal.value = counter
                             the_enum.eLiterals.extend([enum_literal])
-                            context.enum_literals_map[the_enum.name+":" + enum_literal.literal] = enum_literal
+                            context.enum_literals_map[the_enum.name + ":" + enum_literal.literal] = enum_literal
                         except KeyError:
                             print("missing domain: " + enum_id)
 
                     except IndexError:
-                        print(
-                            "row in DM_Domain_AVT.csv skipped  due to improper formatting at row number")
+                        print("row in DM_Domain_AVT.csv skipped  due to improper formatting at row number")
                         print(counter)
 
     def create_ldm_types_map(self, context):
-        '''
+        """
         for each type in the LDM, create a type in the ELPackage
-        '''
+        """
         # for each logicalDatatype for orcle 12c, make a Datatype if we have an
         # equivalent
 
-        file_location = context.file_directory + os.sep + 'ldm' + os.sep + "DM_Logical_To_Native.csv"
+        file_location = context.file_directory + os.sep + "ldm" + os.sep + "DM_Logical_To_Native.csv"
         header_skipped = False
-        with open(file_location,  encoding='utf-8') as csvfile:
-            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
+        with open(file_location, encoding="utf-8") as csvfile:
+            filereader = csv.reader(csvfile, delimiter=",", quotechar='"')
             for row in filereader:
                 if not header_skipped:
                     header_skipped = True
                 else:
                     rdbms_type = row[3]
@@ -619,30 +621,29 @@
                         if native_type.strip() == "UNKNOWN":
 
                             context.datatype_map[datatype_id] = context.types.e_string
 
     def add_ldm_attributes_to_classes(self, context):
-        '''
+        """
         For each attribute on an entity of the LDM, add an attribute
         to the relevant class in the package
-        '''
-
-        file_location = context.file_directory + os.sep + 'ldm' + os.sep + "DM_Attributes.csv"
+        """
+
+        file_location = context.file_directory + os.sep + "ldm" + os.sep + "DM_Attributes.csv"
         header_skipped = False
         # For each attribute add an ELAttribute to the correct ELClass representing the Entity
         # the attribute should have the correct type, which may be a specific
         # enumeration
 
-        with open(file_location,  encoding='utf-8') as csvfile:
-            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
+        with open(file_location, encoding="utf-8") as csvfile:
+            filereader = csv.reader(csvfile, delimiter=",", quotechar='"')
             for row in filereader:
                 if not header_skipped:
                     header_skipped = True
                 else:
                     attribute_name = row[0]
-                    amended_attribute_name = Utils.make_valid_id(
-                         attribute_name)
+                    amended_attribute_name = Utils.make_valid_id(attribute_name)
                     attribute_kind = row[7]
 
                     preferred_abbreviation = row[31]
                     class_id = row[4]
 
@@ -655,24 +656,21 @@
                         print("no preferred abbreviation for attribute: " + attribute_name)
 
                     the_attribute_name = Utils.make_valid_id(preferred_abbreviation)
 
                     process_attribute = True
-                    if context.skip_reference_data_in_ldm and \
-                        (class_id in context.reference_data_class_list):
+                    if context.skip_reference_data_in_ldm and (class_id in context.reference_data_class_list):
                         process_attribute = False
                     if process_attribute:
                         the_class = context.classes_map[class_id]
 
-
                         if attribute_kind == "Domain":
                             enum_id = row[12]
                             the_enum = context.enum_map[enum_id]
 
                             attribute = ELAttribute()
 
-
                             attribute.lowerBound = 0
                             attribute.upperBound = 1
                             if the_enum.name == "String":
                                 attribute.name = the_attribute_name
                                 attribute.eType = context.types.e_string
@@ -736,68 +734,63 @@
                             elif the_enum.name.startswith("BLN"):
                                 attribute.name = the_attribute_name
                                 attribute.eType = context.types.e_date
                                 attribute.eAttributeType = context.types.e_boolean
 
-
-
                             # This is a common domain used for String identifiers in BIRD
                             # in SQLDeveloper
 
                             else:
                                 attribute.name = the_attribute_name
                                 attribute.eType = the_enum
                                 attribute.eAttributeType = the_enum
 
                         context.enums_used.append(attribute.eType)
 
-                        if (attribute_kind == "Logical Type"):
+                        if attribute_kind == "Logical Type":
                             datatype_id = row[14]
                             try:
 
                                 attribute = ELAttribute()
                                 attribute.lowerBound = 0
                                 attribute.upperBound = 1
                                 attribute.name = the_attribute_name
-                                attribute.eType = Utils.get_ecore_datatype_for_datatype(
-                                    self)
-                                attribute.eAttributeType = Utils.get_ecore_datatype_for_datatype(
-                                    self)
+                                attribute.eType = Utils.get_ecore_datatype_for_datatype(self)
+                                attribute.eAttributeType = Utils.get_ecore_datatype_for_datatype(self)
 
                             except KeyError:
                                 print("missing datatype: ")
                                 print(datatype_id)
 
                         try:
 
-
                             the_class = context.classes_map[class_id]
                             the_class.eStructuralFeatures.extend([attribute])
                             the_long_name_annotation = ELAnnotation()
-                            the_long_name_directive = Utils.get_annotation_directive(the_class.eContainer(), "long_name")
+                            the_long_name_directive = Utils.get_annotation_directive(
+                                the_class.eContainer(), "long_name"
+                            )
                             the_long_name_annotation.source = the_long_name_directive
                             details = the_long_name_annotation.details
-                            mapentry  = ELStringToStringMapEntry()
+                            mapentry = ELStringToStringMapEntry()
                             mapentry.key = "long_name"
                             mapentry.value = amended_attribute_name
                             details.append(mapentry)
                             attribute.eAnnotations.append(the_long_name_annotation)
 
                         except:
                             print("missing class2: ")
                             print(class_id)
 
-
-
     def add_ldm_relationships_between_classes(self, context):
-        '''
+        """
         For each relationship in the LDM, add a reference between the relevant classes
-        '''
-        file_location = context.file_directory + os.sep + 'ldm' + os.sep + "DM_Relations.csv"
+        """
+        file_location = context.file_directory + os.sep + "ldm" + os.sep + "DM_Relations.csv"
         header_skipped = False
-        with open(file_location,  encoding='utf-8') as csvfile:
-            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
+        with open(file_location, encoding="utf-8") as csvfile:
+            filereader = csv.reader(csvfile, delimiter=",", quotechar='"')
             for row in filereader:
                 if not header_skipped:
                     header_skipped = True
                 else:
                     source_id = row[16]
@@ -817,12 +810,11 @@
                         reference_name = Utils.make_valid_id(relation_name)
                     else:
                         reference_name = Utils.make_valid_id(relation_name)
 
                     process_relationship = True
-                    if context.skip_reference_data_in_ldm and \
-                        (source_id in context.reference_data_class_list):
+                    if context.skip_reference_data_in_ldm and (source_id in context.reference_data_class_list):
                         process_relationship = False
                     if process_relationship:
                         try:
                             the_class = context.classes_map[source_id]
                         except KeyError:
@@ -835,11 +827,11 @@
 
                         ereference = ELReference()
                         ereference.name = reference_name
                         ereference.eType = target_class
 
-                        if (not (the_class is None) ) and (not (target_class is None)):
+                        if (not (the_class is None)) and (not (target_class is None)):
                             ereference = ELReference()
                             ereference.name = reference_name
                             ereference.eType = target_class
 
                             if target_optional == "N":
@@ -853,22 +845,22 @@
                                 linked_reference.containment = False
                                 linked_reference.eOpposite = ereference
 
                                 ereference.eOpposite = linked_reference
 
-                                target_class.eStructuralFeatures.append(
-                                    linked_reference)
-
+                                target_class.eStructuralFeatures.append(linked_reference)
 
                             else:
                                 ereference.containment = False
 
                                 the_associated_class_annotation = ELAnnotation()
-                                the_associated_class_directive = Utils.get_annotation_directive(the_class.eContainer(), "relationship_type")
+                                the_associated_class_directive = Utils.get_annotation_directive(
+                                    the_class.eContainer(), "relationship_type"
+                                )
                                 the_associated_class_annotation.source = the_associated_class_directive
                                 details = the_associated_class_annotation.details
-                                mapentry  = ELStringToStringMapEntry()
+                                mapentry = ELStringToStringMapEntry()
                                 mapentry.key = "is_associated_with"
                                 mapentry.value = the_class.name + "." + ereference.name
                                 details.append(mapentry)
                                 target_class.eAnnotations.append(the_associated_class_annotation)
 
@@ -889,121 +881,127 @@
                                     ereference.lowerBound = 1
                                 else:
                                     ereference.upperBound = 1
                                     ereference.lowerBound = 1
 
-
                             if not the_class is None:
                                 the_class.eStructuralFeatures.append(ereference)
 
-
-    def mark_root_class_as_entity_group_annotation(self,context):
+    def mark_root_class_as_entity_group_annotation(self, context):
         for the_class in context.classes_map.values():
-            ultimate_superclass = SQLDevLDMImport.get_ultimate_superclass(self,context,the_class)
-            if not (ultimate_superclass == the_class) :
+            ultimate_superclass = SQLDevLDMImport.get_ultimate_superclass(self, context, the_class)
+            if not (ultimate_superclass == the_class):
                 the_entity_annotation = Utils.get_annotation_with_source(the_class, "entity_hierarchy")
                 if the_entity_annotation is None:
                     the_entity_annotation = ELAnnotation()
-                    the_entity_annotation_directive = Utils.get_annotation_directive(the_class.eContainer(), "entity_hierarchy")
+                    the_entity_annotation_directive = Utils.get_annotation_directive(
+                        the_class.eContainer(), "entity_hierarchy"
+                    )
                     the_entity_annotation.source = the_entity_annotation_directive
                     the_class.eAnnotations.append(the_entity_annotation)
 
                 details = the_entity_annotation.details
-                mapentry  = ELStringToStringMapEntry()
+                mapentry = ELStringToStringMapEntry()
                 mapentry.key = "entity_hierarchy"
                 mapentry.value = ultimate_superclass.name
                 details.append(mapentry)
 
-            if (ultimate_superclass == the_class) and ( SQLDevLDMImport.has_subclasses(self,context,the_class) or SQLDevLDMImport.has_delegate(self,context,the_class)):
+            if (ultimate_superclass == the_class) and (
+                SQLDevLDMImport.has_subclasses(self, context, the_class)
+                or SQLDevLDMImport.has_delegate(self, context, the_class)
+            ):
                 the_entity_annotation = Utils.get_annotation_with_source(the_class, "entity_hierarchy")
                 if the_entity_annotation is None:
                     the_entity_annotation = ELAnnotation()
-                    the_entity_annotation_directive = Utils.get_annotation_directive(the_class.eContainer(), "entity_hierarchy")
+                    the_entity_annotation_directive = Utils.get_annotation_directive(
+                        the_class.eContainer(), "entity_hierarchy"
+                    )
                     the_entity_annotation.source = the_entity_annotation_directive
                     the_class.eAnnotations.append(the_entity_annotation)
 
                 details = the_entity_annotation.details
-                mapentry  = ELStringToStringMapEntry()
+                mapentry = ELStringToStringMapEntry()
                 mapentry.key = "entity_hierarchy"
                 mapentry.value = ultimate_superclass.name
                 details.append(mapentry)
 
-    def remove_enums_not_used_by_attributes(self,context):
+    def remove_enums_not_used_by_attributes(self, context):
 
         for enum in context.ldm_domains_package.eClassifiers:
             if not (enum in context.enums_used):
                 context.ldm_domains_package.eClassifiers.remove(enum)
 
-    def get_ultimate_superclass(self,context,the_class):
+    def get_ultimate_superclass(self, context, the_class):
 
         return_class = None
         if len(the_class.eSuperTypes) > 0:
-            return_class = SQLDevLDMImport.get_ultimate_superclass(self,context,the_class.eSuperTypes[0])
-        elif SQLDevLDMImport.is_delegate_class(self,context,the_class):
-            return_class = SQLDevLDMImport.get_ultimate_superclass(self,context,
-                                SQLDevLDMImport.get_delegate_class(self,context,the_class))
+            return_class = SQLDevLDMImport.get_ultimate_superclass(self, context, the_class.eSuperTypes[0])
+        elif SQLDevLDMImport.is_delegate_class(self, context, the_class):
+            return_class = SQLDevLDMImport.get_ultimate_superclass(
+                self, context, SQLDevLDMImport.get_delegate_class(self, context, the_class)
+            )
         else:
             return_class = the_class
 
         return return_class
 
-    def has_subclasses(self,context,the_class):
+    def has_subclasses(self, context, the_class):
         for a_class in context.classes_map.values():
             if len(a_class.eSuperTypes) > 0:
                 superclass = a_class.eSuperTypes[0]
                 if superclass == the_class:
                     return True
 
         return False
 
-    def has_delegate(self,context,the_class):
+    def has_delegate(self, context, the_class):
         for a_class in context.classes_map.values():
             for ref in a_class.eStructuralFeatures:
-                if ref.name.endswith('_delegate'):
+                if ref.name.endswith("_delegate"):
                     return True
 
         return False
 
-    def is_delegate_class(self,context,the_class):
-
-        if not (SQLDevLDMImport.get_delegate_class(self,context,the_class) is None):
-                return True
+    def is_delegate_class(self, context, the_class):
+
+        if not (SQLDevLDMImport.get_delegate_class(self, context, the_class) is None):
+            return True
         return False
 
-    def get_delegate_class(self,context,the_class):
+    def get_delegate_class(self, context, the_class):
         # find the calss that has a containment reference to this class
         for a_class in context.classes_map.values():
             for reference in a_class.eStructuralFeatures:
-                if (reference.name.endswith('_delegate')) and reference.eType == the_class:
+                if (reference.name.endswith("_delegate")) and reference.eType == the_class:
                     return a_class
 
         return None
 
     def remove_duplicate_attributes_in_subclasses(self, context):
 
         for classifier in context.ldm_entities_package.eClassifiers:
             if isinstance(classifier, ELClass):
                 feaures_to_remove = []
-                for feature in  classifier.eStructuralFeatures:
+                for feature in classifier.eStructuralFeatures:
                     if isinstance(feature, ELAttribute):
-                        if SQLDevLDMImport.attribute_exists_in_any_superclass(self, classifier,feature, context):
+                        if SQLDevLDMImport.attribute_exists_in_any_superclass(self, classifier, feature, context):
                             feaures_to_remove.append(feature)
 
                 for feature_to_remove in feaures_to_remove:
                     classifier.eStructuralFeatures.remove(feature_to_remove)
 
-    def attribute_exists_in_any_superclass(self, el_class,attribute, context):
+    def attribute_exists_in_any_superclass(self, el_class, attribute, context):
         return_value = False
         if len(el_class.eSuperTypes) > 0:
             super_class = el_class.eSuperTypes[0]
             for feature in super_class.eStructuralFeatures:
                 if isinstance(feature, ELAttribute):
                     if feature.name == attribute.name:
                         return_value = True
 
             if return_value == False:
-                return SQLDevLDMImport.attribute_exists_in_any_superclass(self, super_class,attribute, context)
+                return SQLDevLDMImport.attribute_exists_in_any_superclass(self, super_class, attribute, context)
             else:
                 return True
 
         else:
             return False
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/sqldeveloper_import/import_sqldev_ldm_to_regdna.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py	2025-09-15 13:18:11.388542+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py	2025-09-21 17:07:37.287815+00:00
@@ -18,24 +18,26 @@
 from pathlib import Path
 from django.db import connection, transaction
 import subprocess
 import platform
 
+
 class ImportWebsiteToSDDModel(object):
-    '''
+    """
     Class responsible for importing SDD csv files into an instance of the analysis model
-    '''
-    def import_report_templates_from_sdd(self, sdd_context, dpm:bool=False):
-        '''
+    """
+
+    def import_report_templates_from_sdd(self, sdd_context, dpm: bool = False):
+        """
         Import SDD csv files into an instance of the analysis model
-        '''
+        """
 
         ImportWebsiteToSDDModel.create_maintenance_agencies(self, sdd_context)
         ImportWebsiteToSDDModel.create_frameworks(self, sdd_context)
-        ImportWebsiteToSDDModel.create_all_domains(self, sdd_context,False)
-        ImportWebsiteToSDDModel.create_all_members(self, sdd_context,False)
-        ImportWebsiteToSDDModel.create_all_variables(self, sdd_context,False)
+        ImportWebsiteToSDDModel.create_all_domains(self, sdd_context, False)
+        ImportWebsiteToSDDModel.create_all_members(self, sdd_context, False)
+        ImportWebsiteToSDDModel.create_all_variables(self, sdd_context, False)
 
         ImportWebsiteToSDDModel.create_report_tables(self, sdd_context)
         ImportWebsiteToSDDModel.create_axis(self, sdd_context)
         ImportWebsiteToSDDModel.create_axis_ordinates(self, sdd_context)
 
@@ -46,105 +48,101 @@
         else:
             ImportWebsiteToSDDModel.create_table_cells(self, sdd_context)
             ImportWebsiteToSDDModel.create_ordinate_items(self, sdd_context)
             ImportWebsiteToSDDModel.create_cell_positions(self, sdd_context)
 
-
-
     def import_semantic_integrations_from_sdd(self, sdd_context):
-        '''
+        """
         Import SDD csv files into an instance of the analysis model
-        '''
+        """
         ImportWebsiteToSDDModel.delete_mapping_warnings_files(self, sdd_context)
         ImportWebsiteToSDDModel.create_all_variable_mappings(self, sdd_context)
         ImportWebsiteToSDDModel.create_all_variable_mapping_items(self, sdd_context)
         ImportWebsiteToSDDModel.create_member_mappings(self, sdd_context)
         ImportWebsiteToSDDModel.create_all_member_mappings_items(self, sdd_context)
         ImportWebsiteToSDDModel.create_all_mapping_definitions(self, sdd_context)
         ImportWebsiteToSDDModel.create_all_mapping_to_cubes(self, sdd_context)
 
     def import_hierarchies_from_sdd(self, sdd_context):
-        '''
+        """
         Import hierarchies from CSV file
-        '''
+        """
         ImportWebsiteToSDDModel.delete_hierarchy_warnings_files(self, sdd_context)
         ImportWebsiteToSDDModel.create_all_member_hierarchies(self, sdd_context)
         ImportWebsiteToSDDModel.create_all_parent_members_with_children_locally(self, sdd_context)
         ImportWebsiteToSDDModel.create_all_member_hierarchies_nodes(self, sdd_context)
 
     def create_maintenance_agencies(self, context):
-        '''
+        """
         Import maintenance agencies from CSV file using bulk create
-        '''
+        """
         file_location = context.file_directory + os.sep + "technical_export" + os.sep + "maintenance_agency.csv"
         header_skipped = False
         agencies_to_create = []
 
-        with open(file_location, encoding='utf-8') as csvfile:
-            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
+        with open(file_location, encoding="utf-8") as csvfile:
+            filereader = csv.reader(csvfile, delimiter=",", quotechar='"')
             for row in filereader:
                 if not header_skipped:
                     header_skipped = True
                 else:
                     code = row[ColumnIndexes().maintenance_agency_code]
                     description = row[ColumnIndexes().maintenance_agency_description]
                     id = row[ColumnIndexes().maintenance_agency_id]
                     name = row[ColumnIndexes().maintenance_agency_name]
 
-                    maintenance_agency = MAINTENANCE_AGENCY(
-                        name=ImportWebsiteToSDDModel.replace_dots(self, id))
+                    maintenance_agency = MAINTENANCE_AGENCY(name=ImportWebsiteToSDDModel.replace_dots(self, id))
                     maintenance_agency.code = code
                     maintenance_agency.description = description
                     maintenance_agency.maintenance_agency_id = ImportWebsiteToSDDModel.replace_dots(self, id)
 
                     agencies_to_create.append(maintenance_agency)
                     context.agency_dictionary[id] = maintenance_agency
 
         if context.save_sdd_to_db and agencies_to_create:
-            MAINTENANCE_AGENCY.objects.bulk_create(agencies_to_create, batch_size=1000,ignore_conflicts=True)
+            MAINTENANCE_AGENCY.objects.bulk_create(agencies_to_create, batch_size=1000, ignore_conflicts=True)
 
     def create_frameworks(self, context):
-        '''
+        """
         Import frameworks from CSV file using bulk create
-        '''
+        """
         file_location = context.file_directory + os.sep + "technical_export" + os.sep + "framework.csv"
         header_skipped = False
         frameworks_to_create = []
 
-        with open(file_location, encoding='utf-8') as csvfile:
-            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
+        with open(file_location, encoding="utf-8") as csvfile:
+            filereader = csv.reader(csvfile, delimiter=",", quotechar='"')
             for row in filereader:
                 if not header_skipped:
                     header_skipped = True
                 else:
                     code = row[ColumnIndexes().framework_code]
                     description = row[ColumnIndexes().framework_description]
                     id = row[ColumnIndexes().framework_id]
                     name = row[ColumnIndexes().framework_name]
 
-                    framework = FRAMEWORK(
-                        name=ImportWebsiteToSDDModel.replace_dots(self, id))
+                    framework = FRAMEWORK(name=ImportWebsiteToSDDModel.replace_dots(self, id))
                     framework.code = code
                     framework.description = description
                     framework.framework_id = ImportWebsiteToSDDModel.replace_dots(self, id)
 
                     frameworks_to_create.append(framework)
                     context.framework_dictionary[ImportWebsiteToSDDModel.replace_dots(self, id)] = framework
 
         if context.save_sdd_to_db and frameworks_to_create:
-            FRAMEWORK.objects.bulk_create(frameworks_to_create, batch_size=1000,ignore_conflicts=True)
+            FRAMEWORK.objects.bulk_create(frameworks_to_create, batch_size=1000, ignore_conflicts=True)
 
     def create_all_domains(self, context, ref):
-        '''
+        """
         Import all domains from CSV file using bulk create
-        '''
+        """
         file_location = context.file_directory + os.sep + "technical_export" + os.sep + "domain.csv"
         header_skipped = False
         domains_to_create = []
 
-        with open(file_location, encoding='utf-8') as csvfile:
-            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
+        with open(file_location, encoding="utf-8") as csvfile:
+            filereader = csv.reader(csvfile, delimiter=",", quotechar='"')
             for row in filereader:
                 if not header_skipped:
                     header_skipped = True
                 else:
                     maintenence_agency = row[ColumnIndexes().domain_maintenence_agency]
@@ -163,13 +161,17 @@
                         include = True
 
                     if include:
                         domain = DOMAIN(name=ImportWebsiteToSDDModel.replace_dots(self, domain_id))
                         if maintenence_agency == "":
-                            maintenence_agency = ImportWebsiteToSDDModel.find_maintenance_agency_with_id(self,context,"SDD_DOMAIN")
+                            maintenence_agency = ImportWebsiteToSDDModel.find_maintenance_agency_with_id(
+                                self, context, "SDD_DOMAIN"
+                            )
                         else:
-                            maintenence_agency = ImportWebsiteToSDDModel.find_maintenance_agency_with_id(self,context,maintenence_agency)
+                            maintenence_agency = ImportWebsiteToSDDModel.find_maintenance_agency_with_id(
+                                self, context, maintenence_agency
+                            )
                         domain.maintenance_agency_id = maintenence_agency
                         domain.code = code
                         domain.description = description
                         domain.domain_id = ImportWebsiteToSDDModel.replace_dots(self, domain_id)
                         domain.name = domain_name
@@ -181,22 +183,22 @@
                             context.domain_dictionary[domain.domain_id] = domain
                         else:
                             context.domain_dictionary[domain.domain_id] = domain
 
         if context.save_sdd_to_db and domains_to_create:
-            DOMAIN.objects.bulk_create(domains_to_create, batch_size=1000,ignore_conflicts=True)
+            DOMAIN.objects.bulk_create(domains_to_create, batch_size=1000, ignore_conflicts=True)
 
     def create_all_members(self, context, ref):
-        '''
+        """
         Import all members from CSV file using bulk create
-        '''
+        """
         file_location = context.file_directory + os.sep + "technical_export" + os.sep + "member.csv"
         header_skipped = False
         members_to_create = []
 
-        with open(file_location, encoding='utf-8') as csvfile:
-            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
+        with open(file_location, encoding="utf-8") as csvfile:
+            filereader = csv.reader(csvfile, delimiter=",", quotechar='"')
             for row in filereader:
                 if not header_skipped:
                     header_skipped = True
                 else:
                     code = row[ColumnIndexes().member_member_code_index]
@@ -219,11 +221,13 @@
                         member = MEMBER(name=ImportWebsiteToSDDModel.replace_dots(self, member_id))
                         member.member_id = ImportWebsiteToSDDModel.replace_dots(self, member_id)
                         member.code = code
                         member.description = description
                         member.name = member_name
-                        maintenance_agency = ImportWebsiteToSDDModel.find_maintenance_agency_with_id(self,context,maintenence_agency)
+                        maintenance_agency = ImportWebsiteToSDDModel.find_maintenance_agency_with_id(
+                            self, context, maintenence_agency
+                        )
                         member.maintenance_agency_id = maintenance_agency
                         domain = ImportWebsiteToSDDModel.find_domain_with_id(self, context, domain_id)
                         member.domain_id = domain
 
                         members_to_create.append(member)
@@ -232,22 +236,22 @@
                         if not (domain_id is None) and not (domain_id == ""):
                             context.member_id_to_domain_map[member] = domain
                             context.member_id_to_member_code_map[member.member_id] = code
 
         if context.save_sdd_to_db and members_to_create:
-            MEMBER.objects.bulk_create(members_to_create, batch_size=1000,ignore_conflicts=True)
+            MEMBER.objects.bulk_create(members_to_create, batch_size=1000, ignore_conflicts=True)
 
     def create_all_variables(self, context, ref):
-        '''
+        """
         Import all variables from CSV file using bulk create
-        '''
+        """
         file_location = context.file_directory + os.sep + "technical_export" + os.sep + "variable.csv"
         header_skipped = False
         variables_to_create = []
 
-        with open(file_location, encoding='utf-8') as csvfile:
-            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
+        with open(file_location, encoding="utf-8") as csvfile:
+            filereader = csv.reader(csvfile, delimiter=",", quotechar='"')
             for row in filereader:
                 if not header_skipped:
                     header_skipped = True
                 else:
                     maintenence_agency = row[ColumnIndexes().variable_variable_maintenence_agency]
@@ -264,11 +268,13 @@
                     if (not ref) and not (maintenence_agency == "ECB"):
                         include = True
 
                     if include:
                         variable = VARIABLE(name=ImportWebsiteToSDDModel.replace_dots(self, variable_id))
-                        maintenance_agency_id = ImportWebsiteToSDDModel.find_maintenance_agency_with_id(self,context,maintenence_agency)
+                        maintenance_agency_id = ImportWebsiteToSDDModel.find_maintenance_agency_with_id(
+                            self, context, maintenence_agency
+                        )
                         variable.code = code
                         variable.variable_id = ImportWebsiteToSDDModel.replace_dots(self, variable_id)
                         variable.name = name
                         domain = ImportWebsiteToSDDModel.find_domain_with_id(self, context, domain_id)
                         variable.domain_id = domain
@@ -277,15 +283,15 @@
 
                         variables_to_create.append(variable)
                         context.variable_dictionary[variable.variable_id] = variable
                         context.variable_to_domain_map[variable.variable_id] = domain
                         context.variable_to_long_names_map[variable.variable_id] = name
-                        if not((primary_concept == "") or (primary_concept == None)):
+                        if not ((primary_concept == "") or (primary_concept == None)):
                             context.variable_to_primary_concept_map[variable.variable_id] = primary_concept
 
         if context.save_sdd_to_db and variables_to_create:
-            VARIABLE.objects.bulk_create(variables_to_create, batch_size=1000,ignore_conflicts=True)
+            VARIABLE.objects.bulk_create(variables_to_create, batch_size=1000, ignore_conflicts=True)
 
     def create_all_parent_members_with_children_locally(self, context):
         print("Creating all parent members with children locally")
         parent_members = set()  # Using set for faster lookups
         parent_members_to_create = []
@@ -293,98 +299,112 @@
         missing_children = []
 
         # Pre-fetch all hierarchies for faster lookup
         hierarchy_cache = {}
 
-        with open(f"{context.file_directory}/technical_export/member_hierarchy_node.csv", encoding='utf-8') as csvfile:
+        with open(f"{context.file_directory}/technical_export/member_hierarchy_node.csv", encoding="utf-8") as csvfile:
             header_skipped = False
             id_increment = 0
             for row in csv.reader(csvfile):
                 if not header_skipped:
                     header_skipped = True
-                    if row[0].upper() == 'ID': #sometimes exported data without a  primary key has an ID field added at the time of export, exported data is re-imported
+                    if (
+                        row[0].upper() == "ID"
+                    ):  # sometimes exported data without a  primary key has an ID field added at the time of export, exported data is re-imported
                         id_increment = 1
                 else:
                     parent_member_id = row[ColumnIndexes().member_hierarchy_node_parent_member_id + id_increment]
                     member_id = row[ColumnIndexes().member_hierarchy_node_member_id + id_increment]
                     hierarchy_id = row[ColumnIndexes().member_hierarchy_node_hierarchy_id + id_increment]
 
                     if not parent_member_id:
                         continue
 
                     if hierarchy_id not in hierarchy_cache:
-                        hierarchy_cache[hierarchy_id] = ImportWebsiteToSDDModel.find_member_hierarchy_with_id(self,hierarchy_id,context)
+                        hierarchy_cache[hierarchy_id] = ImportWebsiteToSDDModel.find_member_hierarchy_with_id(
+                            self, hierarchy_id, context
+                        )
 
                     hierarchy = hierarchy_cache[hierarchy_id]
                     if hierarchy:
                         domain = hierarchy.domain_id
-                        parent_members_child_triples.append((parent_member_id,member_id,domain))
+                        parent_members_child_triples.append((parent_member_id, member_id, domain))
                         parent_members.add(parent_member_id)
 
         # Process parent-child relationships in batches
         for parent_member_id, member_id, domain in parent_members_child_triples:
             if member_id in parent_members:
-                if not any(parent_member_id in d for d in (context.members_that_are_nodes,
-                                                         context.member_dictionary,
-                                                         context.member_dictionary)):
+                if not any(
+                    parent_member_id in d
+                    for d in (context.members_that_are_nodes, context.member_dictionary, context.member_dictionary)
+                ):
                     parent_member = MEMBER(
                         name=ImportWebsiteToSDDModel.replace_dots(self, parent_member_id),
                         member_id=ImportWebsiteToSDDModel.replace_dots(self, parent_member_id),
-                        maintenance_agency_id=ImportWebsiteToSDDModel.find_maintenance_agency_with_id(self,context,"NODE"),
-                        domain_id=domain
+                        maintenance_agency_id=ImportWebsiteToSDDModel.find_maintenance_agency_with_id(
+                            self, context, "NODE"
+                        ),
+                        domain_id=domain,
                     )
                     parent_members_to_create.append(parent_member)
                     context.member_dictionary[parent_member.member_id] = parent_member
                     if not (parent_member.domain_id is None) and not (parent_member.domain_id == ""):
                         context.member_id_to_domain_map[parent_member] = domain
                         context.member_id_to_member_code_map[parent_member.member_id] = parent_member.member_id
 
                     context.members_that_are_nodes[parent_member_id] = parent_member
             else:
-                member = ImportWebsiteToSDDModel.find_member_with_id(self,member_id,context)
+                member = ImportWebsiteToSDDModel.find_member_with_id(self, member_id, context)
                 if member is None:
-                    missing_children.append((parent_member_id,member_id))
-                elif not any(parent_member_id in d for d in (context.members_that_are_nodes,
-                                                          context.member_dictionary,
-                                                          context.member_dictionary)):
+                    missing_children.append((parent_member_id, member_id))
+                elif not any(
+                    parent_member_id in d
+                    for d in (context.members_that_are_nodes, context.member_dictionary, context.member_dictionary)
+                ):
                     parent_member = MEMBER(
                         name=ImportWebsiteToSDDModel.replace_dots(self, parent_member_id),
                         member_id=ImportWebsiteToSDDModel.replace_dots(self, parent_member_id),
-                        maintenance_agency_id=ImportWebsiteToSDDModel.find_maintenance_agency_with_id(self,context,"NODE"),
-                        domain_id=domain
+                        maintenance_agency_id=ImportWebsiteToSDDModel.find_maintenance_agency_with_id(
+                            self, context, "NODE"
+                        ),
+                        domain_id=domain,
                     )
                     parent_members_to_create.append(parent_member)
                     context.members_that_are_nodes[parent_member_id] = parent_member
 
                     context.member_dictionary[parent_member.member_id] = parent_member
                     if not (parent_member.domain_id is None) and not (parent_member.domain_id == ""):
                         context.member_id_to_domain_map[parent_member] = domain
                         context.member_id_to_member_code_map[parent_member.member_id] = parent_member.member_id
 
         if context.save_sdd_to_db and parent_members_to_create:
-            MEMBER.objects.bulk_create(parent_members_to_create, batch_size=5000,ignore_conflicts=True)  # Increased batch size
-
-        ImportWebsiteToSDDModel.save_missing_children_to_csv(context,missing_children)
+            MEMBER.objects.bulk_create(
+                parent_members_to_create, batch_size=5000, ignore_conflicts=True
+            )  # Increased batch size
+
+        ImportWebsiteToSDDModel.save_missing_children_to_csv(context, missing_children)
 
     def create_all_member_hierarchies(self, context):
-        '''
+        """
         Import all member hierarchies with batch processing
-        '''
+        """
         missing_domains = set()  # Using set for faster lookups
         hierarchies_to_create = []
 
-        with open(f"{context.file_directory}/technical_export/member_hierarchy.csv", encoding='utf-8') as csvfile:
+        with open(f"{context.file_directory}/technical_export/member_hierarchy.csv", encoding="utf-8") as csvfile:
             next(csvfile)  # Skip header more efficiently
             for row in csv.reader(csvfile):
                 maintenance_agency_id = row[ColumnIndexes().member_hierarchy_maintenance_agency]
                 code = row[ColumnIndexes().member_hierarchy_code]
                 id = row[ColumnIndexes().member_hierarchy_id]
                 domain_id = row[ColumnIndexes().member_hierarchy_domain_id]
                 description = row[ColumnIndexes().member_hierarchy_description]
 
-                maintenance_agency = ImportWebsiteToSDDModel.find_maintenance_agency_with_id(self,context,maintenance_agency_id)
-                domain = ImportWebsiteToSDDModel.find_domain_with_id(self,context,domain_id)
+                maintenance_agency = ImportWebsiteToSDDModel.find_maintenance_agency_with_id(
+                    self, context, maintenance_agency_id
+                )
+                domain = ImportWebsiteToSDDModel.find_domain_with_id(self, context, domain_id)
 
                 if domain is None:
                     missing_domains.add(domain_id)
                     continue
 
@@ -392,67 +412,68 @@
                     name=ImportWebsiteToSDDModel.replace_dots(self, id),
                     member_hierarchy_id=ImportWebsiteToSDDModel.replace_dots(self, id),
                     code=code,
                     description=description,
                     maintenance_agency_id=maintenance_agency,
-                    domain_id=domain
+                    domain_id=domain,
                 )
 
                 if hierarchy.member_hierarchy_id not in context.member_hierarchy_dictionary:
                     hierarchies_to_create.append(hierarchy)
                     context.member_hierarchy_dictionary[hierarchy.member_hierarchy_id] = hierarchy
 
         if context.save_sdd_to_db and hierarchies_to_create:
-            MEMBER_HIERARCHY.objects.bulk_create(hierarchies_to_create, batch_size=5000,ignore_conflicts=True)  # Increased batch size
+            MEMBER_HIERARCHY.objects.bulk_create(
+                hierarchies_to_create, batch_size=5000, ignore_conflicts=True
+            )  # Increased batch size
 
         if missing_domains:
             ImportWebsiteToSDDModel.save_missing_domains_to_csv(context, list(missing_domains))
 
-    def save_missing_domains_to_csv(context,missing_domains):
+    def save_missing_domains_to_csv(context, missing_domains):
         filename = context.output_directory + os.sep + "generated_hierarchy_warnings" + os.sep + "missing_domains.csv"
-        with open(filename, 'w', newline='') as csvfile:
+        with open(filename, "w", newline="") as csvfile:
             writer = csv.writer(csvfile)
             writer.writerows(missing_domains)
 
-    def save_missing_members_to_csv(context,missing_members):
+    def save_missing_members_to_csv(context, missing_members):
         filename = context.output_directory + os.sep + "generated_hierarchy_warnings" + os.sep + "missing_members.csv"
-        with open(filename, 'w', newline='') as csvfile:
+        with open(filename, "w", newline="") as csvfile:
             writer = csv.writer(csvfile)
             writer.writerows(missing_members)
 
-    def save_missing_variables_to_csv(context,missing_variables):
+    def save_missing_variables_to_csv(context, missing_variables):
         filename = context.output_directory + os.sep + "generated_hierarchy_warnings" + os.sep + "missing_variables.csv"
-        with open(filename, 'w', newline='') as csvfile:
+        with open(filename, "w", newline="") as csvfile:
             writer = csv.writer(csvfile)
             writer.writerows(missing_variables)
 
-    def save_missing_children_to_csv(context,missing_children):
+    def save_missing_children_to_csv(context, missing_children):
         filename = context.output_directory + os.sep + "generated_hierarchy_warnings" + os.sep + "missing_children.csv"
-        with open(filename, 'w', newline='') as csvfile:
+        with open(filename, "w", newline="") as csvfile:
             writer = csv.writer(csvfile)
             writer.writerows(missing_children)
 
-
-
-
     def create_all_member_hierarchies_nodes(self, context):
-        '''
+        """
         Import all member hierarchy nodes from CSV file
-        '''
+        """
         file_location = context.file_directory + os.sep + "technical_export" + os.sep + "member_hierarchy_node.csv"
         header_skipped = False
         missing_members = []
         missing_hierarchies = []
         nodes_to_create = []
 
-        with open(file_location, encoding='utf-8') as csvfile:
-            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
+        with open(file_location, encoding="utf-8") as csvfile:
+            filereader = csv.reader(csvfile, delimiter=",", quotechar='"')
             id_increment = 0
             for row in filereader:
                 if not header_skipped:
                     header_skipped = True
-                    if row[0].upper() == 'ID': #sometimes exported data without a  primary key has an ID field added at the time of export, exported data is re-imported
+                    if (
+                        row[0].upper() == "ID"
+                    ):  # sometimes exported data without a  primary key has an ID field added at the time of export, exported data is re-imported
                         id_increment = 1
                 else:
                     hierarchy_id = row[ColumnIndexes().member_hierarchy_node_hierarchy_id + id_increment]
                     member_id = row[ColumnIndexes().member_hierarchy_node_member_id + id_increment]
                     parent_member_id = row[ColumnIndexes().member_hierarchy_node_parent_member_id + id_increment]
@@ -460,59 +481,65 @@
                     comparator = row[ColumnIndexes().member_hierarchy_node_comparator + id_increment]
                     operator = row[ColumnIndexes().member_hierarchy_node_operator + id_increment]
                     valid_from = row[ColumnIndexes().member_hierarchy_node_valid_from + id_increment]
                     valid_to = row[ColumnIndexes().member_hierarchy_node_valid_to + id_increment]
 
-                    hierarchy = ImportWebsiteToSDDModel.find_member_hierarchy_with_id(self,hierarchy_id,context)
+                    hierarchy = ImportWebsiteToSDDModel.find_member_hierarchy_with_id(self, hierarchy_id, context)
                     if hierarchy is None:
                         print(f"Hierarchy {hierarchy_id} not found")
                         missing_hierarchies.append(hierarchy_id)
                     else:
-                        member = ImportWebsiteToSDDModel.find_member_with_id_for_hierarchy(self,member_id,hierarchy,context)
+                        member = ImportWebsiteToSDDModel.find_member_with_id_for_hierarchy(
+                            self, member_id, hierarchy, context
+                        )
                         if member is None:
                             print(f"Member {member_id} not found in the database for hierarchy {hierarchy_id}")
-                            missing_members.append((hierarchy_id,member_id))
+                            missing_members.append((hierarchy_id, member_id))
                         else:
-                            parent_member = ImportWebsiteToSDDModel.find_member_with_id(self,parent_member_id,context)
+                            parent_member = ImportWebsiteToSDDModel.find_member_with_id(self, parent_member_id, context)
                             if not (parent_member is None):
                                 hierarchy_node = MEMBER_HIERARCHY_NODE()
                                 hierarchy_node.member_hierarchy_id = hierarchy
                                 hierarchy_node.comparator = comparator
                                 hierarchy_node.operator = operator
                                 hierarchy_node.member_id = member
                                 hierarchy_node.level = int(node_level)
                                 hierarchy_node.parent_member_id = parent_member
                                 nodes_to_create.append(hierarchy_node)
-                                context.member_hierarchy_node_dictionary[hierarchy_id + ":" + member_id] = hierarchy_node
+                                context.member_hierarchy_node_dictionary[hierarchy_id + ":" + member_id] = (
+                                    hierarchy_node
+                                )
 
         if context.save_sdd_to_db and nodes_to_create:
-            MEMBER_HIERARCHY_NODE.objects.bulk_create(nodes_to_create, batch_size=1000,ignore_conflicts=True)
-
-        ImportWebsiteToSDDModel.save_missing_members_to_csv(context,missing_members)
-        ImportWebsiteToSDDModel.save_missing_hierarchies_to_csv(context,missing_hierarchies)
-
-    def save_missing_hierarchies_to_csv(context,missing_hierarchies):
-        filename = context.output_directory + os.sep + "generated_hierarchy_warnings" + os.sep + "missing_hierarchies.csv"
-        with open(filename, 'w', newline='') as csvfile:
+            MEMBER_HIERARCHY_NODE.objects.bulk_create(nodes_to_create, batch_size=1000, ignore_conflicts=True)
+
+        ImportWebsiteToSDDModel.save_missing_members_to_csv(context, missing_members)
+        ImportWebsiteToSDDModel.save_missing_hierarchies_to_csv(context, missing_hierarchies)
+
+    def save_missing_hierarchies_to_csv(context, missing_hierarchies):
+        filename = (
+            context.output_directory + os.sep + "generated_hierarchy_warnings" + os.sep + "missing_hierarchies.csv"
+        )
+        with open(filename, "w", newline="") as csvfile:
             writer = csv.writer(csvfile)
             writer.writerows(missing_hierarchies)
 
-    def find_member_with_id_for_hierarchy(self,member_id,hierarchy,context):
+    def find_member_with_id_for_hierarchy(self, member_id, hierarchy, context):
         domain = hierarchy.domain_id
-        member = MEMBER.objects.filter(domain_id=domain,member_id=member_id).first()
+        member = MEMBER.objects.filter(domain_id=domain, member_id=member_id).first()
         return member
 
     def create_report_tables(self, context):
-        '''
+        """
         Import all tables from the rendering package CSV file using bulk create
-        '''
+        """
         file_location = context.file_directory + os.sep + "technical_export" + os.sep + "table.csv"
         header_skipped = False
         tables_to_create = []
 
-        with open(file_location, encoding='utf-8') as csvfile:
-            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
+        with open(file_location, encoding="utf-8") as csvfile:
+            filereader = csv.reader(csvfile, delimiter=",", quotechar='"')
             for row in filereader:
                 if not header_skipped:
                     header_skipped = True
                 else:
                     table_id = row[ColumnIndexes().table_table_id]
@@ -522,36 +549,37 @@
                     maintenance_agency_id = row[ColumnIndexes().table_maintenance_agency_id]
                     version = row[ColumnIndexes().table_version]
                     valid_from = row[ColumnIndexes().table_valid_from]
                     valid_to = row[ColumnIndexes().table_valid_to]
 
-                    table = TABLE(
-                        name=ImportWebsiteToSDDModel.replace_dots(self, table_id))
+                    table = TABLE(name=ImportWebsiteToSDDModel.replace_dots(self, table_id))
                     table.table_id = ImportWebsiteToSDDModel.replace_dots(self, table_id)
                     table.name = display_name
                     table.code = code
                     table.description = description
-                    maintenance_agency = ImportWebsiteToSDDModel.find_maintenance_agency_with_id(self,context,maintenance_agency_id)
+                    maintenance_agency = ImportWebsiteToSDDModel.find_maintenance_agency_with_id(
+                        self, context, maintenance_agency_id
+                    )
                     table.maintenance_agency_id = maintenance_agency
                     table.version = version
 
                     tables_to_create.append(table)
                     context.report_tables_dictionary[table.table_id] = table
 
         if context.save_sdd_to_db and tables_to_create:
-            TABLE.objects.bulk_create(tables_to_create, batch_size=1000,ignore_conflicts=True)
+            TABLE.objects.bulk_create(tables_to_create, batch_size=1000, ignore_conflicts=True)
 
     def create_axis(self, context):
-        '''
+        """
         Import all axes from the rendering package CSV file using bulk create
-        '''
+        """
         file_location = context.file_directory + os.sep + "technical_export" + os.sep + "axis.csv"
         header_skipped = False
         axes_to_create = []
 
-        with open(file_location, encoding='utf-8') as csvfile:
-            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
+        with open(file_location, encoding="utf-8") as csvfile:
+            filereader = csv.reader(csvfile, delimiter=",", quotechar='"')
             for row in filereader:
                 if not header_skipped:
                     header_skipped = True
                 else:
                     axis_id = row[ColumnIndexes().axis_id]
@@ -560,33 +588,32 @@
                     axis_name = row[ColumnIndexes().axis_name]
                     axis_description = row[ColumnIndexes().axis_description]
                     axis_table_id = row[ColumnIndexes().axis_table_id]
                     axis_is_open_axis = row[ColumnIndexes().axis_is_open_axis]
 
-                    axis = AXIS(
-                        name=ImportWebsiteToSDDModel.replace_dots(self, axis_id))
+                    axis = AXIS(name=ImportWebsiteToSDDModel.replace_dots(self, axis_id))
                     axis.axis_id = ImportWebsiteToSDDModel.replace_dots(self, axis_id)
                     axis.orientation = axis_orientation
                     axis.description = axis_description
                     axis.table_id = ImportWebsiteToSDDModel.find_table_with_id(self, context, axis_table_id)
 
                     axes_to_create.append(axis)
                     context.axis_dictionary[axis.axis_id] = axis
 
         if context.save_sdd_to_db and axes_to_create:
-            AXIS.objects.bulk_create(axes_to_create, batch_size=1000,ignore_conflicts=True)
+            AXIS.objects.bulk_create(axes_to_create, batch_size=1000, ignore_conflicts=True)
 
     def create_axis_ordinates(self, context):
-        '''
+        """
         Import all axis ordinates from the rendering package CSV file using bulk create
-        '''
+        """
         file_location = context.file_directory + os.sep + "technical_export" + os.sep + "axis_ordinate.csv"
         header_skipped = False
         ordinates_to_create = []
 
-        with open(file_location, encoding='utf-8') as csvfile:
-            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
+        with open(file_location, encoding="utf-8") as csvfile:
+            filereader = csv.reader(csvfile, delimiter=",", quotechar='"')
             for row in filereader:
                 if not header_skipped:
                     header_skipped = True
                 else:
                     axis_ordinate_id = row[ColumnIndexes().axis_ordinate_axis_ordinate_id]
@@ -597,24 +624,25 @@
                     axis_ordinate_axis_id = row[ColumnIndexes().axis_ordinate_axis_id]
                     axis_ordinate_parent_axis_ordinate_id = row[ColumnIndexes().axis_ordinate_parent_axis_ordinate_id]
                     axis_ordinate_name = row[ColumnIndexes().axis_ordinate_name]
                     axis_ordinate_description = row[ColumnIndexes().axis_ordinate_description]
 
-                    axis_ordinate = AXIS_ORDINATE(
-                        name=ImportWebsiteToSDDModel.replace_dots(self, axis_ordinate_id))
+                    axis_ordinate = AXIS_ORDINATE(name=ImportWebsiteToSDDModel.replace_dots(self, axis_ordinate_id))
                     axis_ordinate.axis_ordinate_id = ImportWebsiteToSDDModel.replace_dots(self, axis_ordinate_id)
                     axis_ordinate.code = axis_ordinate_code
                     axis_ordinate.path = axis_ordinate_path
-                    axis_ordinate.axis_id = ImportWebsiteToSDDModel.find_axis_with_id(self, context, ImportWebsiteToSDDModel.replace_dots(self,axis_ordinate_axis_id))
+                    axis_ordinate.axis_id = ImportWebsiteToSDDModel.find_axis_with_id(
+                        self, context, ImportWebsiteToSDDModel.replace_dots(self, axis_ordinate_axis_id)
+                    )
                     axis_ordinate.name = axis_ordinate_name
                     axis_ordinate.description = axis_ordinate_description
 
                     ordinates_to_create.append(axis_ordinate)
                     context.axis_ordinate_dictionary[axis_ordinate.axis_ordinate_id] = axis_ordinate
 
         if context.save_sdd_to_db and ordinates_to_create:
-            AXIS_ORDINATE.objects.bulk_create(ordinates_to_create, batch_size=1000,ignore_conflicts=True)
+            AXIS_ORDINATE.objects.bulk_create(ordinates_to_create, batch_size=1000, ignore_conflicts=True)
 
     def _create_instances_from_csv_copy(self, context, cls):
         sdd_table_name = cls.__name__.lower()
         table_name = f"pybirdai_{sdd_table_name}"
 
@@ -627,95 +655,84 @@
             raise FileNotFoundError(f"CSV file not found: {csv_file}")
 
         try:
 
             # Define allowed table names to prevent SQL injection
-            ALLOWED_TABLES = {
-                'pybirdai_table_cell',
-                'pybirdai_ordinate_item', 
-                'pybirdai_cell_position'
-            }
-            
+            ALLOWED_TABLES = {"pybirdai_table_cell", "pybirdai_ordinate_item", "pybirdai_cell_position"}
+
             if table_name not in ALLOWED_TABLES:
                 raise ValueError(f"Table '{table_name}' not allowed for deletion")
-            
+
             with connection.cursor() as cursor:
-                if connection.vendor == 'sqlite':
+                if connection.vendor == "sqlite":
                     cursor.execute("PRAGMA foreign_keys = 0;")
                     # Table name is validated against whitelist above
                     cursor.execute(f"DELETE FROM {table_name};")
                     cursor.execute("PRAGMA foreign_keys = 1;")
-                elif connection.vendor == 'postgresql':
+                elif connection.vendor == "postgresql":
                     # Table name is validated against whitelist above
                     cursor.execute(f"TRUNCATE TABLE {table_name} CASCADE;")
-                elif connection.vendor in ['microsoft', 'mssql']:
+                elif connection.vendor in ["microsoft", "mssql"]:
                     # Table name is validated against whitelist above
                     cursor.execute(f"TRUNCATE TABLE {table_name};")
                 else:
                     # Table name is validated against whitelist above
                     cursor.execute(f"DELETE FROM {table_name};")
 
             # Import CSV data based on database vendor
-            if connection.vendor == 'sqlite':
+            if connection.vendor == "sqlite":
                 # SQLite needs to be handled outside transaction for subprocess
                 # Get database file path
-                db_file = Path(connection.settings_dict['NAME']).absolute()
+                db_file = Path(connection.settings_dict["NAME"]).absolute()
 
                 # Create the SQLite commands
-                commands = [
-                    ".mode csv",
-                    f".separator '{delimiter}'",
-                    f".import --skip 1 '{csv_file}' {table_name}"
-                ]
+                commands = [".mode csv", f".separator '{delimiter}'", f".import --skip 1 '{csv_file}' {table_name}"]
 
                 # Join commands with newlines
-                sqlite_script = '\n'.join(commands)
+                sqlite_script = "\n".join(commands)
 
                 sqlite_program = "sqlite3"
-                if platform.system() == 'Windows':
+                if platform.system() == "Windows":
                     sqlite_program += ".exe"
 
                 result = subprocess.run(
-                        [sqlite_program, str(db_file)],
-                        input=sqlite_script,
-                        text=True,
-                        capture_output=True,
-                        check=True
-                    )
+                    [sqlite_program, str(db_file)], input=sqlite_script, text=True, capture_output=True, check=True
+                )
                 if result.stderr:
                     raise Exception(f"SQLite import error: {result.stderr}")
                 return result
 
-            elif connection.vendor == 'postgresql':
+            elif connection.vendor == "postgresql":
                 # PostgreSQL COPY command
                 with connection.cursor() as cursor:
-                    with open(csv_file, 'r', encoding='utf-8') as f:
+                    with open(csv_file, "r", encoding="utf-8") as f:
                         # Skip header line
                         next(f)
                         # Use COPY FROM STDIN
                         cursor.copy_expert(
-                            f"COPY {table_name} FROM STDIN WITH (FORMAT CSV, DELIMITER '{delimiter}')",
-                            f
+                            f"COPY {table_name} FROM STDIN WITH (FORMAT CSV, DELIMITER '{delimiter}')", f
                         )
                 return None
 
-            elif connection.vendor in ['microsoft', 'mssql']:
+            elif connection.vendor in ["microsoft", "mssql"]:
                 # MSSQL bulk insert - requires special handling
                 # Note: BULK INSERT requires the file to be accessible by SQL Server
                 with connection.cursor() as cursor:
                     # Try using BULK INSERT if file is accessible
                     try:
-                        cursor.execute(f"""
+                        cursor.execute(
+                            f"""
                             BULK INSERT {table_name}
                             FROM '{csv_file}'
                             WITH (
                                 FORMAT = 'CSV',
                                 FIRSTROW = 2,
                                 FIELDTERMINATOR = '{delimiter}',
                                 ROWTERMINATOR = '\\n'
                             )
-                        """)
+                        """
+                        )
                     except Exception as e:
                         # Fallback to row-by-row insert if BULK INSERT fails
                         print(f"BULK INSERT failed: {e}. Falling back to row-by-row insert.")
                         self._fallback_csv_import(csv_file, table_name, delimiter)
                 return None
@@ -729,21 +746,21 @@
         except Exception as e:
             print(f"Error importing CSV for {table_name}: {str(e)}")
             raise
 
     def _fallback_csv_import(self, context, cls):
-        '''
+        """
         Fallback method for CSV import using raw SQL inserts
         This method is database-agnostic and works with any database backend
-        '''
+        """
         import csv
 
         fallback_import_func = {
             TABLE_CELL: self.create_table_cells,
             ORDINATE_ITEM: self.create_ordinate_items,
-            CELL_POSITION: self.create_cell_positions
-            }[cls](context)
+            CELL_POSITION: self.create_cell_positions,
+        }[cls](context)
 
         try:
             self.fallback_import_func(context)
         except Exception as e:
             print(f"Error in fallback CSV import for {table_name}: {str(e)}")
@@ -760,148 +777,171 @@
     def create_cell_positions_csv_copy(self, context):
         # Ensure paths are absolute
         self._create_instances_from_csv_copy(context, CELL_POSITION)
 
     def create_ordinate_items(self, context):
-        '''
+        """
         Import all ordinate items from the rendering package CSV file
-        '''
+        """
         file_location = context.file_directory + os.sep + "technical_export" + os.sep + "ordinate_item.csv"
         header_skipped = False
         ordinate_items_to_create = []
 
-        with open(file_location, encoding='utf-8') as csvfile:
-            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
+        with open(file_location, encoding="utf-8") as csvfile:
+            filereader = csv.reader(csvfile, delimiter=",", quotechar='"')
             id_increment = 0
             for row in filereader:
                 if not header_skipped:
                     header_skipped = True
-                    if row[0].upper() == 'ID': #sometimes exported data without a  primary key has an ID field added at the time of export, exported data is re-imported
+                    if (
+                        row[0].upper() == "ID"
+                    ):  # sometimes exported data without a  primary key has an ID field added at the time of export, exported data is re-imported
                         id_increment = 1
                 else:
                     axis_ordinate_id = row[ColumnIndexes().ordinate_item_axis_ordinate_id + id_increment]
                     variable_id = row[ColumnIndexes().ordinate_item_variable_id + id_increment]
                     member_id = row[ColumnIndexes().ordinate_item_member_id + id_increment]
                     member_hierarchy_id = row[ColumnIndexes().ordinate_item_member_hierarchy_id + id_increment]
                     starting_member_id = row[ColumnIndexes().ordinate_item_starting_member_id + id_increment]
-                    is_starting_member_included = row[ColumnIndexes().ordinate_item_is_starting_member_included + id_increment]
+                    is_starting_member_included = row[
+                        ColumnIndexes().ordinate_item_is_starting_member_included + id_increment
+                    ]
 
                     print(f"member_id: {member_id}")
                     print(f"variable_id: {variable_id}")
                     print(f"axis_ordinate_id: {axis_ordinate_id}")
                     print(f"member_hierarchy_id: {member_hierarchy_id}")
                     print(f"starting_member_id: {starting_member_id}")
                     print(f"is_starting_member_included: {is_starting_member_included}")
 
                     ordinate_item = ORDINATE_ITEM()
                     ordinate_item.axis_ordinate_id = ImportWebsiteToSDDModel.find_axis_ordinate_with_id(
-                        self, context, ImportWebsiteToSDDModel.replace_dots(self, axis_ordinate_id))
+                        self, context, ImportWebsiteToSDDModel.replace_dots(self, axis_ordinate_id)
+                    )
 
                     print(ordinate_item.axis_ordinate_id)
                     ordinate_item.variable_id = ImportWebsiteToSDDModel.find_variable_with_id(
-                        self, context, ImportWebsiteToSDDModel.replace_dots(self, variable_id))
+                        self, context, ImportWebsiteToSDDModel.replace_dots(self, variable_id)
+                    )
                     ordinate_item.member_id = ImportWebsiteToSDDModel.find_member_with_id(
-                        self, ImportWebsiteToSDDModel.replace_dots(self, member_id), context)
+                        self, ImportWebsiteToSDDModel.replace_dots(self, member_id), context
+                    )
                     ordinate_item.member_hierarchy_id = ImportWebsiteToSDDModel.find_member_hierarchy_with_id(
-                        self, ImportWebsiteToSDDModel.replace_dots(self, member_hierarchy_id), context)
+                        self, ImportWebsiteToSDDModel.replace_dots(self, member_hierarchy_id), context
+                    )
                     ordinate_item.starting_member_id = ImportWebsiteToSDDModel.find_member_with_id(
-                        self, ImportWebsiteToSDDModel.replace_dots(self, starting_member_id), context)
+                        self, ImportWebsiteToSDDModel.replace_dots(self, starting_member_id), context
+                    )
                     ordinate_item.is_starting_member_included = is_starting_member_included
 
                     ordinate_items_to_create.append(ordinate_item)
 
                     try:
-                        ordinate_items = context.axis_ordinate_to_ordinate_items_map[ordinate_item.axis_ordinate_id.axis_ordinate_id]
+                        ordinate_items = context.axis_ordinate_to_ordinate_items_map[
+                            ordinate_item.axis_ordinate_id.axis_ordinate_id
+                        ]
                         ordinate_items.append(ordinate_item)
                     except KeyError:
-                        context.axis_ordinate_to_ordinate_items_map[ordinate_item.axis_ordinate_id.axis_ordinate_id] = [ordinate_item]
+                        context.axis_ordinate_to_ordinate_items_map[ordinate_item.axis_ordinate_id.axis_ordinate_id] = [
+                            ordinate_item
+                        ]
 
         if context.save_sdd_to_db and ordinate_items_to_create:
-            ORDINATE_ITEM.objects.bulk_create(ordinate_items_to_create, batch_size=50000,ignore_conflicts=True)
-
-    def create_table_cells(self, context, dpm:bool=False):
-        '''
+            ORDINATE_ITEM.objects.bulk_create(ordinate_items_to_create, batch_size=50000, ignore_conflicts=True)
+
+    def create_table_cells(self, context, dpm: bool = False):
+        """
         Import all table cells from the rendering package CSV file
-        '''
+        """
         file_location = context.file_directory + os.sep + "technical_export" + os.sep + "table_cell.csv"
         header_skipped = False
         table_cells_to_create = []
 
-        with open(file_location, encoding='utf-8') as csvfile:
-            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
+        with open(file_location, encoding="utf-8") as csvfile:
+            filereader = csv.reader(csvfile, delimiter=",", quotechar='"')
             id_increment = 0
             for row in filereader:
                 if not header_skipped:
                     header_skipped = True
-                    if row[0].upper() == 'ID': #sometimes exported data without a  primary key has an ID field added at the time of export, exported data is re-imported
+                    if (
+                        row[0].upper() == "ID"
+                    ):  # sometimes exported data without a  primary key has an ID field added at the time of export, exported data is re-imported
                         id_increment = 1
                 else:
                     table_cell_cell_id = row[ColumnIndexes().table_cell_cell_id + id_increment]
                     table_cell_combination_id = row[ColumnIndexes().table_cell_combination_id + id_increment]
                     table_cell_table_id = row[ColumnIndexes().table_cell_table_id + id_increment]
 
                     if table_cell_cell_id.endswith("_REF") or dpm:
-                        table_cell = TABLE_CELL(
-                            name=ImportWebsiteToSDDModel.replace_dots(self, table_cell_cell_id))
+                        table_cell = TABLE_CELL(name=ImportWebsiteToSDDModel.replace_dots(self, table_cell_cell_id))
                         table_cell.cell_id = ImportWebsiteToSDDModel.replace_dots(self, table_cell_cell_id)
                         table_cell.table_id = ImportWebsiteToSDDModel.find_table_with_id(
-                            self, context, ImportWebsiteToSDDModel.replace_dots(self, table_cell_table_id))
+                            self, context, ImportWebsiteToSDDModel.replace_dots(self, table_cell_table_id)
+                        )
                         table_cell.table_cell_combination_id = table_cell_combination_id
 
                         table_cells_to_create.append(table_cell)
                         context.table_cell_dictionary[table_cell.cell_id] = table_cell
 
                         table_cell_list = context.table_to_table_cell_dictionary.setdefault(table_cell.table_id, [])
                         table_cell_list.append(table_cell)
 
         if context.save_sdd_to_db and table_cells_to_create:
-            TABLE_CELL.objects.bulk_create(table_cells_to_create, batch_size=1000,ignore_conflicts=True)
-
-    def create_cell_positions(self, context, dpm:bool = False):
-        '''
+            TABLE_CELL.objects.bulk_create(table_cells_to_create, batch_size=1000, ignore_conflicts=True)
+
+    def create_cell_positions(self, context, dpm: bool = False):
+        """
         Import all cell positions from the rendering package CSV file
-        '''
+        """
         file_location = context.file_directory + os.sep + "technical_export" + os.sep + "cell_position.csv"
         header_skipped = False
         cell_positions_to_create = []
         id_increment = 0
-        with open(file_location, encoding='utf-8') as csvfile:
-            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
-            for row in filereader:
-                if not header_skipped:
-                    header_skipped = True
-                    if row[0].upper() == 'ID': #sometimes exported data without a  primary key has an ID field added at the time of export, exported data is re-imported
+        with open(file_location, encoding="utf-8") as csvfile:
+            filereader = csv.reader(csvfile, delimiter=",", quotechar='"')
+            for row in filereader:
+                if not header_skipped:
+                    header_skipped = True
+                    if (
+                        row[0].upper() == "ID"
+                    ):  # sometimes exported data without a  primary key has an ID field added at the time of export, exported data is re-imported
                         id_increment = 1
                 else:
                     cell_positions_cell_id = row[ColumnIndexes().cell_positions_cell_id + id_increment]
-                    cell_positions_axis_ordinate_id = row[ColumnIndexes().cell_positions_axis_ordinate_id + id_increment]
+                    cell_positions_axis_ordinate_id = row[
+                        ColumnIndexes().cell_positions_axis_ordinate_id + id_increment
+                    ]
 
                     if cell_positions_cell_id.endswith("_REF") or dpm:
                         cell_position = CELL_POSITION()
                         cell_position.axis_ordinate_id = ImportWebsiteToSDDModel.find_axis_ordinate_with_id(
-                            self, context, ImportWebsiteToSDDModel.replace_dots(self, cell_positions_axis_ordinate_id))
+                            self, context, ImportWebsiteToSDDModel.replace_dots(self, cell_positions_axis_ordinate_id)
+                        )
                         cell_position.cell_id = ImportWebsiteToSDDModel.find_table_cell_with_id(
-                            self, context, ImportWebsiteToSDDModel.replace_dots(self, cell_positions_cell_id))
+                            self, context, ImportWebsiteToSDDModel.replace_dots(self, cell_positions_cell_id)
+                        )
 
                         cell_positions_to_create.append(cell_position)
 
-                        cell_positions_list = context.cell_positions_dictionary.setdefault(cell_position.cell_id.cell_id, [])
+                        cell_positions_list = context.cell_positions_dictionary.setdefault(
+                            cell_position.cell_id.cell_id, []
+                        )
                         cell_positions_list.append(cell_position)
 
         if context.save_sdd_to_db and cell_positions_to_create:
-            CELL_POSITION.objects.bulk_create(cell_positions_to_create, batch_size=1000,ignore_conflicts=True)
+            CELL_POSITION.objects.bulk_create(cell_positions_to_create, batch_size=1000, ignore_conflicts=True)
 
     def create_member_mappings(self, context):
-        '''
+        """
         Import all member mappings from the rendering package CSV file
-        '''
+        """
         file_location = context.file_directory + os.sep + "technical_export" + os.sep + "member_mapping.csv"
         header_skipped = False
         member_mappings_to_create = []
 
-        with open(file_location, encoding='utf-8') as csvfile:
-            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
+        with open(file_location, encoding="utf-8") as csvfile:
+            filereader = csv.reader(csvfile, delimiter=",", quotechar='"')
             for row in filereader:
                 if not header_skipped:
                     header_skipped = True
                 else:
                     maintenance_agency_id = row[ColumnIndexes().member_map_maintenance_agency_id]
@@ -912,263 +952,291 @@
                         member_mapping = MEMBER_MAPPING()
                         member_mapping.member_mapping_id = member_mapping_id
                         member_mapping.name = name
                         member_mapping.code = code
                         member_mapping.maintenance_agency_id = ImportWebsiteToSDDModel.find_maintenance_agency_with_id(
-                            self, context, maintenance_agency_id)
+                            self, context, maintenance_agency_id
+                        )
 
                         member_mappings_to_create.append(member_mapping)
                         context.member_mapping_dictionary[member_mapping_id] = member_mapping
 
         if context.save_sdd_to_db and member_mappings_to_create:
-            MEMBER_MAPPING.objects.bulk_create(member_mappings_to_create, batch_size=1000,ignore_conflicts=True)
+            MEMBER_MAPPING.objects.bulk_create(member_mappings_to_create, batch_size=1000, ignore_conflicts=True)
 
     def create_all_member_mappings_items(self, context):
-        '''
+        """
         Import all member mapping items from the rendering package CSV file using bulk create
-        '''
+        """
 
         file_location = context.file_directory + os.sep + "technical_export" + os.sep + "member_mapping_item.csv"
         header_skipped = False
         missing_members = []
         missing_variables = []
         member_mapping_items_to_create = []
         id_increment = 0
-        with open(file_location, encoding='utf-8') as csvfile:
-            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
-            for row in filereader:
-                if not header_skipped:
-                    header_skipped = True
-                    if row[0].upper() == 'ID': #sometimes exported data without a  primary key has an ID field added at the time of export, exported data is re-imported
+        with open(file_location, encoding="utf-8") as csvfile:
+            filereader = csv.reader(csvfile, delimiter=",", quotechar='"')
+            for row in filereader:
+                if not header_skipped:
+                    header_skipped = True
+                    if (
+                        row[0].upper() == "ID"
+                    ):  # sometimes exported data without a  primary key has an ID field added at the time of export, exported data is re-imported
                         id_increment = 1
                 else:
                     member_mapping_id = row[ColumnIndexes().member_mapping_id + id_increment]
                     row_number = row[ColumnIndexes().member_mapping_row + id_increment]
                     variable_id = row[ColumnIndexes().member_mapping_variable_id + id_increment]
                     is_source = row[ColumnIndexes().member_mapping_is_source + id_increment]
                     member_id = row[ColumnIndexes().member_mapping_member_id + id_increment]
                     if not member_mapping_id.startswith("SHS_"):
-                        member = ImportWebsiteToSDDModel.find_member_with_id(
-                                                            self,member_id,context)
-                        variable = ImportWebsiteToSDDModel.find_variable_with_id(
-                                                            self,context,variable_id)
+                        member = ImportWebsiteToSDDModel.find_member_with_id(self, member_id, context)
+                        variable = ImportWebsiteToSDDModel.find_variable_with_id(self, context, variable_id)
 
                         if member is None:
                             if member_id not in missing_members:
-                                missing_members.append((member_id,member_mapping_id,row_number,variable_id))
+                                missing_members.append((member_id, member_mapping_id, row_number, variable_id))
                         if variable is None:
                             if variable_id not in missing_variables:
-                                missing_variables.append((variable_id,'',''))
-
+                                missing_variables.append((variable_id, "", ""))
 
                         if member is None or variable is None:
                             pass
                         else:
                             member_mapping_item = MEMBER_MAPPING_ITEM()
                             member_mapping_item.is_source = is_source
                             member_mapping_item.member_id = member
                             member_mapping_item.variable_id = variable
                             member_mapping_item.member_mapping_row = row_number
-                            member_mapping_item.member_mapping_id  = ImportWebsiteToSDDModel.find_member_mapping_with_id(
-                                                self,context,member_mapping_id)
+                            member_mapping_item.member_mapping_id = ImportWebsiteToSDDModel.find_member_mapping_with_id(
+                                self, context, member_mapping_id
+                            )
 
                             if context.save_sdd_to_db:
                                 member_mapping_items_to_create.append(member_mapping_item)
                             try:
                                 member_mapping_items_list = context.member_mapping_items_dictionary[member_mapping_id]
                                 member_mapping_items_list.append(member_mapping_item)
                             except KeyError:
                                 context.member_mapping_items_dictionary[member_mapping_id] = [member_mapping_item]
         if context.save_sdd_to_db and member_mapping_items_to_create:
-            MEMBER_MAPPING_ITEM.objects.bulk_create(member_mapping_items_to_create, batch_size=1000,ignore_conflicts=True)
+            MEMBER_MAPPING_ITEM.objects.bulk_create(
+                member_mapping_items_to_create, batch_size=1000, ignore_conflicts=True
+            )
         for missing_member in missing_members:
             print(f"Missing member {missing_member}")
         for missing_variable in missing_variables:
             print(f"Missing variable {missing_variable}")
-        ImportWebsiteToSDDModel.save_missing_mapping_variables_to_csv(context,missing_variables)
-        ImportWebsiteToSDDModel.save_missing_mapping_members_to_csv(context,missing_members)
-
-    def save_missing_mapping_variables_to_csv(context,missing_variables):
-        filename = context.output_directory + os.sep + "generated_mapping_warnings" + os.sep + "mappings_missing_variables.csv"
-        with open(filename, 'w', newline='') as csvfile:
+        ImportWebsiteToSDDModel.save_missing_mapping_variables_to_csv(context, missing_variables)
+        ImportWebsiteToSDDModel.save_missing_mapping_members_to_csv(context, missing_members)
+
+    def save_missing_mapping_variables_to_csv(context, missing_variables):
+        filename = (
+            context.output_directory + os.sep + "generated_mapping_warnings" + os.sep + "mappings_missing_variables.csv"
+        )
+        with open(filename, "w", newline="") as csvfile:
             writer = csv.writer(csvfile)
-            writer.writerow(["Varaible","Mapping","Valid_to"])
+            writer.writerow(["Varaible", "Mapping", "Valid_to"])
             for var in missing_variables:
-                writer.writerow([var[0],var[1],var[2]])
-
-    def save_missing_mapping_members_to_csv(context,missing_members):
-        filename = context.output_directory + os.sep + "generated_mapping_warnings" + os.sep + "mappings_missing_members.csv"
-        with open(filename, 'w', newline='') as csvfile:
+                writer.writerow([var[0], var[1], var[2]])
+
+    def save_missing_mapping_members_to_csv(context, missing_members):
+        filename = (
+            context.output_directory + os.sep + "generated_mapping_warnings" + os.sep + "mappings_missing_members.csv"
+        )
+        with open(filename, "w", newline="") as csvfile:
             writer = csv.writer(csvfile)
-            writer.writerow(["Member","Mapping","Row","Variable"])
+            writer.writerow(["Member", "Mapping", "Row", "Variable"])
             for mem in missing_members:
-                writer.writerow([mem[0],mem[1],mem[2],mem[3]])
-
-        ImportWebsiteToSDDModel.create_mappings_warnings_summary(context,missing_members)
-
-    def create_mappings_warnings_summary(context,missing_members):
-        filename = context.output_directory + os.sep + "generated_mapping_warnings" + os.sep + "mappings_warnings_summary.csv"
-        #create a list of unique missing variable ids
+                writer.writerow([mem[0], mem[1], mem[2], mem[3]])
+
+        ImportWebsiteToSDDModel.create_mappings_warnings_summary(context, missing_members)
+
+    def create_mappings_warnings_summary(context, missing_members):
+        filename = (
+            context.output_directory + os.sep + "generated_mapping_warnings" + os.sep + "mappings_warnings_summary.csv"
+        )
+        # create a list of unique missing variable ids
         # read mappings_missing_variables file into a dictionary
-        missing_variables= []
+        missing_variables = []
         written_members = []
-        varaibles_filename = context.output_directory + os.sep + "generated_mapping_warnings" + os.sep + "mappings_missing_variables.csv"
-        with open(varaibles_filename, 'r', newline='') as csvfile:
+        varaibles_filename = (
+            context.output_directory + os.sep + "generated_mapping_warnings" + os.sep + "mappings_missing_variables.csv"
+        )
+        with open(varaibles_filename, "r", newline="") as csvfile:
             reader = csv.reader(csvfile)
             for row in reader:
                 if row[0] not in missing_variables:
                     missing_variables.append(row[0])
 
-        with open(filename, 'w', newline='') as csvfile:
+        with open(filename, "w", newline="") as csvfile:
             writer = csv.writer(csvfile)
-            writer.writerow(["Variable","Member"])
+            writer.writerow(["Variable", "Member"])
             for var in missing_variables:
-                writer.writerow([var,''])
+                writer.writerow([var, ""])
 
             for mem in missing_members:
                 variable = mem[3]
                 member = mem[0]
                 if member not in written_members:
                     if variable not in missing_variables:
-                        writer.writerow([variable,member])
+                        writer.writerow([variable, member])
                     written_members.append(member)
 
     def create_all_mapping_definitions(self, context):
-        '''
+        """
         Import all mapping definitions from the rendering package CSV file using bulk create
-        '''
+        """
         file_location = context.file_directory + os.sep + "technical_export" + os.sep + "mapping_definition.csv"
         mapping_definitions_to_create = []
 
         # Cache lookups
         member_mapping_cache = {}
         variable_mapping_cache = {}
 
-        with open(file_location, encoding='utf-8') as csvfile:
+        with open(file_location, encoding="utf-8") as csvfile:
             rows = list(csv.reader(csvfile))[1:]  # Skip header
 
             for row in rows:
                 mapping_id = row[ColumnIndexes().mapping_definition_mapping_id]
                 if mapping_id.startswith("SHS_"):
                     continue
 
                 member_mapping_id = row[ColumnIndexes().mapping_definition_member_mapping_id]
                 if member_mapping_id not in member_mapping_cache:
                     member_mapping_cache[member_mapping_id] = ImportWebsiteToSDDModel.find_member_mapping_with_id(
-                        self, context, member_mapping_id)
+                        self, context, member_mapping_id
+                    )
 
                 variable_mapping_id = row[ColumnIndexes().mapping_definition_variable_mapping_id]
                 if variable_mapping_id not in variable_mapping_cache:
                     variable_mapping_cache[variable_mapping_id] = ImportWebsiteToSDDModel.find_variable_mapping_with_id(
-                        self, context, variable_mapping_id)
+                        self, context, variable_mapping_id
+                    )
 
                 mapping_definition = MAPPING_DEFINITION(
                     mapping_id=mapping_id,
                     name=row[ColumnIndexes().mapping_definition_name],
                     code=row[ColumnIndexes().mapping_definition_code],
                     mapping_type=row[ColumnIndexes().mapping_definition_mapping_type],
                     member_mapping_id=member_mapping_cache[member_mapping_id],
-                    variable_mapping_id=variable_mapping_cache[variable_mapping_id]
+                    variable_mapping_id=variable_mapping_cache[variable_mapping_id],
                 )
 
                 mapping_definitions_to_create.append(mapping_definition)
                 context.mapping_definition_dictionary[mapping_id] = mapping_definition
 
         if context.save_sdd_to_db and mapping_definitions_to_create:
-            MAPPING_DEFINITION.objects.bulk_create(mapping_definitions_to_create, batch_size=5000,ignore_conflicts=True)
+            MAPPING_DEFINITION.objects.bulk_create(
+                mapping_definitions_to_create, batch_size=5000, ignore_conflicts=True
+            )
 
     def create_all_mapping_to_cubes(self, context):
-        '''
+        """
         Import all mapping to cubes from the rendering package CSV file
-        '''
+        """
         file_location = context.file_directory + os.sep + "technical_export" + os.sep + "mapping_to_cube.csv"
         header_skipped = False
         mapping_to_cubes_to_create = []
 
-        with open(file_location, encoding='utf-8') as csvfile:
-            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
+        with open(file_location, encoding="utf-8") as csvfile:
+            filereader = csv.reader(csvfile, delimiter=",", quotechar='"')
             id_increment = 0
             for row in filereader:
                 if not header_skipped:
                     header_skipped = True
-                    if row[0].upper() == 'ID': #sometimes exported data without a  primary key has an ID field added at the time of export, exported data is re-imported
+                    if (
+                        row[0].upper() == "ID"
+                    ):  # sometimes exported data without a  primary key has an ID field added at the time of export, exported data is re-imported
                         id_increment = 1
                 else:
                     mapping_to_cube_mapping_id = row[ColumnIndexes().mapping_to_cube_mapping_id + id_increment]
-                    mapping_to_cube_cube_mapping_id = row[ColumnIndexes().mapping_to_cube_cube_mapping_id + id_increment]
+                    mapping_to_cube_cube_mapping_id = row[
+                        ColumnIndexes().mapping_to_cube_cube_mapping_id + id_increment
+                    ]
                     mapping_to_cube_valid_from = row[ColumnIndexes().mapping_to_cube_valid_from + id_increment]
                     mapping_to_cube_valid_to = row[ColumnIndexes().mapping_to_cube_valid_to + id_increment]
 
                     if not mapping_to_cube_mapping_id.startswith("M_SHS"):
                         mapping_to_cube = MAPPING_TO_CUBE(
-                            mapping_id=ImportWebsiteToSDDModel.find_mapping_definition_with_id(self, context, mapping_to_cube_mapping_id),
+                            mapping_id=ImportWebsiteToSDDModel.find_mapping_definition_with_id(
+                                self, context, mapping_to_cube_mapping_id
+                            ),
                             cube_mapping_id=ImportWebsiteToSDDModel.replace_dots(self, mapping_to_cube_cube_mapping_id),
                             valid_from=mapping_to_cube_valid_from,
-                            valid_to=mapping_to_cube_valid_to
+                            valid_to=mapping_to_cube_valid_to,
                         )
 
                         mapping_to_cubes_to_create.append(mapping_to_cube)
 
                         mapping_to_cube_list = context.mapping_to_cube_dictionary.setdefault(
-                            mapping_to_cube.cube_mapping_id, [])
+                            mapping_to_cube.cube_mapping_id, []
+                        )
                         mapping_to_cube_list.append(mapping_to_cube)
 
         if context.save_sdd_to_db and mapping_to_cubes_to_create:
-            MAPPING_TO_CUBE.objects.bulk_create(mapping_to_cubes_to_create, batch_size=1000,ignore_conflicts=True)
+            MAPPING_TO_CUBE.objects.bulk_create(mapping_to_cubes_to_create, batch_size=1000, ignore_conflicts=True)
 
     def create_all_variable_mappings(self, context):
-        '''
+        """
         Import all variable mappings from the rendering package CSV file
-        '''
+        """
         file_location = context.file_directory + os.sep + "technical_export" + os.sep + "variable_mapping.csv"
 
         # Pre-filter SHS_ entries and build batch
         variable_mappings_to_create = []
 
         # Read entire CSV at once instead of line by line
-        with open(file_location, encoding='utf-8') as csvfile:
+        with open(file_location, encoding="utf-8") as csvfile:
             rows = list(csv.reader(csvfile))[1:]  # Skip header
 
             # Process in a single pass
             for row in rows:
                 variable_mapping_id = row[ColumnIndexes().variable_mapping_variable_mapping_id]
 
-                if not variable_mapping_id.startswith("SHS_") and variable_mapping_id not in context.variable_mapping_dictionary:
+                if (
+                    not variable_mapping_id.startswith("SHS_")
+                    and variable_mapping_id not in context.variable_mapping_dictionary
+                ):
                     variable_mapping = VARIABLE_MAPPING(
                         variable_mapping_id=variable_mapping_id,
                         maintenance_agency_id=ImportWebsiteToSDDModel.find_maintenance_agency_with_id(
-                            self, context, row[ColumnIndexes().variable_mapping_maintenance_agency_id]),
+                            self, context, row[ColumnIndexes().variable_mapping_maintenance_agency_id]
+                        ),
                         code=row[ColumnIndexes().variable_mapping_code],
-                        name=row[ColumnIndexes().variable_mapping_name]
+                        name=row[ColumnIndexes().variable_mapping_name],
                     )
 
                     variable_mappings_to_create.append(variable_mapping)
                     context.variable_mapping_dictionary[variable_mapping_id] = variable_mapping
 
         # Single bulk create with larger batch size
         if context.save_sdd_to_db and variable_mappings_to_create:
-            VARIABLE_MAPPING.objects.bulk_create(variable_mappings_to_create, batch_size=5000,ignore_conflicts=True)
+            VARIABLE_MAPPING.objects.bulk_create(variable_mappings_to_create, batch_size=5000, ignore_conflicts=True)
 
     def create_all_variable_mapping_items(self, context):
-        '''
+        """
         Import all variable mapping items from the rendering package CSV file
-        '''
+        """
         file_location = context.file_directory + os.sep + "technical_export" + os.sep + "variable_mapping_item.csv"
         missing_variables = []
         variable_mapping_items_to_create = []
         id_increment = 0
         # Cache variable lookups
         variable_cache = {}
 
-        with open(file_location, encoding='utf-8') as csvfile:
-            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
+        with open(file_location, encoding="utf-8") as csvfile:
+            filereader = csv.reader(csvfile, delimiter=",", quotechar='"')
             header_skipped = False
             id_increment = 0
             for row in filereader:
                 if not header_skipped:
                     header_skipped = True
-                    if row[0].upper() == 'ID': #sometimes exported data without a  primary key has an ID field added at the time of export, exported data is re-imported
+                    if (
+                        row[0].upper() == "ID"
+                    ):  # sometimes exported data without a  primary key has an ID field added at the time of export, exported data is re-imported
                         id_increment = 1
                 else:
                     mapping_id = row[ColumnIndexes().varaible_mapping_item_variable_mapping_id + id_increment]
                     if mapping_id.startswith("SHS_"):
                         continue
@@ -1176,56 +1244,62 @@
                     variable_id = row[ColumnIndexes().variable_mapping_item_variable_id + id_increment]
 
                     # Use cached variable lookup
                     if variable_id not in variable_cache:
                         variable_cache[variable_id] = ImportWebsiteToSDDModel.find_variable_with_id(
-                            self, context, variable_id)
+                            self, context, variable_id
+                        )
 
                     variable = variable_cache[variable_id]
 
                     if variable is None:
-                        missing_variables.append((
-                            variable_id,
-                            mapping_id,
-                            row[ColumnIndexes().variable_mapping_item_valid_to + id_increment]
-                        ))
+                        missing_variables.append(
+                            (
+                                variable_id,
+                                mapping_id,
+                                row[ColumnIndexes().variable_mapping_item_valid_to + id_increment],
+                            )
+                        )
                         continue
 
                     variable_mapping_item = VARIABLE_MAPPING_ITEM(
                         variable_id=variable,
                         variable_mapping_id=ImportWebsiteToSDDModel.find_variable_mapping_with_id(
-                            self, context, mapping_id),
+                            self, context, mapping_id
+                        ),
                         is_source=row[ColumnIndexes().variable_mapping_item_is_source + id_increment],
                         valid_from=row[ColumnIndexes().variable_mapping_item_valid_from + id_increment],
-                        valid_to=row[ColumnIndexes().variable_mapping_item_valid_to + id_increment]
+                        valid_to=row[ColumnIndexes().variable_mapping_item_valid_to + id_increment],
                     )
 
                     variable_mapping_items_to_create.append(variable_mapping_item)
 
                     # Build dictionary in a single operation
                     context.variable_mapping_item_dictionary.setdefault(mapping_id, []).append(variable_mapping_item)
 
         # Single bulk create with larger batch size
         if context.save_sdd_to_db and variable_mapping_items_to_create:
-            VARIABLE_MAPPING_ITEM.objects.bulk_create(variable_mapping_items_to_create, batch_size=5000,ignore_conflicts=True)
+            VARIABLE_MAPPING_ITEM.objects.bulk_create(
+                variable_mapping_items_to_create, batch_size=5000, ignore_conflicts=True
+            )
 
         if missing_variables:
             ImportWebsiteToSDDModel.save_missing_mapping_variables_to_csv(context, missing_variables)
 
-    def find_member_mapping_with_id(self,context,member_mapping_id):
-        '''
+    def find_member_mapping_with_id(self, context, member_mapping_id):
+        """
         Find an existing member mapping with this id
-        '''
+        """
         try:
             return context.member_mapping_dictionary[member_mapping_id]
         except KeyError:
             return None
 
-    def find_member_with_id(self,element_id,context):
-        '''
+    def find_member_with_id(self, element_id, context):
+        """
         Find an existing member with this id
-        '''
+        """
         try:
             return context.member_dictionary[element_id]
         except:
             try:
                 return context.member_dictionary[element_id]
@@ -1233,122 +1307,121 @@
                 try:
                     return context.members_that_are_nodes[element_id]
                 except KeyError:
                     return None
 
-    def find_member_hierarchy_with_id(self,element_id,context):
-        '''
+    def find_member_hierarchy_with_id(self, element_id, context):
+        """
         Find an existing member hierarchy with this id
-        '''
+        """
         try:
             return context.member_hierarchy_dictionary[element_id]
         except KeyError:
             return None
 
-    def find_variable_with_id(self,context, element_id):
-        '''
+    def find_variable_with_id(self, context, element_id):
+        """
         Find an existing variable with this id
-        '''
+        """
         try:
             return context.variable_dictionary[element_id]
         except KeyError:
             try:
                 return context.variable_dictionary[element_id]
             except KeyError:
                 return None
 
-    def find_maintenance_agency_with_id(self,context, element_id):
-        '''
+    def find_maintenance_agency_with_id(self, context, element_id):
+        """
         Find an existing maintenance agency with this id
-        '''
+        """
         try:
             return context.agency_dictionary[element_id]
         except KeyError:
             return None
 
-    def find_domain_with_id(self,context, element_id):
-        '''
+    def find_domain_with_id(self, context, element_id):
+        """
         Find an existing domain with this id
-        '''
+        """
         try:
             return context.domain_dictionary[element_id]
         except KeyError:
             try:
                 return_item = context.domain_dictionary[element_id]
                 return return_item
             except KeyError:
                 return None
 
     def find_table_with_id(self, context, table_id):
-        '''
+        """
         Get the report table with the given id
-        '''
+        """
         try:
             return context.report_tables_dictionary[table_id]
         except KeyError:
             return None
 
     def find_axis_with_id(self, context, axis_id):
-        '''
+        """
         Get the axis with the given id
-        '''
+        """
         try:
             return context.axis_dictionary[axis_id]
         except KeyError:
             return None
 
     def find_table_cell_with_id(self, context, table_cell_id):
-        '''
+        """
         Get the table cell with the given id
-        '''
+        """
         try:
             return context.table_cell_dictionary[table_cell_id]
         except KeyError:
             return None
 
     def find_axis_ordinate_with_id(self, context, axis_ordinate_id):
-        '''
+        """
         Get the existing ordinate with the given id
-        '''
+        """
         try:
             return context.axis_ordinate_dictionary[axis_ordinate_id]
         except KeyError:
             return None
 
     def replace_dots(self, text):
-        '''
+        """
         Replace dots with underscores in the given text
-        '''
-        return text.replace('.', '_')
+        """
+        return text.replace(".", "_")
 
     def find_variable_mapping_with_id(self, context, variable_mapping_id):
-        '''
+        """
         Get the variable mapping with the given id
-        '''
+        """
         try:
             return context.variable_mapping_dictionary[variable_mapping_id]
         except KeyError:
             return None
 
     def find_mapping_definition_with_id(self, context, mapping_definition_id):
-        '''
+        """
         get the mapping definition with the given id
-        '''
+        """
         try:
             return context.mapping_definition_dictionary[mapping_definition_id]
         except KeyError:
             return None
 
-
     def delete_hierarchy_warnings_files(self, context):
-        '''
+        """
         Delete warning files more efficiently using pathlib
-        '''
-        warnings_dir = Path(settings.BASE_DIR) / 'results' / 'generated_hierarchy_warnings'
-        for file in warnings_dir.glob('*'):
+        """
+        warnings_dir = Path(settings.BASE_DIR) / "results" / "generated_hierarchy_warnings"
+        for file in warnings_dir.glob("*"):
             file.unlink()
 
     def delete_mapping_warnings_files(self, context):
         base_dir = settings.BASE_DIR
-        mapping_warnings_dir = os.path.join(base_dir, 'results', 'generated_mapping_warnings')
+        mapping_warnings_dir = os.path.join(base_dir, "results", "generated_mapping_warnings")
         for file in os.listdir(mapping_warnings_dir):
             os.remove(os.path.join(mapping_warnings_dir, file))
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py
--- /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/pybird/orchestration.py	2025-09-02 15:09:38.717593+00:00
+++ /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/pybird/orchestration.py	2025-09-21 17:07:37.293240+00:00
@@ -12,2225 +12,2160 @@
 
 
 from django.apps import apps
 from pybirdai.process_steps.pybird.csv_converter import CSVConverter
 from pybirdai.models import (
-    Trail, MetaDataTrail, DatabaseTable, DerivedTable,
-    DatabaseField, Function, FunctionText, TableCreationFunction,
-    PopulatedDataBaseTable, EvaluatedDerivedTable, DatabaseRow,
-    DerivedTableRow, DatabaseColumnValue, EvaluatedFunction,
-    AortaTableReference, FunctionColumnReference, DerivedRowSourceReference,
-    EvaluatedFunctionSourceValue, TableCreationSourceTable,
-    CalculationUsedRow, CalculationUsedField
+    Trail,
+    MetaDataTrail,
+    DatabaseTable,
+    DerivedTable,
+    DatabaseField,
+    Function,
+    FunctionText,
+    TableCreationFunction,
+    PopulatedDataBaseTable,
+    EvaluatedDerivedTable,
+    DatabaseRow,
+    DerivedTableRow,
+    DatabaseColumnValue,
+    EvaluatedFunction,
+    AortaTableReference,
+    FunctionColumnReference,
+    DerivedRowSourceReference,
+    EvaluatedFunctionSourceValue,
+    TableCreationSourceTable,
+    CalculationUsedRow,
+    CalculationUsedField,
 )
 from datetime import datetime
 from django.contrib.contenttypes.models import ContentType
 
 import importlib
+
+
 class OrchestrationWithLineage:
-	# Class variable to track initialized objects
-	_initialized_objects = set()
-
-	# AORTA lineage tracking
-	def __init__(self):
-		self.trail = None
-		self.metadata_trail = None
-		self.current_populated_tables = {}  # Map table names to PopulatedTable instances
-		self.current_rows = {}  # Track current row being processed
-		self.lineage_enabled = True  # Can be disabled for performance
-		self.evaluated_functions_cache = {}  # Cache to track evaluated functions per row
-		self.object_contexts = {}  # Map object id -> derived row context
-
-		# Note: Do not automatically register this instance globally
-		# Global registration should be done explicitly when setting up lineage tracking
-
-	def init_with_lineage(self, theObject, execution_name=None):
-		"""Initialize object with AORTA lineage tracking"""
-		# Create trail if not exists
-		if not self.trail and self.lineage_enabled:
-			execution_name = execution_name or f"Execution_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
-			self.metadata_trail = MetaDataTrail.objects.create()
-			self.trail = Trail.objects.create(
-				name=execution_name,
-				metadata_trail=self.metadata_trail
-			)
-			print(f"Created AORTA Trail: {self.trail.name}")
-
-		# Track object initialization in AORTA
-		if self.lineage_enabled:
-			self._track_object_initialization(theObject)
-
-		# Perform standard initialization
-		return self.init(theObject)
-
-	def _is_django_model(self, table_name):
-		"""Check if a table name corresponds to a Django model"""
-		try:
-			# Try to get the model from Django's app registry
-			apps.get_model('pybirdai', table_name)
-			return True
-		except LookupError:
-			# Not a Django model
-			return False
-
-	def _track_object_initialization(self, obj):
-		"""Track object in AORTA metadata trail"""
-		if not obj:
-			return
-
-		obj_class_name = obj.__class__.__name__
-
-		# Check if this is a table object
-		if hasattr(obj, '__class__') and obj_class_name.endswith('_Table'):
-			# Extract meaningful table name
-			table_name = obj_class_name.replace('_Table', '')
-
-			# Skip dummy objects
-			if table_name == 'Dummy':
-				return
-
-			# Check if we already have a populated table for this name in the current trail
-			if table_name in self.current_populated_tables:
-				print(f"Reusing existing table for: {table_name}")
-				return
-
-			# Determine if this is a Django model or a derived table
-			is_django_model = self._is_django_model(table_name)
-
-			# Check for existing table with same name in this trail
-			aorta_table = None
-			populated_table = None
-			table_exists = False
-
-			if self.trail:
-				if is_django_model:
-					# Look for existing DatabaseTable with same name in this trail
-					existing_populated = PopulatedDataBaseTable.objects.filter(
-						trail=self.trail,
-						table__name=table_name
-					).select_related('table').first()
-					if existing_populated:
-						aorta_table = existing_populated.table
-						populated_table = existing_populated
-						table_exists = True
-						print(f"Found existing DatabaseTable for: {table_name}")
-				else:
-					# Look for existing DerivedTable with same name in this trail
-					existing_evaluated = EvaluatedDerivedTable.objects.filter(
-						trail=self.trail,
-						table__name=table_name
-					).select_related('table').first()
-					if existing_evaluated:
-						aorta_table = existing_evaluated.table
-						populated_table = existing_evaluated
-						table_exists = True
-						print(f"Found existing DerivedTable for: {table_name}")
-
-			# Create new table if not found
-			if not aorta_table:
-				if is_django_model:
-					# Create DatabaseTable for Django model classes
-					aorta_table = DatabaseTable.objects.create(name=table_name)
-				else:
-					# Create DerivedTable for non-Django model classes
-					aorta_table = DerivedTable.objects.create(name=table_name)
-				print(f"Created new table for: {table_name}")
-
-			# Add to metadata trail if not already added
-			if self.metadata_trail:
-				table_type = 'DatabaseTable' if is_django_model else 'DerivedTable'
-				existing_ref = AortaTableReference.objects.filter(
-					metadata_trail=self.metadata_trail,
-					table_content_type=table_type,
-					table_id=aorta_table.id
-				).exists()
-
-				if not existing_ref:
-					AortaTableReference.objects.create(
-						metadata_trail=self.metadata_trail,
-						table_content_type=table_type,
-						table_id=aorta_table.id
-					)
-
-			# Create populated/evaluated table only if we didn't find an existing one
-			if self.trail and not populated_table:
-				if is_django_model:
-					# Create PopulatedDataBaseTable for Django model tables
-					populated_table = PopulatedDataBaseTable.objects.create(
-						trail=self.trail,
-						table=aorta_table
-					)
-				else:
-					# Create EvaluatedDerivedTable for derived tables
-					populated_table = EvaluatedDerivedTable.objects.create(
-						trail=self.trail,
-						table=aorta_table
-					)
-			elif not self.trail:
-				print(f"Warning: No trail available for populated table {table_name}")
-				return
-
-			# Store the populated table in our tracking dictionary
-			self.current_populated_tables[table_name] = populated_table
-
-			# Track table columns/fields only if this is a new table
-			if not table_exists:
-				self._track_table_columns(obj, aorta_table)
-
-				# Analyze table creation functions (calc_ methods)
-				self._analyze_table_creation_functions(obj, aorta_table)
-
-			# print(f"Tracked table initialization: {table_name}")
-
-	def _track_table_columns(self, table_obj, aorta_table):
-		"""Track columns/fields in a table"""
-		try:
-			# Only track fields for DatabaseTable instances
-			# For DerivedTable instances, columns are tracked as Functions
-			if isinstance(aorta_table, DatabaseTable):
-				fields_to_track = []
-				table_name = aorta_table.name
-
-				# For Django models, use the model's _meta.fields
-				if self._is_django_model(table_name):
-					try:
-						from django.apps import apps
-						model_class = apps.get_model('pybirdai', table_name)
-						fields_to_track = [field.name for field in model_class._meta.fields]
-						print(f"Using Django model fields for {table_name}: {len(fields_to_track)} fields")
-					except Exception as e:
-						print(f"Error getting Django model fields for {table_name}: {e}")
-						fields_to_track = []
-				else:
-					# For non-Django tables, detect column methods from the row objects they contain
-					fields_to_track = self._detect_table_fields_from_row_type(table_obj)
-
-				# Create DatabaseField instances
-				for field_name in fields_to_track:
-					db_field = DatabaseField.objects.create(
-						name=field_name,
-						table=aorta_table
-					)
-					# print(f"Tracked column: {aorta_table.name}.{field_name}")
-		except Exception as e:
-			print(f"Error tracking columns for {aorta_table.name}: {e}")
-
-	def _detect_table_fields_from_row_type(self, table_obj):
-		"""Detect fields by examining the row object type that this table contains"""
-		try:
-			# For derived tables, try to determine the row object type
-			table_class_name = table_obj.__class__.__name__
-
-			# Pattern: F_01_01_REF_FINREP_3_0_Table contains F_01_01_REF_FINREP_3_0 objects
-			if table_class_name.endswith('_Table'):
-				row_class_name = table_class_name[:-6]  # Remove '_Table' suffix
-
-				# Try to import and inspect the row class
-				try:
-					# Look for the row class in the same module
-					table_module = table_obj.__class__.__module__
-					module = __import__(table_module, fromlist=[row_class_name])
-
-					if hasattr(module, row_class_name):
-						row_class = getattr(module, row_class_name)
-						# Create a temporary instance to inspect its methods
-						try:
-							row_instance = row_class()
-							fields = self._detect_column_methods(row_instance)
-							print(f"Detected {len(fields)} fields from row type {row_class_name}: {fields[:5]}...")
-							return fields
-						except Exception as e:
-							print(f"Could not instantiate {row_class_name}: {e}")
-
-				except Exception as e:
-					print(f"Could not find row class {row_class_name}: {e}")
-
-			# Fallback: try to detect from table attributes that might be lists of row objects
-			for attr_name in dir(table_obj):
-				if not attr_name.startswith('_'):
-					attr_value = getattr(table_obj, attr_name, None)
-					if isinstance(attr_value, list) and len(attr_value) > 0:
-						# Try to get column methods from the first item in the list
-						first_item = attr_value[0]
-						fields = self._detect_column_methods(first_item)
-						if fields:
-							print(f"Detected {len(fields)} fields from list attribute {attr_name}: {fields[:5]}...")
-							return fields
-
-			print(f"No fields detected for non-Django table {table_class_name}")
-			return []
-
-		except Exception as e:
-			print(f"Error detecting fields for table {table_obj.__class__.__name__}: {e}")
-			return []
-
-	def _detect_column_methods(self, table_obj):
-		"""Detect column methods in non-Django table objects using robust approaches"""
-		import inspect
-		column_methods = set()
-
-		# Get all callable methods
-		methods = [name for name in dir(table_obj)
-				  if (not name.startswith('_') and
-					  callable(getattr(table_obj, name, None)))]
-
-		for method_name in methods:
-			try:
-				method = getattr(table_obj, method_name)
-
-				# Approach 1: Check for @lineage decorator (most reliable)
-				if self._has_lineage_decorator(method):
-					column_methods.add(method_name)
-					continue
-
-				# Approach 2: Method signature and naming patterns
-				if self._is_likely_column_method(method, method_name):
-					column_methods.add(method_name)
-
-			except Exception:
-				continue
-
-		# Filter out known infrastructure methods
-		excluded_methods = {'init', 'metric_value'}
-		excluded_prefixes = {'calc_'}
-
-		final_methods = []
-		for method_name in column_methods:
-			if (method_name not in excluded_methods and
-				not any(method_name.startswith(prefix) for prefix in excluded_prefixes)):
-				final_methods.append(method_name)
-
-		print(f"Detected {len(final_methods)} column methods for non-Django table: {final_methods[:5]}...")
-		return final_methods
-
-	def _has_lineage_decorator(self, method):
-		"""Check if a method has the @lineage decorator"""
-		try:
-			# Check if method has wrapper attributes indicating decoration
-			if hasattr(method, '__wrapped__'):
-				return True
-
-			# Check method name or qualname for lineage wrapper signs
-			if hasattr(method, '__qualname__') and 'lineage' in str(method.__qualname__):
-				return True
-
-			# Check for common decorator attributes
-			if hasattr(method, '__dict__') and any('lineage' in str(key) for key in method.__dict__):
-				return True
-
-			return False
-		except:
-			return False
-
-	def _is_likely_column_method(self, method, method_name):
-		"""Check if method is likely a column method based on signature and naming"""
-		try:
-			import inspect
-
-			# Check method name - should be all uppercase (common column pattern)
-			if not method_name.isupper():
-				return False
-
-			# Check method signature - should only have 'self' parameter
-			sig = inspect.signature(method)
-			params = list(sig.parameters.keys())
-			if len(params) != 1 or params[0] != 'self':
-				return False
-
-			# Check return type annotation if present
-			if sig.return_annotation != inspect.Signature.empty:
-				valid_types = [int, str, 'int', 'str']
-				if sig.return_annotation in valid_types:
-					return True
-
-			# Check docstring for enumeration mentions (common in column methods)
-			if method.__doc__ and 'enumeration' in method.__doc__.lower():
-				return True
-
-			return True  # If all checks pass, likely a column method
-
-		except Exception:
-			return False
-
-	def init(self,theObject):
-		# Check if this object has already been initialized
-		object_id = id(theObject)
-		if object_id in self.__class__._initialized_objects:
-			print(f"Object of type {theObject.__class__.__name__} already initialized, skipping.")
-			# Even if we're skipping full initialization, we still need to ensure references are set
-			self._ensure_references_set(theObject)
-			return
-
-		# Mark this object as initialized
-		self.__class__._initialized_objects.add(object_id)
-
-		# Check if we have lineage tracking enabled globally and this looks like a table
-		from pybirdai.annotations.decorators import _lineage_context
-		global_orchestration = _lineage_context.get('orchestration')
-
-		if (global_orchestration and
-			hasattr(theObject, '__class__') and
-			theObject.__class__.__name__.endswith('_Table')):
-
-			print(f"DEBUG: Using global orchestration for {theObject.__class__.__name__}")
-			print(f"  Global Trail: {global_orchestration.trail.id if global_orchestration.trail else None}")
-			print(f"  Global MetaDataTrail: {global_orchestration.metadata_trail.id if global_orchestration.metadata_trail else None}")
-
-			# This is a table and we have lineage tracking - track it
-			if global_orchestration and global_orchestration.lineage_enabled:
-				global_orchestration._track_object_initialization(theObject)
-
-		# Set up references for the object (use global orchestration if available)
-		if global_orchestration and global_orchestration.lineage_enabled:
-			# Use the global orchestration for reference setup to maintain lineage context
-			global_orchestration._ensure_references_set(theObject)
-		else:
-			# Fallback to local orchestration
-			self._ensure_references_set(theObject)
-
-	def _ensure_references_set(self, theObject):
-		"""
-		Ensure that all table references are properly set for the object.
-		This is called both during full initialization and when initialization is skipped.
-		"""
-		references = [method for method in dir(theObject.__class__) if not callable(
-		getattr(theObject.__class__, method)) and not method.startswith('__')]
-		for eReference in references:
-			if eReference.endswith("Table"):
-				# Only set the reference if it's currently None
-				if getattr(theObject, eReference) is None:
-					from django.apps import apps
-					table_name = eReference.split('_Table')[0]
-					relevant_model = None
-					try:
-						relevant_model = apps.get_model('pybirdai',table_name)
-					except LookupError:
-						print("LookupError: " + table_name)
-
-					if relevant_model:
-						print("relevant_model: " + str(relevant_model))
-						newObject = relevant_model.objects.all()
-						print("newObject: " + str(newObject))
-						if newObject:
-							setattr(theObject,eReference,newObject)
-							# Original CSV persistence
-							CSVConverter.persist_object_as_csv(newObject,True);
-							
-							# Enhanced lineage tracking - track when tables are created but distinguish from usage tracking
-							if self.lineage_enabled and self.trail and hasattr(newObject, '__iter__'):
-								try:
-									# Extract Django model objects from the queryset
-									django_objects = list(newObject) if hasattr(newObject, '__iter__') else [newObject]
-									if django_objects:
-										# Create table structure and register for potential tracking
-										# Only mark as used when actually accessed via automatic tracking wrapper
-										print(f"📊 Table Created: {len(django_objects)} {table_name} objects available")
-								except Exception as e:
-									print(f"Warning: Could not process {table_name} objects for lineage: {e}")
-
-					else:
-						newObject = OrchestrationWithLineage.createObjectFromReferenceType(eReference);
-
-						operations = [method for method in dir(newObject.__class__) if callable(
-							getattr(newObject.__class__, method)) and not method.startswith('__')]
-
-						for operation in operations:
-							if operation == "init":
-								try:
-									getattr(newObject, operation)()
-
-									# Check if lineage tracking is enabled and track data after initialization
-									from pybirdai.annotations.decorators import _lineage_context
-									orchestration = _lineage_context.get('orchestration')
-									if (orchestration and orchestration.lineage_enabled and
-										hasattr(newObject, '__class__') and
-										newObject.__class__.__name__.endswith('_Table')):
-
-										# Debug: print orchestration state
-										print(f"DEBUG: Orchestration for {newObject.__class__.__name__}:")
-										print(f"  Trail: {orchestration.trail.id if orchestration.trail else None}")
-										print(f"  MetaDataTrail: {orchestration.metadata_trail.id if orchestration.metadata_trail else None}")
-
-										# First track the table itself if not already tracked
-										if orchestration.metadata_trail:
-											orchestration._track_object_initialization(newObject)
-										else:
-											print(f"WARNING: No metadata_trail for {newObject.__class__.__name__}")
-
-										# Track any data that was populated during initialization
-										table_name = newObject.__class__.__name__.replace('_Table', '')
-										for attr_name in dir(newObject):
-											if (not attr_name.startswith('_') and
-												hasattr(newObject, attr_name)):
-												attr_value = getattr(newObject, attr_name)
-												if isinstance(attr_value, list) and len(attr_value) > 0:
-													# CRITICAL FIX: DO NOT auto-track derived table data during initialization
-													# Only track when explicitly used in calculations
-													if orchestration.metadata_trail:
-														print(f"📊 Found {len(attr_value)} items in {table_name}_{attr_name} (not tracking as used yet)")
-													else:
-														print(f"WARNING: Cannot process data for {table_name}_{attr_name} - no metadata_trail")
-
-								except Exception as e:
-									print(f"Could not call function called {operation}: {e}")
-
-						setattr(theObject,eReference,newObject)
-
-	@classmethod
-	def reset_initialization(cls):
-		"""
-		Reset the initialization tracking.
-		This can be useful for testing or when re-initialization is required.
-		"""
-		cls._initialized_objects.clear()
-		print("Initialization tracking has been reset.")
-
-	@classmethod
-	def is_initialized(cls, obj):
-		"""
-		Check if an object has been initialized.
-
-		Args:
-			obj: The object to check
-
-		Returns:
-			bool: True if the object has been initialized, False otherwise
-		"""
-		return id(obj) in cls._initialized_objects
-
-	@staticmethod
-	def createObjectFromReferenceType(eReference):
-		try:
-			cls = getattr(importlib.import_module('pybirdai.process_steps.filter_code.output_tables'), eReference)
-			new_object = cls()
-			return new_object;
-		except:
-			print("Error: " + eReference)
-
-	# AORTA Lineage Tracking Methods
-
-	def track_function_execution(self, function_name, source_columns, result_column=None, source_code=None):
-		"""Track the execution of a function in AORTA"""
-		if not self.lineage_enabled or not self.trail:
-			return
-
-		# Create or get derived table for this function
-		class_name = function_name.split('.')[0] if '.' in function_name else 'DynamicFunctions'
-
-		# For individual objects, use their parent table name
-		if not class_name.endswith('_Table'):
-			parent_table_name = self._get_parent_table_name(class_name)
-			if parent_table_name and parent_table_name != class_name:
-				class_name = parent_table_name
-
-		derived_table = None
-
-		# Check if we already have a derived table for this class in the current trail
-		existing_tables = None
-		if self.metadata_trail:
-			# Look for tables in the current metadata trail
-			existing_refs = AortaTableReference.objects.filter(
-				metadata_trail=self.metadata_trail,
-				table_content_type='DerivedTable'
-			)
-			for ref in existing_refs:
-				table = DerivedTable.objects.get(id=ref.table_id)
-				if table.name == class_name:
-					existing_tables = [table]
-					break
-
-		if existing_tables:
-			derived_table = existing_tables[0]
-		else:
-			derived_table = DerivedTable.objects.create(name=class_name)
-
-			# Add to metadata trail
-			if self.metadata_trail:
-				AortaTableReference.objects.create(
-					metadata_trail=self.metadata_trail,
-					table_content_type='DerivedTable',
-					table_id=derived_table.id
-				)
-
-			# Create EvaluatedDerivedTable
-			if self.trail:
-				evaluated_table = EvaluatedDerivedTable.objects.create(
-					trail=self.trail,
-					table=derived_table
-				)
-				self.current_populated_tables[class_name] = evaluated_table
-
-		# Check if Function already exists for this name and table
-		existing_functions = Function.objects.filter(
-			name=function_name,
-			table=derived_table
-		)
-
-		if existing_functions.exists():
-			# Reuse existing function
-			function = existing_functions.first()
-			# print(f"Reusing existing function: {function_name}")
-		else:
-			# Create new Function record
-			function_text = FunctionText.objects.create(
-				text=source_code or function_name,
-				language='python'
-			)
-
-			function = Function.objects.create(
-				name=function_name,
-				function_text=function_text,
-				table=derived_table
-			)
-			# print(f"Created new function: {function_name}")
-
-		# Note: TableCreationFunction instances are now created in _analyze_table_creation_functions
-		# during table initialization, which analyzes class variables for source tables
-
-		# Track column references (only for newly created functions to avoid duplicates)
-		if not existing_functions.exists():
-			for col_ref in source_columns:
-				try:
-					# Try to resolve the actual column object
-					resolved_field = self._resolve_column_reference(col_ref)
-					if resolved_field:
-						# Check if this column reference already exists
-						content_type = ContentType.objects.get_for_model(resolved_field.__class__)
-						existing_col_refs = FunctionColumnReference.objects.filter(
-							function=function,
-							content_type=content_type,
-							object_id=resolved_field.id
-						)
-
-						if not existing_col_refs.exists():
-							# Create FunctionColumnReference
-							FunctionColumnReference.objects.create(
-								function=function,
-								content_type=content_type,
-								object_id=resolved_field.id
-							)
-							# print(f"Tracked column reference: {function_name} -> {col_ref}")
-				except Exception as e:
-					print(f"Could not resolve column reference {col_ref}: {e}")
-
-		return function
-
-	def track_polymorphic_function_execution(self, function_name, base_class_name, 
-											source_columns, result_column=None, 
-											wrapper_obj=None, base_obj=None):
-		"""Track the execution of a polymorphic function in AORTA"""
-		if not self.lineage_enabled or not self.trail:
-			return
-
-		# Use the wrapper class name for the table/function organization
-		# CRITICAL FIX: Extract the FULL wrapper class name, not just the first part
-		# For function_name like "F_05_01_REF_FINREP_3_0_UnionItem.GRSS_CRRYNG_AMNT"
-		# we want "F_05_01_REF_FINREP_3_0_UnionItem", not "F_05_01_REF_FINREP_3_0"
-		if '.' in function_name:
-			# Get everything before the last dot (the method name)
-			wrapper_class_name = function_name.rsplit('.', 1)[0]
-		else:
-			wrapper_class_name = 'DynamicFunctions'
-		
-		print(f"🔧 track_polymorphic_function_execution: function_name={function_name}, wrapper_class_name={wrapper_class_name}, base_class_name={base_class_name}")
-		
-		# Create more descriptive function name that includes the base class
-		polymorphic_function_name = f"{function_name}@{base_class_name}"
-
-		# Create or get derived table for the wrapper class
-		derived_table = None
-		existing_tables = None
-		if self.metadata_trail:
-			# Look for tables in the current metadata trail
-			existing_refs = AortaTableReference.objects.filter(
-				metadata_trail=self.metadata_trail,
-				table_content_type='DerivedTable'
-			)
-			for ref in existing_refs:
-				table = DerivedTable.objects.get(id=ref.table_id)
-				if table.name == wrapper_class_name:
-					existing_tables = [table]
-					break
-
-		if existing_tables:
-			derived_table = existing_tables[0]
-		else:
-			derived_table = DerivedTable.objects.create(name=wrapper_class_name)
-
-			# Add to metadata trail
-			if self.metadata_trail:
-				AortaTableReference.objects.create(
-					metadata_trail=self.metadata_trail,
-					table_content_type='DerivedTable',
-					table_id=derived_table.id
-				)
-
-			# Create EvaluatedDerivedTable
-			if self.trail:
-				evaluated_table = EvaluatedDerivedTable.objects.create(
-					trail=self.trail,
-					table=derived_table
-				)
-				self.current_populated_tables[wrapper_class_name] = evaluated_table
-
-		# Check if Function already exists for this polymorphic name and table
-		existing_functions = Function.objects.filter(
-			name=polymorphic_function_name,
-			table=derived_table
-		)
-
-		if existing_functions.exists():
-			# Reuse existing function
-			function = existing_functions.first()
-		else:
-			# Create source code that shows the polymorphic delegation
-			method_name = function_name.split('.')[-1] if '.' in function_name else function_name
-			source_code = f"def {method_name}(self) -> Any: return self.base.{method_name}()  # Polymorphic delegation to {base_class_name}"
-			
-			# Create new Function record
-			function_text = FunctionText.objects.create(
-				text=source_code,
-				language='python'
-			)
-
-			function = Function.objects.create(
-				name=polymorphic_function_name,
-				function_text=function_text,
-				table=derived_table
-			)
-			print(f"Created polymorphic function: {polymorphic_function_name}")
-
-		# Track column references for polymorphic dependencies
-		for col_ref in source_columns:
-			try:
-				# Try to resolve the actual column object
-				resolved_field = self._resolve_column_reference(col_ref)
-				if resolved_field:
-					# Check if this column reference already exists
-					content_type = ContentType.objects.get_for_model(resolved_field.__class__)
-					existing_col_refs = FunctionColumnReference.objects.filter(
-						function=function,
-						content_type=content_type,
-						object_id=resolved_field.id
-					)
-
-					if not existing_col_refs.exists():
-						# Create FunctionColumnReference
-						FunctionColumnReference.objects.create(
-							function=function,
-							content_type=content_type,
-							object_id=resolved_field.id
-						)
-			except Exception as e:
-				print(f"Could not resolve polymorphic column reference {col_ref}: {e}")
-
-		return function
-
-	def _extract_source_table_names(self, source_columns):
-		"""Extract unique table names from column dependencies"""
-		table_names = set()
-
-		for col_ref in source_columns:
-			try:
-				# Column references are in format "TABLE_NAME.column_name" or nested
-				parts = col_ref.split('.')
-				if len(parts) >= 2:
-					# First part is typically the table name
-					table_name = parts[0]
-					# Only add if it looks like a table name (not a lowercase attribute)
-					if table_name and table_name.isupper():
-						table_names.add(table_name)
-			except Exception as e:
-				print(f"Error extracting table name from {col_ref}: {e}")
-
-		return list(table_names)
-
-	def _analyze_table_creation_functions(self, table_obj, aorta_table):
-		"""Analyze calc_ methods in table classes and create TableCreationFunction instances"""
-		try:
-			class_name = table_obj.__class__.__name__
-
-			# Find all calc_ methods in this class
-			calc_methods = [name for name in dir(table_obj)
-						   if name.startswith('calc_') and callable(getattr(table_obj, name))]
-
-			for calc_method_name in calc_methods:
-				calc_method = getattr(table_obj, calc_method_name)
-				full_function_name = f"{class_name}.{calc_method_name}"
-
-				# Extract source table names from class variables ending with _Table
-				source_table_names = self._extract_source_tables_from_class_variables(table_obj)
-
-				# Get function source code
-				try:
-					import inspect
-					source_code = inspect.getsource(calc_method)
-				except:
-					source_code = f"def {calc_method_name}(self): # Source code not available"
-
-				# Check if this calc_ method has a @lineage decorator and extract dependencies
-				lineage_dependencies = self._extract_lineage_dependencies(calc_method)
-				lineage_column_references = []
-				if lineage_dependencies:
-					# Use lineage dependencies for more detailed function text
-					source_code += f"\n# Lineage dependencies: {lineage_dependencies}"
-					# Extract column references from the lineage dependencies
-					lineage_column_references = self._parse_lineage_dependencies(lineage_dependencies)
-
-				# Check if TableCreationFunction already exists
-				existing_table_creation_functions = TableCreationFunction.objects.filter(
-					name=full_function_name
-				)
-
-				if existing_table_creation_functions.exists():
-					# Reuse existing table creation function
-					table_creation_function = existing_table_creation_functions.first()
-					# print(f"Reusing existing table creation function: {full_function_name}")
-				else:
-					# Create FunctionText
-					function_text = FunctionText.objects.create(
-						text=source_code,
-						language='python'
-					)
-
-					# Create TableCreationFunction
-					table_creation_function = TableCreationFunction.objects.create(
-						name=full_function_name,
-						function_text=function_text
-					)
-					# print(f"Created new table creation function: {full_function_name}")
-
-				# Create TableCreationSourceTable entries (only for new functions)
-				if not existing_table_creation_functions.exists():
-					for source_table_name in source_table_names:
-						# Find the source table in DatabaseTable or DerivedTable
-						source_table = self._find_table_by_name(source_table_name)
-						if source_table:
-							content_type = ContentType.objects.get_for_model(source_table.__class__)
-							# Check if this source table reference already exists
-							existing_source_refs = TableCreationSourceTable.objects.filter(
-								table_creation_function=table_creation_function,
-								content_type=content_type,
-								object_id=source_table.id
-							)
-
-							if not existing_source_refs.exists():
-								TableCreationSourceTable.objects.create(
-									table_creation_function=table_creation_function,
-									content_type=content_type,
-									object_id=source_table.id
-								)
-								# print(f"Tracked table creation source: {full_function_name} -> {source_table_name}")
-
-					# Create TableCreationFunctionColumn entries for lineage dependencies
-					for column_ref in lineage_column_references:
-						column_obj = column_ref['column']
-						reference_text = column_ref['reference_text']
-
-						content_type = ContentType.objects.get_for_model(column_obj.__class__)
-						from pybirdai.models import TableCreationFunctionColumn
-
-						# Check if this column reference already exists
-						existing_column_refs = TableCreationFunctionColumn.objects.filter(
-							table_creation_function=table_creation_function,
-							content_type=content_type,
-							object_id=column_obj.id,
-							reference_text=reference_text
-						)
-
-						if not existing_column_refs.exists():
-							TableCreationFunctionColumn.objects.create(
-								table_creation_function=table_creation_function,
-								content_type=content_type,
-								object_id=column_obj.id,
-								reference_text=reference_text
-							)
-							# print(f"Tracked column reference: {full_function_name} -> {column_obj}")
-
-				print(f"Created TableCreationFunction for {full_function_name} with {len(source_table_names)} source tables and {len(lineage_column_references)} column references")
-
-		except Exception as e:
-			print(f"Error analyzing table creation functions for {table_obj.__class__.__name__}: {e}")
-
-	def _extract_source_tables_from_class_variables(self, table_obj):
-		"""Extract source table names from class variables ending with _Table"""
-		source_table_names = set()
-
-		# Get all attributes that end with _Table
-		for attr_name in dir(table_obj):
-			if attr_name.endswith('_Table') and not attr_name.startswith('_'):
-				# Remove _Table suffix to get the table name
-				table_name = attr_name.replace('_Table', '')
-				source_table_names.add(table_name)
-
-		return list(source_table_names)
-
-	def _extract_lineage_dependencies(self, method):
-		"""Extract dependencies from @lineage decorator if present"""
-		try:
-			import inspect
-			import re
-
-			# Get the source code of the method
-			source_code = inspect.getsource(method)
-
-			# Look for @lineage decorator with dependencies parameter
-			# Pattern matches: @lineage(dependencies={"base.COLUMN", "table.COLUMN"}) including multiline
-			lineage_pattern = r'@lineage\s*\(\s*dependencies\s*=\s*\{([^}]+)\}\s*\)'
-			matches = re.search(lineage_pattern, source_code, re.DOTALL)
-
-			if matches:
-				# Extract the dependencies content
-				dependencies_content = matches.group(1)
-
-				# Extract individual dependency strings (remove quotes and whitespace)
-				dependency_pattern = r'\"([^\"]+)\"'
-				dependency_matches = re.findall(dependency_pattern, dependencies_content)
-
-				if dependency_matches:
-					dependencies_text = ', '.join(dependency_matches)
-					print(f"Extracted lineage dependencies: {dependencies_text}")
-					return dependencies_text
-
-			return None
-
-		except Exception as e:
-			print(f"Error extracting lineage dependencies: {e}")
-			return None
-
-	def _parse_lineage_dependencies(self, lineage_dependencies_text):
-		"""Parse lineage dependencies text to extract column references"""
-		column_references = []
-
-		if not lineage_dependencies_text:
-			return column_references
-
-		try:
-			# Extract column references from the lineage dependencies text
-			# Look for patterns like "base.COLUMN_NAME", "table.COLUMN_NAME"
-			import re
-
-			# Find all patterns that look like column references
-			# This regex looks for word.WORD patterns (table.column references)
-			pattern = r'\b(\w+)\.([A-Za-z_][A-Za-z0-9_]*)\b'
-			matches = re.findall(pattern, lineage_dependencies_text)
-
-			for table_ref, column_name in matches:
-				# Try to find the actual column object
-				column_obj = self._find_column_by_name(column_name, table_ref)
-				if column_obj:
-					column_references.append({
-						'column': column_obj,
-						'reference_text': f"{table_ref}.{column_name}"
-					})
-				else:
-					# If we can't find the specific column, try a broader search
-					column_obj = self._find_column_by_name(column_name)
-					if column_obj:
-						column_references.append({
-							'column': column_obj,
-							'reference_text': f"{table_ref}.{column_name}"
-						})
-
-			print(f"Parsed {len(column_references)} column references from lineage dependencies")
-			return column_references
-
-		except Exception as e:
-			print(f"Error parsing lineage dependencies: {e}")
-			return column_references
-
-	def _find_column_by_name(self, column_name, table_hint=None):
-		"""Find a column (DatabaseField or Function) by name, optionally with table hint"""
-		try:
-			# First try to find in DatabaseField
-			database_fields = DatabaseField.objects.filter(name=column_name)
-			if table_hint:
-				# Filter by table name if hint provided
-				database_fields = database_fields.filter(table__name__icontains=table_hint)
-
-			if database_fields.exists():
-				return database_fields.first()
-
-			# Then try to find in Function
-			functions = Function.objects.filter(name=column_name)
-			if table_hint:
-				# Filter by table name if hint provided
-				functions = functions.filter(table__name__icontains=table_hint)
-
-			if functions.exists():
-				return functions.first()
-
-			# Fallback: try without table hint if we had one
-			if table_hint:
-				return self._find_column_by_name(column_name, None)
-
-			return None
-
-		except Exception as e:
-			print(f"Error finding column {column_name}: {e}")
-			return None
-
-	def _find_table_by_name(self, table_name):
-		"""Find a table by name in DatabaseTable or DerivedTable"""
-		try:
-			# First try DatabaseTable
-			database_tables = DatabaseTable.objects.filter(name=table_name)
-			if database_tables.exists():
-				return database_tables.first()
-
-			# Then try DerivedTable
-			derived_tables = DerivedTable.objects.filter(name=table_name)
-			if derived_tables.exists():
-				return derived_tables.first()
-
-			return None
-		except Exception as e:
-			print(f"Error finding table {table_name}: {e}")
-			return None
-
-	def _resolve_column_reference(self, column_ref):
-		"""Resolve a column reference string to an actual DatabaseField object"""
-		try:
-			# Handle nested references like "base.CRRYNG_AMNT"
-			parts = column_ref.split('.')
-
-			# For simple column references, look in existing tables
-			if len(parts) == 1:
-				# Look for this column in any tracked table
-				for table_name, populated_table in self.current_populated_tables.items():
-					if hasattr(populated_table, 'table') and hasattr(populated_table.table, 'database_fields'):
-						fields = populated_table.table.database_fields.filter(name=parts[0])
-						if fields.exists():
-							return fields.first()
-
-			# For complex references, try to resolve based on patterns
-			elif len(parts) > 1:
-				# Look for the column name in the last part
-				column_name = parts[-1]
-				for table_name, populated_table in self.current_populated_tables.items():
-					if hasattr(populated_table, 'table') and hasattr(populated_table.table, 'database_fields'):
-						fields = populated_table.table.database_fields.filter(name=column_name)
-						if fields.exists():
-							return fields.first()
-
-		except Exception as e:
-			print(f"Error resolving column reference {column_ref}: {e}")
-
-		return None
-
-	def track_row_processing(self, table_name, row_data, row_identifier=None):
-		"""Track row-level lineage"""
-		if not self.lineage_enabled or not self.trail:
-			return
-
-		try:
-			# Get the populated table for this table name
-			populated_table = self.current_populated_tables.get(table_name)
-			if not populated_table:
-				print(f"No populated table found for {table_name}")
-				return
-
-			# Determine if this is a derived table or database table
-			is_derived_table = isinstance(populated_table, EvaluatedDerivedTable)
-
-			# Check for existing row with same data to prevent duplicates
-			existing_row = None
-			if not is_derived_table and isinstance(row_data, dict):
-				# For database tables, check if a row with the same data already exists
-				existing_rows = populated_table.databaserow_set.all()
-				for existing in existing_rows:
-					if self._rows_have_same_data(existing, row_data):
-						existing_row = existing
-						print(f"Found existing row for {table_name}, reusing instead of creating duplicate")
-						break
-
-			if existing_row:
-				db_row = existing_row
-			else:
-				# Create row identifier if not provided
-				if not row_identifier:
-					if is_derived_table:
-						row_identifier = f"row_{len(populated_table.derivedtablerow_set.all()) + 1}"
-					else:
-						row_identifier = f"row_{len(populated_table.databaserow_set.all()) + 1}"
-
-				# Create appropriate row type
-				if is_derived_table:
-					# Create DerivedTableRow for derived tables
-					db_row = DerivedTableRow.objects.create(
-						populated_table=populated_table,
-						row_identifier=row_identifier
-					)
-				else:
-					# Create DatabaseRow for database tables
-					db_row = DatabaseRow.objects.create(
-						populated_table=populated_table,
-						row_identifier=row_identifier
-					)
-
-			# Track individual column values (only for DatabaseRow and only if this is a new row)
-			if not is_derived_table and not existing_row:
-				if isinstance(row_data, dict):
-					for column_name, value in row_data.items():
-						self._track_column_value(db_row, column_name, value)
-				elif hasattr(row_data, '__dict__'):
-					# Handle object with attributes
-					for attr_name in dir(row_data):
-						if not attr_name.startswith('_') and not callable(getattr(row_data, attr_name)):
-							value = getattr(row_data, attr_name)
-							self._track_column_value(db_row, attr_name, value)
-
-			# Store current row context for value tracking
-			self.current_rows['source'] = db_row.id
-			self.current_rows['table'] = table_name
-
-			# Clear evaluated functions cache when switching to a new row
-			self.evaluated_functions_cache.clear()
-
-			# print(f"Tracked row processing: {table_name} row {row_identifier}")
-			return db_row
-
-		except Exception as e:
-			print(f"Error tracking row processing: {e}")
-			return None
-
-	def _track_column_value(self, db_row, column_name, value):
-		"""Track individual column values"""
-		try:
-			# Find the corresponding DatabaseField
-			table = db_row.populated_table.table
-			fields = table.database_fields.filter(name=column_name)
-
-			if not fields.exists():
-				# Create the field if it doesn't exist
-				field = DatabaseField.objects.create(
-					name=column_name,
-					table=table
-				)
-				print(f"Created missing column: {table.name}.{column_name}")
-			else:
-				field = fields.first()
-
-			# Create DatabaseColumnValue
-			# Try to convert to float, otherwise use string_value
-			numeric_value = None
-			string_value = None
-
-			if value is not None:
-				try:
-					numeric_value = float(value)
-				except (ValueError, TypeError):
-					string_value = str(value)
-
-			DatabaseColumnValue.objects.create(
-				value=numeric_value,
-				string_value=string_value,
-				column=field,
-				row=db_row
-			)
-
-			# print(f"Tracked column value: {table.name}.{column_name} = {value}")
-		except Exception as e:
-			print(f"Error tracking column value {column_name}: {e}")
-
-	def _rows_have_same_data(self, existing_row, new_row_data):
-		"""Check if an existing DatabaseRow has the same data as new_row_data"""
-		try:
-			# Get all column values for the existing row
-			existing_values = {}
-			for column_value in existing_row.column_values.all():
-				column_name = column_value.column.name
-				value = column_value.value if column_value.value is not None else column_value.string_value
-				existing_values[column_name] = value
-
-			# Compare with new row data
-			if len(existing_values) != len(new_row_data):
-				return False
-
-			for column_name, new_value in new_row_data.items():
-				existing_value = existing_values.get(column_name)
-
-				# Handle numeric vs string comparison
-				if existing_value is None and new_value is None:
-					continue
-				elif existing_value is None or new_value is None:
-					return False
-
-				# Try numeric comparison first
-				try:
-					if float(existing_value) == float(new_value):
-						continue
-				except (ValueError, TypeError):
-					pass
-
-				# Fall back to string comparison
-				if str(existing_value) != str(new_value):
-					return False
-
-			return True
-
-		except Exception as e:
-			print(f"Error comparing row data: {e}")
-			return False
-
-	def track_derived_row_processing(self, table_name, derived_row_data, source_row_ids=None):
-		"""Track derived/computed row processing"""
-		if not self.lineage_enabled or not self.trail:
-			return
-
-		try:
-			# Get the evaluated derived table
-			evaluated_table = self.current_populated_tables.get(table_name)
-			if not evaluated_table or not isinstance(evaluated_table, EvaluatedDerivedTable):
-				print(f"No evaluated derived table found for {table_name}")
-				return
-
-			# Create DerivedTableRow
-			derived_row = DerivedTableRow.objects.create(
-				populated_table=evaluated_table
-			)
-
-			# Track source row references
-			if source_row_ids:
-				for source_row_id in source_row_ids:
-					try:
-						source_row = DatabaseRow.objects.get(id=source_row_id)
-						DerivedRowSourceReference.objects.create(
-							derived_row=derived_row,
-							content_type=ContentType.objects.get_for_model(DatabaseRow),
-							object_id=source_row.id
-						)
-					except DatabaseRow.DoesNotExist:
-						print(f"Source row {source_row_id} not found")
-
-			# Store current derived row context
-			self.current_rows['derived'] = derived_row.id
-
-			# Clear evaluated functions cache when switching to a new derived row
-			self.evaluated_functions_cache.clear()
-
-			# print(f"Tracked derived row processing: {table_name}")
-			return derived_row
-
-		except Exception as e:
-			print(f"Error tracking derived row processing: {e}")
-			return None
-
-	def track_value_computation(self, function_name, source_values, computed_value):
-		"""Track value-level lineage"""
-		print(f"🔍 track_value_computation called: {function_name}, value={computed_value}")
-		
-		if not self.lineage_enabled or not self.trail:
-			print(f"🔍 track_value_computation: lineage disabled or no trail")
-			return
-
-		try:
-			# Get the current derived row if available
-			derived_row_id = self.current_rows.get('derived')
-			if not derived_row_id:
-				print(f"🔍 track_value_computation: No derived row context for value computation: {function_name}")
-				print(f"🔍 track_value_computation: Current rows context: {self.current_rows}")
-				return
-			
-			print(f"🔍 track_value_computation: Using derived_row_id: {derived_row_id}")
-
-			# Check cache first
-			cache_key = f"{derived_row_id}:{function_name}"
-			if cache_key in self.evaluated_functions_cache:
-				# Return cached evaluated function
-				return self.evaluated_functions_cache[cache_key]
-
-			# Get the derived row
-			derived_row = DerivedTableRow.objects.get(id=derived_row_id)
-
-			# Find the corresponding Function object
-			function_parts = function_name.split('.')
-			class_name = function_parts[0] if len(function_parts) > 1 else 'DynamicFunctions'
-			method_name = function_parts[-1]
-
-			# Look for the function by name across all Function objects
-			functions = Function.objects.filter(name=function_name)
-
-			if not functions.exists():
-				print(f"Function {function_name} not found for value computation")
-				return
-
-			# CRITICAL FIX: When multiple functions exist with the same name,
-			# prefer the one that belongs to the same table as the derived row
-			function = functions.first()  # Default fallback
-			
-			# Try to find a function from the same table as the derived row
-			derived_table = derived_row.populated_table.table
-			for func in functions:
-				if func.table.id == derived_table.id:
-					function = func
-					print(f"🎯 track_value_computation: Using function {func.id} from correct table {derived_table.name}")
-					break
-			else:
-				# If no exact table match, try by table name (for data consistency)
-				for func in functions:
-					if func.table.name == derived_table.name:
-						function = func
-						print(f"🎯 track_value_computation: Using function {func.id} from table with matching name {derived_table.name}")
-						break
-
-			# Check if we already have an EvaluatedFunction for this function and row
-			existing_evaluated = EvaluatedFunction.objects.filter(
-				function=function,
-				row=derived_row
-			).first()
-
-			if existing_evaluated:
-				# We already have this function evaluated for this row
-				# Since functions are immutable, the result should be the same
-				# Cache it and return
-				self.evaluated_functions_cache[cache_key] = existing_evaluated
-				# print(f"Reusing existing EvaluatedFunction for {function_name} on row {derived_row_id}")
-				return existing_evaluated
-
-			# Create EvaluatedFunction only if it doesn't exist
-			# Try to store as numeric value if possible
-			numeric_value = None
-			string_value = None
-
-			if computed_value is not None:
-				try:
-					numeric_value = float(computed_value)
-				except (ValueError, TypeError):
-					string_value = str(computed_value)
-
-			print(f"🔍 track_value_computation: Creating EvaluatedFunction for {function.name} (ID: {function.id}) on row {derived_row.id}")
-			evaluated_function = EvaluatedFunction.objects.create(
-				value=numeric_value,
-				string_value=string_value,
-				function=function,
-				row=derived_row
-			)
-			print(f"🔍 track_value_computation: ✅ Created EvaluatedFunction ID: {evaluated_function.id}")
-
-			# Track source values (optional - don't fail if this doesn't work)
-			for source_value in source_values:
-				if source_value is not None:
-					try:
-						# Try to find the corresponding DatabaseColumnValue
-						source_value_obj = self._find_source_value_object(source_value)
-						if source_value_obj:
-							EvaluatedFunctionSourceValue.objects.create(
-								evaluated_function=evaluated_function,
-								content_type=ContentType.objects.get_for_model(source_value_obj.__class__),
-								object_id=source_value_obj.id
-							)
-					except Exception as e:
-						# Source value tracking is optional - don't fail the main function evaluation
-						print(f"Debug: Could not create source value link for '{source_value}': {e}")
-
-			# Cache the evaluated function
-			self.evaluated_functions_cache[cache_key] = evaluated_function
-
-			# print(f"Tracked value computation: {function_name} with {len(source_values)} source values = {computed_value}")
-			return evaluated_function
-
-		except Exception as e:
-			print(f"Error tracking value computation: {e}")
-			return None
-
-	def _ensure_derived_row_context(self, derived_obj, function_name):
-		"""Ensure a derived row context exists for the given derived object"""
-		if not self.lineage_enabled or not self.trail:
-			return None
-
-		try:
-			# Check if we already have a context for this specific object
-			obj_id = id(derived_obj)
-			if obj_id in self.object_contexts:
-				return self.object_contexts[obj_id]
-
-			# Get the class name to determine table name
-			class_name = derived_obj.__class__.__name__
-
-			# Only create derived tables for *_Table classes
-			# Individual objects should be treated as rows within their parent table
-			if class_name.endswith('_Table'):
-				table_name = class_name.replace('_Table', '')
-			else:
-				# For individual objects, use their class name as the table name
-				# This ensures proper isolation between different object types
-				table_name = class_name
-
-			# Ensure we have an EvaluatedDerivedTable for this specific class
-			if table_name not in self.current_populated_tables:
-				# Check if a DerivedTable already exists for this name
-				existing_derived_tables = DerivedTable.objects.filter(name=table_name)
-				if existing_derived_tables.exists():
-					derived_table = existing_derived_tables.first()
-				else:
-					# Create a new DerivedTable
-					derived_table = DerivedTable.objects.create(name=table_name)
-
-				if self.metadata_trail:
-					# Check if reference already exists
-					existing_refs = AortaTableReference.objects.filter(
-						metadata_trail=self.metadata_trail,
-						table_content_type='DerivedTable',
-						table_id=derived_table.id
-					)
-
-					if not existing_refs.exists():
-						AortaTableReference.objects.create(
-							metadata_trail=self.metadata_trail,
-							table_content_type='DerivedTable',
-							table_id=derived_table.id
-						)
-
-
-				# Create EvaluatedDerivedTable
-				evaluated_table = EvaluatedDerivedTable.objects.create(
-					trail=self.trail,
-					table=derived_table
-				)
-
-				self.current_populated_tables[table_name] = evaluated_table
-				print(f"Created EvaluatedDerivedTable for {table_name}")
-
-			# Get the EvaluatedDerivedTable
-			evaluated_table = self.current_populated_tables[table_name]
-
-			# Create a unique identifier for this derived row based on object identity
-			row_identifier = f"{class_name}_{id(derived_obj)}"
-
-			# Check if we already have a DerivedTableRow for this object
-			existing_rows = evaluated_table.derivedtablerow_set.filter(row_identifier=row_identifier)
-			if existing_rows.exists():
-				derived_row_id = existing_rows.first().id
-			else:
-				# Create a new DerivedTableRow
-				derived_row = DerivedTableRow.objects.create(
-					populated_table=evaluated_table,
-					row_identifier=row_identifier
-				)
-				derived_row_id = derived_row.id
-				print(f"Created DerivedTableRow {derived_row_id} for {function_name}")
-
-			# Store the context for this specific object
-			self.object_contexts[obj_id] = derived_row_id
-			return derived_row_id
-
-		except Exception as e:
-			print(f"Error ensuring derived row context: {e}")
-			return None
-
-	def _get_parent_table_name(self, class_name):
-		"""Determine the parent table name for an individual object"""
-		# Handle special cases first
-		if class_name.endswith('_UnionItem'):
-			return class_name.replace('_UnionItem', '_UnionTable').replace('_Table', '')
-
-		# Direct match - if the class itself is a table
-		if class_name in self.current_populated_tables:
-			return class_name
-
-		# For objects with specific report prefixes (e.g., F_05_01_REF_FINREP_3_0_Other_loans)
-		if '_' in class_name and class_name.split('_')[0].startswith('F_'):
-			# Extract the report prefix (e.g., F_05_01_REF_FINREP_3_0)
-			parts = class_name.split('_')
-			report_prefix_parts = []
-			for i, part in enumerate(parts):
-				report_prefix_parts.append(part)
-				# Stop when we hit a part that looks like a class name (starts with uppercase after numbers)
-				if i > 0 and len(part) > 0 and part[0].isupper() and not part.isdigit():
-					# Check if this forms a valid report table name
-					report_table_name = '_'.join(report_prefix_parts)
-					if report_table_name in self.current_populated_tables:
-						return report_table_name
-
-			# If no exact match, return the full class name as table name
-			# This ensures F_05_01_REF_FINREP_3_0_Other_loans gets its own table
-			return class_name
-
-		# For base objects (e.g., Other_loans), create their own table
-		# Don't try to match them to other tables - this was causing the mixing issue
-		return class_name
-
-	def get_derived_context_for_object(self, obj):
-		"""Get the correct derived context for a specific object"""
-		obj_id = id(obj)
-		if obj_id in self.object_contexts:
-			return self.object_contexts[obj_id]
-		return None
-	
-	def track_calculation_used_row(self, calculation_name, row):
-		"""Track that a specific row was used in a calculation (passed filters)"""
-		print(f"🔍 track_calculation_used_row called: {calculation_name}, {type(row).__name__}")
-		
-		if not self.lineage_enabled or not self.trail:
-			print(f"❌ Lineage tracking disabled or no trail: lineage_enabled={self.lineage_enabled}, trail={self.trail}")
-			return
-		
-		try:
-			# Determine the type of row
-			if isinstance(row, DatabaseRow):
-				content_type = ContentType.objects.get_for_model(DatabaseRow)
-			elif isinstance(row, DerivedTableRow):
-				content_type = ContentType.objects.get_for_model(DerivedTableRow)
-			# Check if this is a Django model instance (database record)
-			elif hasattr(row, '_meta') and hasattr(row._meta, 'model'):
-				# This is a Django model instance - we need to create/find the appropriate DatabaseRow
-				model_name = type(row).__name__
-				
-				# Ensure we have a database table for this model
-				# Check if we already have the wrong type of table stored
-				existing_table = self.current_populated_tables.get(model_name)
-				if not existing_table or not isinstance(existing_table, PopulatedDataBaseTable):
-					# Create database table
-					db_table = DatabaseTable.objects.create(name=model_name)
-					
-					# Add to metadata trail
-					if self.metadata_trail:
-						AortaTableReference.objects.create(
-							metadata_trail=self.metadata_trail,
-							table_content_type='DatabaseTable',
-							table_id=db_table.id
-						)
-					
-					# Create PopulatedDataBaseTable
-					populated_table = PopulatedDataBaseTable.objects.create(
-						trail=self.trail,
-						table=db_table
-					)
-					
-					self.current_populated_tables[model_name] = populated_table
-					print(f"Created database table for Django model: {model_name}")
-				else:
-					populated_table = existing_table
-				
-				# Get the populated database table
-				populated_table = self.current_populated_tables[model_name]
-				
-				# Create unique identifier for this model instance
-				if hasattr(row, 'pk') and row.pk:
-					object_identifier = f"{model_name}_{row.pk}"
-				else:
-					object_identifier = f"{model_name}_{id(row)}"
-				
-				# Check if we already have a database row for this model instance
-				existing_rows = populated_table.databaserow_set.filter(
-					row_identifier=object_identifier
-				)
-				
-				if existing_rows.exists():
-					db_row = existing_rows.first()
-				else:
-					# Create new database row for this model instance
-					db_row = DatabaseRow.objects.create(
-						populated_table=populated_table,
-						row_identifier=object_identifier
-					)
-					
-					# Create column values for the model fields
-					for field in row._meta.fields:
-						if hasattr(row, field.name):
-							field_value = getattr(row, field.name)
-							if field_value is not None:
-								self._track_column_value_for_django_field(db_row, field.name, field_value, populated_table.table)
-					
-					print(f"Created database row for Django model {model_name}")
-					
-					# DISABLED: Don't automatically track all Django model fields as used
-					# Only track fields that are actually accessed during calculations (via wrapper)
-					# self._track_django_model_fields_as_used(calculation_name, populated_table.table, row)
-				
-				row = db_row
-				
-				content_type = ContentType.objects.get_for_model(DatabaseRow)
-			else:
-				# For business objects, determine the appropriate tracking strategy
-				row_class_name = type(row).__name__
-				
-				# Check if this is a business object that should be tracked as a derived table row
-				if hasattr(row, '__dict__'):
-					# This is a business object - create/find appropriate derived table
-					tracked_row = None
-					
-					# Look for or create an appropriate derived table for this object type
-					table_name = row_class_name
-					
-					# Check if we already have a derived table for this object type
-					if table_name not in self.current_populated_tables:
-						# Create derived table for this object type
-						derived_table = DerivedTable.objects.create(name=table_name)
-						
-						# Add to metadata trail
-						if self.metadata_trail:
-							AortaTableReference.objects.create(
-								metadata_trail=self.metadata_trail,
-								table_content_type='DerivedTable',
-								table_id=derived_table.id
-							)
-						
-						# Create EvaluatedDerivedTable
-						evaluated_table = EvaluatedDerivedTable.objects.create(
-							trail=self.trail,
-							table=derived_table
-						)
-						
-						self.current_populated_tables[table_name] = evaluated_table
-						print(f"Created derived table for business object type: {table_name}")
-					
-					# Get the evaluated derived table
-					evaluated_table = self.current_populated_tables[table_name]
-					
-					# Create unique identifier for this specific object instance
-					object_identifier = f"{row_class_name}_{id(row)}"
-					
-					# Check if we already have a derived row for this object
-					existing_rows = evaluated_table.derivedtablerow_set.filter(
-						row_identifier=object_identifier
-					)
-					
-					if existing_rows.exists():
-						tracked_row = existing_rows.first()
-					else:
-						# Create new derived table row for this object
-						tracked_row = DerivedTableRow.objects.create(
-							populated_table=evaluated_table,
-							row_identifier=object_identifier
-						)
-						print(f"Created derived table row for {row_class_name}")
-					
-					if tracked_row:
-						# ENHANCED: Track object relationships via DerivedRowSourceReference
-						# Do this for both new AND existing rows to ensure relationships are captured
-						self._track_object_relationships(tracked_row, row)
-						
-						# ENHANCED: Also track transitively referenced objects as used
-						# This ensures that objects referenced via unionOfLayers.base etc. are also marked as used
-						self._track_transitive_used_objects(calculation_name, row)
-						
-						row = tracked_row
-						content_type = ContentType.objects.get_for_model(DerivedTableRow)
-					else:
-						print(f"Failed to create derived table row for {row_class_name}")
-						return
-				else:
-					print(f"Cannot track row of type {type(row)} - not a trackable object")
-					return
-			
-			# Check if this row is already tracked for this calculation
-			existing = CalculationUsedRow.objects.filter(
-				trail=self.trail,
-				calculation_name=calculation_name,
-				content_type=content_type,
-				object_id=row.id
-			).exists()
-			
-			if not existing:
-				used_row = CalculationUsedRow.objects.create(
-					trail=self.trail,
-					calculation_name=calculation_name,
-					content_type=content_type,
-					object_id=row.id
-				)
-				print(f"✅ Created CalculationUsedRow: {calculation_name} -> {type(row).__name__} (id: {row.id})")
-			else:
-				print(f"⚠️ CalculationUsedRow already exists for {calculation_name} -> {type(row).__name__}")
-		
-		except Exception as e:
-			print(f"❌ Error tracking calculation used row: {e}")
-			import traceback
-			traceback.print_exc()
-	
-	def _track_django_model_fields_as_used(self, calculation_name, database_table, django_model_instance):
-		"""Track all fields of a Django model as used fields when the model instance is tracked as a used row"""
-		try:
-			# Get all database fields for this table
-			database_fields = database_table.database_fields.all()
-			
-			# Track each database field as a used field
-			for db_field in database_fields:
-				# Check if the Django model actually has this field
-				if hasattr(django_model_instance, db_field.name):
-					try:
-						# Track this field as used
-						self.track_calculation_used_field(calculation_name, db_field.name, django_model_instance)
-						print(f"  Tracked Django model field as used: {database_table.name}.{db_field.name}")
-					except Exception as e:
-						print(f"  Failed to track Django model field {db_field.name}: {e}")
-		except Exception as e:
-			print(f"Error tracking Django model fields as used: {e}")
-
-	def track_calculation_used_field(self, calculation_name, field_name, row=None, function_obj=None):
-		"""Track that a specific field was accessed during a calculation"""
-		if not self.lineage_enabled or not self.trail:
-			return
-		
-		try:
-			# Find the field object
-			field = None
-			content_type = None
-			
-			# If function_obj is provided, use it directly to avoid ID mismatch issues
-			if function_obj:
-				field = function_obj
-				content_type = ContentType.objects.get_for_model(Function)
-				print(f"🔍 Using provided function object: {field.name} (ID: {field.id})")
-			else:
-				# First try to find as DatabaseField
-				database_fields = DatabaseField.objects.filter(name=field_name)
-				if database_fields.exists():
-					field = database_fields.first()
-					content_type = ContentType.objects.get_for_model(DatabaseField)
-				else:
-					# Try to find as Function
-					functions = Function.objects.filter(name=field_name)
-					if functions.exists():
-						field = functions.first()
-						content_type = ContentType.objects.get_for_model(Function)
-			
-			if not field:
-				# Try to find with more context if field_name includes table reference
-				if '.' in field_name:
-					parts = field_name.split('.')
-					actual_field_name = parts[-1]
-					database_fields = DatabaseField.objects.filter(name=actual_field_name)
-					if database_fields.exists():
-						field = database_fields.first()
-						content_type = ContentType.objects.get_for_model(DatabaseField)
-					else:
-						functions = Function.objects.filter(name=actual_field_name)
-						if functions.exists():
-							field = functions.first()
-							content_type = ContentType.objects.get_for_model(Function)
-			
-			if not field:
-				print(f"Cannot find field {field_name} to track")
-				return
-			
-			# Prepare row tracking if provided
-			row_content_type = None
-			row_object_id = None
-			if row:
-				if isinstance(row, DatabaseRow):
-					row_content_type = ContentType.objects.get_for_model(DatabaseRow)
-					row_object_id = row.id
-				elif isinstance(row, DerivedTableRow):
-					row_content_type = ContentType.objects.get_for_model(DerivedTableRow)
-					row_object_id = row.id
-			
-			# Check if this field is already tracked for this calculation
-			query = CalculationUsedField.objects.filter(
-				trail=self.trail,
-				calculation_name=calculation_name,
-				content_type=content_type,
-				object_id=field.id
-			)
-			
-			if row_content_type and row_object_id:
-				query = query.filter(
-					row_content_type=row_content_type,
-					row_object_id=row_object_id
-				)
-			
-			if not query.exists():
-				CalculationUsedField.objects.create(
-					trail=self.trail,
-					calculation_name=calculation_name,
-					content_type=content_type,
-					object_id=field.id,
-					row_content_type=row_content_type,
-					row_object_id=row_object_id
-				)
-				# print(f"Tracked used field for {calculation_name}: {field_name}")
-		
-		except Exception as e:
-			print(f"Error tracking calculation used field: {e}")
-	
-	def get_calculation_used_rows(self, calculation_name):
-		"""Get all rows that were used in a specific calculation"""
-		if not self.trail:
-			return []
-		
-		used_rows = CalculationUsedRow.objects.filter(
-			trail=self.trail,
-			calculation_name=calculation_name
-		)
-		
-		return [ur.used_row for ur in used_rows]
-	
-	def get_calculation_used_fields(self, calculation_name):
-		"""Get all fields that were accessed during a specific calculation"""
-		if not self.trail:
-			return []
-		
-		used_fields = CalculationUsedField.objects.filter(
-			trail=self.trail,
-			calculation_name=calculation_name
-		)
-		
-		return [uf.used_field for uf in used_fields]
-	
-	def _track_object_relationships(self, tracked_row, business_object):
-		"""Track object relationships via DerivedRowSourceReference based on common relationship attributes"""
-		try:
-			if not isinstance(tracked_row, DerivedTableRow):
-				return
-			
-			print(f"Tracking object relationships for {type(business_object).__name__}")
-			
-			# Common relationship attributes to check
-			relationship_attrs = ['unionOfLayers', 'base', 'INSTRMNT', 'INSTRMNT_RL','PRTY', 'INSTRMNT_ENTTY_RL_ASSGNMNT','source_row', 'parent_row']
-			
-			for attr_name in relationship_attrs:
-				if hasattr(business_object, attr_name):
-					source_obj = getattr(business_object, attr_name)
-					if source_obj:
-						# Try to find the corresponding DerivedTableRow for the source object
-						source_row = self._find_derived_row_for_object(source_obj)
-						if source_row:
-							# Create DerivedRowSourceReference
-							existing_ref = DerivedRowSourceReference.objects.filter(
-								derived_row_id=tracked_row.id,
-								content_type=ContentType.objects.get_for_model(DerivedTableRow),
-								object_id=source_row.id
-							).exists()
-							
-							if not existing_ref:
-								DerivedRowSourceReference.objects.create(
-									derived_row=tracked_row,
-									content_type=ContentType.objects.get_for_model(DerivedTableRow),
-									object_id=source_row.id
-								)
-								print(f"Created relationship: {tracked_row.populated_table.table.name} row {tracked_row.id} <- {source_row.populated_table.table.name} row {source_row.id} via {attr_name}")
-			
-			# Also check class name-based relationships (e.g. F_05_01_REF_FINREP_3_0_UnionItem -> Other_loans)
-			obj_class_name = type(business_object).__name__
-			if '_' in obj_class_name:
-				parts = obj_class_name.split('_')
-				# Look for potential source class names in the parts
-				for i in range(len(parts)):
-					potential_source_class = '_'.join(parts[i:])
-					if potential_source_class != obj_class_name:
-						# Try to find objects of this source class
-						source_obj = self._find_object_by_class_suffix(business_object, potential_source_class)
-						if source_obj:
-							source_row = self._find_derived_row_for_object(source_obj)
-							if source_row:
-								existing_ref = DerivedRowSourceReference.objects.filter(
-									derived_row_id=tracked_row.id,
-									content_type=ContentType.objects.get_for_model(DerivedTableRow),
-									object_id=source_row.id
-								).exists()
-								
-								if not existing_ref:
-									DerivedRowSourceReference.objects.create(
-										derived_row=tracked_row,
-										content_type=ContentType.objects.get_for_model(DerivedTableRow),
-										object_id=source_row.id
-									)
-									print(f"Created class-based relationship: {tracked_row.populated_table.table.name} <- {source_row.populated_table.table.name} via class hierarchy")
-		
-		except Exception as e:
-			print(f"Error tracking object relationships: {e}")
-	
-	def _find_derived_row_for_object(self, obj):
-		"""Find the DerivedTableRow that corresponds to a business object"""
-		try:
-			obj_class_name = type(obj).__name__
-			object_identifier = f"{obj_class_name}_{id(obj)}"
-			
-			# Look in current populated tables for matching row
-			for table_name, populated_table in self.current_populated_tables.items():
-				if hasattr(populated_table, 'derivedtablerow_set'):
-					matching_rows = populated_table.derivedtablerow_set.filter(
-						row_identifier=object_identifier
-					)
-					if matching_rows.exists():
-						return matching_rows.first()
-			
-			return None
-		except Exception as e:
-			print(f"Error finding derived row for object: {e}")
-			return None
-	
-	def _find_object_by_class_suffix(self, business_object, suffix):
-		"""Find a related object by class name suffix"""
-		try:
-			# This is a heuristic approach - look for attributes that might contain objects of the target class
-			for attr_name in dir(business_object):
-				if not attr_name.startswith('_') and not callable(getattr(business_object, attr_name, None)):
-					attr_value = getattr(business_object, attr_name)
-					if attr_value and hasattr(attr_value, '__class__'):
-						if type(attr_value).__name__.endswith(suffix):
-							return attr_value
-			return None
-		except Exception as e:
-			print(f"Error finding object by class suffix: {e}")
-			return None
-
-	def _track_transitive_used_objects(self, calculation_name, business_object):
-		"""Track objects that are transitively referenced by the current object as also being used"""
-		try:
-			print(f"Tracking transitive used objects for {type(business_object).__name__}")
-			
-			# Common relationship attributes that point to other business objects
-			relationship_attrs = ['unionOfLayers', 'base', 'INSTRMNT', 'INSTRMNT_RL','PRTY', 'INSTRMNT_ENTTY_RL_ASSGNMNT', 'source_row', 'parent_row']
-			
-			for attr_name in relationship_attrs:
-				if hasattr(business_object, attr_name):
-					referenced_obj = getattr(business_object, attr_name)
-					if referenced_obj and hasattr(referenced_obj, '__class__'):
-						# This object is transitively used - track it as used as well
-						print(f"Found transitive reference: {type(business_object).__name__}.{attr_name} -> {type(referenced_obj).__name__}")
-						
-						# Recursively track this object as used (this will create its DerivedTableRow if needed)
-						self.track_calculation_used_row(calculation_name, referenced_obj)
-						
-		except Exception as e:
-			print(f"Error tracking transitive used objects: {e}")
-
-	def _track_column_value_for_django_field(self, db_row, field_name, field_value, table):
-		"""Helper method to track column values for Django model fields"""
-		try:
-			# Find or create the DatabaseField
-			fields = table.database_fields.filter(name=field_name)
-			
-			if not fields.exists():
-				field = DatabaseField.objects.create(
-					name=field_name,
-					table=table
-				)
-			else:
-				field = fields.first()
-			
-			# Create DatabaseColumnValue
-			numeric_value = None
-			string_value = None
-			
-			if field_value is not None:
-				try:
-					numeric_value = float(field_value)
-				except (ValueError, TypeError):
-					string_value = str(field_value)
-			
-			DatabaseColumnValue.objects.create(
-				value=numeric_value,
-				string_value=string_value,
-				column=field,
-				row=db_row
-			)
-			
-		except Exception as e:
-			print(f"Error tracking Django field {field_name}: {e}")
-
-	def _find_source_value_object(self, source_value):
-		"""Find the DatabaseColumnValue object for a given source value"""
-		try:
-			# Look for DatabaseColumnValue with matching value
-			source_row_id = self.current_rows.get('source') if hasattr(self, 'current_rows') and self.current_rows else None
-			if source_row_id:
-				source_row = DatabaseRow.objects.get(id=source_row_id)
-				column_values = source_row.column_values.filter(value=str(source_value))
-				if column_values.exists():
-					return column_values.first()
-
-			# Fallback: look across all current rows for derived table rows (most common case for polymorphic functions)
-			if hasattr(self, 'current_populated_tables'):
-				for table_name, populated_table in self.current_populated_tables.items():
-					# Check DerivedTableRow objects instead of DatabaseRow for business objects
-					if hasattr(populated_table, 'derivedtablerow_set'):
-						for row in populated_table.derivedtablerow_set.all():
-							# Look for matching values in EvaluatedFunction records
-							evaluated_funcs = row.evaluatedfunction_set.filter(
-								string_value=str(source_value)
-							)
-							if evaluated_funcs.exists():
-								# Return a dummy object that represents the source
-								return evaluated_funcs.first()
-					
-					# Also check traditional DatabaseRow objects		
-					if hasattr(populated_table, 'databaserow_set'):
-						for row in populated_table.databaserow_set.all():
-							column_values = row.column_values.filter(value=str(source_value))
-							if column_values.exists():
-								return column_values.first()
-
-		except Exception as e:
-			# Make this a debug message instead of error to reduce noise
-			print(f"Debug: Could not find source value object for '{source_value}': {e}")
-
-		return None
-
-	def track_data_processing(self, table_name, data_items, django_model_objects=None):
-		"""Track processing of data items in a table"""
-		if not self.lineage_enabled or not self.trail or not self.metadata_trail:
-			return
-		
-		# Also track that these rows and tables are being used in calculations
-		current_calculation = getattr(self, 'current_calculation', None)
-		print(f"🔗 track_data_processing: table={table_name}, items={len(data_items)}, django_objects={len(django_model_objects) if django_model_objects else 0}, current_calculation={current_calculation}")
-		# CRITICAL FIX: Do NOT auto-track all processed items here.
-		# Rows should only be marked as used when they pass a cell's calc_referenced_items filter
-		# or are explicitly tracked by targeted logic (e.g., wrapper or explicit calls).
-		# This prevents unrelated rows (e.g., Advances_that_are_not_loans) from appearing as used.
-		
-		try:
-			# Ensure we have a populated table for this table name
-			if table_name not in self.current_populated_tables:
-				# Determine if this is a Django model or a derived table
-				is_django_model = self._is_django_model(table_name)
-
-				# Check for existing PopulatedDataBaseTable/EvaluatedDerivedTable first
-				populated_table = None
-				temp_table = None
-				table_exists = False
-
-				if is_django_model:
-					# Look for existing DatabaseTable with same name in this trail
-					existing_populated = PopulatedDataBaseTable.objects.filter(
-						trail=self.trail,
-						table__name=table_name
-					).select_related('table').first()
-					if existing_populated:
-						temp_table = existing_populated.table
-						populated_table = existing_populated
-						table_exists = True
-						print(f"Found existing DatabaseTable for: {table_name}")
-				else:
-					# Look for existing DerivedTable with same name in this trail
-					existing_evaluated = EvaluatedDerivedTable.objects.filter(
-						trail=self.trail,
-						table__name=table_name
-					).select_related('table').first()
-					if existing_evaluated:
-						temp_table = existing_evaluated.table
-						populated_table = existing_evaluated
-						table_exists = True
-						print(f"Found existing DerivedTable for: {table_name}")
-
-				# Create new table only if not found
-				if not temp_table:
-					if is_django_model:
-						temp_table = DatabaseTable.objects.create(name=table_name)
-						table_type = 'DatabaseTable'
-					else:
-						temp_table = DerivedTable.objects.create(name=table_name)
-						table_type = 'DerivedTable'
-
-					if self.metadata_trail:
-						AortaTableReference.objects.create(
-							metadata_trail=self.metadata_trail,
-							table_content_type=table_type,
-							table_id=temp_table.id
-						)
-					else:
-						print(f"Warning: No metadata_trail available for tracking table {table_name}")
-
-				# Create populated table only if not found
-				if self.trail and not populated_table:
-					if is_django_model:
-						populated_table = PopulatedDataBaseTable.objects.create(
-							trail=self.trail,
-							table=temp_table
-						)
-					else:
-						populated_table = EvaluatedDerivedTable.objects.create(
-							trail=self.trail,
-							table=temp_table
-						)
-				elif not self.trail:
-					print(f"Warning: No trail available for PopulatedDataBaseTable {table_name}")
-					return
-				self.current_populated_tables[table_name] = populated_table
-
-			# Track each data item as a row
-			for i, item in enumerate(data_items):
-				row_id = f"{table_name}_row_{i}"
-
-				# Extract data from the item
-				if isinstance(item, dict):
-					# Item is already a dictionary
-					row_data = item
-				elif hasattr(item, '__dict__'):
-					row_data = {}
-					for attr in dir(item):
-						if not attr.startswith('_') and not callable(getattr(item, attr)):
-							try:
-								value = getattr(item, attr)
-								# Only include non-callable attributes to avoid unwanted method evaluations
-								if not callable(value):
-									row_data[attr] = value
-							except:
-								pass
-				else:
-					row_data = {'value': str(item)}
-
-				# Track the row
-				self.track_row_processing(table_name, row_data, row_id)
-
-			# print(f"Tracked data processing for {table_name}: {len(data_items)} items")
-
-		except Exception as e:
-			print(f"Error tracking data processing: {e}")
-
-	def get_lineage_trail(self):
-		"""Get the current lineage trail"""
-		return self.trail
-
-	def export_lineage_graph(self, trail_id=None):
-		"""Export lineage as a graph structure for visualization"""
-		if trail_id:
-			trail = Trail.objects.get(id=trail_id)
-		else:
-			trail = self.trail
-
-		if not trail:
-			return None
-
-		# Build graph structure
-		graph = {
-			'nodes': [],
-			'edges': [],
-			'trail': {
-				'id': trail.id,
-				'name': trail.name,
-				'created_at': trail.created_at.isoformat()
-			}
-		}
-
-		# Add table nodes
-		for table_ref in trail.metadata_trail.table_references.all():
-			if table_ref.table_content_type == 'DatabaseTable':
-				table = DatabaseTable.objects.get(id=table_ref.table_id)
-				graph['nodes'].append({
-					'id': f'table_{table.id}',
-					'type': 'DatabaseTable',
-					'name': table.name
-				})
-			elif table_ref.table_content_type == 'DerivedTable':
-				table = DerivedTable.objects.get(id=table_ref.table_id)
-				graph['nodes'].append({
-					'id': f'table_{table.id}',
-					'type': 'DerivedTable',
-					'name': table.name
-				})
-
-		# TODO: Add edges based on function references and data flow
-
-		return graph
+    # Class variable to track initialized objects
+    _initialized_objects = set()
+
+    # AORTA lineage tracking
+    def __init__(self):
+        self.trail = None
+        self.metadata_trail = None
+        self.current_populated_tables = {}  # Map table names to PopulatedTable instances
+        self.current_rows = {}  # Track current row being processed
+        self.lineage_enabled = True  # Can be disabled for performance
+        self.evaluated_functions_cache = {}  # Cache to track evaluated functions per row
+        self.object_contexts = {}  # Map object id -> derived row context
+
+        # Note: Do not automatically register this instance globally
+        # Global registration should be done explicitly when setting up lineage tracking
+
+    def init_with_lineage(self, theObject, execution_name=None):
+        """Initialize object with AORTA lineage tracking"""
+        # Create trail if not exists
+        if not self.trail and self.lineage_enabled:
+            execution_name = execution_name or f"Execution_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
+            self.metadata_trail = MetaDataTrail.objects.create()
+            self.trail = Trail.objects.create(name=execution_name, metadata_trail=self.metadata_trail)
+            print(f"Created AORTA Trail: {self.trail.name}")
+
+        # Track object initialization in AORTA
+        if self.lineage_enabled:
+            self._track_object_initialization(theObject)
+
+        # Perform standard initialization
+        return self.init(theObject)
+
+    def _is_django_model(self, table_name):
+        """Check if a table name corresponds to a Django model"""
+        try:
+            # Try to get the model from Django's app registry
+            apps.get_model("pybirdai", table_name)
+            return True
+        except LookupError:
+            # Not a Django model
+            return False
+
+    def _track_object_initialization(self, obj):
+        """Track object in AORTA metadata trail"""
+        if not obj:
+            return
+
+        obj_class_name = obj.__class__.__name__
+
+        # Check if this is a table object
+        if hasattr(obj, "__class__") and obj_class_name.endswith("_Table"):
+            # Extract meaningful table name
+            table_name = obj_class_name.replace("_Table", "")
+
+            # Skip dummy objects
+            if table_name == "Dummy":
+                return
+
+            # Check if we already have a populated table for this name in the current trail
+            if table_name in self.current_populated_tables:
+                print(f"Reusing existing table for: {table_name}")
+                return
+
+            # Determine if this is a Django model or a derived table
+            is_django_model = self._is_django_model(table_name)
+
+            # Check for existing table with same name in this trail
+            aorta_table = None
+            populated_table = None
+            table_exists = False
+
+            if self.trail:
+                if is_django_model:
+                    # Look for existing DatabaseTable with same name in this trail
+                    existing_populated = (
+                        PopulatedDataBaseTable.objects.filter(trail=self.trail, table__name=table_name)
+                        .select_related("table")
+                        .first()
+                    )
+                    if existing_populated:
+                        aorta_table = existing_populated.table
+                        populated_table = existing_populated
+                        table_exists = True
+                        print(f"Found existing DatabaseTable for: {table_name}")
+                else:
+                    # Look for existing DerivedTable with same name in this trail
+                    existing_evaluated = (
+                        EvaluatedDerivedTable.objects.filter(trail=self.trail, table__name=table_name)
+                        .select_related("table")
+                        .first()
+                    )
+                    if existing_evaluated:
+                        aorta_table = existing_evaluated.table
+                        populated_table = existing_evaluated
+                        table_exists = True
+                        print(f"Found existing DerivedTable for: {table_name}")
+
+            # Create new table if not found
+            if not aorta_table:
+                if is_django_model:
+                    # Create DatabaseTable for Django model classes
+                    aorta_table = DatabaseTable.objects.create(name=table_name)
+                else:
+                    # Create DerivedTable for non-Django model classes
+                    aorta_table = DerivedTable.objects.create(name=table_name)
+                print(f"Created new table for: {table_name}")
+
+            # Add to metadata trail if not already added
+            if self.metadata_trail:
+                table_type = "DatabaseTable" if is_django_model else "DerivedTable"
+                existing_ref = AortaTableReference.objects.filter(
+                    metadata_trail=self.metadata_trail, table_content_type=table_type, table_id=aorta_table.id
+                ).exists()
+
+                if not existing_ref:
+                    AortaTableReference.objects.create(
+                        metadata_trail=self.metadata_trail, table_content_type=table_type, table_id=aorta_table.id
+                    )
+
+            # Create populated/evaluated table only if we didn't find an existing one
+            if self.trail and not populated_table:
+                if is_django_model:
+                    # Create PopulatedDataBaseTable for Django model tables
+                    populated_table = PopulatedDataBaseTable.objects.create(trail=self.trail, table=aorta_table)
+                else:
+                    # Create EvaluatedDerivedTable for derived tables
+                    populated_table = EvaluatedDerivedTable.objects.create(trail=self.trail, table=aorta_table)
+            elif not self.trail:
+                print(f"Warning: No trail available for populated table {table_name}")
+                return
+
+            # Store the populated table in our tracking dictionary
+            self.current_populated_tables[table_name] = populated_table
+
+            # Track table columns/fields only if this is a new table
+            if not table_exists:
+                self._track_table_columns(obj, aorta_table)
+
+                # Analyze table creation functions (calc_ methods)
+                self._analyze_table_creation_functions(obj, aorta_table)
+
+            # print(f"Tracked table initialization: {table_name}")
+
+    def _track_table_columns(self, table_obj, aorta_table):
+        """Track columns/fields in a table"""
+        try:
+            # Only track fields for DatabaseTable instances
+            # For DerivedTable instances, columns are tracked as Functions
+            if isinstance(aorta_table, DatabaseTable):
+                fields_to_track = []
+                table_name = aorta_table.name
+
+                # For Django models, use the model's _meta.fields
+                if self._is_django_model(table_name):
+                    try:
+                        from django.apps import apps
+
+                        model_class = apps.get_model("pybirdai", table_name)
+                        fields_to_track = [field.name for field in model_class._meta.fields]
+                        print(f"Using Django model fields for {table_name}: {len(fields_to_track)} fields")
+                    except Exception as e:
+                        print(f"Error getting Django model fields for {table_name}: {e}")
+                        fields_to_track = []
+                else:
+                    # For non-Django tables, detect column methods from the row objects they contain
+                    fields_to_track = self._detect_table_fields_from_row_type(table_obj)
+
+                # Create DatabaseField instances
+                for field_name in fields_to_track:
+                    db_field = DatabaseField.objects.create(name=field_name, table=aorta_table)
+                    # print(f"Tracked column: {aorta_table.name}.{field_name}")
+        except Exception as e:
+            print(f"Error tracking columns for {aorta_table.name}: {e}")
+
+    def _detect_table_fields_from_row_type(self, table_obj):
+        """Detect fields by examining the row object type that this table contains"""
+        try:
+            # For derived tables, try to determine the row object type
+            table_class_name = table_obj.__class__.__name__
+
+            # Pattern: F_01_01_REF_FINREP_3_0_Table contains F_01_01_REF_FINREP_3_0 objects
+            if table_class_name.endswith("_Table"):
+                row_class_name = table_class_name[:-6]  # Remove '_Table' suffix
+
+                # Try to import and inspect the row class
+                try:
+                    # Look for the row class in the same module
+                    table_module = table_obj.__class__.__module__
+                    module = __import__(table_module, fromlist=[row_class_name])
+
+                    if hasattr(module, row_class_name):
+                        row_class = getattr(module, row_class_name)
+                        # Create a temporary instance to inspect its methods
+                        try:
+                            row_instance = row_class()
+                            fields = self._detect_column_methods(row_instance)
+                            print(f"Detected {len(fields)} fields from row type {row_class_name}: {fields[:5]}...")
+                            return fields
+                        except Exception as e:
+                            print(f"Could not instantiate {row_class_name}: {e}")
+
+                except Exception as e:
+                    print(f"Could not find row class {row_class_name}: {e}")
+
+            # Fallback: try to detect from table attributes that might be lists of row objects
+            for attr_name in dir(table_obj):
+                if not attr_name.startswith("_"):
+                    attr_value = getattr(table_obj, attr_name, None)
+                    if isinstance(attr_value, list) and len(attr_value) > 0:
+                        # Try to get column methods from the first item in the list
+                        first_item = attr_value[0]
+                        fields = self._detect_column_methods(first_item)
+                        if fields:
+                            print(f"Detected {len(fields)} fields from list attribute {attr_name}: {fields[:5]}...")
+                            return fields
+
+            print(f"No fields detected for non-Django table {table_class_name}")
+            return []
+
+        except Exception as e:
+            print(f"Error detecting fields for table {table_obj.__class__.__name__}: {e}")
+            return []
+
+    def _detect_column_methods(self, table_obj):
+        """Detect column methods in non-Django table objects using robust approaches"""
+        import inspect
+
+        column_methods = set()
+
+        # Get all callable methods
+        methods = [
+            name for name in dir(table_obj) if (not name.startswith("_") and callable(getattr(table_obj, name, None)))
+        ]
+
+        for method_name in methods:
+            try:
+                method = getattr(table_obj, method_name)
+
+                # Approach 1: Check for @lineage decorator (most reliable)
+                if self._has_lineage_decorator(method):
+                    column_methods.add(method_name)
+                    continue
+
+                # Approach 2: Method signature and naming patterns
+                if self._is_likely_column_method(method, method_name):
+                    column_methods.add(method_name)
+
+            except Exception:
+                continue
+
+        # Filter out known infrastructure methods
+        excluded_methods = {"init", "metric_value"}
+        excluded_prefixes = {"calc_"}
+
+        final_methods = []
+        for method_name in column_methods:
+            if method_name not in excluded_methods and not any(
+                method_name.startswith(prefix) for prefix in excluded_prefixes
+            ):
+                final_methods.append(method_name)
+
+        print(f"Detected {len(final_methods)} column methods for non-Django table: {final_methods[:5]}...")
+        return final_methods
+
+    def _has_lineage_decorator(self, method):
+        """Check if a method has the @lineage decorator"""
+        try:
+            # Check if method has wrapper attributes indicating decoration
+            if hasattr(method, "__wrapped__"):
+                return True
+
+            # Check method name or qualname for lineage wrapper signs
+            if hasattr(method, "__qualname__") and "lineage" in str(method.__qualname__):
+                return True
+
+            # Check for common decorator attributes
+            if hasattr(method, "__dict__") and any("lineage" in str(key) for key in method.__dict__):
+                return True
+
+            return False
+        except:
+            return False
+
+    def _is_likely_column_method(self, method, method_name):
+        """Check if method is likely a column method based on signature and naming"""
+        try:
+            import inspect
+
+            # Check method name - should be all uppercase (common column pattern)
+            if not method_name.isupper():
+                return False
+
+            # Check method signature - should only have 'self' parameter
+            sig = inspect.signature(method)
+            params = list(sig.parameters.keys())
+            if len(params) != 1 or params[0] != "self":
+                return False
+
+            # Check return type annotation if present
+            if sig.return_annotation != inspect.Signature.empty:
+                valid_types = [int, str, "int", "str"]
+                if sig.return_annotation in valid_types:
+                    return True
+
+            # Check docstring for enumeration mentions (common in column methods)
+            if method.__doc__ and "enumeration" in method.__doc__.lower():
+                return True
+
+            return True  # If all checks pass, likely a column method
+
+        except Exception:
+            return False
+
+    def init(self, theObject):
+        # Check if this object has already been initialized
+        object_id = id(theObject)
+        if object_id in self.__class__._initialized_objects:
+            print(f"Object of type {theObject.__class__.__name__} already initialized, skipping.")
+            # Even if we're skipping full initialization, we still need to ensure references are set
+            self._ensure_references_set(theObject)
+            return
+
+        # Mark this object as initialized
+        self.__class__._initialized_objects.add(object_id)
+
+        # Check if we have lineage tracking enabled globally and this looks like a table
+        from pybirdai.annotations.decorators import _lineage_context
+
+        global_orchestration = _lineage_context.get("orchestration")
+
+        if global_orchestration and hasattr(theObject, "__class__") and theObject.__class__.__name__.endswith("_Table"):
+
+            print(f"DEBUG: Using global orchestration for {theObject.__class__.__name__}")
+            print(f"  Global Trail: {global_orchestration.trail.id if global_orchestration.trail else None}")
+            print(
+                f"  Global MetaDataTrail: {global_orchestration.metadata_trail.id if global_orchestration.metadata_trail else None}"
+            )
+
+            # This is a table and we have lineage tracking - track it
+            if global_orchestration and global_orchestration.lineage_enabled:
+                global_orchestration._track_object_initialization(theObject)
+
+        # Set up references for the object (use global orchestration if available)
+        if global_orchestration and global_orchestration.lineage_enabled:
+            # Use the global orchestration for reference setup to maintain lineage context
+            global_orchestration._ensure_references_set(theObject)
+        else:
+            # Fallback to local orchestration
+            self._ensure_references_set(theObject)
+
+    def _ensure_references_set(self, theObject):
+        """
+        Ensure that all table references are properly set for the object.
+        This is called both during full initialization and when initialization is skipped.
+        """
+        references = [
+            method
+            for method in dir(theObject.__class__)
+            if not callable(getattr(theObject.__class__, method)) and not method.startswith("__")
+        ]
+        for eReference in references:
+            if eReference.endswith("Table"):
+                # Only set the reference if it's currently None
+                if getattr(theObject, eReference) is None:
+                    from django.apps import apps
+
+                    table_name = eReference.split("_Table")[0]
+                    relevant_model = None
+                    try:
+                        relevant_model = apps.get_model("pybirdai", table_name)
+                    except LookupError:
+                        print("LookupError: " + table_name)
+
+                    if relevant_model:
+                        print("relevant_model: " + str(relevant_model))
+                        newObject = relevant_model.objects.all()
+                        print("newObject: " + str(newObject))
+                        if newObject:
+                            setattr(theObject, eReference, newObject)
+                            # Original CSV persistence
+                            CSVConverter.persist_object_as_csv(newObject, True)
+
+                            # Enhanced lineage tracking - track when tables are created but distinguish from usage tracking
+                            if self.lineage_enabled and self.trail and hasattr(newObject, "__iter__"):
+                                try:
+                                    # Extract Django model objects from the queryset
+                                    django_objects = list(newObject) if hasattr(newObject, "__iter__") else [newObject]
+                                    if django_objects:
+                                        # Create table structure and register for potential tracking
+                                        # Only mark as used when actually accessed via automatic tracking wrapper
+                                        print(f"📊 Table Created: {len(django_objects)} {table_name} objects available")
+                                except Exception as e:
+                                    print(f"Warning: Could not process {table_name} objects for lineage: {e}")
+
+                    else:
+                        newObject = OrchestrationWithLineage.createObjectFromReferenceType(eReference)
+
+                        operations = [
+                            method
+                            for method in dir(newObject.__class__)
+                            if callable(getattr(newObject.__class__, method)) and not method.startswith("__")
+                        ]
+
+                        for operation in operations:
+                            if operation == "init":
+                                try:
+                                    getattr(newObject, operation)()
+
+                                    # Check if lineage tracking is enabled and track data after initialization
+                                    from pybirdai.annotations.decorators import _lineage_context
+
+                                    orchestration = _lineage_context.get("orchestration")
+                                    if (
+                                        orchestration
+                                        and orchestration.lineage_enabled
+                                        and hasattr(newObject, "__class__")
+                                        and newObject.__class__.__name__.endswith("_Table")
+                                    ):
+
+                                        # Debug: print orchestration state
+                                        print(f"DEBUG: Orchestration for {newObject.__class__.__name__}:")
+                                        print(f"  Trail: {orchestration.trail.id if orchestration.trail else None}")
+                                        print(
+                                            f"  MetaDataTrail: {orchestration.metadata_trail.id if orchestration.metadata_trail else None}"
+                                        )
+
+                                        # First track the table itself if not already tracked
+                                        if orchestration.metadata_trail:
+                                            orchestration._track_object_initialization(newObject)
+                                        else:
+                                            print(f"WARNING: No metadata_trail for {newObject.__class__.__name__}")
+
+                                        # Track any data that was populated during initialization
+                                        table_name = newObject.__class__.__name__.replace("_Table", "")
+                                        for attr_name in dir(newObject):
+                                            if not attr_name.startswith("_") and hasattr(newObject, attr_name):
+                                                attr_value = getattr(newObject, attr_name)
+                                                if isinstance(attr_value, list) and len(attr_value) > 0:
+                                                    # CRITICAL FIX: DO NOT auto-track derived table data during initialization
+                                                    # Only track when explicitly used in calculations
+                                                    if orchestration.metadata_trail:
+                                                        print(
+                                                            f"📊 Found {len(attr_value)} items in {table_name}_{attr_name} (not tracking as used yet)"
+                                                        )
+                                                    else:
+                                                        print(
+                                                            f"WARNING: Cannot process data for {table_name}_{attr_name} - no metadata_trail"
+                                                        )
+
+                                except Exception as e:
+                                    print(f"Could not call function called {operation}: {e}")
+
+                        setattr(theObject, eReference, newObject)
+
+    @classmethod
+    def reset_initialization(cls):
+        """
+        Reset the initialization tracking.
+        This can be useful for testing or when re-initialization is required.
+        """
+        cls._initialized_objects.clear()
+        print("Initialization tracking has been reset.")
+
+    @classmethod
+    def is_initialized(cls, obj):
+        """
+        Check if an object has been initialized.
+
+        Args:
+                obj: The object to check
+
+        Returns:
+                bool: True if the object has been initialized, False otherwise
+        """
+        return id(obj) in cls._initialized_objects
+
+    @staticmethod
+    def createObjectFromReferenceType(eReference):
+        try:
+            cls = getattr(importlib.import_module("pybirdai.process_steps.filter_code.output_tables"), eReference)
+            new_object = cls()
+            return new_object
+        except:
+            print("Error: " + eReference)
+
+    # AORTA Lineage Tracking Methods
+
+    def track_function_execution(self, function_name, source_columns, result_column=None, source_code=None):
+        """Track the execution of a function in AORTA"""
+        if not self.lineage_enabled or not self.trail:
+            return
+
+        # Create or get derived table for this function
+        class_name = function_name.split(".")[0] if "." in function_name else "DynamicFunctions"
+
+        # For individual objects, use their parent table name
+        if not class_name.endswith("_Table"):
+            parent_table_name = self._get_parent_table_name(class_name)
+            if parent_table_name and parent_table_name != class_name:
+                class_name = parent_table_name
+
+        derived_table = None
+
+        # Check if we already have a derived table for this class in the current trail
+        existing_tables = None
+        if self.metadata_trail:
+            # Look for tables in the current metadata trail
+            existing_refs = AortaTableReference.objects.filter(
+                metadata_trail=self.metadata_trail, table_content_type="DerivedTable"
+            )
+            for ref in existing_refs:
+                table = DerivedTable.objects.get(id=ref.table_id)
+                if table.name == class_name:
+                    existing_tables = [table]
+                    break
+
+        if existing_tables:
+            derived_table = existing_tables[0]
+        else:
+            derived_table = DerivedTable.objects.create(name=class_name)
+
+            # Add to metadata trail
+            if self.metadata_trail:
+                AortaTableReference.objects.create(
+                    metadata_trail=self.metadata_trail, table_content_type="DerivedTable", table_id=derived_table.id
+                )
+
+            # Create EvaluatedDerivedTable
+            if self.trail:
+                evaluated_table = EvaluatedDerivedTable.objects.create(trail=self.trail, table=derived_table)
+                self.current_populated_tables[class_name] = evaluated_table
+
+        # Check if Function already exists for this name and table
+        existing_functions = Function.objects.filter(name=function_name, table=derived_table)
+
+        if existing_functions.exists():
+            # Reuse existing function
+            function = existing_functions.first()
+            # print(f"Reusing existing function: {function_name}")
+        else:
+            # Create new Function record
+            function_text = FunctionText.objects.create(text=source_code or function_name, language="python")
+
+            function = Function.objects.create(name=function_name, function_text=function_text, table=derived_table)
+            # print(f"Created new function: {function_name}")
+
+        # Note: TableCreationFunction instances are now created in _analyze_table_creation_functions
+        # during table initialization, which analyzes class variables for source tables
+
+        # Track column references (only for newly created functions to avoid duplicates)
+        if not existing_functions.exists():
+            for col_ref in source_columns:
+                try:
+                    # Try to resolve the actual column object
+                    resolved_field = self._resolve_column_reference(col_ref)
+                    if resolved_field:
+                        # Check if this column reference already exists
+                        content_type = ContentType.objects.get_for_model(resolved_field.__class__)
+                        existing_col_refs = FunctionColumnReference.objects.filter(
+                            function=function, content_type=content_type, object_id=resolved_field.id
+                        )
+
+                        if not existing_col_refs.exists():
+                            # Create FunctionColumnReference
+                            FunctionColumnReference.objects.create(
+                                function=function, content_type=content_type, object_id=resolved_field.id
+                            )
+                            # print(f"Tracked column reference: {function_name} -> {col_ref}")
+                except Exception as e:
+                    print(f"Could not resolve column reference {col_ref}: {e}")
+
+        return function
+
+    def track_polymorphic_function_execution(
+        self, function_name, base_class_name, source_columns, result_column=None, wrapper_obj=None, base_obj=None
+    ):
+        """Track the execution of a polymorphic function in AORTA"""
+        if not self.lineage_enabled or not self.trail:
+            return
+
+        # Use the wrapper class name for the table/function organization
+        # CRITICAL FIX: Extract the FULL wrapper class name, not just the first part
+        # For function_name like "F_05_01_REF_FINREP_3_0_UnionItem.GRSS_CRRYNG_AMNT"
+        # we want "F_05_01_REF_FINREP_3_0_UnionItem", not "F_05_01_REF_FINREP_3_0"
+        if "." in function_name:
+            # Get everything before the last dot (the method name)
+            wrapper_class_name = function_name.rsplit(".", 1)[0]
+        else:
+            wrapper_class_name = "DynamicFunctions"
+
+        print(
+            f"🔧 track_polymorphic_function_execution: function_name={function_name}, wrapper_class_name={wrapper_class_name}, base_class_name={base_class_name}"
+        )
+
+        # Create more descriptive function name that includes the base class
+        polymorphic_function_name = f"{function_name}@{base_class_name}"
+
+        # Create or get derived table for the wrapper class
+        derived_table = None
+        existing_tables = None
+        if self.metadata_trail:
+            # Look for tables in the current metadata trail
+            existing_refs = AortaTableReference.objects.filter(
+                metadata_trail=self.metadata_trail, table_content_type="DerivedTable"
+            )
+            for ref in existing_refs:
+                table = DerivedTable.objects.get(id=ref.table_id)
+                if table.name == wrapper_class_name:
+                    existing_tables = [table]
+                    break
+
+        if existing_tables:
+            derived_table = existing_tables[0]
+        else:
+            derived_table = DerivedTable.objects.create(name=wrapper_class_name)
+
+            # Add to metadata trail
+            if self.metadata_trail:
+                AortaTableReference.objects.create(
+                    metadata_trail=self.metadata_trail, table_content_type="DerivedTable", table_id=derived_table.id
+                )
+
+            # Create EvaluatedDerivedTable
+            if self.trail:
+                evaluated_table = EvaluatedDerivedTable.objects.create(trail=self.trail, table=derived_table)
+                self.current_populated_tables[wrapper_class_name] = evaluated_table
+
+        # Check if Function already exists for this polymorphic name and table
+        existing_functions = Function.objects.filter(name=polymorphic_function_name, table=derived_table)
+
+        if existing_functions.exists():
+            # Reuse existing function
+            function = existing_functions.first()
+        else:
+            # Create source code that shows the polymorphic delegation
+            method_name = function_name.split(".")[-1] if "." in function_name else function_name
+            source_code = f"def {method_name}(self) -> Any: return self.base.{method_name}()  # Polymorphic delegation to {base_class_name}"
+
+            # Create new Function record
+            function_text = FunctionText.objects.create(text=source_code, language="python")
+
+            function = Function.objects.create(
+                name=polymorphic_function_name, function_text=function_text, table=derived_table
+            )
+            print(f"Created polymorphic function: {polymorphic_function_name}")
+
+        # Track column references for polymorphic dependencies
+        for col_ref in source_columns:
+            try:
+                # Try to resolve the actual column object
+                resolved_field = self._resolve_column_reference(col_ref)
+                if resolved_field:
+                    # Check if this column reference already exists
+                    content_type = ContentType.objects.get_for_model(resolved_field.__class__)
+                    existing_col_refs = FunctionColumnReference.objects.filter(
+                        function=function, content_type=content_type, object_id=resolved_field.id
+                    )
+
+                    if not existing_col_refs.exists():
+                        # Create FunctionColumnReference
+                        FunctionColumnReference.objects.create(
+                            function=function, content_type=content_type, object_id=resolved_field.id
+                        )
+            except Exception as e:
+                print(f"Could not resolve polymorphic column reference {col_ref}: {e}")
+
+        return function
+
+    def _extract_source_table_names(self, source_columns):
+        """Extract unique table names from column dependencies"""
+        table_names = set()
+
+        for col_ref in source_columns:
+            try:
+                # Column references are in format "TABLE_NAME.column_name" or nested
+                parts = col_ref.split(".")
+                if len(parts) >= 2:
+                    # First part is typically the table name
+                    table_name = parts[0]
+                    # Only add if it looks like a table name (not a lowercase attribute)
+                    if table_name and table_name.isupper():
+                        table_names.add(table_name)
+            except Exception as e:
+                print(f"Error extracting table name from {col_ref}: {e}")
+
+        return list(table_names)
+
+    def _analyze_table_creation_functions(self, table_obj, aorta_table):
+        """Analyze calc_ methods in table classes and create TableCreationFunction instances"""
+        try:
+            class_name = table_obj.__class__.__name__
+
+            # Find all calc_ methods in this class
+            calc_methods = [
+                name for name in dir(table_obj) if name.startswith("calc_") and callable(getattr(table_obj, name))
+            ]
+
+            for calc_method_name in calc_methods:
+                calc_method = getattr(table_obj, calc_method_name)
+                full_function_name = f"{class_name}.{calc_method_name}"
+
+                # Extract source table names from class variables ending with _Table
+                source_table_names = self._extract_source_tables_from_class_variables(table_obj)
+
+                # Get function source code
+                try:
+                    import inspect
+
+                    source_code = inspect.getsource(calc_method)
+                except:
+                    source_code = f"def {calc_method_name}(self): # Source code not available"
+
+                # Check if this calc_ method has a @lineage decorator and extract dependencies
+                lineage_dependencies = self._extract_lineage_dependencies(calc_method)
+                lineage_column_references = []
+                if lineage_dependencies:
+                    # Use lineage dependencies for more detailed function text
+                    source_code += f"\n# Lineage dependencies: {lineage_dependencies}"
+                    # Extract column references from the lineage dependencies
+                    lineage_column_references = self._parse_lineage_dependencies(lineage_dependencies)
+
+                # Check if TableCreationFunction already exists
+                existing_table_creation_functions = TableCreationFunction.objects.filter(name=full_function_name)
+
+                if existing_table_creation_functions.exists():
+                    # Reuse existing table creation function
+                    table_creation_function = existing_table_creation_functions.first()
+                    # print(f"Reusing existing table creation function: {full_function_name}")
+                else:
+                    # Create FunctionText
+                    function_text = FunctionText.objects.create(text=source_code, language="python")
+
+                    # Create TableCreationFunction
+                    table_creation_function = TableCreationFunction.objects.create(
+                        name=full_function_name, function_text=function_text
+                    )
+                    # print(f"Created new table creation function: {full_function_name}")
+
+                # Create TableCreationSourceTable entries (only for new functions)
+                if not existing_table_creation_functions.exists():
+                    for source_table_name in source_table_names:
+                        # Find the source table in DatabaseTable or DerivedTable
+                        source_table = self._find_table_by_name(source_table_name)
+                        if source_table:
+                            content_type = ContentType.objects.get_for_model(source_table.__class__)
+                            # Check if this source table reference already exists
+                            existing_source_refs = TableCreationSourceTable.objects.filter(
+                                table_creation_function=table_creation_function,
+                                content_type=content_type,
+                                object_id=source_table.id,
+                            )
+
+                            if not existing_source_refs.exists():
+                                TableCreationSourceTable.objects.create(
+                                    table_creation_function=table_creation_function,
+                                    content_type=content_type,
+                                    object_id=source_table.id,
+                                )
+                                # print(f"Tracked table creation source: {full_function_name} -> {source_table_name}")
+
+                    # Create TableCreationFunctionColumn entries for lineage dependencies
+                    for column_ref in lineage_column_references:
+                        column_obj = column_ref["column"]
+                        reference_text = column_ref["reference_text"]
+
+                        content_type = ContentType.objects.get_for_model(column_obj.__class__)
+                        from pybirdai.models import TableCreationFunctionColumn
+
+                        # Check if this column reference already exists
+                        existing_column_refs = TableCreationFunctionColumn.objects.filter(
+                            table_creation_function=table_creation_function,
+                            content_type=content_type,
+                            object_id=column_obj.id,
+                            reference_text=reference_text,
+                        )
+
+                        if not existing_column_refs.exists():
+                            TableCreationFunctionColumn.objects.create(
+                                table_creation_function=table_creation_function,
+                                content_type=content_type,
+                                object_id=column_obj.id,
+                                reference_text=reference_text,
+                            )
+                            # print(f"Tracked column reference: {full_function_name} -> {column_obj}")
+
+                print(
+                    f"Created TableCreationFunction for {full_function_name} with {len(source_table_names)} source tables and {len(lineage_column_references)} column references"
+                )
+
+        except Exception as e:
+            print(f"Error analyzing table creation functions for {table_obj.__class__.__name__}: {e}")
+
+    def _extract_source_tables_from_class_variables(self, table_obj):
+        """Extract source table names from class variables ending with _Table"""
+        source_table_names = set()
+
+        # Get all attributes that end with _Table
+        for attr_name in dir(table_obj):
+            if attr_name.endswith("_Table") and not attr_name.startswith("_"):
+                # Remove _Table suffix to get the table name
+                table_name = attr_name.replace("_Table", "")
+                source_table_names.add(table_name)
+
+        return list(source_table_names)
+
+    def _extract_lineage_dependencies(self, method):
+        """Extract dependencies from @lineage decorator if present"""
+        try:
+            import inspect
+            import re
+
+            # Get the source code of the method
+            source_code = inspect.getsource(method)
+
+            # Look for @lineage decorator with dependencies parameter
+            # Pattern matches: @lineage(dependencies={"base.COLUMN", "table.COLUMN"}) including multiline
+            lineage_pattern = r"@lineage\s*\(\s*dependencies\s*=\s*\{([^}]+)\}\s*\)"
+            matches = re.search(lineage_pattern, source_code, re.DOTALL)
+
+            if matches:
+                # Extract the dependencies content
+                dependencies_content = matches.group(1)
+
+                # Extract individual dependency strings (remove quotes and whitespace)
+                dependency_pattern = r"\"([^\"]+)\""
+                dependency_matches = re.findall(dependency_pattern, dependencies_content)
+
+                if dependency_matches:
+                    dependencies_text = ", ".join(dependency_matches)
+                    print(f"Extracted lineage dependencies: {dependencies_text}")
+                    return dependencies_text
+
+            return None
+
+        except Exception as e:
+            print(f"Error extracting lineage dependencies: {e}")
+            return None
+
+    def _parse_lineage_dependencies(self, lineage_dependencies_text):
+        """Parse lineage dependencies text to extract column references"""
+        column_references = []
+
+        if not lineage_dependencies_text:
+            return column_references
+
+        try:
+            # Extract column references from the lineage dependencies text
+            # Look for patterns like "base.COLUMN_NAME", "table.COLUMN_NAME"
+            import re
+
+            # Find all patterns that look like column references
+            # This regex looks for word.WORD patterns (table.column references)
+            pattern = r"\b(\w+)\.([A-Za-z_][A-Za-z0-9_]*)\b"
+            matches = re.findall(pattern, lineage_dependencies_text)
+
+            for table_ref, column_name in matches:
+                # Try to find the actual column object
+                column_obj = self._find_column_by_name(column_name, table_ref)
+                if column_obj:
+                    column_references.append({"column": column_obj, "reference_text": f"{table_ref}.{column_name}"})
+                else:
+                    # If we can't find the specific column, try a broader search
+                    column_obj = self._find_column_by_name(column_name)
+                    if column_obj:
+                        column_references.append({"column": column_obj, "reference_text": f"{table_ref}.{column_name}"})
+
+            print(f"Parsed {len(column_references)} column references from lineage dependencies")
+            return column_references
+
+        except Exception as e:
+            print(f"Error parsing lineage dependencies: {e}")
+            return column_references
+
+    def _find_column_by_name(self, column_name, table_hint=None):
+        """Find a column (DatabaseField or Function) by name, optionally with table hint"""
+        try:
+            # First try to find in DatabaseField
+            database_fields = DatabaseField.objects.filter(name=column_name)
+            if table_hint:
+                # Filter by table name if hint provided
+                database_fields = database_fields.filter(table__name__icontains=table_hint)
+
+            if database_fields.exists():
+                return database_fields.first()
+
+            # Then try to find in Function
+            functions = Function.objects.filter(name=column_name)
+            if table_hint:
+                # Filter by table name if hint provided
+                functions = functions.filter(table__name__icontains=table_hint)
+
+            if functions.exists():
+                return functions.first()
+
+            # Fallback: try without table hint if we had one
+            if table_hint:
+                return self._find_column_by_name(column_name, None)
+
+            return None
+
+        except Exception as e:
+            print(f"Error finding column {column_name}: {e}")
+            return None
+
+    def _find_table_by_name(self, table_name):
+        """Find a table by name in DatabaseTable or DerivedTable"""
+        try:
+            # First try DatabaseTable
+            database_tables = DatabaseTable.objects.filter(name=table_name)
+            if database_tables.exists():
+                return database_tables.first()
+
+            # Then try DerivedTable
+            derived_tables = DerivedTable.objects.filter(name=table_name)
+            if derived_tables.exists():
+                return derived_tables.first()
+
+            return None
+        except Exception as e:
+            print(f"Error finding table {table_name}: {e}")
+            return None
+
+    def _resolve_column_reference(self, column_ref):
+        """Resolve a column reference string to an actual DatabaseField object"""
+        try:
+            # Handle nested references like "base.CRRYNG_AMNT"
+            parts = column_ref.split(".")
+
+            # For simple column references, look in existing tables
+            if len(parts) == 1:
+                # Look for this column in any tracked table
+                for table_name, populated_table in self.current_populated_tables.items():
+                    if hasattr(populated_table, "table") and hasattr(populated_table.table, "database_fields"):
+                        fields = populated_table.table.database_fields.filter(name=parts[0])
+                        if fields.exists():
+                            return fields.first()
+
+            # For complex references, try to resolve based on patterns
+            elif len(parts) > 1:
+                # Look for the column name in the last part
+                column_name = parts[-1]
+                for table_name, populated_table in self.current_populated_tables.items():
+                    if hasattr(populated_table, "table") and hasattr(populated_table.table, "database_fields"):
+                        fields = populated_table.table.database_fields.filter(name=column_name)
+                        if fields.exists():
+                            return fields.first()
+
+        except Exception as e:
+            print(f"Error resolving column reference {column_ref}: {e}")
+
+        return None
+
+    def track_row_processing(self, table_name, row_data, row_identifier=None):
+        """Track row-level lineage"""
+        if not self.lineage_enabled or not self.trail:
+            return
+
+        try:
+            # Get the populated table for this table name
+            populated_table = self.current_populated_tables.get(table_name)
+            if not populated_table:
+                print(f"No populated table found for {table_name}")
+                return
+
+            # Determine if this is a derived table or database table
+            is_derived_table = isinstance(populated_table, EvaluatedDerivedTable)
+
+            # Check for existing row with same data to prevent duplicates
+            existing_row = None
+            if not is_derived_table and isinstance(row_data, dict):
+                # For database tables, check if a row with the same data already exists
+                existing_rows = populated_table.databaserow_set.all()
+                for existing in existing_rows:
+                    if self._rows_have_same_data(existing, row_data):
+                        existing_row = existing
+                        print(f"Found existing row for {table_name}, reusing instead of creating duplicate")
+                        break
+
+            if existing_row:
+                db_row = existing_row
+            else:
+                # Create row identifier if not provided
+                if not row_identifier:
+                    if is_derived_table:
+                        row_identifier = f"row_{len(populated_table.derivedtablerow_set.all()) + 1}"
+                    else:
+                        row_identifier = f"row_{len(populated_table.databaserow_set.all()) + 1}"
+
+                # Create appropriate row type
+                if is_derived_table:
+                    # Create DerivedTableRow for derived tables
+                    db_row = DerivedTableRow.objects.create(
+                        populated_table=populated_table, row_identifier=row_identifier
+                    )
+                else:
+                    # Create DatabaseRow for database tables
+                    db_row = DatabaseRow.objects.create(populated_table=populated_table, row_identifier=row_identifier)
+
+            # Track individual column values (only for DatabaseRow and only if this is a new row)
+            if not is_derived_table and not existing_row:
+                if isinstance(row_data, dict):
+                    for column_name, value in row_data.items():
+                        self._track_column_value(db_row, column_name, value)
+                elif hasattr(row_data, "__dict__"):
+                    # Handle object with attributes
+                    for attr_name in dir(row_data):
+                        if not attr_name.startswith("_") and not callable(getattr(row_data, attr_name)):
+                            value = getattr(row_data, attr_name)
+                            self._track_column_value(db_row, attr_name, value)
+
+            # Store current row context for value tracking
+            self.current_rows["source"] = db_row.id
+            self.current_rows["table"] = table_name
+
+            # Clear evaluated functions cache when switching to a new row
+            self.evaluated_functions_cache.clear()
+
+            # print(f"Tracked row processing: {table_name} row {row_identifier}")
+            return db_row
+
+        except Exception as e:
+            print(f"Error tracking row processing: {e}")
+            return None
+
+    def _track_column_value(self, db_row, column_name, value):
+        """Track individual column values"""
+        try:
+            # Find the corresponding DatabaseField
+            table = db_row.populated_table.table
+            fields = table.database_fields.filter(name=column_name)
+
+            if not fields.exists():
+                # Create the field if it doesn't exist
+                field = DatabaseField.objects.create(name=column_name, table=table)
+                print(f"Created missing column: {table.name}.{column_name}")
+            else:
+                field = fields.first()
+
+            # Create DatabaseColumnValue
+            # Try to convert to float, otherwise use string_value
+            numeric_value = None
+            string_value = None
+
+            if value is not None:
+                try:
+                    numeric_value = float(value)
+                except (ValueError, TypeError):
+                    string_value = str(value)
+
+            DatabaseColumnValue.objects.create(value=numeric_value, string_value=string_value, column=field, row=db_row)
+
+            # print(f"Tracked column value: {table.name}.{column_name} = {value}")
+        except Exception as e:
+            print(f"Error tracking column value {column_name}: {e}")
+
+    def _rows_have_same_data(self, existing_row, new_row_data):
+        """Check if an existing DatabaseRow has the same data as new_row_data"""
+        try:
+            # Get all column values for the existing row
+            existing_values = {}
+            for column_value in existing_row.column_values.all():
+                column_name = column_value.column.name
+                value = column_value.value if column_value.value is not None else column_value.string_value
+                existing_values[column_name] = value
+
+            # Compare with new row data
+            if len(existing_values) != len(new_row_data):
+                return False
+
+            for column_name, new_value in new_row_data.items():
+                existing_value = existing_values.get(column_name)
+
+                # Handle numeric vs string comparison
+                if existing_value is None and new_value is None:
+                    continue
+                elif existing_value is None or new_value is None:
+                    return False
+
+                # Try numeric comparison first
+                try:
+                    if float(existing_value) == float(new_value):
+                        continue
+                except (ValueError, TypeError):
+                    pass
+
+                # Fall back to string comparison
+                if str(existing_value) != str(new_value):
+                    return False
+
+            return True
+
+        except Exception as e:
+            print(f"Error comparing row data: {e}")
+            return False
+
+    def track_derived_row_processing(self, table_name, derived_row_data, source_row_ids=None):
+        """Track derived/computed row processing"""
+        if not self.lineage_enabled or not self.trail:
+            return
+
+        try:
+            # Get the evaluated derived table
+            evaluated_table = self.current_populated_tables.get(table_name)
+            if not evaluated_table or not isinstance(evaluated_table, EvaluatedDerivedTable):
+                print(f"No evaluated derived table found for {table_name}")
+                return
+
+            # Create DerivedTableRow
+            derived_row = DerivedTableRow.objects.create(populated_table=evaluated_table)
+
+            # Track source row references
+            if source_row_ids:
+                for source_row_id in source_row_ids:
+                    try:
+                        source_row = DatabaseRow.objects.get(id=source_row_id)
+                        DerivedRowSourceReference.objects.create(
+                            derived_row=derived_row,
+                            content_type=ContentType.objects.get_for_model(DatabaseRow),
+                            object_id=source_row.id,
+                        )
+                    except DatabaseRow.DoesNotExist:
+                        print(f"Source row {source_row_id} not found")
+
+            # Store current derived row context
+            self.current_rows["derived"] = derived_row.id
+
+            # Clear evaluated functions cache when switching to a new derived row
+            self.evaluated_functions_cache.clear()
+
+            # print(f"Tracked derived row processing: {table_name}")
+            return derived_row
+
+        except Exception as e:
+            print(f"Error tracking derived row processing: {e}")
+            return None
+
+    def track_value_computation(self, function_name, source_values, computed_value):
+        """Track value-level lineage"""
+        print(f"🔍 track_value_computation called: {function_name}, value={computed_value}")
+
+        if not self.lineage_enabled or not self.trail:
+            print(f"🔍 track_value_computation: lineage disabled or no trail")
+            return
+
+        try:
+            # Get the current derived row if available
+            derived_row_id = self.current_rows.get("derived")
+            if not derived_row_id:
+                print(f"🔍 track_value_computation: No derived row context for value computation: {function_name}")
+                print(f"🔍 track_value_computation: Current rows context: {self.current_rows}")
+                return
+
+            print(f"🔍 track_value_computation: Using derived_row_id: {derived_row_id}")
+
+            # Check cache first
+            cache_key = f"{derived_row_id}:{function_name}"
+            if cache_key in self.evaluated_functions_cache:
+                # Return cached evaluated function
+                return self.evaluated_functions_cache[cache_key]
+
+            # Get the derived row
+            derived_row = DerivedTableRow.objects.get(id=derived_row_id)
+
+            # Find the corresponding Function object
+            function_parts = function_name.split(".")
+            class_name = function_parts[0] if len(function_parts) > 1 else "DynamicFunctions"
+            method_name = function_parts[-1]
+
+            # Look for the function by name across all Function objects
+            functions = Function.objects.filter(name=function_name)
+
+            if not functions.exists():
+                print(f"Function {function_name} not found for value computation")
+                return
+
+            # CRITICAL FIX: When multiple functions exist with the same name,
+            # prefer the one that belongs to the same table as the derived row
+            function = functions.first()  # Default fallback
+
+            # Try to find a function from the same table as the derived row
+            derived_table = derived_row.populated_table.table
+            for func in functions:
+                if func.table.id == derived_table.id:
+                    function = func
+                    print(
+                        f"🎯 track_value_computation: Using function {func.id} from correct table {derived_table.name}"
+                    )
+                    break
+            else:
+                # If no exact table match, try by table name (for data consistency)
+                for func in functions:
+                    if func.table.name == derived_table.name:
+                        function = func
+                        print(
+                            f"🎯 track_value_computation: Using function {func.id} from table with matching name {derived_table.name}"
+                        )
+                        break
+
+            # Check if we already have an EvaluatedFunction for this function and row
+            existing_evaluated = EvaluatedFunction.objects.filter(function=function, row=derived_row).first()
+
+            if existing_evaluated:
+                # We already have this function evaluated for this row
+                # Since functions are immutable, the result should be the same
+                # Cache it and return
+                self.evaluated_functions_cache[cache_key] = existing_evaluated
+                # print(f"Reusing existing EvaluatedFunction for {function_name} on row {derived_row_id}")
+                return existing_evaluated
+
+            # Create EvaluatedFunction only if it doesn't exist
+            # Try to store as numeric value if possible
+            numeric_value = None
+            string_value = None
+
+            if computed_value is not None:
+                try:
+                    numeric_value = float(computed_value)
+                except (ValueError, TypeError):
+                    string_value = str(computed_value)
+
+            print(
+                f"🔍 track_value_computation: Creating EvaluatedFunction for {function.name} (ID: {function.id}) on row {derived_row.id}"
+            )
+            evaluated_function = EvaluatedFunction.objects.create(
+                value=numeric_value, string_value=string_value, function=function, row=derived_row
+            )
+            print(f"🔍 track_value_computation: ✅ Created EvaluatedFunction ID: {evaluated_function.id}")
+
+            # Track source values (optional - don't fail if this doesn't work)
+            for source_value in source_values:
+                if source_value is not None:
+                    try:
+                        # Try to find the corresponding DatabaseColumnValue
+                        source_value_obj = self._find_source_value_object(source_value)
+                        if source_value_obj:
+                            EvaluatedFunctionSourceValue.objects.create(
+                                evaluated_function=evaluated_function,
+                                content_type=ContentType.objects.get_for_model(source_value_obj.__class__),
+                                object_id=source_value_obj.id,
+                            )
+                    except Exception as e:
+                        # Source value tracking is optional - don't fail the main function evaluation
+                        print(f"Debug: Could not create source value link for '{source_value}': {e}")
+
+            # Cache the evaluated function
+            self.evaluated_functions_cache[cache_key] = evaluated_function
+
+            # print(f"Tracked value computation: {function_name} with {len(source_values)} source values = {computed_value}")
+            return evaluated_function
+
+        except Exception as e:
+            print(f"Error tracking value computation: {e}")
+            return None
+
+    def _ensure_derived_row_context(self, derived_obj, function_name):
+        """Ensure a derived row context exists for the given derived object"""
+        if not self.lineage_enabled or not self.trail:
+            return None
+
+        try:
+            # Check if we already have a context for this specific object
+            obj_id = id(derived_obj)
+            if obj_id in self.object_contexts:
+                return self.object_contexts[obj_id]
+
+            # Get the class name to determine table name
+            class_name = derived_obj.__class__.__name__
+
+            # Only create derived tables for *_Table classes
+            # Individual objects should be treated as rows within their parent table
+            if class_name.endswith("_Table"):
+                table_name = class_name.replace("_Table", "")
+            else:
+                # For individual objects, use their class name as the table name
+                # This ensures proper isolation between different object types
+                table_name = class_name
+
+            # Ensure we have an EvaluatedDerivedTable for this specific class
+            if table_name not in self.current_populated_tables:
+                # Check if a DerivedTable already exists for this name
+                existing_derived_tables = DerivedTable.objects.filter(name=table_name)
+                if existing_derived_tables.exists():
+                    derived_table = existing_derived_tables.first()
+                else:
+                    # Create a new DerivedTable
+                    derived_table = DerivedTable.objects.create(name=table_name)
+
+                if self.metadata_trail:
+                    # Check if reference already exists
+                    existing_refs = AortaTableReference.objects.filter(
+                        metadata_trail=self.metadata_trail, table_content_type="DerivedTable", table_id=derived_table.id
+                    )
+
+                    if not existing_refs.exists():
+                        AortaTableReference.objects.create(
+                            metadata_trail=self.metadata_trail,
+                            table_content_type="DerivedTable",
+                            table_id=derived_table.id,
+                        )
+
+                # Create EvaluatedDerivedTable
+                evaluated_table = EvaluatedDerivedTable.objects.create(trail=self.trail, table=derived_table)
+
+                self.current_populated_tables[table_name] = evaluated_table
+                print(f"Created EvaluatedDerivedTable for {table_name}")
+
+            # Get the EvaluatedDerivedTable
+            evaluated_table = self.current_populated_tables[table_name]
+
+            # Create a unique identifier for this derived row based on object identity
+            row_identifier = f"{class_name}_{id(derived_obj)}"
+
+            # Check if we already have a DerivedTableRow for this object
+            existing_rows = evaluated_table.derivedtablerow_set.filter(row_identifier=row_identifier)
+            if existing_rows.exists():
+                derived_row_id = existing_rows.first().id
+            else:
+                # Create a new DerivedTableRow
+                derived_row = DerivedTableRow.objects.create(
+                    populated_table=evaluated_table, row_identifier=row_identifier
+                )
+                derived_row_id = derived_row.id
+                print(f"Created DerivedTableRow {derived_row_id} for {function_name}")
+
+            # Store the context for this specific object
+            self.object_contexts[obj_id] = derived_row_id
+            return derived_row_id
+
+        except Exception as e:
+            print(f"Error ensuring derived row context: {e}")
+            return None
+
+    def _get_parent_table_name(self, class_name):
+        """Determine the parent table name for an individual object"""
+        # Handle special cases first
+        if class_name.endswith("_UnionItem"):
+            return class_name.replace("_UnionItem", "_UnionTable").replace("_Table", "")
+
+        # Direct match - if the class itself is a table
+        if class_name in self.current_populated_tables:
+            return class_name
+
+        # For objects with specific report prefixes (e.g., F_05_01_REF_FINREP_3_0_Other_loans)
+        if "_" in class_name and class_name.split("_")[0].startswith("F_"):
+            # Extract the report prefix (e.g., F_05_01_REF_FINREP_3_0)
+            parts = class_name.split("_")
+            report_prefix_parts = []
+            for i, part in enumerate(parts):
+                report_prefix_parts.append(part)
+                # Stop when we hit a part that looks like a class name (starts with uppercase after numbers)
+                if i > 0 and len(part) > 0 and part[0].isupper() and not part.isdigit():
+                    # Check if this forms a valid report table name
+                    report_table_name = "_".join(report_prefix_parts)
+                    if report_table_name in self.current_populated_tables:
+                        return report_table_name
+
+            # If no exact match, return the full class name as table name
+            # This ensures F_05_01_REF_FINREP_3_0_Other_loans gets its own table
+            return class_name
+
+        # For base objects (e.g., Other_loans), create their own table
+        # Don't try to match them to other tables - this was causing the mixing issue
+        return class_name
+
+    def get_derived_context_for_object(self, obj):
+        """Get the correct derived context for a specific object"""
+        obj_id = id(obj)
+        if obj_id in self.object_contexts:
+            return self.object_contexts[obj_id]
+        return None
+
+    def track_calculation_used_row(self, calculation_name, row):
+        """Track that a specific row was used in a calculation (passed filters)"""
+        print(f"🔍 track_calculation_used_row called: {calculation_name}, {type(row).__name__}")
+
+        if not self.lineage_enabled or not self.trail:
+            print(
+                f"❌ Lineage tracking disabled or no trail: lineage_enabled={self.lineage_enabled}, trail={self.trail}"
+            )
+            return
+
+        try:
+            # Determine the type of row
+            if isinstance(row, DatabaseRow):
+                content_type = ContentType.objects.get_for_model(DatabaseRow)
+            elif isinstance(row, DerivedTableRow):
+                content_type = ContentType.objects.get_for_model(DerivedTableRow)
+            # Check if this is a Django model instance (database record)
+            elif hasattr(row, "_meta") and hasattr(row._meta, "model"):
+                # This is a Django model instance - we need to create/find the appropriate DatabaseRow
+                model_name = type(row).__name__
+
+                # Ensure we have a database table for this model
+                # Check if we already have the wrong type of table stored
+                existing_table = self.current_populated_tables.get(model_name)
+                if not existing_table or not isinstance(existing_table, PopulatedDataBaseTable):
+                    # Create database table
+                    db_table = DatabaseTable.objects.create(name=model_name)
+
+                    # Add to metadata trail
+                    if self.metadata_trail:
+                        AortaTableReference.objects.create(
+                            metadata_trail=self.metadata_trail, table_content_type="DatabaseTable", table_id=db_table.id
+                        )
+
+                    # Create PopulatedDataBaseTable
+                    populated_table = PopulatedDataBaseTable.objects.create(trail=self.trail, table=db_table)
+
+                    self.current_populated_tables[model_name] = populated_table
+                    print(f"Created database table for Django model: {model_name}")
+                else:
+                    populated_table = existing_table
+
+                # Get the populated database table
+                populated_table = self.current_populated_tables[model_name]
+
+                # Create unique identifier for this model instance
+                if hasattr(row, "pk") and row.pk:
+                    object_identifier = f"{model_name}_{row.pk}"
+                else:
+                    object_identifier = f"{model_name}_{id(row)}"
+
+                # Check if we already have a database row for this model instance
+                existing_rows = populated_table.databaserow_set.filter(row_identifier=object_identifier)
+
+                if existing_rows.exists():
+                    db_row = existing_rows.first()
+                else:
+                    # Create new database row for this model instance
+                    db_row = DatabaseRow.objects.create(
+                        populated_table=populated_table, row_identifier=object_identifier
+                    )
+
+                    # Create column values for the model fields
+                    for field in row._meta.fields:
+                        if hasattr(row, field.name):
+                            field_value = getattr(row, field.name)
+                            if field_value is not None:
+                                self._track_column_value_for_django_field(
+                                    db_row, field.name, field_value, populated_table.table
+                                )
+
+                    print(f"Created database row for Django model {model_name}")
+
+                    # DISABLED: Don't automatically track all Django model fields as used
+                    # Only track fields that are actually accessed during calculations (via wrapper)
+                    # self._track_django_model_fields_as_used(calculation_name, populated_table.table, row)
+
+                row = db_row
+
+                content_type = ContentType.objects.get_for_model(DatabaseRow)
+            else:
+                # For business objects, determine the appropriate tracking strategy
+                row_class_name = type(row).__name__
+
+                # Check if this is a business object that should be tracked as a derived table row
+                if hasattr(row, "__dict__"):
+                    # This is a business object - create/find appropriate derived table
+                    tracked_row = None
+
+                    # Look for or create an appropriate derived table for this object type
+                    table_name = row_class_name
+
+                    # Check if we already have a derived table for this object type
+                    if table_name not in self.current_populated_tables:
+                        # Create derived table for this object type
+                        derived_table = DerivedTable.objects.create(name=table_name)
+
+                        # Add to metadata trail
+                        if self.metadata_trail:
+                            AortaTableReference.objects.create(
+                                metadata_trail=self.metadata_trail,
+                                table_content_type="DerivedTable",
+                                table_id=derived_table.id,
+                            )
+
+                        # Create EvaluatedDerivedTable
+                        evaluated_table = EvaluatedDerivedTable.objects.create(trail=self.trail, table=derived_table)
+
+                        self.current_populated_tables[table_name] = evaluated_table
+                        print(f"Created derived table for business object type: {table_name}")
+
+                    # Get the evaluated derived table
+                    evaluated_table = self.current_populated_tables[table_name]
+
+                    # Create unique identifier for this specific object instance
+                    object_identifier = f"{row_class_name}_{id(row)}"
+
+                    # Check if we already have a derived row for this object
+                    existing_rows = evaluated_table.derivedtablerow_set.filter(row_identifier=object_identifier)
+
+                    if existing_rows.exists():
+                        tracked_row = existing_rows.first()
+                    else:
+                        # Create new derived table row for this object
+                        tracked_row = DerivedTableRow.objects.create(
+                            populated_table=evaluated_table, row_identifier=object_identifier
+                        )
+                        print(f"Created derived table row for {row_class_name}")
+
+                    if tracked_row:
+                        # ENHANCED: Track object relationships via DerivedRowSourceReference
+                        # Do this for both new AND existing rows to ensure relationships are captured
+                        self._track_object_relationships(tracked_row, row)
+
+                        # ENHANCED: Also track transitively referenced objects as used
+                        # This ensures that objects referenced via unionOfLayers.base etc. are also marked as used
+                        self._track_transitive_used_objects(calculation_name, row)
+
+                        row = tracked_row
+                        content_type = ContentType.objects.get_for_model(DerivedTableRow)
+                    else:
+                        print(f"Failed to create derived table row for {row_class_name}")
+                        return
+                else:
+                    print(f"Cannot track row of type {type(row)} - not a trackable object")
+                    return
+
+            # Check if this row is already tracked for this calculation
+            existing = CalculationUsedRow.objects.filter(
+                trail=self.trail, calculation_name=calculation_name, content_type=content_type, object_id=row.id
+            ).exists()
+
+            if not existing:
+                used_row = CalculationUsedRow.objects.create(
+                    trail=self.trail, calculation_name=calculation_name, content_type=content_type, object_id=row.id
+                )
+                print(f"✅ Created CalculationUsedRow: {calculation_name} -> {type(row).__name__} (id: {row.id})")
+            else:
+                print(f"⚠️ CalculationUsedRow already exists for {calculation_name} -> {type(row).__name__}")
+
+        except Exception as e:
+            print(f"❌ Error tracking calculation used row: {e}")
+            import traceback
+
+            traceback.print_exc()
+
+    def _track_django_model_fields_as_used(self, calculation_name, database_table, django_model_instance):
+        """Track all fields of a Django model as used fields when the model instance is tracked as a used row"""
+        try:
+            # Get all database fields for this table
+            database_fields = database_table.database_fields.all()
+
+            # Track each database field as a used field
+            for db_field in database_fields:
+                # Check if the Django model actually has this field
+                if hasattr(django_model_instance, db_field.name):
+                    try:
+                        # Track this field as used
+                        self.track_calculation_used_field(calculation_name, db_field.name, django_model_instance)
+                        print(f"  Tracked Django model field as used: {database_table.name}.{db_field.name}")
+                    except Exception as e:
+                        print(f"  Failed to track Django model field {db_field.name}: {e}")
+        except Exception as e:
+            print(f"Error tracking Django model fields as used: {e}")
+
+    def track_calculation_used_field(self, calculation_name, field_name, row=None, function_obj=None):
+        """Track that a specific field was accessed during a calculation"""
+        if not self.lineage_enabled or not self.trail:
+            return
+
+        try:
+            # Find the field object
+            field = None
+            content_type = None
+
+            # If function_obj is provided, use it directly to avoid ID mismatch issues
+            if function_obj:
+                field = function_obj
+                content_type = ContentType.objects.get_for_model(Function)
+                print(f"🔍 Using provided function object: {field.name} (ID: {field.id})")
+            else:
+                # First try to find as DatabaseField
+                database_fields = DatabaseField.objects.filter(name=field_name)
+                if database_fields.exists():
+                    field = database_fields.first()
+                    content_type = ContentType.objects.get_for_model(DatabaseField)
+                else:
+                    # Try to find as Function
+                    functions = Function.objects.filter(name=field_name)
+                    if functions.exists():
+                        field = functions.first()
+                        content_type = ContentType.objects.get_for_model(Function)
+
+            if not field:
+                # Try to find with more context if field_name includes table reference
+                if "." in field_name:
+                    parts = field_name.split(".")
+                    actual_field_name = parts[-1]
+                    database_fields = DatabaseField.objects.filter(name=actual_field_name)
+                    if database_fields.exists():
+                        field = database_fields.first()
+                        content_type = ContentType.objects.get_for_model(DatabaseField)
+                    else:
+                        functions = Function.objects.filter(name=actual_field_name)
+                        if functions.exists():
+                            field = functions.first()
+                            content_type = ContentType.objects.get_for_model(Function)
+
+            if not field:
+                print(f"Cannot find field {field_name} to track")
+                return
+
+            # Prepare row tracking if provided
+            row_content_type = None
+            row_object_id = None
+            if row:
+                if isinstance(row, DatabaseRow):
+                    row_content_type = ContentType.objects.get_for_model(DatabaseRow)
+                    row_object_id = row.id
+                elif isinstance(row, DerivedTableRow):
+                    row_content_type = ContentType.objects.get_for_model(DerivedTableRow)
+                    row_object_id = row.id
+
+            # Check if this field is already tracked for this calculation
+            query = CalculationUsedField.objects.filter(
+                trail=self.trail, calculation_name=calculation_name, content_type=content_type, object_id=field.id
+            )
+
+            if row_content_type and row_object_id:
+                query = query.filter(row_content_type=row_content_type, row_object_id=row_object_id)
+
+            if not query.exists():
+                CalculationUsedField.objects.create(
+                    trail=self.trail,
+                    calculation_name=calculation_name,
+                    content_type=content_type,
+                    object_id=field.id,
+                    row_content_type=row_content_type,
+                    row_object_id=row_object_id,
+                )
+                # print(f"Tracked used field for {calculation_name}: {field_name}")
+
+        except Exception as e:
+            print(f"Error tracking calculation used field: {e}")
+
+    def get_calculation_used_rows(self, calculation_name):
+        """Get all rows that were used in a specific calculation"""
+        if not self.trail:
+            return []
+
+        used_rows = CalculationUsedRow.objects.filter(trail=self.trail, calculation_name=calculation_name)
+
+        return [ur.used_row for ur in used_rows]
+
+    def get_calculation_used_fields(self, calculation_name):
+        """Get all fields that were accessed during a specific calculation"""
+        if not self.trail:
+            return []
+
+        used_fields = CalculationUsedField.objects.filter(trail=self.trail, calculation_name=calculation_name)
+
+        return [uf.used_field for uf in used_fields]
+
+    def _track_object_relationships(self, tracked_row, business_object):
+        """Track object relationships via DerivedRowSourceReference based on common relationship attributes"""
+        try:
+            if not isinstance(tracked_row, DerivedTableRow):
+                return
+
+            print(f"Tracking object relationships for {type(business_object).__name__}")
+
+            # Common relationship attributes to check
+            relationship_attrs = [
+                "unionOfLayers",
+                "base",
+                "INSTRMNT",
+                "INSTRMNT_RL",
+                "PRTY",
+                "INSTRMNT_ENTTY_RL_ASSGNMNT",
+                "source_row",
+                "parent_row",
+            ]
+
+            for attr_name in relationship_attrs:
+                if hasattr(business_object, attr_name):
+                    source_obj = getattr(business_object, attr_name)
+                    if source_obj:
+                        # Try to find the corresponding DerivedTableRow for the source object
+                        source_row = self._find_derived_row_for_object(source_obj)
+                        if source_row:
+                            # Create DerivedRowSourceReference
+                            existing_ref = DerivedRowSourceReference.objects.filter(
+                                derived_row_id=tracked_row.id,
+                                content_type=ContentType.objects.get_for_model(DerivedTableRow),
+                                object_id=source_row.id,
+                            ).exists()
+
+                            if not existing_ref:
+                                DerivedRowSourceReference.objects.create(
+                                    derived_row=tracked_row,
+                                    content_type=ContentType.objects.get_for_model(DerivedTableRow),
+                                    object_id=source_row.id,
+                                )
+                                print(
+                                    f"Created relationship: {tracked_row.populated_table.table.name} row {tracked_row.id} <- {source_row.populated_table.table.name} row {source_row.id} via {attr_name}"
+                                )
+
+            # Also check class name-based relationships (e.g. F_05_01_REF_FINREP_3_0_UnionItem -> Other_loans)
+            obj_class_name = type(business_object).__name__
+            if "_" in obj_class_name:
+                parts = obj_class_name.split("_")
+                # Look for potential source class names in the parts
+                for i in range(len(parts)):
+                    potential_source_class = "_".join(parts[i:])
+                    if potential_source_class != obj_class_name:
+                        # Try to find objects of this source class
+                        source_obj = self._find_object_by_class_suffix(business_object, potential_source_class)
+                        if source_obj:
+                            source_row = self._find_derived_row_for_object(source_obj)
+                            if source_row:
+                                existing_ref = DerivedRowSourceReference.objects.filter(
+                                    derived_row_id=tracked_row.id,
+                                    content_type=ContentType.objects.get_for_model(DerivedTableRow),
+                                    object_id=source_row.id,
+                                ).exists()
+
+                                if not existing_ref:
+                                    DerivedRowSourceReference.objects.create(
+                                        derived_row=tracked_row,
+                                        content_type=ContentType.objects.get_for_model(DerivedTableRow),
+                                        object_id=source_row.id,
+                                    )
+                                    print(
+                                        f"Created class-based relationship: {tracked_row.populated_table.table.name} <- {source_row.populated_table.table.name} via class hierarchy"
+                                    )
+
+        except Exception as e:
+            print(f"Error tracking object relationships: {e}")
+
+    def _find_derived_row_for_object(self, obj):
+        """Find the DerivedTableRow that corresponds to a business object"""
+        try:
+            obj_class_name = type(obj).__name__
+            object_identifier = f"{obj_class_name}_{id(obj)}"
+
+            # Look in current populated tables for matching row
+            for table_name, populated_table in self.current_populated_tables.items():
+                if hasattr(populated_table, "derivedtablerow_set"):
+                    matching_rows = populated_table.derivedtablerow_set.filter(row_identifier=object_identifier)
+                    if matching_rows.exists():
+                        return matching_rows.first()
+
+            return None
+        except Exception as e:
+            print(f"Error finding derived row for object: {e}")
+            return None
+
+    def _find_object_by_class_suffix(self, business_object, suffix):
+        """Find a related object by class name suffix"""
+        try:
+            # This is a heuristic approach - look for attributes that might contain objects of the target class
+            for attr_name in dir(business_object):
+                if not attr_name.startswith("_") and not callable(getattr(business_object, attr_name, None)):
+                    attr_value = getattr(business_object, attr_name)
+                    if attr_value and hasattr(attr_value, "__class__"):
+                        if type(attr_value).__name__.endswith(suffix):
+                            return attr_value
+            return None
+        except Exception as e:
+            print(f"Error finding object by class suffix: {e}")
+            return None
+
+    def _track_transitive_used_objects(self, calculation_name, business_object):
+        """Track objects that are transitively referenced by the current object as also being used"""
+        try:
+            print(f"Tracking transitive used objects for {type(business_object).__name__}")
+
+            # Common relationship attributes that point to other business objects
+            relationship_attrs = [
+                "unionOfLayers",
+                "base",
+                "INSTRMNT",
+                "INSTRMNT_RL",
+                "PRTY",
+                "INSTRMNT_ENTTY_RL_ASSGNMNT",
+                "source_row",
+                "parent_row",
+            ]
+
+            for attr_name in relationship_attrs:
+                if hasattr(business_object, attr_name):
+                    referenced_obj = getattr(business_object, attr_name)
+                    if referenced_obj and hasattr(referenced_obj, "__class__"):
+                        # This object is transitively used - track it as used as well
+                        print(
+                            f"Found transitive reference: {type(business_object).__name__}.{attr_name} -> {type(referenced_obj).__name__}"
+                        )
+
+                        # Recursively track this object as used (this will create its DerivedTableRow if needed)
+                        self.track_calculation_used_row(calculation_name, referenced_obj)
+
+        except Exception as e:
+            print(f"Error tracking transitive used objects: {e}")
+
+    def _track_column_value_for_django_field(self, db_row, field_name, field_value, table):
+        """Helper method to track column values for Django model fields"""
+        try:
+            # Find or create the DatabaseField
+            fields = table.database_fields.filter(name=field_name)
+
+            if not fields.exists():
+                field = DatabaseField.objects.create(name=field_name, table=table)
+            else:
+                field = fields.first()
+
+            # Create DatabaseColumnValue
+            numeric_value = None
+            string_value = None
+
+            if field_value is not None:
+                try:
+                    numeric_value = float(field_value)
+                except (ValueError, TypeError):
+                    string_value = str(field_value)
+
+            DatabaseColumnValue.objects.create(value=numeric_value, string_value=string_value, column=field, row=db_row)
+
+        except Exception as e:
+            print(f"Error tracking Django field {field_name}: {e}")
+
+    def _find_source_value_object(self, source_value):
+        """Find the DatabaseColumnValue object for a given source value"""
+        try:
+            # Look for DatabaseColumnValue with matching value
+            source_row_id = (
+                self.current_rows.get("source") if hasattr(self, "current_rows") and self.current_rows else None
+            )
+            if source_row_id:
+                source_row = DatabaseRow.objects.get(id=source_row_id)
+                column_values = source_row.column_values.filter(value=str(source_value))
+                if column_values.exists():
+                    return column_values.first()
+
+            # Fallback: look across all current rows for derived table rows (most common case for polymorphic functions)
+            if hasattr(self, "current_populated_tables"):
+                for table_name, populated_table in self.current_populated_tables.items():
+                    # Check DerivedTableRow objects instead of DatabaseRow for business objects
+                    if hasattr(populated_table, "derivedtablerow_set"):
+                        for row in populated_table.derivedtablerow_set.all():
+                            # Look for matching values in EvaluatedFunction records
+                            evaluated_funcs = row.evaluatedfunction_set.filter(string_value=str(source_value))
+                            if evaluated_funcs.exists():
+                                # Return a dummy object that represents the source
+                                return evaluated_funcs.first()
+
+                    # Also check traditional DatabaseRow objects
+                    if hasattr(populated_table, "databaserow_set"):
+                        for row in populated_table.databaserow_set.all():
+                            column_values = row.column_values.filter(value=str(source_value))
+                            if column_values.exists():
+                                return column_values.first()
+
+        except Exception as e:
+            # Make this a debug message instead of error to reduce noise
+            print(f"Debug: Could not find source value object for '{source_value}': {e}")
+
+        return None
+
+    def track_data_processing(self, table_name, data_items, django_model_objects=None):
+        """Track processing of data items in a table"""
+        if not self.lineage_enabled or not self.trail or not self.metadata_trail:
+            return
+
+        # Also track that these rows and tables are being used in calculations
+        current_calculation = getattr(self, "current_calculation", None)
+        print(
+            f"🔗 track_data_processing: table={table_name}, items={len(data_items)}, django_objects={len(django_model_objects) if django_model_objects else 0}, current_calculation={current_calculation}"
+        )
+        # CRITICAL FIX: Do NOT auto-track all processed items here.
+        # Rows should only be marked as used when they pass a cell's calc_referenced_items filter
+        # or are explicitly tracked by targeted logic (e.g., wrapper or explicit calls).
+        # This prevents unrelated rows (e.g., Advances_that_are_not_loans) from appearing as used.
+
+        try:
+            # Ensure we have a populated table for this table name
+            if table_name not in self.current_populated_tables:
+                # Determine if this is a Django model or a derived table
+                is_django_model = self._is_django_model(table_name)
+
+                # Check for existing PopulatedDataBaseTable/EvaluatedDerivedTable first
+                populated_table = None
+                temp_table = None
+                table_exists = False
+
+                if is_django_model:
+                    # Look for existing DatabaseTable with same name in this trail
+                    existing_populated = (
+                        PopulatedDataBaseTable.objects.filter(trail=self.trail, table__name=table_name)
+                        .select_related("table")
+                        .first()
+                    )
+                    if existing_populated:
+                        temp_table = existing_populated.table
+                        populated_table = existing_populated
+                        table_exists = True
+                        print(f"Found existing DatabaseTable for: {table_name}")
+                else:
+                    # Look for existing DerivedTable with same name in this trail
+                    existing_evaluated = (
+                        EvaluatedDerivedTable.objects.filter(trail=self.trail, table__name=table_name)
+                        .select_related("table")
+                        .first()
+                    )
+                    if existing_evaluated:
+                        temp_table = existing_evaluated.table
+                        populated_table = existing_evaluated
+                        table_exists = True
+                        print(f"Found existing DerivedTable for: {table_name}")
+
+                # Create new table only if not found
+                if not temp_table:
+                    if is_django_model:
+                        temp_table = DatabaseTable.objects.create(name=table_name)
+                        table_type = "DatabaseTable"
+                    else:
+                        temp_table = DerivedTable.objects.create(name=table_name)
+                        table_type = "DerivedTable"
+
+                    if self.metadata_trail:
+                        AortaTableReference.objects.create(
+                            metadata_trail=self.metadata_trail, table_content_type=table_type, table_id=temp_table.id
+                        )
+                    else:
+                        print(f"Warning: No metadata_trail available for tracking table {table_name}")
+
+                # Create populated table only if not found
+                if self.trail and not populated_table:
+                    if is_django_model:
+                        populated_table = PopulatedDataBaseTable.objects.create(trail=self.trail, table=temp_table)
+                    else:
+                        populated_table = EvaluatedDerivedTable.objects.create(trail=self.trail, table=temp_table)
+                elif not self.trail:
+                    print(f"Warning: No trail available for PopulatedDataBaseTable {table_name}")
+                    return
+                self.current_populated_tables[table_name] = populated_table
+
+            # Track each data item as a row
+            for i, item in enumerate(data_items):
+                row_id = f"{table_name}_row_{i}"
+
+                # Extract data from the item
+                if isinstance(item, dict):
+                    # Item is already a dictionary
+                    row_data = item
+                elif hasattr(item, "__dict__"):
+                    row_data = {}
+                    for attr in dir(item):
+                        if not attr.startswith("_") and not callable(getattr(item, attr)):
+                            try:
+                                value = getattr(item, attr)
+                                # Only include non-callable attributes to avoid unwanted method evaluations
+                                if not callable(value):
+                                    row_data[attr] = value
+                            except:
+                                pass
+                else:
+                    row_data = {"value": str(item)}
+
+                # Track the row
+                self.track_row_processing(table_name, row_data, row_id)
+
+            # print(f"Tracked data processing for {table_name}: {len(data_items)} items")
+
+        except Exception as e:
+            print(f"Error tracking data processing: {e}")
+
+    def get_lineage_trail(self):
+        """Get the current lineage trail"""
+        return self.trail
+
+    def export_lineage_graph(self, trail_id=None):
+        """Export lineage as a graph structure for visualization"""
+        if trail_id:
+            trail = Trail.objects.get(id=trail_id)
+        else:
+            trail = self.trail
+
+        if not trail:
+            return None
+
+        # Build graph structure
+        graph = {
+            "nodes": [],
+            "edges": [],
+            "trail": {"id": trail.id, "name": trail.name, "created_at": trail.created_at.isoformat()},
+        }
+
+        # Add table nodes
+        for table_ref in trail.metadata_trail.table_references.all():
+            if table_ref.table_content_type == "DatabaseTable":
+                table = DatabaseTable.objects.get(id=table_ref.table_id)
+                graph["nodes"].append({"id": f"table_{table.id}", "type": "DatabaseTable", "name": table.name})
+            elif table_ref.table_content_type == "DerivedTable":
+                table = DerivedTable.objects.get(id=table_ref.table_id)
+                graph["nodes"].append({"id": f"table_{table.id}", "type": "DerivedTable", "name": table.name})
+
+        # TODO: Add edges based on function references and data flow
+
+        return graph
 
 
 # Original Orchestration class from develop branch
 class OrchestrationOriginal:
-	# Class variable to track initialized objects
-	_initialized_objects = set()
-
-	def init(self,theObject):
-		# Check if this object has already been initialized
-		object_id = id(theObject)
-		if object_id in OrchestrationOriginal._initialized_objects:
-			print(f"Object of type {theObject.__class__.__name__} already initialized, skipping.")
-			# Even if we're skipping full initialization, we still need to ensure references are set
-			self._ensure_references_set(theObject)
-			return
-
-		# Mark this object as initialized
-		OrchestrationOriginal._initialized_objects.add(object_id)
-
-		# Set up references for the object
-		self._ensure_references_set(theObject)
-
-	def _ensure_references_set(self, theObject):
-		"""
-		Ensure that all table references are properly set for the object.
-		This is called both during full initialization and when initialization is skipped.
-		"""
-		references = [method for method in dir(theObject.__class__) if not callable(
-		getattr(theObject.__class__, method)) and not method.startswith('__')]
-		for eReference in references:
-			if eReference.endswith("Table"):
-				# Only set the reference if it's currently None
-				if getattr(theObject, eReference) is None:
-					from django.apps import apps
-					table_name = eReference.split('_Table')[0]
-					relevant_model = None
-					try:
-						relevant_model = apps.get_model('pybirdai',table_name)
-					except LookupError:
-						print("LookupError: " + table_name)
-
-					if relevant_model:
-						print("relevant_model: " + str(relevant_model))
-						newObject = relevant_model.objects.all()
-						print("newObject: " + str(newObject))
-						if newObject:
-							setattr(theObject,eReference,newObject)
-							CSVConverter.persist_object_as_csv(newObject,True);
-
-					else:
-						newObject = OrchestrationOriginal.createObjectFromReferenceType(eReference);
-
-						operations = [method for method in dir(newObject.__class__) if callable(
-							getattr(newObject.__class__, method)) and not method.startswith('__')]
-
-						for operation in operations:
-							if operation == "init":
-								try:
-									getattr(newObject, operation)()
-								except:
-									print (" could not call function called " + operation)
-
-						setattr(theObject,eReference,newObject)
-
-	@classmethod
-	def reset_initialization(cls):
-		"""
-		Reset the initialization tracking.
-		This can be useful for testing or when re-initialization is required.
-		"""
-		cls._initialized_objects.clear()
-		print("Initialization tracking has been reset.")
-
-	@classmethod
-	def is_initialized(cls, obj):
-		"""
-		Check if an object has been initialized.
-
-		Args:
-			obj: The object to check
-
-		Returns:
-			bool: True if the object has been initialized, False otherwise
-		"""
-		return id(obj) in cls._initialized_objects
-
-	@staticmethod
-	def createObjectFromReferenceType(eReference):
-		try:
-			cls = getattr(importlib.import_module('pybirdai.process_steps.filter_code.output_tables'), eReference)
-			new_object = cls()
-			return new_object;
-		except:
-			print("Error: " + eReference)
+    # Class variable to track initialized objects
+    _initialized_objects = set()
+
+    def init(self, theObject):
+        # Check if this object has already been initialized
+        object_id = id(theObject)
+        if object_id in OrchestrationOriginal._initialized_objects:
+            print(f"Object of type {theObject.__class__.__name__} already initialized, skipping.")
+            # Even if we're skipping full initialization, we still need to ensure references are set
+            self._ensure_references_set(theObject)
+            return
+
+        # Mark this object as initialized
+        OrchestrationOriginal._initialized_objects.add(object_id)
+
+        # Set up references for the object
+        self._ensure_references_set(theObject)
+
+    def _ensure_references_set(self, theObject):
+        """
+        Ensure that all table references are properly set for the object.
+        This is called both during full initialization and when initialization is skipped.
+        """
+        references = [
+            method
+            for method in dir(theObject.__class__)
+            if not callable(getattr(theObject.__class__, method)) and not method.startswith("__")
+        ]
+        for eReference in references:
+            if eReference.endswith("Table"):
+                # Only set the reference if it's currently None
+                if getattr(theObject, eReference) is None:
+                    from django.apps import apps
+
+                    table_name = eReference.split("_Table")[0]
+                    relevant_model = None
+                    try:
+                        relevant_model = apps.get_model("pybirdai", table_name)
+                    except LookupError:
+                        print("LookupError: " + table_name)
+
+                    if relevant_model:
+                        print("relevant_model: " + str(relevant_model))
+                        newObject = relevant_model.objects.all()
+                        print("newObject: " + str(newObject))
+                        if newObject:
+                            setattr(theObject, eReference, newObject)
+                            CSVConverter.persist_object_as_csv(newObject, True)
+
+                    else:
+                        newObject = OrchestrationOriginal.createObjectFromReferenceType(eReference)
+
+                        operations = [
+                            method
+                            for method in dir(newObject.__class__)
+                            if callable(getattr(newObject.__class__, method)) and not method.startswith("__")
+                        ]
+
+                        for operation in operations:
+                            if operation == "init":
+                                try:
+                                    getattr(newObject, operation)()
+                                except:
+                                    print(" could not call function called " + operation)
+
+                        setattr(theObject, eReference, newObject)
+
+    @classmethod
+    def reset_initialization(cls):
+        """
+        Reset the initialization tracking.
+        This can be useful for testing or when re-initialization is required.
+        """
+        cls._initialized_objects.clear()
+        print("Initialization tracking has been reset.")
+
+    @classmethod
+    def is_initialized(cls, obj):
+        """
+        Check if an object has been initialized.
+
+        Args:
+                obj: The object to check
+
+        Returns:
+                bool: True if the object has been initialized, False otherwise
+        """
+        return id(obj) in cls._initialized_objects
+
+    @staticmethod
+    def createObjectFromReferenceType(eReference):
+        try:
+            cls = getattr(importlib.import_module("pybirdai.process_steps.filter_code.output_tables"), eReference)
+            new_object = cls()
+            return new_object
+        except:
+            print("Error: " + eReference)
 
 
 # Factory function to create the appropriate Orchestration instance
 def create_orchestration():
-	"""
-	Factory function that returns the appropriate Orchestration instance
-	based on the context configuration.
-	"""
-	from pybirdai.context.context import Context
-
-	if hasattr(Context, 'enable_lineage_tracking') and Context.enable_lineage_tracking:
-		print("Using lineage-enhanced orchestrator")
-		return OrchestrationWithLineage()
-	else:
-		print("Using original orchestrator")
-		return OrchestrationOriginal()
+    """
+    Factory function that returns the appropriate Orchestration instance
+    based on the context configuration.
+    """
+    from pybirdai.context.context import Context
+
+    if hasattr(Context, "enable_lineage_tracking") and Context.enable_lineage_tracking:
+        print("Using lineage-enhanced orchestrator")
+        return OrchestrationWithLineage()
+    else:
+        print("Using original orchestrator")
+        return OrchestrationOriginal()
 
 
 # For backwards compatibility - Orchestration points to the factory function result
 def Orchestration():
-	"""
-	Factory function that returns the appropriate Orchestration instance.
-	This maintains backwards compatibility while allowing version selection.
-	"""
-	return create_orchestration()
+    """
+    Factory function that returns the appropriate Orchestration instance.
+    This maintains backwards compatibility while allowing version selection.
+    """
+    return create_orchestration()
would reformat /Volumes/External Memory/benjaminarfa/Desktop/programming/github.com/benjamin-arfa/efbt/efbt-develop/birds_nest/pybirdai/process_steps/pybird/orchestration.py

Oh no! 💥 💔 💥
63 files would be reformatted, 16 files would be left unchanged.


=== PYLINT CHECKS ===
Running pylint on: pybirdai/utils/
-----------------------------------
************* Module pybirdai.utils.logger_factory
pybirdai/utils/logger_factory.py:15:0: W0611: Unused Path imported from pathlib (unused-import)
************* Module pybirdai.utils.mapping_library
pybirdai/utils/mapping_library.py:40:4: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/mapping_library.py:41:18: W0612: Unused variable 'created' (unused-variable)
pybirdai/utils/mapping_library.py:85:23: W0612: Unused variable 'source_vars' (unused-variable)
pybirdai/utils/mapping_library.py:208:4: W0612: Unused variable 'mapping_def' (unused-variable)
pybirdai/utils/mapping_library.py:225:4: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/mapping_library.py:244:4: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/mapping_library.py:274:4: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/mapping_library.py:278:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/mapping_library.py:293:4: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/mapping_library.py:14:0: W0611: Unused import json (unused-import)
pybirdai/utils/mapping_library.py:16:0: W0611: Unused Optional imported from typing (unused-import)
pybirdai/utils/mapping_library.py:17:0: W0611: Unused MAPPING_TO_CUBE imported from models.bird_meta_data_model (unused-import)
************* Module pybirdai.utils.bird_ecb_website_fetcher
pybirdai/utils/bird_ecb_website_fetcher.py:144:19: W3101: Missing timeout argument for method 'requests.get' can cause your program to hang indefinitely (missing-timeout)
pybirdai/utils/bird_ecb_website_fetcher.py:159:4: R0914: Too many local variables (17/15) (too-many-locals)
pybirdai/utils/bird_ecb_website_fetcher.py:201:19: W3101: Missing timeout argument for method 'requests.get' can cause your program to hang indefinitely (missing-timeout)
pybirdai/utils/bird_ecb_website_fetcher.py:22:0: C0411: standard import "zipfile" should be placed before third party import "requests" (wrong-import-order)
pybirdai/utils/bird_ecb_website_fetcher.py:23:0: C0411: standard import "os" should be placed before third party import "requests" (wrong-import-order)
pybirdai/utils/bird_ecb_website_fetcher.py:24:0: C0411: standard import "io" should be placed before third party import "requests" (wrong-import-order)
pybirdai/utils/bird_ecb_website_fetcher.py:24:0: W0611: Unused import io (unused-import)
************* Module pybirdai.utils.secure_error_handling
pybirdai/utils/secure_error_handling.py:26:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/secure_error_handling.py:40:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/secure_error_handling.py:42:77: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/secure_error_handling.py:58:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/secure_error_handling.py:72:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/secure_error_handling.py:78:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/secure_error_handling.py:95:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/secure_error_handling.py:112:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/secure_error_handling.py:125:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/secure_error_handling.py:127:71: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/secure_error_handling.py:139:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/secure_error_handling.py:143:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/secure_error_handling.py:145:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/secure_error_handling.py:160:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/secure_error_handling.py:167:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/secure_error_handling.py:174:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/secure_error_handling.py:190:76: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/secure_error_handling.py:192:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/secure_error_handling.py:199:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/secure_error_handling.py:214:75: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/secure_error_handling.py:216:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/secure_error_handling.py:228:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/secure_error_handling.py:249:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/secure_error_handling.py:252:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/secure_error_handling.py:259:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/secure_error_handling.py:264:0: C0304: Final newline missing (missing-final-newline)
pybirdai/utils/secure_error_handling.py:190:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/secure_error_handling.py:214:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/secure_error_handling.py:16:0: W0611: Unused HttpResponseBadRequest imported from django.http (unused-import)
************* Module pybirdai.utils.model_meta_data
pybirdai/utils/model_meta_data.py:17:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/model_meta_data.py:21:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/model_meta_data.py:22:8: C0415: Import outside toplevel (django.apps.apps) (import-outside-toplevel)
pybirdai/utils/model_meta_data.py:27:21: W0212: Access to a protected member _meta of a client class (protected-access)
pybirdai/utils/model_meta_data.py:29:18: W0212: Access to a protected member _meta of a client class (protected-access)
pybirdai/utils/model_meta_data.py:30:25: W0212: Access to a protected member _meta of a client class (protected-access)
pybirdai/utils/model_meta_data.py:40:16: W0702: No exception type(s) specified (bare-except)
************* Module pybirdai.utils.utils_views
pybirdai/utils/utils_views.py:31:16: R1735: Consider using '{}' instead of a call to 'dict'. (use-dict-literal)
pybirdai/utils/utils_views.py:39:11: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/utils/utils_views.py:40:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/utils_views.py:49:11: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/utils/utils_views.py:47:13: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/utils/utils_views.py:50:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
************* Module pybirdai.utils.export_db
pybirdai/utils/export_db.py:49:0: R0914: Too many local variables (41/15) (too-many-locals)
pybirdai/utils/export_db.py:50:4: C0415: Import outside toplevel (re) (import-outside-toplevel)
pybirdai/utils/export_db.py:51:4: C0415: Import outside toplevel (pybirdai.models.bird_meta_data_model) (import-outside-toplevel)
pybirdai/utils/export_db.py:52:4: C0415: Import outside toplevel (pybirdai.models.bird_data_model) (import-outside-toplevel)
pybirdai/utils/export_db.py:53:4: C0415: Import outside toplevel (django.db.transaction, django.db.connection) (import-outside-toplevel)
pybirdai/utils/export_db.py:54:4: C0415: Import outside toplevel (django.http.HttpResponse, django.http.JsonResponse, django.http.HttpResponseBadRequest) (import-outside-toplevel)
pybirdai/utils/export_db.py:67:34: W0212: Access to a protected member _meta of a client class (protected-access)
pybirdai/utils/export_db.py:68:22: W0212: Access to a protected member _meta of a client class (protected-access)
pybirdai/utils/export_db.py:87:69: W0212: Access to a protected member _meta of a client class (protected-access)
pybirdai/utils/export_db.py:90:25: W0212: Access to a protected member _meta of a client class (protected-access)
pybirdai/utils/export_db.py:101:20: R1724: Unnecessary "elif" after "continue", remove the leading "el" from "elif" (no-else-continue)
pybirdai/utils/export_db.py:49:0: R0912: Too many branches (23/12) (too-many-branches)
pybirdai/utils/export_db.py:49:0: R0915: Too many statements (77/50) (too-many-statements)
pybirdai/utils/export_db.py:65:8: W0612: Unused variable 'name' (unused-variable)
pybirdai/utils/export_db.py:52:4: W0611: Unused bird_data_model imported from pybirdai.models (unused-import)
pybirdai/utils/export_db.py:53:4: W0611: Unused transaction imported from django.db (unused-import)
pybirdai/utils/export_db.py:54:4: W0611: Unused HttpResponse imported from django.http (unused-import)
pybirdai/utils/export_db.py:54:4: W0611: Unused JsonResponse imported from django.http (unused-import)
pybirdai/utils/export_db.py:54:4: W0611: Unused HttpResponseBadRequest imported from django.http (unused-import)
pybirdai/utils/export_db.py:17:0: C0411: standard import "sys" should be placed before third party imports "django", "django.db.models", "django.conf.settings" (wrong-import-order)
pybirdai/utils/export_db.py:19:0: C0411: standard import "logging" should be placed before third party imports "django", "django.db.models", "django.conf.settings" (wrong-import-order)
pybirdai/utils/export_db.py:20:0: C0411: standard import "inspect" should be placed before third party imports "django", "django.db.models", "django.conf.settings" (wrong-import-order)
pybirdai/utils/export_db.py:21:0: C0411: standard import "zipfile" should be placed before third party imports "django", "django.db.models", "django.conf.settings" (wrong-import-order)
pybirdai/utils/export_db.py:22:0: C0411: standard import "traceback" should be placed before third party imports "django", "django.db.models", "django.conf.settings" (wrong-import-order)
pybirdai/utils/export_db.py:22:0: W0611: Unused import traceback (unused-import)
************* Module pybirdai.utils.licensing_code
pybirdai/utils/licensing_code.py:43:27: R1734: Consider using [] instead of list() (use-list-literal)
pybirdai/utils/licensing_code.py:49:61: R1732: Consider using 'with' for resource-allocating operations (consider-using-with)
pybirdai/utils/licensing_code.py:49:61: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/utils/licensing_code.py:53:9: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/utils/licensing_code.py:57:9: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/utils/licensing_code.py:76:27: R1734: Consider using [] instead of list() (use-list-literal)
pybirdai/utils/licensing_code.py:82:61: R1732: Consider using 'with' for resource-allocating operations (consider-using-with)
pybirdai/utils/licensing_code.py:82:61: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/utils/licensing_code.py:86:9: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/utils/licensing_code.py:90:9: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
************* Module pybirdai.utils.visualisation_service
pybirdai/utils/visualisation_service.py:50:32: W0621: Redefining name 'cube_id' from outer scope (line 438) (redefined-outer-name)
pybirdai/utils/visualisation_service.py:54:8: C0415: Import outside toplevel (pybirdai.models.bird_meta_data_model.CUBE_LINK) (import-outside-toplevel)
pybirdai/utils/visualisation_service.py:67:8: C0415: Import outside toplevel (pybirdai.models.bird_meta_data_model.CUBE_LINK) (import-outside-toplevel)
pybirdai/utils/visualisation_service.py:77:8: C0415: Import outside toplevel (pybirdai.models.bird_meta_data_model.CUBE_STRUCTURE_ITEM_LINK) (import-outside-toplevel)
pybirdai/utils/visualisation_service.py:91:8: C0415: Import outside toplevel (pybirdai.models.bird_meta_data_model.CUBE_STRUCTURE_ITEM_LINK, pybirdai.models.bird_meta_data_model.CUBE_STRUCTURE, pybirdai.models.bird_meta_data_model.CUBE_STRUCTURE_ITEM) (import-outside-toplevel)
pybirdai/utils/visualisation_service.py:91:8: W0611: Unused CUBE_STRUCTURE_ITEM_LINK imported from pybirdai.models.bird_meta_data_model (unused-import)
pybirdai/utils/visualisation_service.py:91:8: W0611: Unused CUBE_STRUCTURE imported from pybirdai.models.bird_meta_data_model (unused-import)
pybirdai/utils/visualisation_service.py:91:8: W0611: Unused CUBE_STRUCTURE_ITEM imported from pybirdai.models.bird_meta_data_model (unused-import)
pybirdai/utils/visualisation_service.py:112:8: C0415: Import outside toplevel (pybirdai.models.bird_meta_data_model.CUBE_STRUCTURE_ITEM_LINK, pybirdai.models.bird_meta_data_model.CUBE_STRUCTURE, pybirdai.models.bird_meta_data_model.CUBE_STRUCTURE_ITEM) (import-outside-toplevel)
pybirdai/utils/visualisation_service.py:114:8: W0105: String statement has no effect (pointless-string-statement)
pybirdai/utils/visualisation_service.py:112:8: W0611: Unused CUBE_STRUCTURE_ITEM_LINK imported from pybirdai.models.bird_meta_data_model (unused-import)
pybirdai/utils/visualisation_service.py:192:4: R0914: Too many local variables (29/15) (too-many-locals)
pybirdai/utils/visualisation_service.py:333:8: W0621: Redefining name 'html_content' from outer scope (line 445) (redefined-outer-name)
pybirdai/utils/visualisation_service.py:361:8: W0621: Redefining name 'e' from outer scope (line 447) (redefined-outer-name)
pybirdai/utils/visualisation_service.py:358:17: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/utils/visualisation_service.py:192:4: R0912: Too many branches (23/12) (too-many-branches)
pybirdai/utils/visualisation_service.py:192:4: R0915: Too many statements (78/50) (too-many-statements)
pybirdai/utils/visualisation_service.py:278:8: W0612: Unused variable 'source_cube_color' (unused-variable)
pybirdai/utils/visualisation_service.py:279:8: W0612: Unused variable 'target_cube_color' (unused-variable)
pybirdai/utils/visualisation_service.py:280:8: W0612: Unused variable 'source_item_color' (unused-variable)
pybirdai/utils/visualisation_service.py:281:8: W0612: Unused variable 'target_item_color' (unused-variable)
pybirdai/utils/visualisation_service.py:370:31: W0621: Redefining name 'cube_id' from outer scope (line 438) (redefined-outer-name)
pybirdai/utils/visualisation_service.py:370:40: W0621: Redefining name 'join_identifier' from outer scope (line 439) (redefined-outer-name)
pybirdai/utils/visualisation_service.py:18:0: C0411: standard import "sys" should be placed before third party imports "django", "django.db.models", "django.conf.settings" (wrong-import-order)
pybirdai/utils/visualisation_service.py:20:0: C0411: standard import "logging" should be placed before third party imports "django", "django.db.models", "django.conf.settings" (wrong-import-order)
************* Module pybirdai.utils.advanced_migration_generator
pybirdai/utils/advanced_migration_generator.py:30:0: R0902: Too many instance attributes (20/7) (too-many-instance-attributes)
pybirdai/utils/advanced_migration_generator.py:55:0: R0902: Too many instance attributes (9/7) (too-many-instance-attributes)
pybirdai/utils/advanced_migration_generator.py:170:8: W0107: Unnecessary pass statement (unnecessary-pass)
pybirdai/utils/advanced_migration_generator.py:192:4: R0912: Too many branches (26/12) (too-many-branches)
pybirdai/utils/advanced_migration_generator.py:285:12: R1705: Unnecessary "elif" after "return", remove the leading "el" from "elif" (no-else-return)
pybirdai/utils/advanced_migration_generator.py:291:56: C0207: Use full_path.rsplit('.', maxsplit=1)[-1] instead (use-maxsplit-arg)
pybirdai/utils/advanced_migration_generator.py:300:8: R1702: Too many nested blocks (7/5) (too-many-nested-blocks)
pybirdai/utils/advanced_migration_generator.py:353:19: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/utils/advanced_migration_generator.py:341:8: W0612: Unused variable 'all_models' (unused-variable)
pybirdai/utils/advanced_migration_generator.py:366:4: R0914: Too many local variables (27/15) (too-many-locals)
pybirdai/utils/advanced_migration_generator.py:366:37: W0621: Redefining name 'models' from outer scope (line 913) (redefined-outer-name)
pybirdai/utils/advanced_migration_generator.py:382:12: W0621: Redefining name 'model' from outer scope (line 921) (redefined-outer-name)
pybirdai/utils/advanced_migration_generator.py:366:4: R0912: Too many branches (14/12) (too-many-branches)
pybirdai/utils/advanced_migration_generator.py:527:4: R0912: Too many branches (26/12) (too-many-branches)
pybirdai/utils/advanced_migration_generator.py:527:4: R0915: Too many statements (59/50) (too-many-statements)
pybirdai/utils/advanced_migration_generator.py:662:4: R0912: Too many branches (23/12) (too-many-branches)
pybirdai/utils/advanced_migration_generator.py:662:4: R0915: Too many statements (53/50) (too-many-statements)
pybirdai/utils/advanced_migration_generator.py:773:8: R1705: Unnecessary "elif" after "return", remove the leading "el" from "elif" (no-else-return)
pybirdai/utils/advanced_migration_generator.py:802:12: R1705: Unnecessary "else" after "return", remove the "else" and de-indent the code inside it (no-else-return)
pybirdai/utils/advanced_migration_generator.py:820:37: W0621: Redefining name 'model' from outer scope (line 921) (redefined-outer-name)
pybirdai/utils/advanced_migration_generator.py:844:38: W0621: Redefining name 'models' from outer scope (line 913) (redefined-outer-name)
pybirdai/utils/advanced_migration_generator.py:849:34: W0621: Redefining name 'models' from outer scope (line 913) (redefined-outer-name)
pybirdai/utils/advanced_migration_generator.py:849:59: W0621: Redefining name 'output_path' from outer scope (line 905) (redefined-outer-name)
pybirdai/utils/advanced_migration_generator.py:851:8: W0621: Redefining name 'migration_code' from outer scope (line 934) (redefined-outer-name)
pybirdai/utils/advanced_migration_generator.py:867:4: W0621: Redefining name 'generator' from outer scope (line 910) (redefined-outer-name)
pybirdai/utils/advanced_migration_generator.py:868:4: W0621: Redefining name 'models' from outer scope (line 913) (redefined-outer-name)
pybirdai/utils/advanced_migration_generator.py:880:4: W0621: Redefining name 'generator' from outer scope (line 910) (redefined-outer-name)
pybirdai/utils/advanced_migration_generator.py:881:4: W0621: Redefining name 'models' from outer scope (line 913) (redefined-outer-name)
pybirdai/utils/advanced_migration_generator.py:893:4: W0621: Redefining name 'generator' from outer scope (line 910) (redefined-outer-name)
pybirdai/utils/advanced_migration_generator.py:894:4: W0621: Redefining name 'models' from outer scope (line 913) (redefined-outer-name)
pybirdai/utils/advanced_migration_generator.py:23:0: W0611: Unused import re (unused-import)
pybirdai/utils/advanced_migration_generator.py:24:0: W0611: Unused Dict imported from typing (unused-import)
pybirdai/utils/advanced_migration_generator.py:24:0: W0611: Unused Union imported from typing (unused-import)
************* Module pybirdai.utils.github_file_fetcher
pybirdai/utils/github_file_fetcher.py:479:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/github_file_fetcher.py:481:71: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/github_file_fetcher.py:482:63: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/github_file_fetcher.py:484:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/github_file_fetcher.py:489:75: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/github_file_fetcher.py:490:66: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/github_file_fetcher.py:36:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/github_file_fetcher.py:48:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/github_file_fetcher.py:62:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/github_file_fetcher.py:65:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/github_file_fetcher.py:102:23: W3101: Missing timeout argument for method 'requests.get' can cause your program to hang indefinitely (missing-timeout)
pybirdai/utils/github_file_fetcher.py:111:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/github_file_fetcher.py:125:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/github_file_fetcher.py:129:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/github_file_fetcher.py:149:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/github_file_fetcher.py:152:19: W3101: Missing timeout argument for method 'requests.get' can cause your program to hang indefinitely (missing-timeout)
pybirdai/utils/github_file_fetcher.py:160:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/github_file_fetcher.py:173:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/github_file_fetcher.py:178:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/github_file_fetcher.py:197:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/github_file_fetcher.py:216:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/github_file_fetcher.py:219:23: W3101: Missing timeout argument for method 'requests.get' can cause your program to hang indefinitely (missing-timeout)
pybirdai/utils/github_file_fetcher.py:225:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/github_file_fetcher.py:228:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/github_file_fetcher.py:245:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/github_file_fetcher.py:251:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/github_file_fetcher.py:255:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/github_file_fetcher.py:259:23: W3101: Missing timeout argument for method 'requests.get' can cause your program to hang indefinitely (missing-timeout)
pybirdai/utils/github_file_fetcher.py:262:12: R1705: Unnecessary "else" after "return", remove the "else" and de-indent the code inside it (no-else-return)
pybirdai/utils/github_file_fetcher.py:264:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/github_file_fetcher.py:267:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/github_file_fetcher.py:271:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/github_file_fetcher.py:274:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/github_file_fetcher.py:286:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/github_file_fetcher.py:300:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/github_file_fetcher.py:308:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/github_file_fetcher.py:317:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/github_file_fetcher.py:321:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/github_file_fetcher.py:331:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/github_file_fetcher.py:342:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/github_file_fetcher.py:344:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/github_file_fetcher.py:348:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/github_file_fetcher.py:360:46: W1309: Using an f-string that does not have any interpolated variables (f-string-without-interpolation)
pybirdai/utils/github_file_fetcher.py:366:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/github_file_fetcher.py:375:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/github_file_fetcher.py:383:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/github_file_fetcher.py:385:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/github_file_fetcher.py:426:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/github_file_fetcher.py:430:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/github_file_fetcher.py:432:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/github_file_fetcher.py:434:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/github_file_fetcher.py:448:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/github_file_fetcher.py:452:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/github_file_fetcher.py:454:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/github_file_fetcher.py:471:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/github_file_fetcher.py:487:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/github_file_fetcher.py:493:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/github_file_fetcher.py:498:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/github_file_fetcher.py:505:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/github_file_fetcher.py:508:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/github_file_fetcher.py:510:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/github_file_fetcher.py:14:0: C0411: standard import "os" should be placed before third party import "requests" (wrong-import-order)
pybirdai/utils/github_file_fetcher.py:15:0: C0411: standard import "logging" should be placed before third party import "requests" (wrong-import-order)
************* Module pybirdai.utils.secure_file_utils
pybirdai/utils/secure_file_utils.py:23:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/secure_file_utils.py:28:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/secure_file_utils.py:31:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/secure_file_utils.py:46:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/secure_file_utils.py:49:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/secure_file_utils.py:52:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/secure_file_utils.py:55:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/secure_file_utils.py:58:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/secure_file_utils.py:62:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/secure_file_utils.py:66:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/secure_file_utils.py:71:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/secure_file_utils.py:73:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/secure_file_utils.py:90:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/secure_file_utils.py:93:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/secure_file_utils.py:96:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/secure_file_utils.py:98:69: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/secure_file_utils.py:115:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/secure_file_utils.py:118:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/secure_file_utils.py:121:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/secure_file_utils.py:125:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/secure_file_utils.py:127:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/secure_file_utils.py:141:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/secure_file_utils.py:159:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/secure_file_utils.py:161:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/secure_file_utils.py:167:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/secure_file_utils.py:171:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/secure_file_utils.py:175:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/secure_file_utils.py:180:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/secure_file_utils.py:183:0: C0304: Final newline missing (missing-final-newline)
************* Module pybirdai.utils.utils
pybirdai/utils/utils.py:53:0: C0325: Unnecessary parens after 'not' keyword (superfluous-parens)
pybirdai/utils/utils.py:25:0: R0205: Class 'Utils' inherits from object, can be safely removed from bases in python3 (useless-object-inheritance)
************* Module pybirdai.utils.database_setup_first_use
pybirdai/utils/database_setup_first_use.py:47:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/database_setup_first_use.py:52:4: C0415: Import outside toplevel (pybirdai.entry_points.create_django_models.RunCreateDjangoModels) (import-outside-toplevel)
pybirdai/utils/database_setup_first_use.py:68:4: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/database_setup_first_use.py:72:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/database_setup_first_use.py:74:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/database_setup_first_use.py:80:4: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/database_setup_first_use.py:84:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/database_setup_first_use.py:86:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/database_setup_first_use.py:92:4: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/database_setup_first_use.py:94:9: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/utils/database_setup_first_use.py:96:13: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/utils/database_setup_first_use.py:105:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/database_setup_first_use.py:110:13: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/utils/database_setup_first_use.py:114:13: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/utils/database_setup_first_use.py:118:13: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/utils/database_setup_first_use.py:122:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/database_setup_first_use.py:124:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/database_setup_first_use.py:131:4: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/database_setup_first_use.py:134:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/database_setup_first_use.py:139:13: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/utils/database_setup_first_use.py:143:13: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/utils/database_setup_first_use.py:147:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/database_setup_first_use.py:149:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/database_setup_first_use.py:160:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/database_setup_first_use.py:169:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/database_setup_first_use.py:185:4: E1120: No value for argument 'app_name' in constructor call (no-value-for-parameter)
pybirdai/utils/database_setup_first_use.py:185:4: E1120: No value for argument 'app_module' in constructor call (no-value-for-parameter)
pybirdai/utils/database_setup_first_use.py:14:0: C0411: standard import "os" should be placed before third party import "django" (wrong-import-order)
pybirdai/utils/database_setup_first_use.py:15:0: C0411: standard import "sys" should be placed before third party import "django" (wrong-import-order)
pybirdai/utils/database_setup_first_use.py:18:0: C0411: standard import "logging" should be placed before third party imports "django", "django.apps.AppConfig", "django.conf.settings" (wrong-import-order)
pybirdai/utils/database_setup_first_use.py:19:0: C0411: standard import "importlib.metadata" should be placed before third party imports "django", "django.apps.AppConfig", "django.conf.settings" (wrong-import-order)
pybirdai/utils/database_setup_first_use.py:20:0: C0411: standard import "ast" should be placed before third party imports "django", "django.apps.AppConfig", "django.conf.settings" (wrong-import-order)
pybirdai/utils/database_setup_first_use.py:17:0: W0611: Unused settings imported from django.conf (unused-import)
pybirdai/utils/database_setup_first_use.py:19:0: W0611: Unused metadata imported from importlib (unused-import)
************* Module pybirdai.utils.clone_repo_service
pybirdai/utils/clone_repo_service.py:101:20: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_repo_service.py:98:12: W0612: Unused variable 'source_folder' (unused-variable)
pybirdai/utils/clone_repo_service.py:99:31: W0612: Unused variable 'filter_func' (unused-variable)
pybirdai/utils/clone_repo_service.py:117:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_repo_service.py:125:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_repo_service.py:122:12: W0612: Unused variable 'source_folder' (unused-variable)
pybirdai/utils/clone_repo_service.py:123:31: W0612: Unused variable 'filter_func' (unused-variable)
pybirdai/utils/clone_repo_service.py:139:23: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/utils/clone_repo_service.py:135:24: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_repo_service.py:138:24: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_repo_service.py:140:20: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_repo_service.py:141:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_repo_service.py:143:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_repo_service.py:155:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_repo_service.py:159:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_repo_service.py:162:19: W3101: Missing timeout argument for method 'requests.get' can cause your program to hang indefinitely (missing-timeout)
pybirdai/utils/clone_repo_service.py:180:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_repo_service.py:188:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_repo_service.py:198:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_repo_service.py:214:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_repo_service.py:252:33: R1733: Unnecessary dictionary index lookup, use 'target_mappings' instead (unnecessary-dict-index-lookup)
pybirdai/utils/clone_repo_service.py:220:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_repo_service.py:224:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_repo_service.py:236:28: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_repo_service.py:241:32: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_repo_service.py:247:32: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_repo_service.py:218:8: R1702: Too many nested blocks (7/5) (too-many-nested-blocks)
pybirdai/utils/clone_repo_service.py:259:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_repo_service.py:190:4: R0912: Too many branches (13/12) (too-many-branches)
pybirdai/utils/clone_repo_service.py:269:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_repo_service.py:272:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_repo_service.py:289:4: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_repo_service.py:15:0: C0411: standard import "zipfile" should be placed before third party import "requests" (wrong-import-order)
pybirdai/utils/clone_repo_service.py:16:0: C0411: standard import "os" should be placed before third party import "requests" (wrong-import-order)
pybirdai/utils/clone_repo_service.py:17:0: C0411: standard import "shutil" should be placed before third party import "requests" (wrong-import-order)
pybirdai/utils/clone_repo_service.py:18:0: C0411: standard import "logging" should be placed before third party import "requests" (wrong-import-order)
pybirdai/utils/clone_repo_service.py:19:0: C0411: standard import "time" should be placed before third party import "requests" (wrong-import-order)
************* Module pybirdai.utils.derived_fields_extractor
pybirdai/utils/derived_fields_extractor.py:47:9: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/utils/derived_fields_extractor.py:65:28: R0916: Too many boolean expressions in if statement (9/5) (too-many-boolean-expressions)
pybirdai/utils/derived_fields_extractor.py:53:4: R1702: Too many nested blocks (6/5) (too-many-nested-blocks)
pybirdai/utils/derived_fields_extractor.py:136:16: W0612: Unused variable 'func_def' (unused-variable)
pybirdai/utils/derived_fields_extractor.py:165:9: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/utils/derived_fields_extractor.py:177:0: R0914: Too many local variables (21/15) (too-many-locals)
pybirdai/utils/derived_fields_extractor.py:207:9: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/utils/derived_fields_extractor.py:210:9: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/utils/derived_fields_extractor.py:227:28: R0916: Too many boolean expressions in if statement (9/5) (too-many-boolean-expressions)
pybirdai/utils/derived_fields_extractor.py:218:4: R1702: Too many nested blocks (6/5) (too-many-nested-blocks)
pybirdai/utils/derived_fields_extractor.py:266:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/derived_fields_extractor.py:277:16: R1724: Unnecessary "elif" after "continue", remove the leading "el" from "elif" (no-else-continue)
pybirdai/utils/derived_fields_extractor.py:263:4: R1702: Too many nested blocks (6/5) (too-many-nested-blocks)
pybirdai/utils/derived_fields_extractor.py:303:9: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/utils/derived_fields_extractor.py:306:4: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/derived_fields_extractor.py:177:0: R0912: Too many branches (18/12) (too-many-branches)
pybirdai/utils/derived_fields_extractor.py:328:4: W0612: Unused variable 'logger' (unused-variable)
pybirdai/utils/derived_fields_extractor.py:21:0: C0411: standard import "sys" should be placed before third party imports "django", "django.db.models", "django.conf.settings" (wrong-import-order)
pybirdai/utils/derived_fields_extractor.py:23:0: C0411: standard import "logging" should be placed before third party imports "django", "django.db.models", "django.conf.settings" (wrong-import-order)
pybirdai/utils/derived_fields_extractor.py:14:0: W0611: Unused import inspect (unused-import)
pybirdai/utils/derived_fields_extractor.py:15:0: W0611: Unused import argparse (unused-import)
pybirdai/utils/derived_fields_extractor.py:19:0: W0611: Unused models imported from django.db (unused-import)
************* Module pybirdai.utils.member_hierarchy_editor.from_member_hierarchy_node_to_visualisation
pybirdai/utils/member_hierarchy_editor/from_member_hierarchy_node_to_visualisation.py:117:4: R0914: Too many local variables (32/15) (too-many-locals)
pybirdai/utils/member_hierarchy_editor/from_member_hierarchy_node_to_visualisation.py:117:4: R0912: Too many branches (14/12) (too-many-branches)
pybirdai/utils/member_hierarchy_editor/from_member_hierarchy_node_to_visualisation.py:156:12: W0612: Unused variable 'nodes_in_level' (unused-variable)
pybirdai/utils/member_hierarchy_editor/from_member_hierarchy_node_to_visualisation.py:169:20: W0612: Unused variable 'code' (unused-variable)
pybirdai/utils/member_hierarchy_editor/from_member_hierarchy_node_to_visualisation.py:261:39: W0621: Redefining name 'f' from outer scope (line 332) (redefined-outer-name)
pybirdai/utils/member_hierarchy_editor/from_member_hierarchy_node_to_visualisation.py:261:13: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/utils/member_hierarchy_editor/from_member_hierarchy_node_to_visualisation.py:288:66: W0621: Redefining name 'f' from outer scope (line 332) (redefined-outer-name)
pybirdai/utils/member_hierarchy_editor/from_member_hierarchy_node_to_visualisation.py:306:4: W0621: Redefining name 'integration' from outer scope (line 322) (redefined-outer-name)
pybirdai/utils/member_hierarchy_editor/from_member_hierarchy_node_to_visualisation.py:315:4: W0621: Redefining name 'integration' from outer scope (line 322) (redefined-outer-name)
pybirdai/utils/member_hierarchy_editor/from_member_hierarchy_node_to_visualisation.py:332:9: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/utils/member_hierarchy_editor/from_member_hierarchy_node_to_visualisation.py:335:10: W1309: Using an f-string that does not have any interpolated variables (f-string-without-interpolation)
pybirdai/utils/member_hierarchy_editor/from_member_hierarchy_node_to_visualisation.py:16:0: W0611: Unused Tuple imported from typing (unused-import)
pybirdai/utils/member_hierarchy_editor/from_member_hierarchy_node_to_visualisation.py:16:0: W0611: Unused Optional imported from typing (unused-import)
pybirdai/utils/member_hierarchy_editor/from_member_hierarchy_node_to_visualisation.py:17:0: W0611: Unused import math (unused-import)
************* Module pybirdai.utils.member_hierarchy_editor.test_integration
pybirdai/utils/member_hierarchy_editor/test_integration.py:21:0: R0902: Too many instance attributes (10/7) (too-many-instance-attributes)
pybirdai/utils/member_hierarchy_editor/test_integration.py:15:0: W0611: Unused override_settings imported from django.test.utils (unused-import)
************* Module pybirdai.utils.member_hierarchy_editor.django_model_converter
pybirdai/utils/member_hierarchy_editor/django_model_converter.py:22:4: R0914: Too many local variables (22/15) (too-many-locals)
pybirdai/utils/member_hierarchy_editor/django_model_converter.py:113:15: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/utils/member_hierarchy_editor/django_model_converter.py:67:16: R1705: Unnecessary "elif" after "return", remove the leading "el" from "elif" (no-else-return)
pybirdai/utils/member_hierarchy_editor/django_model_converter.py:117:4: R0914: Too many local variables (28/15) (too-many-locals)
pybirdai/utils/member_hierarchy_editor/django_model_converter.py:13:0: W0611: Unused import json (unused-import)
pybirdai/utils/member_hierarchy_editor/django_model_converter.py:14:0: W0611: Unused Set imported from typing (unused-import)
pybirdai/utils/member_hierarchy_editor/django_model_converter.py:15:0: W0611: Unused ObjectDoesNotExist imported from django.core.exceptions (unused-import)
************* Module pybirdai.utils.member_hierarchy_editor.from_visualisation_to_member_hierarchy_node
pybirdai/utils/member_hierarchy_editor/from_visualisation_to_member_hierarchy_node.py:26:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/member_hierarchy_editor/from_visualisation_to_member_hierarchy_node.py:30:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/member_hierarchy_editor/from_visualisation_to_member_hierarchy_node.py:34:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/member_hierarchy_editor/from_visualisation_to_member_hierarchy_node.py:38:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/member_hierarchy_editor/from_visualisation_to_member_hierarchy_node.py:48:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/member_hierarchy_editor/from_visualisation_to_member_hierarchy_node.py:54:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/member_hierarchy_editor/from_visualisation_to_member_hierarchy_node.py:58:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/member_hierarchy_editor/from_visualisation_to_member_hierarchy_node.py:61:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/member_hierarchy_editor/from_visualisation_to_member_hierarchy_node.py:68:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/member_hierarchy_editor/from_visualisation_to_member_hierarchy_node.py:93:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/member_hierarchy_editor/from_visualisation_to_member_hierarchy_node.py:98:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/member_hierarchy_editor/from_visualisation_to_member_hierarchy_node.py:101:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/member_hierarchy_editor/from_visualisation_to_member_hierarchy_node.py:105:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/member_hierarchy_editor/from_visualisation_to_member_hierarchy_node.py:108:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/member_hierarchy_editor/from_visualisation_to_member_hierarchy_node.py:114:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/member_hierarchy_editor/from_visualisation_to_member_hierarchy_node.py:124:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/member_hierarchy_editor/from_visualisation_to_member_hierarchy_node.py:128:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/member_hierarchy_editor/from_visualisation_to_member_hierarchy_node.py:130:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/member_hierarchy_editor/from_visualisation_to_member_hierarchy_node.py:133:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/member_hierarchy_editor/from_visualisation_to_member_hierarchy_node.py:141:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/member_hierarchy_editor/from_visualisation_to_member_hierarchy_node.py:153:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/member_hierarchy_editor/from_visualisation_to_member_hierarchy_node.py:156:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/member_hierarchy_editor/from_visualisation_to_member_hierarchy_node.py:168:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/member_hierarchy_editor/from_visualisation_to_member_hierarchy_node.py:176:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/member_hierarchy_editor/from_visualisation_to_member_hierarchy_node.py:181:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/member_hierarchy_editor/from_visualisation_to_member_hierarchy_node.py:186:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/member_hierarchy_editor/from_visualisation_to_member_hierarchy_node.py:190:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/member_hierarchy_editor/from_visualisation_to_member_hierarchy_node.py:194:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/member_hierarchy_editor/from_visualisation_to_member_hierarchy_node.py:197:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/member_hierarchy_editor/from_visualisation_to_member_hierarchy_node.py:200:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/member_hierarchy_editor/from_visualisation_to_member_hierarchy_node.py:212:0: C0304: Final newline missing (missing-final-newline)
pybirdai/utils/member_hierarchy_editor/from_visualisation_to_member_hierarchy_node.py:19:9: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/utils/member_hierarchy_editor/from_visualisation_to_member_hierarchy_node.py:69:4: R1705: Unnecessary "elif" after "return", remove the leading "el" from "elif" (no-else-return)
pybirdai/utils/member_hierarchy_editor/from_visualisation_to_member_hierarchy_node.py:89:0: R0914: Too many local variables (16/15) (too-many-locals)
pybirdai/utils/member_hierarchy_editor/from_visualisation_to_member_hierarchy_node.py:142:9: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/utils/member_hierarchy_editor/from_visualisation_to_member_hierarchy_node.py:207:11: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/utils/member_hierarchy_editor/from_visualisation_to_member_hierarchy_node.py:198:14: W1309: Using an f-string that does not have any interpolated variables (f-string-without-interpolation)
************* Module pybirdai.utils.clone_mode.import_from_metadata_export
pybirdai/utils/clone_mode/import_from_metadata_export.py:71:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:618:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:622:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:626:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:629:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:632:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:635:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:638:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:653:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:657:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:661:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:664:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:669:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:673:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:692:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:694:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:702:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:704:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:710:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:714:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:717:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:722:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:724:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:731:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:733:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:743:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:748:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:758:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:760:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:763:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:764:70: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:767:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:789:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:799:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:802:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:806:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:811:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:813:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:819:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:823:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:825:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:828:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:832:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:837:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:843:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:846:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:852:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:856:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:861:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:877:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:880:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:889:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:891:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:897:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:902:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:906:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:909:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:914:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:918:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:927:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:931:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:936:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:961:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:966:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:968:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:981:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:984:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:989:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:995:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1000:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1024:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1033:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1036:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1042:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1045:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1077:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1092:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1098:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1102:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1106:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1110:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1117:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1126:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1130:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1134:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1138:56: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1142:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1150:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1154:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1158:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1162:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1166:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1172:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1178:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1185:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1206:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1218:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1222:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1226:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1233:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1237:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1239:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1271:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1274:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1279:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1281:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1285:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1309:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1324:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1330:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1341:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1345:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1351:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1362:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1370:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1386:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1392:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1395:73: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1425:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1429:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1437:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1455:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1460:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1469:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1481:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1487:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1492:42: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1499:42: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1505:42: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1510:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1514:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1520:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1524:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1528:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1532:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1544:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1551:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1558:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1569:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1570:85: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1576:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1579:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1582:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1585:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1594:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1597:50: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1602:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1607:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1610:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1614:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1617:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1621:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1630:46: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1635:46: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1641:46: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1646:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1649:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1657:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1661:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1667:50: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1863:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1866:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1870:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1873:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1878:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1887:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1907:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1926:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1:0: C0302: Too many lines in module (2079/1000) (too-many-lines)
pybirdai/utils/clone_mode/import_from_metadata_export.py:31:8: C0415: Import outside toplevel (django) (import-outside-toplevel)
pybirdai/utils/clone_mode/import_from_metadata_export.py:32:8: C0415: Import outside toplevel (django.conf.settings) (import-outside-toplevel)
pybirdai/utils/clone_mode/import_from_metadata_export.py:40:0: C0413: Import "from django.db import transaction" should be placed at the top of the module (wrong-import-position)
pybirdai/utils/clone_mode/import_from_metadata_export.py:41:0: C0413: Import "from django.db import models" should be placed at the top of the module (wrong-import-position)
pybirdai/utils/clone_mode/import_from_metadata_export.py:42:0: C0413: Import "from django.db import connection" should be placed at the top of the module (wrong-import-position)
pybirdai/utils/clone_mode/import_from_metadata_export.py:43:0: C0413: Import "from pybirdai import bird_meta_data_model" should be placed at the top of the module (wrong-import-position)
pybirdai/utils/clone_mode/import_from_metadata_export.py:43:0: E0611: No name 'bird_meta_data_model' in module 'pybirdai' (no-name-in-module)
pybirdai/utils/clone_mode/import_from_metadata_export.py:44:0: C0413: Import "from pybirdai.utils.clone_mode.clone_mode_column_index import ColumnIndexes" should be placed at the top of the module (wrong-import-position)
pybirdai/utils/clone_mode/import_from_metadata_export.py:45:0: C0413: Import "import traceback" should be placed at the top of the module (wrong-import-position)
pybirdai/utils/clone_mode/import_from_metadata_export.py:86:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:113:15: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/utils/clone_mode/import_from_metadata_export.py:111:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:114:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:120:8: W0621: Redefining name 'models' from outer scope (line 41) (redefined-outer-name)
pybirdai/utils/clone_mode/import_from_metadata_export.py:119:8: C0415: Import outside toplevel (inspect) (import-outside-toplevel)
pybirdai/utils/clone_mode/import_from_metadata_export.py:120:8: W0404: Reimport 'models' (imported line 41) (reimported)
pybirdai/utils/clone_mode/import_from_metadata_export.py:120:8: C0415: Import outside toplevel (django.db.models) (import-outside-toplevel)
pybirdai/utils/clone_mode/import_from_metadata_export.py:127:31: W0212: Access to a protected member _meta of a client class (protected-access)
pybirdai/utils/clone_mode/import_from_metadata_export.py:129:45: W0212: Access to a protected member _meta of a client class (protected-access)
pybirdai/utils/clone_mode/import_from_metadata_export.py:131:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:131:60: W0212: Access to a protected member _meta of a client class (protected-access)
pybirdai/utils/clone_mode/import_from_metadata_export.py:133:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:509:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:558:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:567:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:579:8: R1705: Unnecessary "elif" after "return", remove the leading "el" from "elif" (no-else-return)
pybirdai/utils/clone_mode/import_from_metadata_export.py:584:12: C0415: Import outside toplevel (decimal.Decimal) (import-outside-toplevel)
pybirdai/utils/clone_mode/import_from_metadata_export.py:588:13: R1701: Consider merging these isinstance calls to isinstance(field, (models.DateField, models.DateTimeField)) (consider-merging-isinstance)
pybirdai/utils/clone_mode/import_from_metadata_export.py:591:16: C0415: Import outside toplevel (django.utils.dateparse.parse_date, django.utils.dateparse.parse_datetime) (import-outside-toplevel)
pybirdai/utils/clone_mode/import_from_metadata_export.py:596:12: R1705: Unnecessary "else" after "return", remove the "else" and de-indent the code inside it (no-else-return)
pybirdai/utils/clone_mode/import_from_metadata_export.py:609:24: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:570:4: R0911: Too many return statements (13/6) (too-many-return-statements)
pybirdai/utils/clone_mode/import_from_metadata_export.py:570:4: R0912: Too many branches (14/12) (too-many-branches)
pybirdai/utils/clone_mode/import_from_metadata_export.py:617:47: W0212: Access to a protected member _meta of a client class (protected-access)
pybirdai/utils/clone_mode/import_from_metadata_export.py:621:26: W0212: Access to a protected member _meta of a client class (protected-access)
pybirdai/utils/clone_mode/import_from_metadata_export.py:636:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:641:4: R0914: Too many local variables (31/15) (too-many-locals)
pybirdai/utils/clone_mode/import_from_metadata_export.py:646:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:649:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:650:12: W0719: Raising too general exception: Exception (broad-exception-raised)
pybirdai/utils/clone_mode/import_from_metadata_export.py:655:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:659:40: W0212: Access to a protected member _meta of a client class (protected-access)
pybirdai/utils/clone_mode/import_from_metadata_export.py:723:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:728:16: W1309: Using an f-string that does not have any interpolated variables (f-string-without-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:735:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:747:16: W0719: Raising too general exception: Exception (broad-exception-raised)
pybirdai/utils/clone_mode/import_from_metadata_export.py:753:20: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:754:20: W0719: Raising too general exception: Exception (broad-exception-raised)
pybirdai/utils/clone_mode/import_from_metadata_export.py:759:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:762:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:762:24: W1309: Using an f-string that does not have any interpolated variables (f-string-without-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:769:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:775:19: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/utils/clone_mode/import_from_metadata_export.py:776:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:641:4: R0912: Too many branches (20/12) (too-many-branches)
pybirdai/utils/clone_mode/import_from_metadata_export.py:641:4: R0915: Too many statements (73/50) (too-many-statements)
pybirdai/utils/clone_mode/import_from_metadata_export.py:793:4: R0914: Too many local variables (28/15) (too-many-locals)
pybirdai/utils/clone_mode/import_from_metadata_export.py:798:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:804:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:809:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:821:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:824:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:875:35: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/utils/clone_mode/import_from_metadata_export.py:867:32: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:873:39: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/utils/clone_mode/import_from_metadata_export.py:872:36: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:874:36: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:876:32: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:890:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:892:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:793:4: R0912: Too many branches (17/12) (too-many-branches)
pybirdai/utils/clone_mode/import_from_metadata_export.py:793:4: R0915: Too many statements (54/50) (too-many-statements)
pybirdai/utils/clone_mode/import_from_metadata_export.py:894:4: R0914: Too many local variables (18/15) (too-many-locals)
pybirdai/utils/clone_mode/import_from_metadata_export.py:896:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:935:24: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:940:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:942:4: R0914: Too many local variables (18/15) (too-many-locals)
pybirdai/utils/clone_mode/import_from_metadata_export.py:947:21: W0212: Access to a protected member _meta of a client class (protected-access)
pybirdai/utils/clone_mode/import_from_metadata_export.py:967:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:970:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:973:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:994:20: W0719: Raising too general exception: Exception (broad-exception-raised)
pybirdai/utils/clone_mode/import_from_metadata_export.py:996:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1016:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1032:16: W0719: Raising too general exception: Exception (broad-exception-raised)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1035:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1041:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1044:20: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1049:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1052:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1054:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:942:4: R0912: Too many branches (16/12) (too-many-branches)
pybirdai/utils/clone_mode/import_from_metadata_export.py:942:4: R0915: Too many statements (61/50) (too-many-statements)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1057:4: R0914: Too many local variables (95/15) (too-many-locals)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1146:20: W0621: Redefining name 'tempfile' from outer scope (line 21) (redefined-outer-name)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1067:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1075:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1076:12: W0719: Raising too general exception: Exception (broad-exception-raised)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1079:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1082:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1086:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1096:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1097:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1105:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1114:23: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1115:20: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1116:20: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1121:24: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1123:24: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1123:37: W1309: Using an f-string that does not have any interpolated variables (f-string-without-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1125:24: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1125:37: W1309: Using an f-string that does not have any interpolated variables (f-string-without-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1129:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1133:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1139:16: W0212: Access to a protected member _meta of a client class (protected-access)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1167:23: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1146:20: W0404: Reimport 'tempfile' (imported line 21) (reimported)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1146:20: C0415: Import outside toplevel (tempfile) (import-outside-toplevel)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1157:24: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1168:20: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1171:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1174:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1176:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1177:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1184:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1204:15: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1189:17: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1196:28: W1309: Using an f-string that does not have any interpolated variables (f-string-without-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1200:24: W1309: Using an f-string that does not have any interpolated variables (f-string-without-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1205:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1209:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1211:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1215:20: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1220:40: W0212: Access to a protected member _meta of a client class (protected-access)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1228:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1229:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1231:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1232:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1238:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1242:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1249:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1252:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1253:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1257:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1264:16: W0719: Raising too general exception: Exception (broad-exception-raised)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1266:16: W0719: Raising too general exception: Exception (broad-exception-raised)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1280:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1284:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1287:16: W0719: Raising too general exception: Exception (broad-exception-raised)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1295:69: W0212: Access to a protected member _meta of a client class (protected-access)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1296:32: W0212: Access to a protected member _meta of a client class (protected-access)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1303:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1320:28: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1306:8: R1702: Too many nested blocks (6/5) (too-many-nested-blocks)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1326:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1334:20: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1342:20: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1344:24: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1350:24: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1356:20: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1365:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1378:24: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1399:36: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1405:32: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1363:8: R1702: Too many nested blocks (7/5) (too-many-nested-blocks)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1411:32: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1413:32: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1363:8: R1702: Too many nested blocks (6/5) (too-many-nested-blocks)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1440:23: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1439:24: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1448:20: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1453:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1454:50: W0212: Access to a protected member _meta of a client class (protected-access)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1471:20: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1608:19: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1479:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1480:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1485:20: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1486:20: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1497:20: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1509:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1513:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1518:20: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1519:20: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1519:35: W1309: Using an f-string that does not have any interpolated variables (f-string-without-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1523:20: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1543:36: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1477:8: R1702: Too many nested blocks (7/5) (too-many-nested-blocks)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1564:40: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1575:48: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1477:8: R1702: Too many nested blocks (10/5) (too-many-nested-blocks)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1578:40: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1581:36: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1477:8: R1702: Too many nested blocks (8/5) (too-many-nested-blocks)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1477:8: R1702: Too many nested blocks (6/5) (too-many-nested-blocks)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1477:8: R1702: Too many nested blocks (6/5) (too-many-nested-blocks)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1601:28: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1604:20: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1604:35: W1309: Using an f-string that does not have any interpolated variables (f-string-without-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1606:20: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1613:20: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1622:24: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1650:31: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1647:28: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1651:28: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1655:24: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1659:20: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1660:20: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1660:33: W1309: Using an f-string that does not have any interpolated variables (f-string-without-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1664:24: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1671:28: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1673:28: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1678:20: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1681:24: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1686:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1688:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1057:4: R0912: Too many branches (125/12) (too-many-branches)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1057:4: R0915: Too many statements (364/50) (too-many-statements)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1694:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1708:15: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1704:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1709:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1722:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1725:12: R1705: Unnecessary "elif" after "return", remove the leading "el" from "elif" (no-else-return)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1726:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1731:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1751:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1762:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1767:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1771:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1782:19: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1781:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1783:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1789:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1796:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1810:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1813:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1823:19: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1822:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1824:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1830:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1835:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1839:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1848:19: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1847:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1849:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1855:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1862:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1865:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1872:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1880:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1883:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1889:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1899:23: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1898:20: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1900:20: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1906:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1911:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1920:23: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1919:20: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1921:20: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1929:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1956:8: W0212: Access to a protected member _save_results of a client class (protected-access)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1958:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1960:8: W0212: Access to a protected member _save_results of a client class (protected-access)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1965:4: W0101: Unreachable code (unreachable)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1965:4: R0914: Too many local variables (16/15) (too-many-locals)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1967:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1974:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1993:23: W0212: Access to a protected member _get_import_order of a client class (protected-access)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1998:28: C0201: Consider iterating the dictionary directly instead of calling .keys() (consider-iterating-dictionary)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1999:19: W0212: Access to a protected member _get_table_name_from_csv_filename of a client class (protected-access)
pybirdai/utils/clone_mode/import_from_metadata_export.py:2004:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:2013:23: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/utils/clone_mode/import_from_metadata_export.py:2012:20: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:2014:20: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:2020:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:2025:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:2033:23: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/utils/clone_mode/import_from_metadata_export.py:2034:20: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:2040:8: W0212: Access to a protected member _save_results of a client class (protected-access)
pybirdai/utils/clone_mode/import_from_metadata_export.py:2041:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1965:4: R0912: Too many branches (15/12) (too-many-branches)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1938:0: R0915: Too many statements (57/50) (too-many-statements)
pybirdai/utils/clone_mode/import_from_metadata_export.py:1965:4: W0612: Unused variable 'import_from_path_ordered' (unused-variable)
pybirdai/utils/clone_mode/import_from_metadata_export.py:2062:8: W0621: Redefining name 'tempfile' from outer scope (line 21) (redefined-outer-name)
pybirdai/utils/clone_mode/import_from_metadata_export.py:2062:8: W0404: Reimport 'tempfile' (imported line 21) (reimported)
pybirdai/utils/clone_mode/import_from_metadata_export.py:2062:8: C0415: Import outside toplevel (tempfile) (import-outside-toplevel)
pybirdai/utils/clone_mode/import_from_metadata_export.py:2069:12: W0212: Access to a protected member _save_results of a client class (protected-access)
pybirdai/utils/clone_mode/import_from_metadata_export.py:2074:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/clone_mode/import_from_metadata_export.py:2076:8: W0212: Access to a protected member _save_results of a client class (protected-access)
pybirdai/utils/clone_mode/import_from_metadata_export.py:45:0: C0411: standard import "traceback" should be placed before third party imports "django.db.transaction", "django.db.models", "django.db.connection" and first party imports "pybirdai.bird_meta_data_model", "pybirdai.utils.clone_mode.clone_mode_column_index.ColumnIndexes"  (wrong-import-order)
pybirdai/utils/clone_mode/import_from_metadata_export.py:40:0: W0611: Unused transaction imported from django.db (unused-import)
pybirdai/utils/clone_mode/import_from_metadata_export.py:45:0: W0611: Unused import traceback (unused-import)
************* Module pybirdai.utils.clone_mode.export_with_ids
pybirdai/utils/clone_mode/export_with_ids.py:48:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/export_with_ids.py:56:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/export_with_ids.py:62:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/export_with_ids.py:66:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/export_with_ids.py:70:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/export_with_ids.py:73:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/export_with_ids.py:78:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/export_with_ids.py:83:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/export_with_ids.py:91:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/export_with_ids.py:98:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/export_with_ids.py:102:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/export_with_ids.py:113:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/export_with_ids.py:118:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/export_with_ids.py:128:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/export_with_ids.py:132:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/export_with_ids.py:145:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/export_with_ids.py:148:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/export_with_ids.py:152:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/export_with_ids.py:156:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/clone_mode/export_with_ids.py:158:0: C0304: Final newline missing (missing-final-newline)
pybirdai/utils/clone_mode/export_with_ids.py:24:0: E0611: No name 'bird_meta_data_model' in module 'pybirdai' (no-name-in-module)
pybirdai/utils/clone_mode/export_with_ids.py:33:0: R0914: Too many local variables (31/15) (too-many-locals)
pybirdai/utils/clone_mode/export_with_ids.py:54:34: W0212: Access to a protected member _meta of a client class (protected-access)
pybirdai/utils/clone_mode/export_with_ids.py:55:22: W0212: Access to a protected member _meta of a client class (protected-access)
pybirdai/utils/clone_mode/export_with_ids.py:72:69: W0212: Access to a protected member _meta of a client class (protected-access)
pybirdai/utils/clone_mode/export_with_ids.py:75:25: W0212: Access to a protected member _meta of a client class (protected-access)
pybirdai/utils/clone_mode/export_with_ids.py:86:20: R1724: Unnecessary "elif" after "continue", remove the leading "el" from "elif" (no-else-continue)
pybirdai/utils/clone_mode/export_with_ids.py:33:0: R0912: Too many branches (22/12) (too-many-branches)
pybirdai/utils/clone_mode/export_with_ids.py:33:0: R0915: Too many statements (67/50) (too-many-statements)
pybirdai/utils/clone_mode/export_with_ids.py:52:8: W0612: Unused variable 'name' (unused-variable)
pybirdai/utils/clone_mode/export_with_ids.py:25:0: C0411: standard import "re" should be placed before third party imports "django.db.connection", "django.conf.settings" and first party import "pybirdai.bird_meta_data_model"  (wrong-import-order)
pybirdai/utils/clone_mode/export_with_ids.py:18:0: W0611: Unused import csv (unused-import)
************* Module pybirdai.utils.clone_mode.clone_mode_column_index
pybirdai/utils/clone_mode/clone_mode_column_index.py:13:0: R0205: Class 'ColumnIndexes' inherits from object, can be safely removed from bases in python3 (useless-object-inheritance)
************* Module pybirdai.utils.speed_improvements_initial_migration.advanced_migration_generator
pybirdai/utils/speed_improvements_initial_migration/advanced_migration_generator.py:30:0: R0902: Too many instance attributes (20/7) (too-many-instance-attributes)
pybirdai/utils/speed_improvements_initial_migration/advanced_migration_generator.py:55:0: R0902: Too many instance attributes (9/7) (too-many-instance-attributes)
pybirdai/utils/speed_improvements_initial_migration/advanced_migration_generator.py:170:8: W0107: Unnecessary pass statement (unnecessary-pass)
pybirdai/utils/speed_improvements_initial_migration/advanced_migration_generator.py:192:4: R0912: Too many branches (26/12) (too-many-branches)
pybirdai/utils/speed_improvements_initial_migration/advanced_migration_generator.py:285:12: R1705: Unnecessary "elif" after "return", remove the leading "el" from "elif" (no-else-return)
pybirdai/utils/speed_improvements_initial_migration/advanced_migration_generator.py:291:56: C0207: Use full_path.rsplit('.', maxsplit=1)[-1] instead (use-maxsplit-arg)
pybirdai/utils/speed_improvements_initial_migration/advanced_migration_generator.py:300:8: R1702: Too many nested blocks (7/5) (too-many-nested-blocks)
pybirdai/utils/speed_improvements_initial_migration/advanced_migration_generator.py:353:19: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/utils/speed_improvements_initial_migration/advanced_migration_generator.py:341:8: W0612: Unused variable 'all_models' (unused-variable)
pybirdai/utils/speed_improvements_initial_migration/advanced_migration_generator.py:366:4: R0914: Too many local variables (27/15) (too-many-locals)
pybirdai/utils/speed_improvements_initial_migration/advanced_migration_generator.py:366:37: W0621: Redefining name 'models' from outer scope (line 913) (redefined-outer-name)
pybirdai/utils/speed_improvements_initial_migration/advanced_migration_generator.py:382:12: W0621: Redefining name 'model' from outer scope (line 921) (redefined-outer-name)
pybirdai/utils/speed_improvements_initial_migration/advanced_migration_generator.py:366:4: R0912: Too many branches (14/12) (too-many-branches)
pybirdai/utils/speed_improvements_initial_migration/advanced_migration_generator.py:527:4: R0912: Too many branches (26/12) (too-many-branches)
pybirdai/utils/speed_improvements_initial_migration/advanced_migration_generator.py:527:4: R0915: Too many statements (59/50) (too-many-statements)
pybirdai/utils/speed_improvements_initial_migration/advanced_migration_generator.py:662:4: R0912: Too many branches (23/12) (too-many-branches)
pybirdai/utils/speed_improvements_initial_migration/advanced_migration_generator.py:662:4: R0915: Too many statements (53/50) (too-many-statements)
pybirdai/utils/speed_improvements_initial_migration/advanced_migration_generator.py:773:8: R1705: Unnecessary "elif" after "return", remove the leading "el" from "elif" (no-else-return)
pybirdai/utils/speed_improvements_initial_migration/advanced_migration_generator.py:802:12: R1705: Unnecessary "else" after "return", remove the "else" and de-indent the code inside it (no-else-return)
pybirdai/utils/speed_improvements_initial_migration/advanced_migration_generator.py:820:37: W0621: Redefining name 'model' from outer scope (line 921) (redefined-outer-name)
pybirdai/utils/speed_improvements_initial_migration/advanced_migration_generator.py:844:38: W0621: Redefining name 'models' from outer scope (line 913) (redefined-outer-name)
pybirdai/utils/speed_improvements_initial_migration/advanced_migration_generator.py:849:34: W0621: Redefining name 'models' from outer scope (line 913) (redefined-outer-name)
pybirdai/utils/speed_improvements_initial_migration/advanced_migration_generator.py:849:59: W0621: Redefining name 'output_path' from outer scope (line 905) (redefined-outer-name)
pybirdai/utils/speed_improvements_initial_migration/advanced_migration_generator.py:851:8: W0621: Redefining name 'migration_code' from outer scope (line 934) (redefined-outer-name)
pybirdai/utils/speed_improvements_initial_migration/advanced_migration_generator.py:867:4: W0621: Redefining name 'generator' from outer scope (line 910) (redefined-outer-name)
pybirdai/utils/speed_improvements_initial_migration/advanced_migration_generator.py:868:4: W0621: Redefining name 'models' from outer scope (line 913) (redefined-outer-name)
pybirdai/utils/speed_improvements_initial_migration/advanced_migration_generator.py:880:4: W0621: Redefining name 'generator' from outer scope (line 910) (redefined-outer-name)
pybirdai/utils/speed_improvements_initial_migration/advanced_migration_generator.py:881:4: W0621: Redefining name 'models' from outer scope (line 913) (redefined-outer-name)
pybirdai/utils/speed_improvements_initial_migration/advanced_migration_generator.py:893:4: W0621: Redefining name 'generator' from outer scope (line 910) (redefined-outer-name)
pybirdai/utils/speed_improvements_initial_migration/advanced_migration_generator.py:894:4: W0621: Redefining name 'models' from outer scope (line 913) (redefined-outer-name)
pybirdai/utils/speed_improvements_initial_migration/advanced_migration_generator.py:23:0: W0611: Unused import re (unused-import)
pybirdai/utils/speed_improvements_initial_migration/advanced_migration_generator.py:24:0: W0611: Unused Dict imported from typing (unused-import)
pybirdai/utils/speed_improvements_initial_migration/advanced_migration_generator.py:24:0: W0611: Unused Union imported from typing (unused-import)
************* Module pybirdai.utils.speed_improvements_initial_migration.artifact_fetcher
pybirdai/utils/speed_improvements_initial_migration/artifact_fetcher.py:42:0: R0902: Too many instance attributes (12/7) (too-many-instance-attributes)
pybirdai/utils/speed_improvements_initial_migration/artifact_fetcher.py:66:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/speed_improvements_initial_migration/artifact_fetcher.py:67:19: W3101: Missing timeout argument for method 'requests.get' can cause your program to hang indefinitely (missing-timeout)
pybirdai/utils/speed_improvements_initial_migration/artifact_fetcher.py:68:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/speed_improvements_initial_migration/artifact_fetcher.py:72:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/speed_improvements_initial_migration/artifact_fetcher.py:82:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/speed_improvements_initial_migration/artifact_fetcher.py:88:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/speed_improvements_initial_migration/artifact_fetcher.py:89:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/speed_improvements_initial_migration/artifact_fetcher.py:91:19: W3101: Missing timeout argument for method 'requests.get' can cause your program to hang indefinitely (missing-timeout)
pybirdai/utils/speed_improvements_initial_migration/artifact_fetcher.py:92:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/speed_improvements_initial_migration/artifact_fetcher.py:118:15: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/utils/speed_improvements_initial_migration/artifact_fetcher.py:101:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/speed_improvements_initial_migration/artifact_fetcher.py:114:31: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/utils/speed_improvements_initial_migration/artifact_fetcher.py:109:32: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/speed_improvements_initial_migration/artifact_fetcher.py:112:28: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/speed_improvements_initial_migration/artifact_fetcher.py:115:28: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/speed_improvements_initial_migration/artifact_fetcher.py:119:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/speed_improvements_initial_migration/artifact_fetcher.py:127:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/speed_improvements_initial_migration/artifact_fetcher.py:131:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/speed_improvements_initial_migration/artifact_fetcher.py:140:15: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/utils/speed_improvements_initial_migration/artifact_fetcher.py:134:17: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/utils/speed_improvements_initial_migration/artifact_fetcher.py:138:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/speed_improvements_initial_migration/artifact_fetcher.py:141:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/speed_improvements_initial_migration/artifact_fetcher.py:146:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/speed_improvements_initial_migration/artifact_fetcher.py:154:15: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/utils/speed_improvements_initial_migration/artifact_fetcher.py:150:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/speed_improvements_initial_migration/artifact_fetcher.py:152:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/speed_improvements_initial_migration/artifact_fetcher.py:155:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/speed_improvements_initial_migration/artifact_fetcher.py:158:4: R0914: Too many local variables (23/15) (too-many-locals)
pybirdai/utils/speed_improvements_initial_migration/artifact_fetcher.py:164:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/speed_improvements_initial_migration/artifact_fetcher.py:173:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/speed_improvements_initial_migration/artifact_fetcher.py:177:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/speed_improvements_initial_migration/artifact_fetcher.py:187:20: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/speed_improvements_initial_migration/artifact_fetcher.py:190:20: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/speed_improvements_initial_migration/artifact_fetcher.py:193:20: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/speed_improvements_initial_migration/artifact_fetcher.py:197:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/speed_improvements_initial_migration/artifact_fetcher.py:200:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/speed_improvements_initial_migration/artifact_fetcher.py:249:19: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/utils/speed_improvements_initial_migration/artifact_fetcher.py:205:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/speed_improvements_initial_migration/artifact_fetcher.py:217:20: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/speed_improvements_initial_migration/artifact_fetcher.py:223:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/speed_improvements_initial_migration/artifact_fetcher.py:235:20: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/speed_improvements_initial_migration/artifact_fetcher.py:240:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/speed_improvements_initial_migration/artifact_fetcher.py:250:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/speed_improvements_initial_migration/artifact_fetcher.py:158:4: R0912: Too many branches (15/12) (too-many-branches)
pybirdai/utils/speed_improvements_initial_migration/artifact_fetcher.py:158:4: R0915: Too many statements (56/50) (too-many-statements)
pybirdai/utils/speed_improvements_initial_migration/artifact_fetcher.py:14:0: C0411: standard import "json" should be placed before third party import "requests" (wrong-import-order)
pybirdai/utils/speed_improvements_initial_migration/artifact_fetcher.py:15:0: C0411: standard import "logging" should be placed before third party import "requests" (wrong-import-order)
pybirdai/utils/speed_improvements_initial_migration/artifact_fetcher.py:16:0: C0411: standard import "dataclasses.dataclass" should be placed before third party import "requests" (wrong-import-order)
pybirdai/utils/speed_improvements_initial_migration/artifact_fetcher.py:17:0: C0411: standard import "typing.List" should be placed before third party import "requests" (wrong-import-order)
pybirdai/utils/speed_improvements_initial_migration/artifact_fetcher.py:18:0: C0411: standard import "hashlib" should be placed before third party import "requests" (wrong-import-order)
pybirdai/utils/speed_improvements_initial_migration/artifact_fetcher.py:19:0: C0411: standard import "zipfile" should be placed before third party import "requests" (wrong-import-order)
pybirdai/utils/speed_improvements_initial_migration/artifact_fetcher.py:20:0: C0411: standard import "io" should be placed before third party import "requests" (wrong-import-order)
pybirdai/utils/speed_improvements_initial_migration/artifact_fetcher.py:21:0: C0411: standard import "collections.defaultdict" should be placed before third party import "requests" (wrong-import-order)
pybirdai/utils/speed_improvements_initial_migration/artifact_fetcher.py:22:0: C0411: standard import "os" should be placed before third party import "requests" (wrong-import-order)
pybirdai/utils/speed_improvements_initial_migration/artifact_fetcher.py:14:0: W0611: Unused import json (unused-import)
pybirdai/utils/speed_improvements_initial_migration/artifact_fetcher.py:18:0: W0611: Unused import hashlib (unused-import)
************* Module pybirdai.utils.speed_improvements_initial_migration.derived_fields_extractor
pybirdai/utils/speed_improvements_initial_migration/derived_fields_extractor.py:47:9: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/utils/speed_improvements_initial_migration/derived_fields_extractor.py:65:28: R0916: Too many boolean expressions in if statement (9/5) (too-many-boolean-expressions)
pybirdai/utils/speed_improvements_initial_migration/derived_fields_extractor.py:53:4: R1702: Too many nested blocks (6/5) (too-many-nested-blocks)
pybirdai/utils/speed_improvements_initial_migration/derived_fields_extractor.py:136:16: W0612: Unused variable 'func_def' (unused-variable)
pybirdai/utils/speed_improvements_initial_migration/derived_fields_extractor.py:165:9: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/utils/speed_improvements_initial_migration/derived_fields_extractor.py:177:0: R0914: Too many local variables (21/15) (too-many-locals)
pybirdai/utils/speed_improvements_initial_migration/derived_fields_extractor.py:207:9: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/utils/speed_improvements_initial_migration/derived_fields_extractor.py:210:9: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/utils/speed_improvements_initial_migration/derived_fields_extractor.py:227:28: R0916: Too many boolean expressions in if statement (9/5) (too-many-boolean-expressions)
pybirdai/utils/speed_improvements_initial_migration/derived_fields_extractor.py:218:4: R1702: Too many nested blocks (6/5) (too-many-nested-blocks)
pybirdai/utils/speed_improvements_initial_migration/derived_fields_extractor.py:266:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/speed_improvements_initial_migration/derived_fields_extractor.py:277:16: R1724: Unnecessary "elif" after "continue", remove the leading "el" from "elif" (no-else-continue)
pybirdai/utils/speed_improvements_initial_migration/derived_fields_extractor.py:263:4: R1702: Too many nested blocks (6/5) (too-many-nested-blocks)
pybirdai/utils/speed_improvements_initial_migration/derived_fields_extractor.py:303:9: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/utils/speed_improvements_initial_migration/derived_fields_extractor.py:306:4: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/speed_improvements_initial_migration/derived_fields_extractor.py:177:0: R0912: Too many branches (18/12) (too-many-branches)
pybirdai/utils/speed_improvements_initial_migration/derived_fields_extractor.py:328:4: W0612: Unused variable 'logger' (unused-variable)
pybirdai/utils/speed_improvements_initial_migration/derived_fields_extractor.py:21:0: C0411: standard import "sys" should be placed before third party imports "django", "django.db.models", "django.conf.settings" (wrong-import-order)
pybirdai/utils/speed_improvements_initial_migration/derived_fields_extractor.py:23:0: C0411: standard import "logging" should be placed before third party imports "django", "django.db.models", "django.conf.settings" (wrong-import-order)
pybirdai/utils/speed_improvements_initial_migration/derived_fields_extractor.py:14:0: W0611: Unused import inspect (unused-import)
pybirdai/utils/speed_improvements_initial_migration/derived_fields_extractor.py:15:0: W0611: Unused import argparse (unused-import)
pybirdai/utils/speed_improvements_initial_migration/derived_fields_extractor.py:19:0: W0611: Unused models imported from django.db (unused-import)
************* Module pybirdai.utils.datapoint_test_run.generator_delete_fixtures
pybirdai/utils/datapoint_test_run/generator_delete_fixtures.py:39:13: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/utils/datapoint_test_run/generator_delete_fixtures.py:43:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/datapoint_test_run/generator_delete_fixtures.py:57:21: W0123: Use of eval (eval-used)
pybirdai/utils/datapoint_test_run/generator_delete_fixtures.py:65:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/datapoint_test_run/generator_delete_fixtures.py:69:18: R1734: Consider using [] instead of list() (use-list-literal)
pybirdai/utils/datapoint_test_run/generator_delete_fixtures.py:71:29: C0321: More than one statement on a single line (multiple-statements)
pybirdai/utils/datapoint_test_run/generator_delete_fixtures.py:88:15: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/utils/datapoint_test_run/generator_delete_fixtures.py:89:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/datapoint_test_run/generator_delete_fixtures.py:90:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/datapoint_test_run/generator_delete_fixtures.py:95:9: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/utils/datapoint_test_run/generator_delete_fixtures.py:15:0: W0611: Unused import os (unused-import)
pybirdai/utils/datapoint_test_run/generator_delete_fixtures.py:19:0: W0611: Unused Path imported from pathlib (unused-import)
************* Module pybirdai.utils.datapoint_test_run.generate_technical_test_report
pybirdai/utils/datapoint_test_run/generate_technical_test_report.py:17:0: W0105: String statement has no effect (pointless-string-statement)
pybirdai/utils/datapoint_test_run/generate_technical_test_report.py:14:0: W0611: Unused import json (unused-import)
pybirdai/utils/datapoint_test_run/generate_technical_test_report.py:15:0: W0611: Unused datetime imported from datetime (unused-import)
************* Module pybirdai.utils.datapoint_test_run.generator_for_tests
pybirdai/utils/datapoint_test_run/generator_for_tests.py:49:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/datapoint_test_run/generator_for_tests.py:56:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/datapoint_test_run/generator_for_tests.py:69:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/datapoint_test_run/generator_for_tests.py:72:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/datapoint_test_run/generator_for_tests.py:80:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/datapoint_test_run/generator_for_tests.py:88:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/datapoint_test_run/generator_for_tests.py:66:20: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/datapoint_test_run/generator_for_tests.py:70:12: C0415: Import outside toplevel (django.conf.settings) (import-outside-toplevel)
pybirdai/utils/datapoint_test_run/generator_for_tests.py:74:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/datapoint_test_run/generator_for_tests.py:76:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/datapoint_test_run/generator_for_tests.py:77:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/datapoint_test_run/generator_for_tests.py:78:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/datapoint_test_run/generator_for_tests.py:79:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/datapoint_test_run/generator_for_tests.py:70:12: W0611: Unused settings imported from django.conf (unused-import)
pybirdai/utils/datapoint_test_run/generator_for_tests.py:405:65: W0621: Redefining name 'logger' from outer scope (line 29) (redefined-outer-name)
pybirdai/utils/datapoint_test_run/generator_for_tests.py:415:13: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/utils/datapoint_test_run/generator_for_tests.py:430:8: W0621: Redefining name 'logger' from outer scope (line 29) (redefined-outer-name)
pybirdai/utils/datapoint_test_run/generator_for_tests.py:434:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/datapoint_test_run/generator_for_tests.py:443:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/datapoint_test_run/generator_for_tests.py:438:8: W0612: Unused variable 'regulatory_template_id' (unused-variable)
************* Module pybirdai.utils.datapoint_test_run.run_tests
pybirdai/utils/datapoint_test_run/run_tests.py:326:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/datapoint_test_run/run_tests.py:361:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/datapoint_test_run/run_tests.py:370:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/datapoint_test_run/run_tests.py:371:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/utils/datapoint_test_run/run_tests.py:73:0: R0205: Class 'FakeArgs' inherits from object, can be safely removed from bases in python3 (useless-object-inheritance)
pybirdai/utils/datapoint_test_run/run_tests.py:180:15: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/utils/datapoint_test_run/run_tests.py:175:17: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/utils/datapoint_test_run/run_tests.py:182:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/datapoint_test_run/run_tests.py:204:15: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/utils/datapoint_test_run/run_tests.py:199:21: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/utils/datapoint_test_run/run_tests.py:200:20: W1510: 'subprocess.run' used without explicitly defining the value for 'check'. (subprocess-run-check)
pybirdai/utils/datapoint_test_run/run_tests.py:202:16: W1510: 'subprocess.run' used without explicitly defining the value for 'check'. (subprocess-run-check)
pybirdai/utils/datapoint_test_run/run_tests.py:205:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/datapoint_test_run/run_tests.py:250:15: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/utils/datapoint_test_run/run_tests.py:223:17: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/utils/datapoint_test_run/run_tests.py:251:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/datapoint_test_run/run_tests.py:254:4: R0914: Too many local variables (21/15) (too-many-locals)
pybirdai/utils/datapoint_test_run/run_tests.py:273:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/datapoint_test_run/run_tests.py:274:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/datapoint_test_run/run_tests.py:324:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/datapoint_test_run/run_tests.py:334:8: W0612: Unused variable 'joined_path' (unused-variable)
pybirdai/utils/datapoint_test_run/run_tests.py:358:15: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/utils/datapoint_test_run/run_tests.py:356:17: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/utils/datapoint_test_run/run_tests.py:359:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/datapoint_test_run/run_tests.py:388:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/datapoint_test_run/run_tests.py:402:23: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/utils/datapoint_test_run/run_tests.py:403:20: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/datapoint_test_run/run_tests.py:407:8: C0415: Import outside toplevel (pybirdai.utils.datapoint_test_run.generate_test_url.main) (import-outside-toplevel)
pybirdai/utils/datapoint_test_run/run_tests.py:465:19: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/utils/datapoint_test_run/run_tests.py:466:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/datapoint_test_run/run_tests.py:470:62: W0123: Use of eval (eval-used)
pybirdai/utils/datapoint_test_run/run_tests.py:477:16: W0123: Use of eval (eval-used)
pybirdai/utils/datapoint_test_run/run_tests.py:27:0: W0611: Unused Path imported from pathlib (unused-import)
************* Module pybirdai.utils.datapoint_test_run.generate_test_url
pybirdai/utils/datapoint_test_run/generate_test_url.py:20:0: R0914: Too many local variables (16/15) (too-many-locals)
pybirdai/utils/datapoint_test_run/generate_test_url.py:33:13: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/utils/datapoint_test_run/generate_test_url.py:80:4: W0612: Unused variable 'base_url' (unused-variable)
************* Module pybirdai.utils.datapoint_test_run.parser_for_tests
pybirdai/utils/datapoint_test_run/parser_for_tests.py:28:0: C0413: Import "from datetime import datetime" should be placed at the top of the module (wrong-import-position)
pybirdai/utils/datapoint_test_run/parser_for_tests.py:102:17: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/utils/datapoint_test_run/parser_for_tests.py:105:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/datapoint_test_run/parser_for_tests.py:125:19: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/utils/datapoint_test_run/parser_for_tests.py:126:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/datapoint_test_run/parser_for_tests.py:169:20: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/datapoint_test_run/parser_for_tests.py:266:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/utils/datapoint_test_run/parser_for_tests.py:17:0: W0611: Unused Path imported from pathlib (unused-import)
pybirdai/utils/datapoint_test_run/parser_for_tests.py:1:0: R0801: Similar lines in 2 files
==pybirdai.utils.advanced_migration_generator:[28:935]
==pybirdai.utils.speed_improvements_initial_migration.advanced_migration_generator:[28:935]
@dataclass
class FieldInfo:
    """Represents a Django model field."""
    name: str
    field_type: str
    verbose_name: Optional[str] = None
    max_length: Optional[int] = None
    blank: bool = False
    null: bool = False
    default: Any = None
    primary_key: bool = False
    choices: Optional[List[Tuple[str, str]]] = None
    db_comment: Optional[str] = None
    foreign_key_to: Optional[str] = None
    on_delete: Optional[str] = None
    related_name: Optional[str] = None
    auto_created: bool = False
    serialize: bool = True
    unique: bool = False
    db_index: bool = False
    editable: bool = True
    help_text: Optional[str] = None
    parent_link: bool = False


@dataclass
class ModelInfo:
    """Represents a Django model."""
    name: str
    fields: List[FieldInfo]
    verbose_name: Optional[str] = None
    verbose_name_plural: Optional[str] = None
    db_table: Optional[str] = None
    ordering: Optional[List[str]] = None
    abstract: bool = False
    parent_model: Optional[str] = None
    bases: List[str] = None

    def __post_init__(self):
        if self.bases is None:
            self.bases = []


class ModelParser(ast.NodeVisitor):
    """AST visitor to parse Django model definitions from source code."""

    def __init__(self):
        self.models = []
        self.current_model = None
        self.current_class_name = None
        self.choice_domains = {}
        self.all_classes = {}  # Track all classes for inheritance detection
        self.potential_models = {}  # Store potential models for second pass

    def visit_ClassDef(self, node: ast.ClassDef):
        """Visit class definitions to find Django models."""
        self.current_class_name = node.name

        # Store all classes for inheritance detection
        self.all_classes[node.name] = node

        # Store potential models for two-pass processing
        self.potential_models[node.name] = node

        # Check for choice domains (dictionaries with domain suffix)
        if node.name.endswith('_domain'):
            self._parse_choice_domain(node)

        self.generic_visit(node)

    def process_models(self):
        """Process all potential models in two passes to handle inheritance."""
        # First pass: identify direct models.Model inheritors
        direct_models = set()

        for class_name, node in self.potential_models.items():
            for base in node.bases:
                if (isinstance(base, ast.Attribute) and isinstance(base.value, ast.Name) and
                    base.value.id == 'models' and base.attr == 'Model'):
                    direct_models.add(class_name)
                    self._parse_model(node, parent_model=None)
                    break

        # Second pass: identify model inheritors (subclasses of existing models)
        for class_name, node in self.potential_models.items():
            if class_name in direct_models:
                continue  # Already processed

            for base in node.bases:
                parent_model = None
                if isinstance(base, ast.Name):
                    # Direct class name (e.g., ADVNC)
                    if base.id in direct_models or base.id.isupper():
                        parent_model = base.id
                        self._parse_model(node, parent_model=parent_model)
                        break
                elif isinstance(base, ast.Attribute) and isinstance(base.value, ast.Name):
                    # Qualified name (e.g., app.Model)
                    base_name = base.attr
                    if base_name in direct_models or base_name.isupper():
                        parent_model = base_name
                        self._parse_model(node, parent_model=parent_model)
                        break

    def _parse_model(self, node: ast.ClassDef, parent_model: Optional[str] = None):
        """Parse a single model class definition."""
        bases = []
        for base in node.bases:
            if isinstance(base, ast.Attribute) and isinstance(base.value, ast.Name):
                bases.append(f'{base.value.id}.{base.attr}')
            elif isinstance(base, ast.Name):
                bases.append(base.id)

        self.current_model = ModelInfo(name=node.name, fields=[], parent_model=parent_model, bases=bases)

        # Parse class body
        for item in node.body:
            if isinstance(item, ast.Assign):
                self._parse_field_assignment(item)
            elif isinstance(item, ast.ClassDef) and item.name == 'Meta':
                self._parse_meta_class(item)

        self.models.append(self.current_model)
        self.current_model = None

    def visit_Assign(self, node: ast.Assign):
        """Visit assignments to find choice domains."""
        for target in node.targets:
            if isinstance(target, ast.Name) and target.id.endswith('_domain'):
                if isinstance(node.value, ast.Dict):
                    choices = []
                    for key, value in zip(node.value.keys, node.value.values):
                        if isinstance(key, ast.Constant) and isinstance(value, ast.Constant):
                            choices.append((str(key.value), str(value.value)))
                    self.choice_domains[target.id] = choices

        self.generic_visit(node)

    def _parse_choice_domain(self, node: ast.ClassDef):
        """Parse choice domain from class definition."""
        # This is for when domains are defined as class attributes
        pass

    def _parse_field_assignment(self, node: ast.Assign):
        """Parse field assignments in model classes."""
        if not self.current_model:
            return

        for target in node.targets:
            if isinstance(target, ast.Name):
                field_name = target.id

                # Skip non-field attributes
                if field_name.startswith('_') or field_name in ['DoesNotExist', 'MultipleObjectsReturned']:
                    continue

                field_info = self._parse_field_call(field_name, node.value)
                if field_info:
                    # Check for duplicate field names
                    existing_field_names = [f.name for f in self.current_model.fields]
                    if field_info.name not in existing_field_names:
                        self.current_model.fields.append(field_info)

    def _parse_field_call(self, field_name: str, node: ast.AST) -> Optional[FieldInfo]:
        """Parse field call to extract field information."""
        if not isinstance(node, ast.Call):
            return None

        field_info = FieldInfo(name=field_name, field_type='CharField')

        # Determine field type
        if isinstance(node.func, ast.Attribute):
            if isinstance(node.func.value, ast.Name) and node.func.value.id == 'models':
                field_info.field_type = node.func.attr

        # Parse arguments
        for i, arg in enumerate(node.args):
            if i == 0 and isinstance(arg, ast.Constant):
                # For ForeignKey{os.sep}OneToOneField, first positional arg is 'to'
                if field_info.field_type in ['ForeignKey', 'OneToOneField']:
                    field_info.foreign_key_to = str(arg.value)
                else:
                    field_info.verbose_name = str(arg.value)

        # Parse keyword arguments
        for keyword in node.keywords:
            if keyword.arg == 'max_length' and isinstance(keyword.value, ast.Constant):
                field_info.max_length = int(keyword.value.value)
            elif keyword.arg == 'blank' and isinstance(keyword.value, ast.Constant):
                field_info.blank = bool(keyword.value.value)
            elif keyword.arg == 'null' and isinstance(keyword.value, ast.Constant):
                field_info.null = bool(keyword.value.value)
            elif keyword.arg == 'default':
                if isinstance(keyword.value, ast.Constant):
                    field_info.default = keyword.value.value
                elif isinstance(keyword.value, ast.Name) and keyword.value.id == 'None':
                    field_info.default = None
            elif keyword.arg == 'primary_key' and isinstance(keyword.value, ast.Constant):
                field_info.primary_key = bool(keyword.value.value)
            elif keyword.arg == 'choices':
                field_info.choices = self._parse_choices(keyword.value)
            elif keyword.arg == 'db_comment' and isinstance(keyword.value, ast.Constant):
                field_info.db_comment = str(keyword.value.value)
            elif keyword.arg == 'to' and isinstance(keyword.value, ast.Constant):
                field_info.foreign_key_to = str(keyword.value.value)
            elif keyword.arg == 'on_delete':
                field_info.on_delete = self._parse_on_delete(keyword.value)
            elif keyword.arg == 'related_name' and isinstance(keyword.value, ast.Constant):
                field_info.related_name = str(keyword.value.value)
            elif keyword.arg == 'auto_created' and isinstance(keyword.value, ast.Constant):
                field_info.auto_created = bool(keyword.value.value)
            elif keyword.arg == 'serialize' and isinstance(keyword.value, ast.Constant):
                field_info.serialize = bool(keyword.value.value)
            elif keyword.arg == 'unique' and isinstance(keyword.value, ast.Constant):
                field_info.unique = bool(keyword.value.value)
            elif keyword.arg == 'db_index' and isinstance(keyword.value, ast.Constant):
                field_info.db_index = bool(keyword.value.value)
            elif keyword.arg == 'help_text' and isinstance(keyword.value, ast.Constant):
                field_info.help_text = str(keyword.value.value)
            elif keyword.arg == 'parent_link' and isinstance(keyword.value, ast.Constant):
                field_info.parent_link = bool(keyword.value.value)

        return field_info

    def _parse_choices(self, node: ast.AST) -> Optional[List[Tuple[str, str]]]:
        """Parse choices field."""
        if isinstance(node, ast.Name):
            # Reference to a domain variable
            domain_name = node.id
            if domain_name in self.choice_domains:
                return self.choice_domains[domain_name]
        elif isinstance(node, ast.List):
            # Inline choices list
            choices = []
            for item in node.elts:
                if isinstance(item, ast.Tuple) and len(item.elts) == 2:
                    key_node, value_node = item.elts
                    if isinstance(key_node, ast.Constant) and isinstance(value_node, ast.Constant):
                        choices.append((str(key_node.value), str(value_node.value)))
            return choices

        return None

    def _parse_on_delete(self, node: ast.AST) -> Optional[str]:
        """Parse on_delete parameter."""
        if isinstance(node, ast.Attribute):
            parts = []
            current = node
            while isinstance(current, ast.Attribute):
                parts.append(current.attr)
                current = current.value
            if isinstance(current, ast.Name):
                parts.append(current.id)
            # Return the full path to ensure complete on_delete reference
            full_path = '.'.join(reversed(parts))
            # Ensure we have the complete django.db.models.deletion.X format
            if 'django' in full_path and 'deletion' in full_path:
                return full_path
            elif full_path.startswith('models.deletion'):
                return f'django.db.{full_path}'
            elif full_path.endswith('CASCADE') or full_path.endswith('SET_NULL') or full_path.endswith('PROTECT'):
                if not full_path.startswith('django.db.models.deletion'):
                    return f'django.db.models.deletion.{full_path.split(".")[-1]}'
            return full_path
        return None

    def _parse_meta_class(self, node: ast.ClassDef):
        """Parse Meta class in Django models."""
        if not self.current_model:
            return

        for item in node.body:
            if isinstance(item, ast.Assign):
                for target in item.targets:
                    if isinstance(target, ast.Name):
                        if target.id == 'verbose_name' and isinstance(item.value, ast.Constant):
                            self.current_model.verbose_name = str(item.value.value)
                        elif target.id == 'verbose_name_plural' and isinstance(item.value, ast.Constant):
                            self.current_model.verbose_name_plural = str(item.value.value)
                        elif target.id == 'db_table' and isinstance(item.value, ast.Constant):
                            self.current_model.db_table = str(item.value.value)
                        elif target.id == 'ordering' and isinstance(item.value, ast.List):
                            ordering = []
                            for elt in item.value.elts:
                                if isinstance(elt, ast.Constant):
                                    ordering.append(str(elt.value))
                            self.current_model.ordering = ordering
                        elif target.id == 'abstract' and isinstance(item.value, ast.Constant):
                            self.current_model.abstract = bool(item.value.value)


class AdvancedMigrationGenerator:
    """Generates Django migration files from parsed model information using only AST."""

    def __init__(self):
        self.models = []

    def parse_file(self, file_path: str) -> List[ModelInfo]:
        """Parse a Python file to extract model definitions."""
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()

        # Parse the file
        tree = ast.parse(content)
        parser = ModelParser()
        parser.visit(tree)
        parser.process_models()  # Two-pass processing for inheritance

        return parser.models

    def parse_files(self, file_paths: List[str]) -> List[ModelInfo]:
        """Parse multiple Python files to extract model definitions."""
        all_models = []

        # Create a single parser for all files to handle cross-file inheritance
        parser = ModelParser()

        # First pass: collect all classes across all files
        for file_path in file_paths:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                tree = ast.parse(content)
                parser.visit(tree)
            except Exception as e:
                print(f"Error parsing {file_path}: {e}")

        # Process models with inheritance awareness
        parser.process_models()
        return parser.models

    def parse_directory(self, directory_path: str, pattern: str = "*.py") -> List[ModelInfo]:
        """Parse all Python files in a directory to extract model definitions."""
        directory = Path(directory_path)
        file_paths = [str(f) for f in directory.glob(pattern) if f.is_file()]
        return self.parse_files(file_paths)

    def generate_migration_ast(self, models: List[ModelInfo], migration_name: str = "0001_initial") -> ast.Module:
        """Generate migration AST from model information."""
        # Import statements
        import_nodes = [
            ast.Import(names=[ast.alias(name='django.db.models.deletion', asname=None)]),
            ast.Import(names=[ast.alias(name='django.utils.timezone', asname=None)]),
            ast.ImportFrom(module='django.db', names=[
                ast.alias(name='migrations', asname=None),
                ast.alias(name='models', asname=None)
            ], level=0)
        ]

        # Create operations list
        operations = []
        add_field_operations = []

        for model in models:
            if model.abstract:
                continue

            # Separate fields: non-ForeignKey fields for CreateModel, ForeignKey fields for AddField
            create_model_fields = []
            foreign_key_fields = []
            reverse_fields = []

            for field in model.fields:
                if self._is_reverse_field(field):
                    reverse_fields.append(field)
                elif field.field_type in ['ForeignKey', 'OneToOneField']:
                    foreign_key_fields.append(field)
                else:
                    create_model_fields.append(field)

            # Build fields list for CreateModel (only non-ForeignKey fields)
            fields_list = []

            # Add ID field if no primary key exists and this is not an inherited model
            has_primary_key = any(field.primary_key for field in create_model_fields + foreign_key_fields)
            if not has_primary_key and not model.parent_model:
                id_field = ast.Tuple(elts=[
                    ast.Constant(value='id'),
                    ast.Call(
                        func=ast.Attribute(value=ast.Name(id='models', ctx=ast.Load()), attr='BigAutoField', ctx=ast.Load()),
                        args=[],
                        keywords=[
                            ast.keyword(arg='auto_created', value=ast.Constant(value=True)),
                            ast.keyword(arg='default', value=ast.Constant(value=None)),
                            ast.keyword(arg='primary_key', value=ast.Constant(value=True)),
                            ast.keyword(arg='serialize', value=ast.Constant(value=False)),
                            ast.keyword(arg='verbose_name', value=ast.Constant(value='ID'))
                        ]
                    )
                ], ctx=ast.Load())
                fields_list.append(id_field)

            # Add non-ForeignKey model fields (check for duplicates)
            seen_field_names = set()
            for field in create_model_fields:
                if field.name not in seen_field_names:
                    field_tuple = self._generate_field_ast(field)
                    fields_list.append(field_tuple)
                    seen_field_names.add(field.name)

            # Create CreateModel call
            create_model_keywords = [
                ast.keyword(arg='name', value=ast.Constant(value=model.name)),
                ast.keyword(arg='fields', value=ast.List(elts=fields_list, ctx=ast.Load()))
            ]

            # Add options if present
            options = self._generate_options_dict(model)
            if options.keys or options.values:
                create_model_keywords.append(ast.keyword(arg='options', value=options))

            # Add bases if this is a model inheritance (subclass)
            if model.parent_model:
                # For inherited models, add parent_link field if not already present
                has_parent_link = any(field.name.endswith('_ptr') for field in create_model_fields + foreign_key_fields)
                if not has_parent_link:
                    parent_link_name = f"{model.parent_model.lower()}_ptr"
                    parent_link_field = ast.Tuple(elts=[
                        ast.Constant(value=parent_link_name),
                        ast.Call(
                            func=ast.Attribute(value=ast.Name(id='models', ctx=ast.Load()), attr='OneToOneField', ctx=ast.Load()),
                            args=[],
                            keywords=[
                                ast.keyword(arg='auto_created', value=ast.Constant(value=True)),
                                ast.keyword(arg='on_delete', value=ast.Attribute(
                                    value=ast.Attribute(
                                        value=ast.Attribute(
                                            value=ast.Attribute(
                                                value=ast.Name(id='django', ctx=ast.Load()),
                                                attr='db', ctx=ast.Load()),
                                            attr='models', ctx=ast.Load()),
                                        attr='deletion', ctx=ast.Load()),
                                    attr='CASCADE', ctx=ast.Load())),
                                ast.keyword(arg='parent_link', value=ast.Constant(value=True)),
                                ast.keyword(arg='primary_key', value=ast.Constant(value=True)),
                                ast.keyword(arg='serialize', value=ast.Constant(value=False)),
                                ast.keyword(arg='to', value=ast.Constant(value=f'pybirdai.{model.parent_model.lower()}'))
                            ]
                        )
                    ], ctx=ast.Load())
                    fields_list.insert(0, parent_link_field)

                # Add bases parameter as tuple (Django convention)
                bases_tuple = ast.Tuple(elts=[ast.Constant(value=f'pybirdai.{model.parent_model.lower()}')], ctx=ast.Load())
                create_model_keywords.append(ast.keyword(arg='bases', value=bases_tuple))

            create_model_call = ast.Call(
                func=ast.Attribute(value=ast.Name(id='migrations', ctx=ast.Load()), attr='CreateModel', ctx=ast.Load()),
                args=[],
                keywords=create_model_keywords
            )
            operations.append(create_model_call)

            # Create AddField operations for ForeignKey fields
            for field in foreign_key_fields:
                add_field_call = self._generate_add_field_ast(model.name, field)
                add_field_operations.append(add_field_call)

            # Create AddField operations for reverse relationship fields
            for field in reverse_fields:
                add_field_call = self._generate_add_field_ast(model.name, field)
                add_field_operations.append(add_field_call)

        # Add all operations (CreateModel first, then AddField)
        all_operations = operations + add_field_operations

        # Create Migration class
        migration_class = ast.ClassDef(
            name='Migration',
            bases=[ast.Attribute(value=ast.Name(id='migrations', ctx=ast.Load()), attr='Migration', ctx=ast.Load())],
            keywords=[],
            body=[
                ast.Assign(
                    targets=[ast.Name(id='initial', ctx=ast.Store())],
                    value=ast.Constant(value=True)
                ),
                ast.Assign(
                    targets=[ast.Name(id='dependencies', ctx=ast.Store())],
                    value=ast.List(elts=[], ctx=ast.Load())
                ),
                ast.Assign(
                    targets=[ast.Name(id='operations', ctx=ast.Store())],
                    value=ast.List(elts=all_operations, ctx=ast.Load())
                )
            ],
            decorator_list=[]
        )

        # Create module
        module = ast.Module(body=import_nodes + [migration_class], type_ignores=[])
        return module

    def _is_reverse_field(self, field: FieldInfo) -> bool:
        """Check if field should be added as a separate AddField operation (typically reverse relationships)."""
        # A field is a reverse field if it has CASCADE deletion and contains '_to_' in the field name
        # has_cascade_deletion = (field.on_delete and ('CASCADE' in field.on_delete or 'models.CASCADE' in field.on_delete))
        return False

    def _generate_field_ast(self, field: FieldInfo) -> ast.Tuple:
        """Generate AST node for field definition."""
        keywords = []

        # Parameters in logical order: auto_created, blank, default, max_length, null, primary_key, serialize, verbose_name, choices, etc.

        # Add auto_created first if applicable
        if field.auto_created:
            keywords.append(ast.keyword(arg='auto_created', value=ast.Constant(value=True)))

        # Add blank parameter
        if field.blank:
            keywords.append(ast.keyword(arg='blank', value=ast.Constant(value=True)))

        # Only add default for non-ForeignKey fields in CreateModel (avoid clutter)
        if field.field_type not in ['ForeignKey', 'OneToOneField']:
            default_value = field.default if field.default is not None else None
            if default_value is not None or field.null:
                keywords.append(ast.keyword(arg='default', value=ast.Constant(value=default_value)))

        # Add field-specific arguments like max_length
        if field.field_type == 'CharField' and field.max_length:
            keywords.append(ast.keyword(arg='max_length', value=ast.Constant(value=field.max_length)))

        # Add null parameter
        if field.null:
            keywords.append(ast.keyword(arg='null', value=ast.Constant(value=True)))

        # For ForeignKey fields, ensure required parameters are present
        if field.field_type in ['ForeignKey', 'OneToOneField']:
            # Add on_delete parameter (required for ForeignKey{os.sep}OneToOneField)
            if field.on_delete:
                on_delete_node = self._create_on_delete_ast(field.on_delete)
                keywords.append(ast.keyword(arg='on_delete', value=on_delete_node))
            else:
                # Choose appropriate default based on field characteristics
                # Use SET_NULL for nullable fields, CASCADE for required fields
                if field.null and not field.primary_key:
                    default_behavior = 'SET_NULL'
                else:
                    default_behavior = 'CASCADE'

                default_on_delete = ast.Attribute(
                    value=ast.Attribute(
                        value=ast.Attribute(
                            value=ast.Attribute(
                                value=ast.Name(id='django', ctx=ast.Load()),
                                attr='db', ctx=ast.Load()),
                            attr='models', ctx=ast.Load()),
                        attr='deletion', ctx=ast.Load()),
                    attr=default_behavior, ctx=ast.Load())
                keywords.append(ast.keyword(arg='on_delete', value=default_on_delete))

            # Add to parameter (required for ForeignKey{os.sep}OneToOneField)
            if field.foreign_key_to:
                keywords.append(ast.keyword(arg='to', value=ast.Constant(value="pybirdai."+field.foreign_key_to.lower())))
            else:
                # Try to infer from field name or use a placeholder
                inferred_to = f'pybirdai.{field.name.lower().replace("_", "")}'
                keywords.append(ast.keyword(arg='to', value=ast.Constant(value="pybirdai."+inferred_to.lower())))
        else:
            # Add on_delete for other field types if present
            if field.on_delete:
                on_delete_node = self._create_on_delete_ast(field.on_delete)
                keywords.append(ast.keyword(arg='on_delete', value=on_delete_node))

        # Add primary_key parameter
        if field.primary_key:
            keywords.append(ast.keyword(arg='primary_key', value=ast.Constant(value=True)))

        # Always explicitly set serialize=False for primary keys
        if field.primary_key or not field.serialize:
            keywords.append(ast.keyword(arg='serialize', value=ast.Constant(value=False)))

        # Add to parameter for non-ForeignKey fields if present
        if field.foreign_key_to and field.field_type not in ['ForeignKey', 'OneToOneField']:
            keywords.append(ast.keyword(arg='to', value=ast.Constant(value="pybirdai."+field.foreign_key_to.lower())))

        # Add other special parameters
        if field.parent_link:
            keywords.append(ast.keyword(arg='parent_link', value=ast.Constant(value=True)))

        # Add verbose_name as named parameter
        if field.verbose_name:
            keywords.append(ast.keyword(arg='verbose_name', value=ast.Constant(value=field.verbose_name)))

        # Add other parameters
        if field.choices:
            choices_list = []
            for k, v in field.choices:
                choice_tuple = ast.Tuple(elts=[ast.Constant(value=k), ast.Constant(value=v)], ctx=ast.Load())
                choices_list.append(choice_tuple)
            keywords.append(ast.keyword(arg='choices', value=ast.List(elts=choices_list, ctx=ast.Load())))

        if field.db_comment:
            keywords.append(ast.keyword(arg='db_comment', value=ast.Constant(value=field.db_comment)))

        if field.related_name:
            keywords.append(ast.keyword(arg='related_name', value=ast.Constant(value=field.related_name)))

        if field.unique:
            keywords.append(ast.keyword(arg='unique', value=ast.Constant(value=True)))

        if field.db_index:
            keywords.append(ast.keyword(arg='db_index', value=ast.Constant(value=True)))

        if field.help_text:
            keywords.append(ast.keyword(arg='help_text', value=ast.Constant(value=field.help_text)))

        # Create field call with no positional arguments, only named parameters
        field_call = ast.Call(
            func=ast.Attribute(value=ast.Name(id='models', ctx=ast.Load()), attr=field.field_type, ctx=ast.Load()),
            args=[],  # No positional arguments
            keywords=keywords
        )

        return ast.Tuple(elts=[ast.Constant(value=field.name), field_call], ctx=ast.Load())

    def _generate_add_field_ast(self, model_name: str, field: FieldInfo) -> ast.Call:
        """Generate AddField operation AST node for reverse relationship fields."""
        # Generate field definition
        field_call = self._generate_field_call_ast(field)

        add_field_call = ast.Call(
            func=ast.Attribute(value=ast.Name(id='migrations', ctx=ast.Load()), attr='AddField', ctx=ast.Load()),
            args=[],
            keywords=[
                ast.keyword(arg='model_name', value=ast.Constant(value=model_name.lower())),
                ast.keyword(arg='name', value=ast.Constant(value=field.name)),
                ast.keyword(arg='field', value=field_call)
            ]
        )

        return add_field_call

    def _generate_field_call_ast(self, field: FieldInfo) -> ast.Call:
        """Generate field call AST node for use in AddField operations."""
        keywords = []

        # Parameters in logical order: auto_created, blank, default, max_length, null, primary_key, serialize, verbose_name, choices, etc.

        # Add auto_created first if applicable
        if field.auto_created:
            keywords.append(ast.keyword(arg='auto_created', value=ast.Constant(value=True)))

        # Add blank parameter
        if field.blank:
            keywords.append(ast.keyword(arg='blank', value=ast.Constant(value=True)))

        # Add field-specific arguments like max_length
        if field.field_type == 'CharField' and field.max_length:
            keywords.append(ast.keyword(arg='max_length', value=ast.Constant(value=field.max_length)))

        # Add null parameter
        if field.null:
            keywords.append(ast.keyword(arg='null', value=ast.Constant(value=True)))

        # For ForeignKey fields, ensure required parameters are present
        if field.field_type in ['ForeignKey', 'OneToOneField']:
            # Add on_delete parameter (required for ForeignKey{os.sep}OneToOneField)
            if field.on_delete:
                on_delete_node = self._create_on_delete_ast(field.on_delete)
                keywords.append(ast.keyword(arg='on_delete', value=on_delete_node))
            else:
                # Choose appropriate default based on field characteristics
                # Use SET_NULL for nullable fields, CASCADE for required fields
                if field.null and not field.primary_key:
                    default_behavior = 'SET_NULL'
                else:
                    default_behavior = 'CASCADE'

                default_on_delete = ast.Attribute(
                    value=ast.Attribute(
                        value=ast.Attribute(
                            value=ast.Attribute(
                                value=ast.Name(id='django', ctx=ast.Load()),
                                attr='db', ctx=ast.Load()),
                            attr='models', ctx=ast.Load()),
                        attr='deletion', ctx=ast.Load()),
                    attr=default_behavior, ctx=ast.Load())
                keywords.append(ast.keyword(arg='on_delete', value=default_on_delete))

            # Add to parameter (required for ForeignKey{os.sep}OneToOneField)
            if field.foreign_key_to:
                keywords.append(ast.keyword(arg='to', value=ast.Constant(value="pybirdai."+field.foreign_key_to.lower())))
            else:
                # Try to infer from field name or use a placeholder
                inferred_to = f'pybirdai.{field.name.lower().replace("_", "")}'
                keywords.append(ast.keyword(arg='to', value=ast.Constant(value="pybirdai."+inferred_to.lower())))
        else:
            # Add on_delete for other field types if present
            if field.on_delete:
                on_delete_node = self._create_on_delete_ast(field.on_delete)
                keywords.append(ast.keyword(arg='on_delete', value=on_delete_node))

        # Add primary_key parameter
        if field.primary_key:
            keywords.append(ast.keyword(arg='primary_key', value=ast.Constant(value=True)))

        # Always explicitly set serialize=False for primary keys
        if field.primary_key or not field.serialize:
            keywords.append(ast.keyword(arg='serialize', value=ast.Constant(value=False)))

        # Add to parameter for non-ForeignKey fields if present
        if field.foreign_key_to and field.field_type not in ['ForeignKey', 'OneToOneField']:
            keywords.append(ast.keyword(arg='to', value=ast.Constant(value="pybirdai."+field.foreign_key_to.lower())))

        # Add verbose_name as named parameter
        if field.verbose_name:
            keywords.append(ast.keyword(arg='verbose_name', value=ast.Constant(value=field.verbose_name)))

        # Add other parameters
        if field.choices:
            choices_list = []
            for k, v in field.choices:
                choice_tuple = ast.Tuple(elts=[ast.Constant(value=k), ast.Constant(value=v)], ctx=ast.Load())
                choices_list.append(choice_tuple)
            keywords.append(ast.keyword(arg='choices', value=ast.List(elts=choices_list, ctx=ast.Load())))

        if field.db_comment:
            keywords.append(ast.keyword(arg='db_comment', value=ast.Constant(value=field.db_comment)))

        if field.related_name:
            keywords.append(ast.keyword(arg='related_name', value=ast.Constant(value=field.related_name)))

        if field.unique:
            keywords.append(ast.keyword(arg='unique', value=ast.Constant(value=True)))

        if field.db_index:
            keywords.append(ast.keyword(arg='db_index', value=ast.Constant(value=True)))

        if field.help_text:
            keywords.append(ast.keyword(arg='help_text', value=ast.Constant(value=field.help_text)))

        # Create field call with no positional arguments, only named parameters
        field_call = ast.Call(
            func=ast.Attribute(value=ast.Name(id='models', ctx=ast.Load()), attr=field.field_type, ctx=ast.Load()),
            args=[],  # No positional arguments
            keywords=keywords
        )

        return field_call

    def _create_on_delete_ast(self, on_delete_str: str) -> ast.AST:
        """Create AST node for on_delete parameter."""
        # Handle complete on_delete paths like 'django.db.models.deletion.CASCADE'
        if 'django.db.models.deletion' in on_delete_str:
            # Extract the specific deletion behavior (CASCADE, SET_NULL, etc.)
            parts = on_delete_str.split('.')
            deletion_behavior = parts[-1] if len(parts) > 1 else 'CASCADE'

            # Create the full AST: django.db.models.deletion.CASCADE
            return ast.Attribute(
                value=ast.Attribute(
                    value=ast.Attribute(
                        value=ast.Attribute(
                            value=ast.Name(id='django', ctx=ast.Load()),
                            attr='db', ctx=ast.Load()),
                        attr='models', ctx=ast.Load()),
                    attr='deletion', ctx=ast.Load()),
                attr=deletion_behavior, ctx=ast.Load())
        elif on_delete_str in ['CASCADE', 'SET_NULL', 'PROTECT', 'SET_DEFAULT', 'DO_NOTHING']:
            # Handle bare deletion behaviors - add full path
            return ast.Attribute(
                value=ast.Attribute(
                    value=ast.Attribute(
                        value=ast.Attribute(
                            value=ast.Name(id='django', ctx=ast.Load()),
                            attr='db', ctx=ast.Load()),
                        attr='models', ctx=ast.Load()),
                    attr='deletion', ctx=ast.Load()),
                attr=on_delete_str, ctx=ast.Load())
        else:
            # Try to parse the existing format
            parts = on_delete_str.split('.')
            if len(parts) == 1:
                # Single part - assume it's a deletion behavior
                return ast.Attribute(
                    value=ast.Attribute(
                        value=ast.Attribute(
                            value=ast.Attribute(
                                value=ast.Name(id='django', ctx=ast.Load()),
                                attr='db', ctx=ast.Load()),
                            attr='models', ctx=ast.Load()),
                        attr='deletion', ctx=ast.Load()),
                    attr=parts[0], ctx=ast.Load())
            else:
                # Multiple parts - reconstruct as attribute chain
                node = ast.Name(id=parts[0], ctx=ast.Load())
                for part in parts[1:]:
                    node = ast.Attribute(value=node, attr=part, ctx=ast.Load())
                return node

    def _generate_options_dict(self, model: ModelInfo) -> ast.Dict:
        """Generate model options dictionary as AST."""
        keys = []
        values = []

        if model.verbose_name:
            keys.append(ast.Constant(value='verbose_name'))
            values.append(ast.Constant(value=model.verbose_name))

        if model.verbose_name_plural:
            keys.append(ast.Constant(value='verbose_name_plural'))
            values.append(ast.Constant(value=model.verbose_name_plural))

        if model.db_table:
            keys.append(ast.Constant(value='db_table'))
            values.append(ast.Constant(value=model.db_table))

        if model.ordering:
            keys.append(ast.Constant(value='ordering'))
            ordering_list = ast.List(elts=[ast.Constant(value=item) for item in model.ordering], ctx=ast.Load())
            values.append(ordering_list)

        return ast.Dict(keys=keys, values=values)

    def generate_migration_code(self, models: List[ModelInfo], migration_name: str = "0001_initial") -> str:
        """Generate migration code from model information."""
        migration_ast = self.generate_migration_ast(models, migration_name)
        return ast.unparse(ast.fix_missing_locations(migration_ast))

    def save_migration_file(self, models: List[ModelInfo], output_path: str, migration_name: str = "0001_initial"):
        """Save migration file to disk."""
        migration_code = self.generate_migration_code(models, migration_name)

        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(migration_code)

        print(f"Migration saved to: {output_path}")


def generate_migration_from_file(source_file: str, output_file: str = "0001_initial.py"):
    """
    Convenience function to generate migration from a single source file.

    Args:
        source_file: Path to the Python file containing model definitions
        output_file: Path for the output migration file
    """
    generator = AdvancedMigrationGenerator()
    models = generator.parse_file(source_file)
    generator.save_migration_file(models, output_file)


def generate_migration_from_files(source_files: List[str], output_file: str = "0001_initial.py"):
    """
    Convenience function to generate migration from multiple source files.

    Args:
        source_files: List of paths to Python files containing model definitions
        output_file: Path for the output migration file
    """
    generator = AdvancedMigrationGenerator()
    models = generator.parse_files(source_files)
    generator.save_migration_file(models, output_file)


def generate_migration_from_directory(source_dir: str, output_file: str = "0001_initial.py"):
    """
    Convenience function to generate migration from all Python files in a directory.

    Args:
        source_dir: Path to the directory containing Python files with model definitions
        output_file: Path for the output migration file
    """
    generator = AdvancedMigrationGenerator()
    models = generator.parse_directory(source_dir)
    generator.save_migration_file(models, output_file)


# Example usage
if __name__ == '__main__':
    # Example: Generate migration from the initial_migration_generation_script.py
    import sys

    if len(sys.argv) > 1:
        source_path = sys.argv[1]
        output_path = sys.argv[2] if len(sys.argv) > 2 else "0001_initial.py"

        print(f"Generating migration from: {source_path}")
        print(f"Output file: {output_path}")

        generator = AdvancedMigrationGenerator()

        if os.path.isfile(source_path):
            models = generator.parse_file(source_path)
        elif os.path.isdir(source_path):
            models = generator.parse_directory(source_path)
        else:
            print(f"Invalid source path: {source_path}")
            sys.exit(1)

        print(f"Found {len(models)} models:")
        for model in models:
            parent_info = f" (inherits from {model.parent_model})" if model.parent_model else ""
            print(f"  - {model.name}{parent_info} with {len(model.fields)} fields")

        generator.save_migration_file(models, output_path)
    else:
        print("Usage: python advanced_migration_generator.py <source_file_or_directory> [output_file]")
        print("Example: python advanced_migration_generator.py models.py 0001_initial.py")
    generator = AdvancedMigrationGenerator()

    models = generator.parse_files([f"pybirdai{os.sep}models{os.sep}bird_data_model.py", f"pybirdai{os.sep}models{os.sep}bird_meta_data_model.py"])

    # Generate migration code
    migration_code = generator.generate_migration_code(models)
    generator.save_migration_file(models, f"pybirdai{os.sep}migrations{os.sep}0001_initial.py") (duplicate-code)
pybirdai/utils/datapoint_test_run/parser_for_tests.py:1:0: R0801: Similar lines in 2 files
==pybirdai.utils.derived_fields_extractor:[25:366]
==pybirdai.utils.speed_improvements_initial_migration.derived_fields_extractor:[25:366]
class DjangoSetup:
    @staticmethod
    def configure_django():
        """Configure Django settings without starting the application"""
        if not settings.configured:
            # Set up Django settings module for birds_nest in parent directory
            project_root = os.path.abspath(
                os.path.join(os.path.dirname(__file__), "../..")
            )
            sys.path.insert(0, project_root)
            os.environ["DJANGO_SETTINGS_MODULE"] = "birds_nest.settings"
            django.setup()


def extract_classes_with_lineage_properties(path: str):
    """Extract classes that have properties with 'lineage' decorator"""
    DjangoSetup.configure_django()

    classes_with_lineage = []

    # Parse the bird_data_model.py file using AST
    with open(path, "r") as f:
        source_code = f.read()

    tree = ast.parse(source_code)

    # Walk through the AST to find classes with lineage decorators
    for node in ast.walk(tree):
        if isinstance(node, ast.ClassDef):
            class_name = node.name
            lineage_properties = []

            # Check each method/property in the class
            for item in node.body:
                if isinstance(item, ast.FunctionDef):
                    # Check if it has decorators
                    for decorator in item.decorator_list:
                        # Check for lineage decorator
                        if (
                            (
                                isinstance(decorator, ast.Name)
                                and decorator.id == "lineage"
                            )
                            or (
                                isinstance(decorator, ast.Attribute)
                                and decorator.attr == "lineage"
                            )
                            or (
                                isinstance(decorator, ast.Call)
                                and (
                                    (
                                        isinstance(decorator.func, ast.Name)
                                        and decorator.func.id == "lineage"
                                    )
                                    or (
                                        isinstance(decorator.func, ast.Attribute)
                                        and decorator.func.attr == "lineage"
                                    )
                                )
                            )
                        ):
                            lineage_properties.append(item.name)
                            break

            if lineage_properties:
                class_info = {
                    "class_name": class_name,
                    "property_names": lineage_properties,
                    "class_node": node,
                }
                classes_with_lineage.append(class_info)

    return classes_with_lineage


def generate_ast_output(classes_info):
    """Generate AST representation of the extracted classes"""
    DjangoSetup.configure_django()

    # Create module node
    module = ast.Module(body=[], type_ignores=[])
    import_sttmt_1 = ast.ImportFrom(
        module="django.db", names=[ast.alias(name="models")], level=0
    )
    import_sttmt_2 = ast.ImportFrom(
        module="pybirdai.annotations.decorators",
        names=[ast.alias(name="lineage")],
        level=0,
    )

    module.body.append(import_sttmt_1)
    module.body.append(import_sttmt_2)

    for class_info in classes_info:
        class_node = class_info["class_node"]
        class_name = class_info["class_name"]

        # Create class definition
        class_def = ast.ClassDef(
            name=class_name,
            bases=class_node.bases,
            keywords=class_node.keywords,
            decorator_list=class_node.decorator_list,
            body=[],
        )

        # Add class body items
        for item in class_node.body:
            if isinstance(item, ast.FunctionDef):
                # Create function def
                func_def = ast.FunctionDef(
                    name=item.name,
                    args=item.args,
                    body=item.body,
                    decorator_list=item.decorator_list,
                    returns=item.returns,
                )
                class_def.body.append(item)
            elif isinstance(item, ast.ClassDef) and item.name == "Meta":
                # Add Meta class
                meta_class = ast.ClassDef(
                    name="Meta",
                    bases=item.bases,
                    keywords=item.keywords,
                    decorator_list=item.decorator_list,
                    body=[ast.Pass()],
                )
                class_def.body.append(meta_class)

        if not class_def.body:
            class_def.body.append(ast.Pass())

        module.body.append(class_def)

    return module


def check_if_file_already_modified(file_path):
    """Check if the file already has lineage imports and decorators"""
    with open(file_path, "r") as f:
        content = f.read()

    # Check for lineage import and @lineage decorator
    has_lineage_import = (
        "from pybirdai.annotations.decorators import lineage" in content
    )
    has_lineage_decorator = "@lineage" in content

    return has_lineage_import and has_lineage_decorator


def merge_derived_fields_into_original_model(
    bird_data_model_path, lineage_classes_ast_path
):
    """
    Merge derived fields from derived_field_configuration.py into the original bird_data_model.py.

    This function:
    1. Checks if the original file has already been modified (has @lineage imports/decorators)
    2. If not modified, processes each class to:
       - Remove existing fields that are overwritten by derived properties
       - Add derived properties before the Meta class and after all other fields
    3. Saves the modified file back to the same location

    Args:
        bird_data_model_path (str): Path to the original bird_data_model.py file
        lineage_classes_ast_path (str): Path to the derived_field_configuration.py file with derived fields

    Returns:
        bool: True if modifications were made, False if file was already modified
    """
    logger = logging.getLogger(__name__)

    # Check if file has already been modified
    if check_if_file_already_modified(bird_data_model_path):
        logger.info(
            "File already contains @lineage decorators and imports, skipping modification"
        )
        return False

    # Parse both files
    with open(bird_data_model_path, "r") as f:
        original_content = f.read()

    with open(lineage_classes_ast_path, "r") as f:
        lineage_content = f.read()

    original_tree = ast.parse(original_content)
    lineage_tree = ast.parse(lineage_content)

    # Extract derived classes and their properties from lineage file
    derived_classes = {}
    for node in ast.walk(lineage_tree):
        if isinstance(node, ast.ClassDef):
            class_name = node.name
            derived_properties = []
            # Find all properties with @lineage decorator
            for item in node.body:
                if isinstance(item, ast.FunctionDef):
                    for decorator in item.decorator_list:
                        if (
                            (
                                isinstance(decorator, ast.Name)
                                and decorator.id == "lineage"
                            )
                            or (
                                isinstance(decorator, ast.Attribute)
                                and decorator.attr == "lineage"
                            )
                            or (
                                isinstance(decorator, ast.Call)
                                and (
                                    (
                                        isinstance(decorator.func, ast.Name)
                                        and decorator.func.id == "lineage"
                                    )
                                    or (
                                        isinstance(decorator.func, ast.Attribute)
                                        and decorator.func.attr == "lineage"
                                    )
                                )
                            )
                        ):
                            derived_properties.append(item)
                            break
            if derived_properties:
                derived_classes[class_name] = derived_properties

    # Add lineage import to original file
    lineage_import = ast.ImportFrom(
        module="pybirdai.annotations.decorators",
        names=[ast.alias(name="lineage")],
        level=0,
    )
    original_tree.body.insert(0, lineage_import)

    # Process each class in the original file
    for node in ast.walk(original_tree):
        if isinstance(node, ast.ClassDef) and node.name in derived_classes:
            class_name = node.name
            logger.info(f"Processing class {class_name}")

            # Get the derived properties to know which fields to remove
            derived_properties = derived_classes[class_name]
            derived_property_names = [prop.name for prop in derived_properties]

            # Remove existing fields that are overwritten by derived properties
            new_body = []
            meta_class = None

            for item in node.body:
                if isinstance(item, ast.ClassDef) and item.name == "Meta":
                    # Store Meta class to add at the end
                    meta_class = item
                    continue
                elif isinstance(item, ast.Assign):
                    # Check if this is a field assignment that should be removed
                    if len(item.targets) == 1 and isinstance(item.targets[0], ast.Name):
                        field_name = item.targets[0].id
                        if field_name in derived_property_names:
                            # Skip this field as it will be replaced by a derived property
                            continue
                new_body.append(item)

            # Add the derived properties before the Meta class
            if class_name in derived_classes:
                for derived_property in derived_classes[class_name]:
                    new_body.append(derived_property)

            # Add Meta class at the end if it exists
            if meta_class:
                new_body.append(meta_class)

            node.body = new_body

    # Write the modified content back to the original file
    modified_content = ast.unparse(original_tree)
    with open(bird_data_model_path, "w") as f:
        f.write(modified_content)

    logger.info(f"Successfully modified {bird_data_model_path} with derived fields")
    return True


def main():
    """
    Main command-line interface for the derived fields extractor.

    Usage examples:
        # Extract and generate AST only
        python derived_fields_extractor.py path/to/bird_meta_data_model.py

        # Extract and save to database
        python derived_fields_extractor.py path/to/bird_meta_data_model.py --save-to-db
    """
    DjangoSetup.configure_django()

    # Configure logging
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    )
    logger = logging.getLogger(__name__)

    #     parser = argparse.ArgumentParser(
    #         description='Extract classes with lineage properties from a Python file and optionally save as cube structure items',
    #         epilog="""
    # Examples:
    #   %(prog)s bird_meta_data_model.py
    #   %(prog)s bird_meta_data_model.py --save-to-db
    #   %(prog)s bird_meta_data_model.py --save-to-db --cube-structure-id my_cube
    #         """,
    #         formatter_class=argparse.RawDescriptionHelpFormatter
    #     )
    #     parser.add_argument('file_path', help='Path to the Python file to analyze')
    #     parser.add_argument('--save-to-db', action='store_true', help='Save derived fields to database as cube structure items')
    #     args = parser.parse_args()

    # Extract classes with lineage properties
    # lineage_classes = extract_classes_with_lineage_properties(args.file_path)

    # # Generate AST
    # ast_module = generate_ast_output(lineage_classes)

    # # Write to file
    # os.makedirs("results/derivation_files/", exist_ok=True)
    # with open('results/derivation_files/derived_field_configuration.py', 'w') as f:
    #     f.write(ast.unparse(ast_module))

    # print(f"Extracted {len(lineage_classes)} classes with lineage properties")
    print("Output written to derived_field_configuration.py")
    model_file_path = f"pybirdai{os.sep}models{os.sep}bird_data_model.py"
    derived_fields_file_path = (
        f"resources{os.sep}derivation_files{os.sep}derived_field_configuration.py"
    )

    merge_derived_fields_into_original_model(model_file_path, derived_fields_file_path)


if __name__ == "__main__":
    main() (duplicate-code)
pybirdai/utils/datapoint_test_run/parser_for_tests.py:1:0: R0801: Similar lines in 2 files
==pybirdai.utils.clone_mode.export_with_ids:[68:129]
==pybirdai.utils.export_db:[83:145]
                model_class = model_map[table_name]

                # Check if model has an explicit primary key
                has_explicit_pk = any(field.primary_key for field in model_class._meta.fields if field.name != 'id')

                # Get fields in the order they're defined in the model
                fields = model_class._meta.fields
                headers = []
                db_headers = []

                # If model uses Django's auto ID and has no explicit PK, include the ID field
                if not has_explicit_pk:
                    headers.append('ID')
                    db_headers.append('id')

                for field in fields:
                    # Skip the id field if we already added it or if there's an explicit PK
                    if field.name == 'id' and has_explicit_pk:
                        continue
                    elif field.name == 'id' and not has_explicit_pk:
                        # We already added it above
                        continue

                    headers.append(field.name.upper())  # Convert header to uppercase
                    # If it's a foreign key, append _id for the actual DB column
                    if isinstance(field, models.ForeignKey):
                        db_headers.append(f"{field.name}_id")
                    else:
                        db_headers.append(field.name)

                # Create CSV in memory
                csv_content = []
                csv_content.append(','.join(headers))

                # Get data with escaped column names and ordered by primary key
                with connection.cursor() as cursor:
                    escaped_headers = [f'"{h}"' if h == 'order' else h for h in db_headers]
                    # Get primary key column name - table_name is validated against valid_table_names set
                    if table_name not in valid_table_names:
                        continue  # Skip unsafe table names
                    # SQLite PRAGMA doesn't support parameterized queries, but table name is validated above
                    cursor.execute(f"PRAGMA table_info({table_name})")
                    table_info = cursor.fetchall()
                    pk_columns = []

                    # Collect all primary key columns for composite keys
                    for col in table_info:
                        if col[5] == 1:  # 5 is the index for pk flag in table_info
                            pk_columns.append(col[1])  # 1 is the index for column name

                    # Build ORDER BY clause
                    if pk_columns:
                        order_by = f"ORDER BY {', '.join(pk_columns)}"
                    else:
                        # If no primary key, sort by id if it exists, otherwise by all columns
                        if 'id' in db_headers:
                            order_by = "ORDER BY id"
                        else:
                            order_by = f"ORDER BY {', '.join(escaped_headers)}"

                    # Table name validated above, column names come from model field definitions (duplicate-code)
pybirdai/utils/datapoint_test_run/parser_for_tests.py:1:0: R0801: Similar lines in 2 files
==pybirdai.utils.derived_fields_extractor:[222:248]
==pybirdai.utils.speed_improvements_initial_migration.derived_fields_extractor:[58:86]
            for item in node.body:
                if isinstance(item, ast.FunctionDef):
                    for decorator in item.decorator_list:
                        if (
                            (
                                isinstance(decorator, ast.Name)
                                and decorator.id == "lineage"
                            )
                            or (
                                isinstance(decorator, ast.Attribute)
                                and decorator.attr == "lineage"
                            )
                            or (
                                isinstance(decorator, ast.Call)
                                and (
                                    (
                                        isinstance(decorator.func, ast.Name)
                                        and decorator.func.id == "lineage"
                                    )
                                    or (
                                        isinstance(decorator.func, ast.Attribute)
                                        and decorator.func.attr == "lineage"
                                    )
                                )
                            )
                        ): (duplicate-code)
pybirdai/utils/datapoint_test_run/parser_for_tests.py:1:0: R0801: Similar lines in 2 files
==pybirdai.utils.derived_fields_extractor:[58:86]
==pybirdai.utils.speed_improvements_initial_migration.derived_fields_extractor:[222:248]
            for item in node.body:
                if isinstance(item, ast.FunctionDef):
                    # Check if it has decorators
                    for decorator in item.decorator_list:
                        # Check for lineage decorator
                        if (
                            (
                                isinstance(decorator, ast.Name)
                                and decorator.id == "lineage"
                            )
                            or (
                                isinstance(decorator, ast.Attribute)
                                and decorator.attr == "lineage"
                            )
                            or (
                                isinstance(decorator, ast.Call)
                                and (
                                    (
                                        isinstance(decorator.func, ast.Name)
                                        and decorator.func.id == "lineage"
                                    )
                                    or (
                                        isinstance(decorator.func, ast.Attribute)
                                        and decorator.func.attr == "lineage"
                                    )
                                )
                            )
                        ): (duplicate-code)
pybirdai/utils/datapoint_test_run/parser_for_tests.py:1:0: R0801: Similar lines in 2 files
==pybirdai.utils.advanced_migration_generator:[734:767]
==pybirdai.utils.speed_improvements_initial_migration.advanced_migration_generator:[609:642]
        if field.verbose_name:
            keywords.append(ast.keyword(arg='verbose_name', value=ast.Constant(value=field.verbose_name)))

        # Add other parameters
        if field.choices:
            choices_list = []
            for k, v in field.choices:
                choice_tuple = ast.Tuple(elts=[ast.Constant(value=k), ast.Constant(value=v)], ctx=ast.Load())
                choices_list.append(choice_tuple)
            keywords.append(ast.keyword(arg='choices', value=ast.List(elts=choices_list, ctx=ast.Load())))

        if field.db_comment:
            keywords.append(ast.keyword(arg='db_comment', value=ast.Constant(value=field.db_comment)))

        if field.related_name:
            keywords.append(ast.keyword(arg='related_name', value=ast.Constant(value=field.related_name)))

        if field.unique:
            keywords.append(ast.keyword(arg='unique', value=ast.Constant(value=True)))

        if field.db_index:
            keywords.append(ast.keyword(arg='db_index', value=ast.Constant(value=True)))

        if field.help_text:
            keywords.append(ast.keyword(arg='help_text', value=ast.Constant(value=field.help_text)))

        # Create field call with no positional arguments, only named parameters
        field_call = ast.Call(
            func=ast.Attribute(value=ast.Name(id='models', ctx=ast.Load()), attr=field.field_type, ctx=ast.Load()),
            args=[],  # No positional arguments
            keywords=keywords
        )
 (duplicate-code)
pybirdai/utils/datapoint_test_run/parser_for_tests.py:1:0: R0801: Similar lines in 2 files
==pybirdai.utils.advanced_migration_generator:[609:642]
==pybirdai.utils.speed_improvements_initial_migration.advanced_migration_generator:[734:767]
        if field.verbose_name:
            keywords.append(ast.keyword(arg='verbose_name', value=ast.Constant(value=field.verbose_name)))

        # Add other parameters
        if field.choices:
            choices_list = []
            for k, v in field.choices:
                choice_tuple = ast.Tuple(elts=[ast.Constant(value=k), ast.Constant(value=v)], ctx=ast.Load())
                choices_list.append(choice_tuple)
            keywords.append(ast.keyword(arg='choices', value=ast.List(elts=choices_list, ctx=ast.Load())))

        if field.db_comment:
            keywords.append(ast.keyword(arg='db_comment', value=ast.Constant(value=field.db_comment)))

        if field.related_name:
            keywords.append(ast.keyword(arg='related_name', value=ast.Constant(value=field.related_name)))

        if field.unique:
            keywords.append(ast.keyword(arg='unique', value=ast.Constant(value=True)))

        if field.db_index:
            keywords.append(ast.keyword(arg='db_index', value=ast.Constant(value=True)))

        if field.help_text:
            keywords.append(ast.keyword(arg='help_text', value=ast.Constant(value=field.help_text)))

        # Create field call with no positional arguments, only named parameters
        field_call = ast.Call(
            func=ast.Attribute(value=ast.Name(id='models', ctx=ast.Load()), attr=field.field_type, ctx=ast.Load()),
            args=[],  # No positional arguments
            keywords=keywords
        )
 (duplicate-code)
pybirdai/utils/datapoint_test_run/parser_for_tests.py:1:0: R0801: Similar lines in 2 files
==pybirdai.utils.member_hierarchy_editor.django_hierarchy_integration:[40:68]
==pybirdai.utils.member_hierarchy_editor.from_member_hierarchy_node_to_visualisation:[45:75]
        return cls(
            id_=str(dict_data.get('id', '')),
            x=dict_data.get('x', 0),
            y=dict_data.get('y', 0),
            width=dict_data.get('width', BOX_WIDTH),
            height=dict_data.get('height', BOX_HEIGHT),
            name=dict_data.get('name', ''),
            text=dict_data.get('text', '')
        )

    @property
    def to_dict(self) -> dict:
        """
        Convert to dictionary format expected by visualization tool
        """
        return {
            'id': self.id_,
            'x': self.x,
            'y': self.y,
            'width': self.width,
            'height': self.height,
            'name': self.name,
            'text': self.text
        }

    @property
    def to_json(self) -> str:
        return json.dumps(self.to_dict)

 (duplicate-code)
pybirdai/utils/datapoint_test_run/parser_for_tests.py:1:0: R0801: Similar lines in 2 files
==pybirdai.utils.advanced_migration_generator:[676:734]
==pybirdai.utils.speed_improvements_initial_migration.advanced_migration_generator:[547:605]
        if field.field_type == 'CharField' and field.max_length:
            keywords.append(ast.keyword(arg='max_length', value=ast.Constant(value=field.max_length)))

        # Add null parameter
        if field.null:
            keywords.append(ast.keyword(arg='null', value=ast.Constant(value=True)))

        # For ForeignKey fields, ensure required parameters are present
        if field.field_type in ['ForeignKey', 'OneToOneField']:
            # Add on_delete parameter (required for ForeignKey{os.sep}OneToOneField)
            if field.on_delete:
                on_delete_node = self._create_on_delete_ast(field.on_delete)
                keywords.append(ast.keyword(arg='on_delete', value=on_delete_node))
            else:
                # Choose appropriate default based on field characteristics
                # Use SET_NULL for nullable fields, CASCADE for required fields
                if field.null and not field.primary_key:
                    default_behavior = 'SET_NULL'
                else:
                    default_behavior = 'CASCADE'

                default_on_delete = ast.Attribute(
                    value=ast.Attribute(
                        value=ast.Attribute(
                            value=ast.Attribute(
                                value=ast.Name(id='django', ctx=ast.Load()),
                                attr='db', ctx=ast.Load()),
                            attr='models', ctx=ast.Load()),
                        attr='deletion', ctx=ast.Load()),
                    attr=default_behavior, ctx=ast.Load())
                keywords.append(ast.keyword(arg='on_delete', value=default_on_delete))

            # Add to parameter (required for ForeignKey{os.sep}OneToOneField)
            if field.foreign_key_to:
                keywords.append(ast.keyword(arg='to', value=ast.Constant(value="pybirdai."+field.foreign_key_to.lower())))
            else:
                # Try to infer from field name or use a placeholder
                inferred_to = f'pybirdai.{field.name.lower().replace("_", "")}'
                keywords.append(ast.keyword(arg='to', value=ast.Constant(value="pybirdai."+inferred_to.lower())))
        else:
            # Add on_delete for other field types if present
            if field.on_delete:
                on_delete_node = self._create_on_delete_ast(field.on_delete)
                keywords.append(ast.keyword(arg='on_delete', value=on_delete_node))

        # Add primary_key parameter
        if field.primary_key:
            keywords.append(ast.keyword(arg='primary_key', value=ast.Constant(value=True)))

        # Always explicitly set serialize=False for primary keys
        if field.primary_key or not field.serialize:
            keywords.append(ast.keyword(arg='serialize', value=ast.Constant(value=False)))

        # Add to parameter for non-ForeignKey fields if present
        if field.foreign_key_to and field.field_type not in ['ForeignKey', 'OneToOneField']:
            keywords.append(ast.keyword(arg='to', value=ast.Constant(value="pybirdai."+field.foreign_key_to.lower())))

        # Add other special parameters (duplicate-code)
pybirdai/utils/datapoint_test_run/parser_for_tests.py:1:0: R0801: Similar lines in 2 files
==pybirdai.utils.advanced_migration_generator:[547:605]
==pybirdai.utils.speed_improvements_initial_migration.advanced_migration_generator:[676:734]
        if field.field_type == 'CharField' and field.max_length:
            keywords.append(ast.keyword(arg='max_length', value=ast.Constant(value=field.max_length)))

        # Add null parameter
        if field.null:
            keywords.append(ast.keyword(arg='null', value=ast.Constant(value=True)))

        # For ForeignKey fields, ensure required parameters are present
        if field.field_type in ['ForeignKey', 'OneToOneField']:
            # Add on_delete parameter (required for ForeignKey{os.sep}OneToOneField)
            if field.on_delete:
                on_delete_node = self._create_on_delete_ast(field.on_delete)
                keywords.append(ast.keyword(arg='on_delete', value=on_delete_node))
            else:
                # Choose appropriate default based on field characteristics
                # Use SET_NULL for nullable fields, CASCADE for required fields
                if field.null and not field.primary_key:
                    default_behavior = 'SET_NULL'
                else:
                    default_behavior = 'CASCADE'

                default_on_delete = ast.Attribute(
                    value=ast.Attribute(
                        value=ast.Attribute(
                            value=ast.Attribute(
                                value=ast.Name(id='django', ctx=ast.Load()),
                                attr='db', ctx=ast.Load()),
                            attr='models', ctx=ast.Load()),
                        attr='deletion', ctx=ast.Load()),
                    attr=default_behavior, ctx=ast.Load())
                keywords.append(ast.keyword(arg='on_delete', value=default_on_delete))

            # Add to parameter (required for ForeignKey{os.sep}OneToOneField)
            if field.foreign_key_to:
                keywords.append(ast.keyword(arg='to', value=ast.Constant(value="pybirdai."+field.foreign_key_to.lower())))
            else:
                # Try to infer from field name or use a placeholder
                inferred_to = f'pybirdai.{field.name.lower().replace("_", "")}'
                keywords.append(ast.keyword(arg='to', value=ast.Constant(value="pybirdai."+inferred_to.lower())))
        else:
            # Add on_delete for other field types if present
            if field.on_delete:
                on_delete_node = self._create_on_delete_ast(field.on_delete)
                keywords.append(ast.keyword(arg='on_delete', value=on_delete_node))

        # Add primary_key parameter
        if field.primary_key:
            keywords.append(ast.keyword(arg='primary_key', value=ast.Constant(value=True)))

        # Always explicitly set serialize=False for primary keys
        if field.primary_key or not field.serialize:
            keywords.append(ast.keyword(arg='serialize', value=ast.Constant(value=False)))

        # Add to parameter for non-ForeignKey fields if present
        if field.foreign_key_to and field.field_type not in ['ForeignKey', 'OneToOneField']:
            keywords.append(ast.keyword(arg='to', value=ast.Constant(value="pybirdai."+field.foreign_key_to.lower())))

        # Add verbose_name as named parameter (duplicate-code)
pybirdai/utils/datapoint_test_run/parser_for_tests.py:1:0: R0801: Similar lines in 2 files
==pybirdai.utils.export_db:[24:55]
==pybirdai.utils.visualisation_service:[22:46]
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("visualization_service.log"),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

class DjangoSetup:
    @staticmethod
    def configure_django():
        """Configure Django settings without starting the application"""
        if not settings.configured:
            # Set up Django settings module for birds_nest in parent directory
            project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '../..'))
            sys.path.insert(0, project_root)
            os.environ['DJANGO_SETTINGS_MODULE'] = 'birds_nest.settings'
            logger.info("Configuring Django with settings module: %s", os.environ['DJANGO_SETTINGS_MODULE'])
            django.setup()
            logger.debug("Django setup complete")

def _export_database_to_csv_logic():
    import re
    from pybirdai.models import bird_meta_data_model
    from pybirdai.models import bird_data_model
    from django.db import transaction, connection
    from django.http import HttpResponse, JsonResponse, HttpResponseBadRequest
    def clean_whitespace(text): (duplicate-code)
pybirdai/utils/datapoint_test_run/parser_for_tests.py:1:0: R0801: Similar lines in 2 files
==pybirdai.utils.clone_mode.export_with_ids:[130:146]
==pybirdai.utils.export_db:[147:161]
                    rows = cursor.fetchall()

                    for row in rows:
                        # Convert all values to strings and handle None values
                        csv_row = [str(clean_whitespace(val)) if val is not None else '' for val in row]
                        # Escape commas and quotes in values
                        processed_row = []
                        for val in csv_row:
                            if ',' in val or '"' in val:
                                escaped_val = val.replace('"', '""')
                                processed_row.append(f'"{escaped_val}"')
                            else:
                                processed_row.append(val)
                        csv_content.append(','.join(processed_row))

                # Determine CSV filename (duplicate-code)
pybirdai/utils/datapoint_test_run/parser_for_tests.py:1:0: R0801: Similar lines in 2 files
==pybirdai.utils.member_hierarchy_editor.django_hierarchy_integration:[78:96]
==pybirdai.utils.member_hierarchy_editor.from_member_hierarchy_node_to_visualisation:[85:105]
        return cls(
            from_=str(dict_data.get("from", "")),
            to_=str(dict_data.get("to", ""))
        )

    @property
    def to_dict(self) -> dict:
        """
        Convert to dictionary format expected by visualization tool
        """
        return {
            "from": self.from_,
            "to": self.to_
        }

    @property
    def to_json(self) -> str:
        return json.dumps(self.to_dict)

 (duplicate-code)
pybirdai/utils/datapoint_test_run/parser_for_tests.py:1:0: R0801: Similar lines in 2 files
==pybirdai.utils.advanced_migration_generator:[779:786]
==pybirdai.utils.speed_improvements_initial_migration.advanced_migration_generator:[698:705]
                value=ast.Attribute(
                    value=ast.Attribute(
                        value=ast.Attribute(
                            value=ast.Name(id='django', ctx=ast.Load()),
                            attr='db', ctx=ast.Load()),
                        attr='models', ctx=ast.Load()),
                    attr='deletion', ctx=ast.Load()), (duplicate-code)
pybirdai/utils/datapoint_test_run/parser_for_tests.py:1:0: R0801: Similar lines in 2 files
==pybirdai.utils.advanced_migration_generator:[698:705]
==pybirdai.utils.speed_improvements_initial_migration.advanced_migration_generator:[779:786]
                value=ast.Attribute(
                    value=ast.Attribute(
                        value=ast.Attribute(
                            value=ast.Name(id='django', ctx=ast.Load()),
                            attr='db', ctx=ast.Load()),
                        attr='models', ctx=ast.Load()),
                    attr='deletion', ctx=ast.Load()), (duplicate-code)
pybirdai/utils/datapoint_test_run/parser_for_tests.py:1:0: R0801: Similar lines in 2 files
==pybirdai.utils.advanced_migration_generator:[569:576]
==pybirdai.utils.speed_improvements_initial_migration.advanced_migration_generator:[453:460]
                    value=ast.Attribute(
                        value=ast.Attribute(
                            value=ast.Attribute(
                                value=ast.Name(id='django', ctx=ast.Load()),
                                attr='db', ctx=ast.Load()),
                            attr='models', ctx=ast.Load()),
                        attr='deletion', ctx=ast.Load()), (duplicate-code)
pybirdai/utils/datapoint_test_run/parser_for_tests.py:1:0: R0801: Similar lines in 2 files
==pybirdai.utils.advanced_migration_generator:[453:460]
==pybirdai.utils.speed_improvements_initial_migration.advanced_migration_generator:[569:576]
                    value=ast.Attribute(
                        value=ast.Attribute(
                            value=ast.Attribute(
                                value=ast.Name(id='django', ctx=ast.Load()),
                                attr='db', ctx=ast.Load()),
                            attr='models', ctx=ast.Load()),
                        attr='deletion', ctx=ast.Load()), (duplicate-code)
pybirdai/utils/datapoint_test_run/parser_for_tests.py:1:0: R0801: Similar lines in 2 files
==pybirdai.utils.datapoint_test_run.generator_for_tests:[21:31]
==pybirdai.utils.datapoint_test_run.run_tests:[63:72]
    return logging.getLogger(__file_name__)

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger()
 (duplicate-code)
pybirdai/utils/datapoint_test_run/parser_for_tests.py:1:0: R0801: Similar lines in 2 files
==pybirdai.utils.clone_mode.export_with_ids:[49:56]
==pybirdai.utils.export_db:[62:69]
    valid_table_names = set()
    model_map = {}  # Store model classes for reference
    for name, obj in inspect.getmembers(bird_meta_data_model):
        if inspect.isclass(obj) and issubclass(obj, models.Model) and obj != models.Model:
            valid_table_names.add(obj._meta.db_table)
            model_map[obj._meta.db_table] = obj
 (duplicate-code)
pybirdai/utils/datapoint_test_run/parser_for_tests.py:1:0: R0801: Similar lines in 2 files
==pybirdai.utils.datapoint_test_run.generator_delete_fixtures:[21:36]
==pybirdai.utils.logger_factory:[17:25]
    return logging.getLogger(__file_name__)

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger()

def process_sql_file(file_path: str):
    """
    Process SQL file and convert insert/update statements to deletes
    Args:
        file_path (str): Path to the SQL file to process
    """ (duplicate-code)
pybirdai/utils/datapoint_test_run/parser_for_tests.py:1:0: R0801: Similar lines in 2 files
==pybirdai.utils.advanced_migration_generator:[789:797]
==pybirdai.utils.speed_improvements_initial_migration.advanced_migration_generator:[778:786]
            return ast.Attribute(
                value=ast.Attribute(
                    value=ast.Attribute(
                        value=ast.Attribute(
                            value=ast.Name(id='django', ctx=ast.Load()),
                            attr='db', ctx=ast.Load()),
                        attr='models', ctx=ast.Load()),
                    attr='deletion', ctx=ast.Load()), (duplicate-code)
pybirdai/utils/datapoint_test_run/parser_for_tests.py:1:0: R0801: Similar lines in 2 files
==pybirdai.utils.advanced_migration_generator:[778:786]
==pybirdai.utils.speed_improvements_initial_migration.advanced_migration_generator:[789:797]
            return ast.Attribute(
                value=ast.Attribute(
                    value=ast.Attribute(
                        value=ast.Attribute(
                            value=ast.Name(id='django', ctx=ast.Load()),
                            attr='db', ctx=ast.Load()),
                        attr='models', ctx=ast.Load()),
                    attr='deletion', ctx=ast.Load()), (duplicate-code)
pybirdai/utils/datapoint_test_run/parser_for_tests.py:1:0: R0801: Similar lines in 2 files
==pybirdai.utils.advanced_migration_generator:[663:676]
==pybirdai.utils.speed_improvements_initial_migration.advanced_migration_generator:[528:541]
        keywords = []

        # Parameters in logical order: auto_created, blank, default, max_length, null, primary_key, serialize, verbose_name, choices, etc.

        # Add auto_created first if applicable
        if field.auto_created:
            keywords.append(ast.keyword(arg='auto_created', value=ast.Constant(value=True)))

        # Add blank parameter
        if field.blank:
            keywords.append(ast.keyword(arg='blank', value=ast.Constant(value=True)))

        # Only add default for non-ForeignKey fields in CreateModel (avoid clutter) (duplicate-code)
pybirdai/utils/datapoint_test_run/parser_for_tests.py:1:0: R0801: Similar lines in 2 files
==pybirdai.utils.advanced_migration_generator:[528:541]
==pybirdai.utils.speed_improvements_initial_migration.advanced_migration_generator:[663:676]
        keywords = []

        # Parameters in logical order: auto_created, blank, default, max_length, null, primary_key, serialize, verbose_name, choices, etc.

        # Add auto_created first if applicable
        if field.auto_created:
            keywords.append(ast.keyword(arg='auto_created', value=ast.Constant(value=True)))

        # Add blank parameter
        if field.blank:
            keywords.append(ast.keyword(arg='blank', value=ast.Constant(value=True)))

        # Add field-specific arguments like max_length (duplicate-code)

-----------------------------------
Your code has been rated at 7.84/10


Running pylint on: pybirdai/entry_points/
-----------------------------------
************* Module automode_database_setup
pybirdai/entry_points/automode_database_setup.py:22:0: E0401: Unable to import 'pybirdai.entry_points.create_django_models' (import-error)
pybirdai/entry_points/automode_database_setup.py:23:0: E0401: Unable to import 'pybirdai.utils.speed_improvements_initial_migration.derived_fields_extractor' (import-error)
pybirdai/entry_points/automode_database_setup.py:26:0: E0401: Unable to import 'pybirdai.utils.speed_improvements_initial_migration.artifact_fetcher' (import-error)
pybirdai/entry_points/automode_database_setup.py:27:0: E0401: Unable to import 'pybirdai.utils.speed_improvements_initial_migration.advanced_migration_generator' (import-error)
pybirdai/entry_points/automode_database_setup.py:28:0: W0404: Reimport 'settings' (imported line 20) (reimported)
pybirdai/entry_points/automode_database_setup.py:42:4: W0231: __init__ method from base class 'AppConfig' is not called (super-init-not-called)
pybirdai/entry_points/automode_database_setup.py:67:20: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/automode_database_setup.py:84:21: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/entry_points/automode_database_setup.py:85:25: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/entry_points/automode_database_setup.py:89:21: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/entry_points/automode_database_setup.py:99:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/automode_database_setup.py:122:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/automode_database_setup.py:123:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/automode_database_setup.py:126:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/automode_database_setup.py:136:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/automode_database_setup.py:166:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/automode_database_setup.py:193:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/automode_database_setup.py:51:4: R0915: Too many statements (54/50) (too-many-statements)
pybirdai/entry_points/automode_database_setup.py:237:15: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/entry_points/automode_database_setup.py:199:12: C0415: Import outside toplevel (tempfile) (import-outside-toplevel)
pybirdai/entry_points/automode_database_setup.py:214:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/automode_database_setup.py:216:12: R1705: Unnecessary "else" after "return", remove the "else" and de-indent the code inside it (no-else-return)
pybirdai/entry_points/automode_database_setup.py:217:21: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/entry_points/automode_database_setup.py:219:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/automode_database_setup.py:224:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/automode_database_setup.py:231:20: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/automode_database_setup.py:232:25: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/entry_points/automode_database_setup.py:238:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/automode_database_setup.py:243:8: C0415: Import outside toplevel (shutil) (import-outside-toplevel)
pybirdai/entry_points/automode_database_setup.py:250:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/automode_database_setup.py:262:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/automode_database_setup.py:276:23: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/entry_points/automode_database_setup.py:275:20: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/automode_database_setup.py:277:20: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/automode_database_setup.py:279:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/automode_database_setup.py:284:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/automode_database_setup.py:374:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/automode_database_setup.py:296:12: W0612: Unused variable 'initial_migration_file' (unused-variable)
pybirdai/entry_points/automode_database_setup.py:300:12: W0612: Unused variable 'db_file' (unused-variable)
pybirdai/entry_points/automode_database_setup.py:307:12: W0612: Unused variable 'results_admin_path' (unused-variable)
pybirdai/entry_points/automode_database_setup.py:419:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/automode_database_setup.py:428:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/automode_database_setup.py:430:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/automode_database_setup.py:440:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/automode_database_setup.py:447:20: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/automode_database_setup.py:449:20: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/automode_database_setup.py:455:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/automode_database_setup.py:458:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/automode_database_setup.py:462:20: C0415: Import outside toplevel (stat) (import-outside-toplevel)
pybirdai/entry_points/automode_database_setup.py:466:20: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/automode_database_setup.py:468:20: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/automode_database_setup.py:473:24: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/automode_database_setup.py:477:24: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/automode_database_setup.py:480:4: R0914: Too many local variables (17/15) (too-many-locals)
pybirdai/entry_points/automode_database_setup.py:484:8: C0415: Import outside toplevel (glob) (import-outside-toplevel)
pybirdai/entry_points/automode_database_setup.py:490:13: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/entry_points/automode_database_setup.py:521:23: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/entry_points/automode_database_setup.py:502:25: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/entry_points/automode_database_setup.py:522:20: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/automode_database_setup.py:536:24: R1705: Unnecessary "elif" after "return", remove the leading "el" from "elif" (no-else-return)
pybirdai/entry_points/automode_database_setup.py:555:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/automode_database_setup.py:557:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/automode_database_setup.py:565:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/automode_database_setup.py:569:13: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/entry_points/automode_database_setup.py:573:13: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/entry_points/automode_database_setup.py:577:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/automode_database_setup.py:601:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/automode_database_setup.py:609:29: W1510: 'subprocess.run' used without explicitly defining the value for 'check'. (subprocess-run-check)
pybirdai/entry_points/automode_database_setup.py:637:4: R0914: Too many local variables (23/15) (too-many-locals)
pybirdai/entry_points/automode_database_setup.py:639:8: W0621: Redefining name 'subprocess' from outer scope (line 18) (redefined-outer-name)
pybirdai/entry_points/automode_database_setup.py:640:8: W0621: Redefining name 'sys' from outer scope (line 14) (redefined-outer-name)
pybirdai/entry_points/automode_database_setup.py:641:8: W0621: Redefining name 'time' from outer scope (line 19) (redefined-outer-name)
pybirdai/entry_points/automode_database_setup.py:639:8: W0404: Reimport 'subprocess' (imported line 18) (reimported)
pybirdai/entry_points/automode_database_setup.py:639:8: C0415: Import outside toplevel (subprocess) (import-outside-toplevel)
pybirdai/entry_points/automode_database_setup.py:640:8: W0404: Reimport 'sys' (imported line 14) (reimported)
pybirdai/entry_points/automode_database_setup.py:640:8: C0415: Import outside toplevel (sys) (import-outside-toplevel)
pybirdai/entry_points/automode_database_setup.py:641:8: W0404: Reimport 'time' (imported line 19) (reimported)
pybirdai/entry_points/automode_database_setup.py:641:8: C0415: Import outside toplevel (time) (import-outside-toplevel)
pybirdai/entry_points/automode_database_setup.py:660:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/automode_database_setup.py:661:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/automode_database_setup.py:666:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/automode_database_setup.py:668:29: W1510: 'subprocess.run' used without explicitly defining the value for 'check'. (subprocess-run-check)
pybirdai/entry_points/automode_database_setup.py:679:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/automode_database_setup.py:682:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/automode_database_setup.py:683:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/automode_database_setup.py:684:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/automode_database_setup.py:685:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/automode_database_setup.py:686:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/automode_database_setup.py:687:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/automode_database_setup.py:690:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/automode_database_setup.py:691:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/automode_database_setup.py:709:27: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/entry_points/automode_database_setup.py:704:27: E0602: Undefined variable 'is_file_locked' (undefined-variable)
pybirdai/entry_points/automode_database_setup.py:705:28: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/automode_database_setup.py:710:24: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/automode_database_setup.py:715:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/automode_database_setup.py:717:33: W1510: 'subprocess.run' used without explicitly defining the value for 'check'. (subprocess-run-check)
pybirdai/entry_points/automode_database_setup.py:726:20: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/automode_database_setup.py:729:20: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/automode_database_setup.py:730:20: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/automode_database_setup.py:731:20: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/automode_database_setup.py:732:20: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/automode_database_setup.py:733:20: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/automode_database_setup.py:734:20: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/automode_database_setup.py:738:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/automode_database_setup.py:739:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/automode_database_setup.py:742:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/automode_database_setup.py:749:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/automode_database_setup.py:750:12: W0707: Consider explicitly re-raising using 'raise RuntimeError(f'Migration subprocess timed out after {e.timeout} seconds') from e' (raise-missing-from)
pybirdai/entry_points/automode_database_setup.py:754:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/automode_database_setup.py:637:4: R0915: Too many statements (77/50) (too-many-statements)
pybirdai/entry_points/automode_database_setup.py:640:8: W0611: Unused import sys (unused-import)
pybirdai/entry_points/automode_database_setup.py:652:12: W0612: Unused variable 'venv_path' (unused-variable)
pybirdai/entry_points/automode_database_setup.py:761:8: W0621: Redefining name 'json' from outer scope (line 16) (redefined-outer-name)
pybirdai/entry_points/automode_database_setup.py:761:8: W0404: Reimport 'json' (imported line 16) (reimported)
pybirdai/entry_points/automode_database_setup.py:761:8: C0415: Import outside toplevel (json) (import-outside-toplevel)
pybirdai/entry_points/automode_database_setup.py:776:15: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/entry_points/automode_database_setup.py:773:17: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/entry_points/automode_database_setup.py:775:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/automode_database_setup.py:777:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/automode_database_setup.py:782:8: W0621: Redefining name 'json' from outer scope (line 16) (redefined-outer-name)
pybirdai/entry_points/automode_database_setup.py:782:8: W0404: Reimport 'json' (imported line 16) (reimported)
pybirdai/entry_points/automode_database_setup.py:782:8: C0415: Import outside toplevel (json) (import-outside-toplevel)
pybirdai/entry_points/automode_database_setup.py:797:15: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/entry_points/automode_database_setup.py:794:17: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/entry_points/automode_database_setup.py:796:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/automode_database_setup.py:798:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/automode_database_setup.py:807:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/automode_database_setup.py:816:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/automode_database_setup.py:828:15: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/entry_points/automode_database_setup.py:827:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/automode_database_setup.py:829:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/automode_database_setup.py:28:0: C0411: third party import "django.conf.settings" should be placed before first party imports "pybirdai.entry_points.create_django_models.RunCreateDjangoModels", "pybirdai.utils.speed_improvements_initial_migration.derived_fields_extractor.merge_derived_fields_into_original_model", "pybirdai.utils.speed_improvements_initial_migration.artifact_fetcher.PreconfiguredDatabaseFetcher", "pybirdai.utils.speed_improvements_initial_migration.advanced_migration_generator.AdvancedMigrationGenerator"  (wrong-import-order)
pybirdai/entry_points/automode_database_setup.py:29:0: C0411: third party import "psutil" should be placed before first party imports "pybirdai.entry_points.create_django_models.RunCreateDjangoModels", "pybirdai.utils.speed_improvements_initial_migration.derived_fields_extractor.merge_derived_fields_into_original_model", "pybirdai.utils.speed_improvements_initial_migration.artifact_fetcher.PreconfiguredDatabaseFetcher", "pybirdai.utils.speed_improvements_initial_migration.advanced_migration_generator.AdvancedMigrationGenerator"  (wrong-import-order)
pybirdai/entry_points/automode_database_setup.py:31:0: C0411: standard import "importlib.metadata" should be placed before third party imports "django.conf.settings", "django.apps.AppConfig", "django.conf.settings", "psutil" and first party imports "pybirdai.entry_points.create_django_models.RunCreateDjangoModels", "pybirdai.utils.speed_improvements_initial_migration.derived_fields_extractor.merge_derived_fields_into_original_model", "pybirdai.utils.speed_improvements_initial_migration.artifact_fetcher.PreconfiguredDatabaseFetcher", "pybirdai.utils.speed_improvements_initial_migration.advanced_migration_generator.AdvancedMigrationGenerator"  (wrong-import-order)
pybirdai/entry_points/automode_database_setup.py:28:0: C0412: Imports from package django are not grouped (ungrouped-imports)
pybirdai/entry_points/automode_database_setup.py:29:0: W0611: Unused import psutil (unused-import)
pybirdai/entry_points/automode_database_setup.py:31:0: W0611: Unused metadata imported from importlib (unused-import)
************* Module delete_semantic_integrations
pybirdai/entry_points/delete_semantic_integrations.py:39:36: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/entry_points/delete_semantic_integrations.py:43:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/entry_points/delete_semantic_integrations.py:57:0: C0304: Final newline missing (missing-final-newline)
pybirdai/entry_points/delete_semantic_integrations.py:57:0: W0311: Bad indentation. Found 8 spaces, expected 4 (bad-indentation)
pybirdai/entry_points/delete_semantic_integrations.py:31:8: E0401: Unable to import 'pybirdai.context.sdd_context_django' (import-error)
pybirdai/entry_points/delete_semantic_integrations.py:31:8: C0415: Import outside toplevel (pybirdai.context.sdd_context_django.SDDContext) (import-outside-toplevel)
pybirdai/entry_points/delete_semantic_integrations.py:32:8: E0401: Unable to import 'pybirdai.context.context' (import-error)
pybirdai/entry_points/delete_semantic_integrations.py:32:8: C0415: Import outside toplevel (pybirdai.context.context.Context) (import-outside-toplevel)
pybirdai/entry_points/delete_semantic_integrations.py:34:8: E0401: Unable to import 'pybirdai.process_steps.joins_meta_data.delete_joins_meta_data' (import-error)
pybirdai/entry_points/delete_semantic_integrations.py:34:8: C0415: Import outside toplevel (pybirdai.process_steps.joins_meta_data.delete_joins_meta_data.TransformationMetaDataDestroyer) (import-outside-toplevel)
pybirdai/entry_points/delete_semantic_integrations.py:16:0: W0611: Unused Path imported from pathlib (unused-import)
pybirdai/entry_points/delete_semantic_integrations.py:18:0: W0611: Unused import django (unused-import)
************* Module mapping_assistant_entry
pybirdai/entry_points/mapping_assistant_entry.py:24:0: E0401: Unable to import 'pybirdai.models.bird_meta_data_model' (import-error)
pybirdai/entry_points/mapping_assistant_entry.py:36:4: R0914: Too many local variables (22/15) (too-many-locals)
pybirdai/entry_points/mapping_assistant_entry.py:52:8: E0401: Unable to import 'pybirdai.process_steps.mapping_assistant' (import-error)
pybirdai/entry_points/mapping_assistant_entry.py:52:8: C0415: Import outside toplevel (pybirdai.process_steps.mapping_assistant.MappingAssistant, pybirdai.process_steps.mapping_assistant.CombinationData, pybirdai.process_steps.mapping_assistant.MappingProposal) (import-outside-toplevel)
pybirdai/entry_points/mapping_assistant_entry.py:52:8: W0611: Unused MappingProposal imported from pybirdai.process_steps.mapping_assistant (unused-import)
pybirdai/entry_points/mapping_assistant_entry.py:158:13: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/entry_points/mapping_assistant_entry.py:220:19: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/entry_points/mapping_assistant_entry.py:191:20: W0612: Unused variable 'source_item' (unused-variable)
pybirdai/entry_points/mapping_assistant_entry.py:211:20: W0612: Unused variable 'target_item' (unused-variable)
pybirdai/entry_points/mapping_assistant_entry.py:367:8: C0415: Import outside toplevel (collections.defaultdict) (import-outside-toplevel)
pybirdai/entry_points/mapping_assistant_entry.py:20:0: W0611: Unused import django (unused-import)
pybirdai/entry_points/mapping_assistant_entry.py:24:0: W0611: Unused MEMBER_MAPPING imported from pybirdai.models.bird_meta_data_model (unused-import)
pybirdai/entry_points/mapping_assistant_entry.py:24:0: W0611: Unused VARIABLE_MAPPING imported from pybirdai.models.bird_meta_data_model (unused-import)
************* Module delete_joins
pybirdai/entry_points/delete_joins.py:39:36: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/entry_points/delete_joins.py:43:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/entry_points/delete_joins.py:57:0: C0304: Final newline missing (missing-final-newline)
pybirdai/entry_points/delete_joins.py:57:0: W0311: Bad indentation. Found 8 spaces, expected 4 (bad-indentation)
pybirdai/entry_points/delete_joins.py:31:8: E0401: Unable to import 'pybirdai.context.sdd_context_django' (import-error)
pybirdai/entry_points/delete_joins.py:31:8: C0415: Import outside toplevel (pybirdai.context.sdd_context_django.SDDContext) (import-outside-toplevel)
pybirdai/entry_points/delete_joins.py:32:8: E0401: Unable to import 'pybirdai.context.context' (import-error)
pybirdai/entry_points/delete_joins.py:32:8: C0415: Import outside toplevel (pybirdai.context.context.Context) (import-outside-toplevel)
pybirdai/entry_points/delete_joins.py:34:8: E0401: Unable to import 'pybirdai.process_steps.joins_meta_data.delete_joins_meta_data' (import-error)
pybirdai/entry_points/delete_joins.py:34:8: C0415: Import outside toplevel (pybirdai.process_steps.joins_meta_data.delete_joins_meta_data.TransformationMetaDataDestroyer) (import-outside-toplevel)
pybirdai/entry_points/delete_joins.py:16:0: W0611: Unused Path imported from pathlib (unused-import)
pybirdai/entry_points/delete_joins.py:18:0: W0611: Unused import django (unused-import)
************* Module create_joins_metadata
pybirdai/entry_points/create_joins_metadata.py:72:0: W0311: Bad indentation. Found 8 spaces, expected 4 (bad-indentation)
pybirdai/entry_points/create_joins_metadata.py:31:8: E0401: Unable to import 'pybirdai.process_steps.input_model.import_database_to_sdd_model' (import-error)
pybirdai/entry_points/create_joins_metadata.py:31:8: C0415: Import outside toplevel (pybirdai.process_steps.input_model.import_database_to_sdd_model.ImportDatabaseToSDDModel) (import-outside-toplevel)
pybirdai/entry_points/create_joins_metadata.py:34:8: E0401: Unable to import 'pybirdai.context.sdd_context_django' (import-error)
pybirdai/entry_points/create_joins_metadata.py:34:8: C0415: Import outside toplevel (pybirdai.context.sdd_context_django.SDDContext) (import-outside-toplevel)
pybirdai/entry_points/create_joins_metadata.py:35:8: E0401: Unable to import 'pybirdai.context.context' (import-error)
pybirdai/entry_points/create_joins_metadata.py:35:8: C0415: Import outside toplevel (pybirdai.context.context.Context) (import-outside-toplevel)
pybirdai/entry_points/create_joins_metadata.py:40:8: E0401: Unable to import 'pybirdai.process_steps.joins_meta_data.create_joins_meta_data_combinations' (import-error)
pybirdai/entry_points/create_joins_metadata.py:40:8: C0415: Import outside toplevel (pybirdai.process_steps.joins_meta_data.create_joins_meta_data_combinations.JoinsMetaDataCreator) (import-outside-toplevel)
pybirdai/entry_points/create_joins_metadata.py:43:8: E0401: Unable to import 'pybirdai.process_steps.joins_meta_data.main_category_finder' (import-error)
pybirdai/entry_points/create_joins_metadata.py:43:8: C0415: Import outside toplevel (pybirdai.process_steps.joins_meta_data.main_category_finder.MainCategoryFinder) (import-outside-toplevel)
pybirdai/entry_points/create_joins_metadata.py:31:8: W0611: Unused ImportDatabaseToSDDModel imported from pybirdai.process_steps.input_model.import_database_to_sdd_model (unused-import)
pybirdai/entry_points/create_joins_metadata.py:16:0: W0611: Unused Path imported from pathlib (unused-import)
pybirdai/entry_points/create_joins_metadata.py:18:0: W0611: Unused import django (unused-import)
************* Module upload_sqldev_eldm_files
pybirdai/entry_points/upload_sqldev_eldm_files.py:43:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/entry_points/upload_sqldev_eldm_files.py:50:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/entry_points/upload_sqldev_eldm_files.py:56:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/entry_points/upload_sqldev_eldm_files.py:65:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/entry_points/upload_sqldev_eldm_files.py:66:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/entry_points/upload_sqldev_eldm_files.py:18:0: E0401: Unable to import 'pybirdai.context.sdd_context_django' (import-error)
pybirdai/entry_points/upload_sqldev_eldm_files.py:39:8: E0401: Unable to import 'pybirdai.models.bird_meta_data_model' (import-error)
pybirdai/entry_points/upload_sqldev_eldm_files.py:39:8: C0415: Import outside toplevel (pybirdai.models.bird_meta_data_model.MAINTENANCE_AGENCY) (import-outside-toplevel)
pybirdai/entry_points/upload_sqldev_eldm_files.py:40:8: E0401: Unable to import 'pybirdai.process_steps.upload_files.file_uploader' (import-error)
pybirdai/entry_points/upload_sqldev_eldm_files.py:40:8: C0415: Import outside toplevel (pybirdai.process_steps.upload_files.file_uploader.FileUploader) (import-outside-toplevel)
pybirdai/entry_points/upload_sqldev_eldm_files.py:44:8: E0401: Unable to import 'pybirdai.context.context' (import-error)
pybirdai/entry_points/upload_sqldev_eldm_files.py:44:8: C0415: Import outside toplevel (pybirdai.context.context.Context) (import-outside-toplevel)
pybirdai/entry_points/upload_sqldev_eldm_files.py:39:8: W0611: Unused MAINTENANCE_AGENCY imported from pybirdai.models.bird_meta_data_model (unused-import)
pybirdai/entry_points/upload_sqldev_eldm_files.py:16:0: C0411: standard import "os" should be placed before third party import "django" (wrong-import-order)
pybirdai/entry_points/upload_sqldev_eldm_files.py:19:0: C0411: third party import "django.conf.settings" should be placed before first party import "pybirdai.context.sdd_context_django.SDDContext"  (wrong-import-order)
pybirdai/entry_points/upload_sqldev_eldm_files.py:19:0: C0412: Imports from package django are not grouped (ungrouped-imports)
pybirdai/entry_points/upload_sqldev_eldm_files.py:15:0: W0611: Unused import django (unused-import)
************* Module create_executable_joins
pybirdai/entry_points/create_executable_joins.py:39:36: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/entry_points/create_executable_joins.py:43:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/entry_points/create_executable_joins.py:63:36: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/entry_points/create_executable_joins.py:67:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/entry_points/create_executable_joins.py:74:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/entry_points/create_executable_joins.py:89:0: C0304: Final newline missing (missing-final-newline)
pybirdai/entry_points/create_executable_joins.py:30:8: E0401: Unable to import 'pybirdai.process_steps.input_model.import_database_to_sdd_model' (import-error)
pybirdai/entry_points/create_executable_joins.py:30:8: C0415: Import outside toplevel (pybirdai.process_steps.input_model.import_database_to_sdd_model.ImportDatabaseToSDDModel) (import-outside-toplevel)
pybirdai/entry_points/create_executable_joins.py:33:8: E0401: Unable to import 'pybirdai.context.sdd_context_django' (import-error)
pybirdai/entry_points/create_executable_joins.py:33:8: C0415: Import outside toplevel (pybirdai.context.sdd_context_django.SDDContext) (import-outside-toplevel)
pybirdai/entry_points/create_executable_joins.py:34:8: E0401: Unable to import 'pybirdai.context.context' (import-error)
pybirdai/entry_points/create_executable_joins.py:34:8: C0415: Import outside toplevel (pybirdai.context.context.Context) (import-outside-toplevel)
pybirdai/entry_points/create_executable_joins.py:35:8: E0401: Unable to import 'pybirdai.process_steps.pybird.create_python_django_transformations' (import-error)
pybirdai/entry_points/create_executable_joins.py:35:8: C0415: Import outside toplevel (pybirdai.process_steps.pybird.create_python_django_transformations.CreatePythonTransformations) (import-outside-toplevel)
pybirdai/entry_points/create_executable_joins.py:30:8: W0611: Unused ImportDatabaseToSDDModel imported from pybirdai.process_steps.input_model.import_database_to_sdd_model (unused-import)
pybirdai/entry_points/create_executable_joins.py:54:8: E0401: Unable to import 'pybirdai.process_steps.input_model.import_database_to_sdd_model' (import-error)
pybirdai/entry_points/create_executable_joins.py:54:8: C0415: Import outside toplevel (pybirdai.process_steps.input_model.import_database_to_sdd_model.ImportDatabaseToSDDModel) (import-outside-toplevel)
pybirdai/entry_points/create_executable_joins.py:57:8: E0401: Unable to import 'pybirdai.context.sdd_context_django' (import-error)
pybirdai/entry_points/create_executable_joins.py:57:8: C0415: Import outside toplevel (pybirdai.context.sdd_context_django.SDDContext) (import-outside-toplevel)
pybirdai/entry_points/create_executable_joins.py:58:8: E0401: Unable to import 'pybirdai.context.context' (import-error)
pybirdai/entry_points/create_executable_joins.py:58:8: C0415: Import outside toplevel (pybirdai.context.context.Context) (import-outside-toplevel)
pybirdai/entry_points/create_executable_joins.py:59:8: E0401: Unable to import 'pybirdai.process_steps.pybird.create_python_django_transformations' (import-error)
pybirdai/entry_points/create_executable_joins.py:59:8: C0415: Import outside toplevel (pybirdai.process_steps.pybird.create_python_django_transformations.CreatePythonTransformations) (import-outside-toplevel)
pybirdai/entry_points/create_executable_joins.py:16:0: W0611: Unused Path imported from pathlib (unused-import)
pybirdai/entry_points/create_executable_joins.py:18:0: W0611: Unused import django (unused-import)
************* Module delete_bird_metadata_database
pybirdai/entry_points/delete_bird_metadata_database.py:39:36: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/entry_points/delete_bird_metadata_database.py:43:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/entry_points/delete_bird_metadata_database.py:57:0: C0304: Final newline missing (missing-final-newline)
pybirdai/entry_points/delete_bird_metadata_database.py:57:0: W0311: Bad indentation. Found 8 spaces, expected 4 (bad-indentation)
pybirdai/entry_points/delete_bird_metadata_database.py:31:8: E0401: Unable to import 'pybirdai.context.sdd_context_django' (import-error)
pybirdai/entry_points/delete_bird_metadata_database.py:31:8: C0415: Import outside toplevel (pybirdai.context.sdd_context_django.SDDContext) (import-outside-toplevel)
pybirdai/entry_points/delete_bird_metadata_database.py:32:8: E0401: Unable to import 'pybirdai.context.context' (import-error)
pybirdai/entry_points/delete_bird_metadata_database.py:32:8: C0415: Import outside toplevel (pybirdai.context.context.Context) (import-outside-toplevel)
pybirdai/entry_points/delete_bird_metadata_database.py:34:8: E0401: Unable to import 'pybirdai.process_steps.joins_meta_data.delete_joins_meta_data' (import-error)
pybirdai/entry_points/delete_bird_metadata_database.py:34:8: C0415: Import outside toplevel (pybirdai.process_steps.joins_meta_data.delete_joins_meta_data.TransformationMetaDataDestroyer) (import-outside-toplevel)
pybirdai/entry_points/delete_bird_metadata_database.py:16:0: W0611: Unused Path imported from pathlib (unused-import)
pybirdai/entry_points/delete_bird_metadata_database.py:18:0: W0611: Unused import django (unused-import)
************* Module metadata_lineage_processor
pybirdai/entry_points/metadata_lineage_processor.py:27:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/entry_points/metadata_lineage_processor.py:29:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/entry_points/metadata_lineage_processor.py:37:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/entry_points/metadata_lineage_processor.py:39:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/entry_points/metadata_lineage_processor.py:43:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/entry_points/metadata_lineage_processor.py:46:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/entry_points/metadata_lineage_processor.py:52:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/entry_points/metadata_lineage_processor.py:56:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/entry_points/metadata_lineage_processor.py:60:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/entry_points/metadata_lineage_processor.py:64:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/entry_points/metadata_lineage_processor.py:69:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/entry_points/metadata_lineage_processor.py:71:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/entry_points/metadata_lineage_processor.py:74:0: C0304: Final newline missing (missing-final-newline)
pybirdai/entry_points/metadata_lineage_processor.py:34:8: E0401: Unable to import 'pybirdai.context.context' (import-error)
pybirdai/entry_points/metadata_lineage_processor.py:34:8: C0415: Import outside toplevel (pybirdai.context.context.Context) (import-outside-toplevel)
pybirdai/entry_points/metadata_lineage_processor.py:35:8: E0401: Unable to import 'pybirdai.context.sdd_context_django' (import-error)
pybirdai/entry_points/metadata_lineage_processor.py:35:8: C0415: Import outside toplevel (pybirdai.context.sdd_context_django.SDDContext) (import-outside-toplevel)
pybirdai/entry_points/metadata_lineage_processor.py:36:8: E0401: Unable to import 'pybirdai.process_steps.metadata_lineage.metadata_lineage_processor' (import-error)
pybirdai/entry_points/metadata_lineage_processor.py:36:8: C0415: Import outside toplevel (pybirdai.process_steps.metadata_lineage.metadata_lineage_processor.MetadataLineageProcessor) (import-outside-toplevel)
pybirdai/entry_points/metadata_lineage_processor.py:34:8: W0611: Unused Context imported from pybirdai.context.context (unused-import)
pybirdai/entry_points/metadata_lineage_processor.py:15:0: C0411: standard import "os" should be placed before third party import "django" (wrong-import-order)
pybirdai/entry_points/metadata_lineage_processor.py:14:0: W0611: Unused import django (unused-import)
************* Module upload_sqldev_eil_files
pybirdai/entry_points/upload_sqldev_eil_files.py:43:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/entry_points/upload_sqldev_eil_files.py:50:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/entry_points/upload_sqldev_eil_files.py:56:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/entry_points/upload_sqldev_eil_files.py:65:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/entry_points/upload_sqldev_eil_files.py:66:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/entry_points/upload_sqldev_eil_files.py:18:0: E0401: Unable to import 'pybirdai.context.sdd_context_django' (import-error)
pybirdai/entry_points/upload_sqldev_eil_files.py:39:8: E0401: Unable to import 'pybirdai.models.bird_meta_data_model' (import-error)
pybirdai/entry_points/upload_sqldev_eil_files.py:39:8: C0415: Import outside toplevel (pybirdai.models.bird_meta_data_model.MAINTENANCE_AGENCY) (import-outside-toplevel)
pybirdai/entry_points/upload_sqldev_eil_files.py:40:8: E0401: Unable to import 'pybirdai.process_steps.upload_files.file_uploader' (import-error)
pybirdai/entry_points/upload_sqldev_eil_files.py:40:8: C0415: Import outside toplevel (pybirdai.process_steps.upload_files.file_uploader.FileUploader) (import-outside-toplevel)
pybirdai/entry_points/upload_sqldev_eil_files.py:44:8: E0401: Unable to import 'pybirdai.context.context' (import-error)
pybirdai/entry_points/upload_sqldev_eil_files.py:44:8: C0415: Import outside toplevel (pybirdai.context.context.Context) (import-outside-toplevel)
pybirdai/entry_points/upload_sqldev_eil_files.py:39:8: W0611: Unused MAINTENANCE_AGENCY imported from pybirdai.models.bird_meta_data_model (unused-import)
pybirdai/entry_points/upload_sqldev_eil_files.py:16:0: C0411: standard import "os" should be placed before third party import "django" (wrong-import-order)
pybirdai/entry_points/upload_sqldev_eil_files.py:19:0: C0411: third party import "django.conf.settings" should be placed before first party import "pybirdai.context.sdd_context_django.SDDContext"  (wrong-import-order)
pybirdai/entry_points/upload_sqldev_eil_files.py:19:0: C0412: Imports from package django are not grouped (ungrouped-imports)
pybirdai/entry_points/upload_sqldev_eil_files.py:15:0: W0611: Unused import django (unused-import)
************* Module upload_joins_configuration
pybirdai/entry_points/upload_joins_configuration.py:43:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/entry_points/upload_joins_configuration.py:50:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/entry_points/upload_joins_configuration.py:56:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/entry_points/upload_joins_configuration.py:65:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/entry_points/upload_joins_configuration.py:66:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/entry_points/upload_joins_configuration.py:18:0: E0401: Unable to import 'pybirdai.context.sdd_context_django' (import-error)
pybirdai/entry_points/upload_joins_configuration.py:39:8: E0401: Unable to import 'pybirdai.models.bird_meta_data_model' (import-error)
pybirdai/entry_points/upload_joins_configuration.py:39:8: C0415: Import outside toplevel (pybirdai.models.bird_meta_data_model.MAINTENANCE_AGENCY) (import-outside-toplevel)
pybirdai/entry_points/upload_joins_configuration.py:40:8: E0401: Unable to import 'pybirdai.process_steps.upload_files.file_uploader' (import-error)
pybirdai/entry_points/upload_joins_configuration.py:40:8: C0415: Import outside toplevel (pybirdai.process_steps.upload_files.file_uploader.FileUploader) (import-outside-toplevel)
pybirdai/entry_points/upload_joins_configuration.py:44:8: E0401: Unable to import 'pybirdai.context.context' (import-error)
pybirdai/entry_points/upload_joins_configuration.py:44:8: C0415: Import outside toplevel (pybirdai.context.context.Context) (import-outside-toplevel)
pybirdai/entry_points/upload_joins_configuration.py:39:8: W0611: Unused MAINTENANCE_AGENCY imported from pybirdai.models.bird_meta_data_model (unused-import)
pybirdai/entry_points/upload_joins_configuration.py:16:0: C0411: standard import "os" should be placed before third party import "django" (wrong-import-order)
pybirdai/entry_points/upload_joins_configuration.py:19:0: C0411: third party import "django.conf.settings" should be placed before first party import "pybirdai.context.sdd_context_django.SDDContext"  (wrong-import-order)
pybirdai/entry_points/upload_joins_configuration.py:19:0: C0412: Imports from package django are not grouped (ungrouped-imports)
pybirdai/entry_points/upload_joins_configuration.py:15:0: W0611: Unused import django (unused-import)
************* Module delete_output_concepts
pybirdai/entry_points/delete_output_concepts.py:39:36: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/entry_points/delete_output_concepts.py:43:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/entry_points/delete_output_concepts.py:57:0: C0304: Final newline missing (missing-final-newline)
pybirdai/entry_points/delete_output_concepts.py:57:0: W0311: Bad indentation. Found 8 spaces, expected 4 (bad-indentation)
pybirdai/entry_points/delete_output_concepts.py:31:8: E0401: Unable to import 'pybirdai.context.sdd_context_django' (import-error)
pybirdai/entry_points/delete_output_concepts.py:31:8: C0415: Import outside toplevel (pybirdai.context.sdd_context_django.SDDContext) (import-outside-toplevel)
pybirdai/entry_points/delete_output_concepts.py:32:8: E0401: Unable to import 'pybirdai.context.context' (import-error)
pybirdai/entry_points/delete_output_concepts.py:32:8: C0415: Import outside toplevel (pybirdai.context.context.Context) (import-outside-toplevel)
pybirdai/entry_points/delete_output_concepts.py:34:8: E0401: Unable to import 'pybirdai.process_steps.joins_meta_data.delete_joins_meta_data' (import-error)
pybirdai/entry_points/delete_output_concepts.py:34:8: C0415: Import outside toplevel (pybirdai.process_steps.joins_meta_data.delete_joins_meta_data.TransformationMetaDataDestroyer) (import-outside-toplevel)
pybirdai/entry_points/delete_output_concepts.py:16:0: W0611: Unused Path imported from pathlib (unused-import)
pybirdai/entry_points/delete_output_concepts.py:18:0: W0611: Unused import django (unused-import)
************* Module import_report_templates_from_website
pybirdai/entry_points/import_report_templates_from_website.py:47:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/entry_points/import_report_templates_from_website.py:54:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/entry_points/import_report_templates_from_website.py:19:0: E0401: Unable to import 'pybirdai.context.sdd_context_django' (import-error)
pybirdai/entry_points/import_report_templates_from_website.py:41:8: W0621: Redefining name 'settings' from outer scope (line 20) (redefined-outer-name)
pybirdai/entry_points/import_report_templates_from_website.py:37:8: E0401: Unable to import 'pybirdai.process_steps.website_to_sddmodel.import_website_to_sdd_model_django' (import-error)
pybirdai/entry_points/import_report_templates_from_website.py:37:8: C0415: Import outside toplevel (pybirdai.process_steps.website_to_sddmodel.import_website_to_sdd_model_django.ImportWebsiteToSDDModel) (import-outside-toplevel)
pybirdai/entry_points/import_report_templates_from_website.py:40:8: E0401: Unable to import 'pybirdai.context.context' (import-error)
pybirdai/entry_points/import_report_templates_from_website.py:40:8: C0415: Import outside toplevel (pybirdai.context.context.Context) (import-outside-toplevel)
pybirdai/entry_points/import_report_templates_from_website.py:41:8: W0404: Reimport 'settings' (imported line 20) (reimported)
pybirdai/entry_points/import_report_templates_from_website.py:41:8: C0415: Import outside toplevel (django.conf.settings) (import-outside-toplevel)
pybirdai/entry_points/import_report_templates_from_website.py:56:12: E0401: Unable to import 'pybirdai.utils.github_file_fetcher' (import-error)
pybirdai/entry_points/import_report_templates_from_website.py:56:12: C0415: Import outside toplevel (pybirdai.utils.github_file_fetcher.GitHubFileFetcher) (import-outside-toplevel)
pybirdai/entry_points/import_report_templates_from_website.py:16:0: C0411: standard import "os" should be placed before third party import "django" (wrong-import-order)
pybirdai/entry_points/import_report_templates_from_website.py:17:0: C0411: standard import "logging" should be placed before third party import "django" (wrong-import-order)
pybirdai/entry_points/import_report_templates_from_website.py:20:0: C0411: third party import "django.conf.settings" should be placed before first party import "pybirdai.context.sdd_context_django.SDDContext"  (wrong-import-order)
pybirdai/entry_points/import_report_templates_from_website.py:20:0: C0412: Imports from package django are not grouped (ungrouped-imports)
pybirdai/entry_points/import_report_templates_from_website.py:15:0: W0611: Unused import django (unused-import)
************* Module delete_joins_metadata
pybirdai/entry_points/delete_joins_metadata.py:39:36: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/entry_points/delete_joins_metadata.py:43:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/entry_points/delete_joins_metadata.py:57:0: C0304: Final newline missing (missing-final-newline)
pybirdai/entry_points/delete_joins_metadata.py:57:0: W0311: Bad indentation. Found 8 spaces, expected 4 (bad-indentation)
pybirdai/entry_points/delete_joins_metadata.py:31:8: E0401: Unable to import 'pybirdai.context.sdd_context_django' (import-error)
pybirdai/entry_points/delete_joins_metadata.py:31:8: C0415: Import outside toplevel (pybirdai.context.sdd_context_django.SDDContext) (import-outside-toplevel)
pybirdai/entry_points/delete_joins_metadata.py:32:8: E0401: Unable to import 'pybirdai.context.context' (import-error)
pybirdai/entry_points/delete_joins_metadata.py:32:8: C0415: Import outside toplevel (pybirdai.context.context.Context) (import-outside-toplevel)
pybirdai/entry_points/delete_joins_metadata.py:34:8: E0401: Unable to import 'pybirdai.process_steps.joins_meta_data.delete_joins_meta_data' (import-error)
pybirdai/entry_points/delete_joins_metadata.py:34:8: C0415: Import outside toplevel (pybirdai.process_steps.joins_meta_data.delete_joins_meta_data.TransformationMetaDataDestroyer) (import-outside-toplevel)
pybirdai/entry_points/delete_joins_metadata.py:16:0: W0611: Unused Path imported from pathlib (unused-import)
pybirdai/entry_points/delete_joins_metadata.py:18:0: W0611: Unused import django (unused-import)
************* Module dpm_output_layer_creation
pybirdai/entry_points/dpm_output_layer_creation.py:78:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/entry_points/dpm_output_layer_creation.py:83:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/entry_points/dpm_output_layer_creation.py:88:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/entry_points/dpm_output_layer_creation.py:93:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/entry_points/dpm_output_layer_creation.py:102:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/entry_points/dpm_output_layer_creation.py:112:0: C0304: Final newline missing (missing-final-newline)
pybirdai/entry_points/dpm_output_layer_creation.py:19:0: E0401: Unable to import 'pybirdai.context.sdd_context_django' (import-error)
pybirdai/entry_points/dpm_output_layer_creation.py:56:8: E0401: Unable to import 'pybirdai.process_steps.report_filters.create_non_reference_output_layers' (import-error)
pybirdai/entry_points/dpm_output_layer_creation.py:56:8: C0415: Import outside toplevel (pybirdai.process_steps.report_filters.create_non_reference_output_layers.CreateNROutputLayers) (import-outside-toplevel)
pybirdai/entry_points/dpm_output_layer_creation.py:57:8: E0401: Unable to import 'pybirdai.context.context' (import-error)
pybirdai/entry_points/dpm_output_layer_creation.py:57:8: C0415: Import outside toplevel (pybirdai.context.context.Context) (import-outside-toplevel)
pybirdai/entry_points/dpm_output_layer_creation.py:103:15: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/entry_points/dpm_output_layer_creation.py:74:12: R1705: Unnecessary "elif" after "return", remove the leading "el" from "elif" (no-else-return)
pybirdai/entry_points/dpm_output_layer_creation.py:76:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/dpm_output_layer_creation.py:81:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/dpm_output_layer_creation.py:86:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/dpm_output_layer_creation.py:91:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/dpm_output_layer_creation.py:104:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/dpm_output_layer_creation.py:17:0: C0411: standard import "os" should be placed before third party import "django" (wrong-import-order)
pybirdai/entry_points/dpm_output_layer_creation.py:20:0: C0411: third party import "django.conf.settings" should be placed before first party import "pybirdai.context.sdd_context_django.SDDContext"  (wrong-import-order)
pybirdai/entry_points/dpm_output_layer_creation.py:21:0: C0411: standard import "logging" should be placed before third party imports "django", "django.apps.AppConfig", "django.conf.settings" and first party import "pybirdai.context.sdd_context_django.SDDContext"  (wrong-import-order)
pybirdai/entry_points/dpm_output_layer_creation.py:20:0: C0412: Imports from package django are not grouped (ungrouped-imports)
pybirdai/entry_points/dpm_output_layer_creation.py:16:0: W0611: Unused import django (unused-import)
************* Module run_create_executable_filters
pybirdai/entry_points/run_create_executable_filters.py:47:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/entry_points/run_create_executable_filters.py:71:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/entry_points/run_create_executable_filters.py:18:0: E0401: Unable to import 'pybirdai.context.sdd_context_django' (import-error)
pybirdai/entry_points/run_create_executable_filters.py:33:8: E0401: Unable to import 'pybirdai.models.bird_meta_data_model' (import-error)
pybirdai/entry_points/run_create_executable_filters.py:33:8: C0415: Import outside toplevel (pybirdai.models.bird_meta_data_model.MAINTENANCE_AGENCY) (import-outside-toplevel)
pybirdai/entry_points/run_create_executable_filters.py:35:8: E0401: Unable to import 'pybirdai.process_steps.input_model.import_database_to_sdd_model' (import-error)
pybirdai/entry_points/run_create_executable_filters.py:35:8: C0415: Import outside toplevel (pybirdai.process_steps.input_model.import_database_to_sdd_model.ImportDatabaseToSDDModel) (import-outside-toplevel)
pybirdai/entry_points/run_create_executable_filters.py:38:8: E0401: Unable to import 'pybirdai.process_steps.pybird.create_executable_filters' (import-error)
pybirdai/entry_points/run_create_executable_filters.py:38:8: C0415: Import outside toplevel (pybirdai.process_steps.pybird.create_executable_filters.CreateExecutableFilters) (import-outside-toplevel)
pybirdai/entry_points/run_create_executable_filters.py:41:8: E0401: Unable to import 'pybirdai.context.context' (import-error)
pybirdai/entry_points/run_create_executable_filters.py:41:8: C0415: Import outside toplevel (pybirdai.context.context.Context) (import-outside-toplevel)
pybirdai/entry_points/run_create_executable_filters.py:33:8: W0611: Unused MAINTENANCE_AGENCY imported from pybirdai.models.bird_meta_data_model (unused-import)
pybirdai/entry_points/run_create_executable_filters.py:35:8: W0611: Unused ImportDatabaseToSDDModel imported from pybirdai.process_steps.input_model.import_database_to_sdd_model (unused-import)
pybirdai/entry_points/run_create_executable_filters.py:57:8: E0401: Unable to import 'pybirdai.models.bird_meta_data_model' (import-error)
pybirdai/entry_points/run_create_executable_filters.py:57:8: C0415: Import outside toplevel (pybirdai.models.bird_meta_data_model.MAINTENANCE_AGENCY) (import-outside-toplevel)
pybirdai/entry_points/run_create_executable_filters.py:59:8: E0401: Unable to import 'pybirdai.process_steps.input_model.import_database_to_sdd_model' (import-error)
pybirdai/entry_points/run_create_executable_filters.py:59:8: C0415: Import outside toplevel (pybirdai.process_steps.input_model.import_database_to_sdd_model.ImportDatabaseToSDDModel) (import-outside-toplevel)
pybirdai/entry_points/run_create_executable_filters.py:62:8: E0401: Unable to import 'pybirdai.process_steps.pybird.create_executable_filters' (import-error)
pybirdai/entry_points/run_create_executable_filters.py:62:8: C0415: Import outside toplevel (pybirdai.process_steps.pybird.create_executable_filters.CreateExecutableFilters) (import-outside-toplevel)
pybirdai/entry_points/run_create_executable_filters.py:65:8: E0401: Unable to import 'pybirdai.context.context' (import-error)
pybirdai/entry_points/run_create_executable_filters.py:65:8: C0415: Import outside toplevel (pybirdai.context.context.Context) (import-outside-toplevel)
pybirdai/entry_points/run_create_executable_filters.py:57:8: W0611: Unused MAINTENANCE_AGENCY imported from pybirdai.models.bird_meta_data_model (unused-import)
pybirdai/entry_points/run_create_executable_filters.py:16:0: C0411: standard import "os" should be placed before third party import "django" (wrong-import-order)
pybirdai/entry_points/run_create_executable_filters.py:19:0: C0411: third party import "django.conf.settings" should be placed before first party import "pybirdai.context.sdd_context_django.SDDContext"  (wrong-import-order)
pybirdai/entry_points/run_create_executable_filters.py:19:0: C0412: Imports from package django are not grouped (ungrouped-imports)
pybirdai/entry_points/run_create_executable_filters.py:15:0: W0611: Unused import django (unused-import)
************* Module upload_technical_export_files
pybirdai/entry_points/upload_technical_export_files.py:43:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/entry_points/upload_technical_export_files.py:50:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/entry_points/upload_technical_export_files.py:56:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/entry_points/upload_technical_export_files.py:65:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/entry_points/upload_technical_export_files.py:66:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/entry_points/upload_technical_export_files.py:18:0: E0401: Unable to import 'pybirdai.context.sdd_context_django' (import-error)
pybirdai/entry_points/upload_technical_export_files.py:39:8: E0401: Unable to import 'pybirdai.models.bird_meta_data_model' (import-error)
pybirdai/entry_points/upload_technical_export_files.py:39:8: C0415: Import outside toplevel (pybirdai.models.bird_meta_data_model.MAINTENANCE_AGENCY) (import-outside-toplevel)
pybirdai/entry_points/upload_technical_export_files.py:40:8: E0401: Unable to import 'pybirdai.process_steps.upload_files.file_uploader' (import-error)
pybirdai/entry_points/upload_technical_export_files.py:40:8: C0415: Import outside toplevel (pybirdai.process_steps.upload_files.file_uploader.FileUploader) (import-outside-toplevel)
pybirdai/entry_points/upload_technical_export_files.py:44:8: E0401: Unable to import 'pybirdai.context.context' (import-error)
pybirdai/entry_points/upload_technical_export_files.py:44:8: C0415: Import outside toplevel (pybirdai.context.context.Context) (import-outside-toplevel)
pybirdai/entry_points/upload_technical_export_files.py:39:8: W0611: Unused MAINTENANCE_AGENCY imported from pybirdai.models.bird_meta_data_model (unused-import)
pybirdai/entry_points/upload_technical_export_files.py:16:0: C0411: standard import "os" should be placed before third party import "django" (wrong-import-order)
pybirdai/entry_points/upload_technical_export_files.py:19:0: C0411: third party import "django.conf.settings" should be placed before first party import "pybirdai.context.sdd_context_django.SDDContext"  (wrong-import-order)
pybirdai/entry_points/upload_technical_export_files.py:19:0: C0412: Imports from package django are not grouped (ungrouped-imports)
pybirdai/entry_points/upload_technical_export_files.py:15:0: W0611: Unused import django (unused-import)
************* Module create_django_models
pybirdai/entry_points/create_django_models.py:42:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/entry_points/create_django_models.py:46:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/entry_points/create_django_models.py:70:0: C0305: Trailing newlines (trailing-newlines)
pybirdai/entry_points/create_django_models.py:17:0: E0401: Unable to import 'pybirdai.context.sdd_context_django' (import-error)
pybirdai/entry_points/create_django_models.py:18:0: E0401: Unable to import 'pybirdai.context.context' (import-error)
pybirdai/entry_points/create_django_models.py:19:0: E0401: Unable to import 'pybirdai.process_steps.sqldeveloper_import.convert_regdna_to_xcore' (import-error)
pybirdai/entry_points/create_django_models.py:23:0: E0401: Unable to import 'pybirdai.process_steps.sqldeveloper_import.import_sqldev_ldm_to_regdna' (import-error)
pybirdai/entry_points/create_django_models.py:26:0: E0401: Unable to import 'pybirdai.process_steps.sqldeveloper_import.import_sqldev_il_to_regdna' (import-error)
pybirdai/entry_points/create_django_models.py:29:0: E0401: Unable to import 'pybirdai.process_steps.sqldeveloper_import.import_sqldev_ldm_to_django' (import-error)
************* Module import_semantic_integrations_from_website
pybirdai/entry_points/import_semantic_integrations_from_website.py:52:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/entry_points/import_semantic_integrations_from_website.py:62:0: W0311: Bad indentation. Found 8 spaces, expected 4 (bad-indentation)
pybirdai/entry_points/import_semantic_integrations_from_website.py:65:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/entry_points/import_semantic_integrations_from_website.py:66:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/entry_points/import_semantic_integrations_from_website.py:18:0: E0401: Unable to import 'pybirdai.context.sdd_context_django' (import-error)
pybirdai/entry_points/import_semantic_integrations_from_website.py:39:8: E0401: Unable to import 'pybirdai.models.bird_meta_data_model' (import-error)
pybirdai/entry_points/import_semantic_integrations_from_website.py:39:8: C0415: Import outside toplevel (pybirdai.models.bird_meta_data_model.MAINTENANCE_AGENCY) (import-outside-toplevel)
pybirdai/entry_points/import_semantic_integrations_from_website.py:40:8: E0401: Unable to import 'pybirdai.process_steps.website_to_sddmodel.import_website_to_sdd_model_django' (import-error)
pybirdai/entry_points/import_semantic_integrations_from_website.py:40:8: C0415: Import outside toplevel (pybirdai.process_steps.website_to_sddmodel.import_website_to_sdd_model_django.ImportWebsiteToSDDModel) (import-outside-toplevel)
pybirdai/entry_points/import_semantic_integrations_from_website.py:43:8: E0401: Unable to import 'pybirdai.process_steps.input_model.import_database_to_sdd_model' (import-error)
pybirdai/entry_points/import_semantic_integrations_from_website.py:43:8: C0415: Import outside toplevel (pybirdai.process_steps.input_model.import_database_to_sdd_model.ImportDatabaseToSDDModel) (import-outside-toplevel)
pybirdai/entry_points/import_semantic_integrations_from_website.py:46:8: E0401: Unable to import 'pybirdai.context.context' (import-error)
pybirdai/entry_points/import_semantic_integrations_from_website.py:46:8: C0415: Import outside toplevel (pybirdai.context.context.Context) (import-outside-toplevel)
pybirdai/entry_points/import_semantic_integrations_from_website.py:39:8: W0611: Unused MAINTENANCE_AGENCY imported from pybirdai.models.bird_meta_data_model (unused-import)
pybirdai/entry_points/import_semantic_integrations_from_website.py:43:8: W0611: Unused ImportDatabaseToSDDModel imported from pybirdai.process_steps.input_model.import_database_to_sdd_model (unused-import)
pybirdai/entry_points/import_semantic_integrations_from_website.py:16:0: C0411: standard import "os" should be placed before third party import "django" (wrong-import-order)
pybirdai/entry_points/import_semantic_integrations_from_website.py:19:0: C0411: third party import "django.conf.settings" should be placed before first party import "pybirdai.context.sdd_context_django.SDDContext"  (wrong-import-order)
pybirdai/entry_points/import_semantic_integrations_from_website.py:19:0: C0412: Imports from package django are not grouped (ungrouped-imports)
pybirdai/entry_points/import_semantic_integrations_from_website.py:15:0: W0611: Unused import django (unused-import)
************* Module convert_ldm_to_sdd_hierarchies
pybirdai/entry_points/convert_ldm_to_sdd_hierarchies.py:40:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/entry_points/convert_ldm_to_sdd_hierarchies.py:49:0: C0304: Final newline missing (missing-final-newline)
pybirdai/entry_points/convert_ldm_to_sdd_hierarchies.py:16:0: E0401: Unable to import 'pybirdai.context.sdd_context_django' (import-error)
pybirdai/entry_points/convert_ldm_to_sdd_hierarchies.py:31:8: E0401: Unable to import 'pybirdai.process_steps.hierarchy_conversion.convert_ldm_to_sdd_hierarchies' (import-error)
pybirdai/entry_points/convert_ldm_to_sdd_hierarchies.py:31:8: C0415: Import outside toplevel (pybirdai.process_steps.hierarchy_conversion.convert_ldm_to_sdd_hierarchies.ConvertLDMToSDDHierarchies) (import-outside-toplevel)
pybirdai/entry_points/convert_ldm_to_sdd_hierarchies.py:34:8: E0401: Unable to import 'pybirdai.context.context' (import-error)
pybirdai/entry_points/convert_ldm_to_sdd_hierarchies.py:34:8: C0415: Import outside toplevel (pybirdai.context.context.Context) (import-outside-toplevel)
pybirdai/entry_points/convert_ldm_to_sdd_hierarchies.py:17:0: C0411: third party import "django.conf.settings" should be placed before first party import "pybirdai.context.sdd_context_django.SDDContext"  (wrong-import-order)
pybirdai/entry_points/convert_ldm_to_sdd_hierarchies.py:17:0: C0412: Imports from package django are not grouped (ungrouped-imports)
************* Module execute_datapoint
pybirdai/entry_points/execute_datapoint.py:51:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/entry_points/execute_datapoint.py:65:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/entry_points/execute_datapoint.py:69:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/entry_points/execute_datapoint.py:70:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/entry_points/execute_datapoint.py:18:0: E0401: Unable to import 'pybirdai.context.sdd_context_django' (import-error)
pybirdai/entry_points/execute_datapoint.py:40:8: E0401: Unable to import 'pybirdai.models.bird_meta_data_model' (import-error)
pybirdai/entry_points/execute_datapoint.py:40:8: C0415: Import outside toplevel (pybirdai.models.bird_meta_data_model.MAINTENANCE_AGENCY) (import-outside-toplevel)
pybirdai/entry_points/execute_datapoint.py:42:8: E0401: Unable to import 'pybirdai.process_steps.pybird.execute_datapoint' (import-error)
pybirdai/entry_points/execute_datapoint.py:42:8: C0415: Import outside toplevel (pybirdai.process_steps.pybird.execute_datapoint.ExecuteDataPoint) (import-outside-toplevel)
pybirdai/entry_points/execute_datapoint.py:45:8: E0401: Unable to import 'pybirdai.context.context' (import-error)
pybirdai/entry_points/execute_datapoint.py:45:8: C0415: Import outside toplevel (pybirdai.context.context.Context) (import-outside-toplevel)
pybirdai/entry_points/execute_datapoint.py:40:8: W0611: Unused MAINTENANCE_AGENCY imported from pybirdai.models.bird_meta_data_model (unused-import)
pybirdai/entry_points/execute_datapoint.py:16:0: C0411: standard import "os" should be placed before third party import "django" (wrong-import-order)
pybirdai/entry_points/execute_datapoint.py:19:0: C0411: third party import "django.conf.settings" should be placed before first party import "pybirdai.context.sdd_context_django.SDDContext"  (wrong-import-order)
pybirdai/entry_points/execute_datapoint.py:19:0: C0412: Imports from package django are not grouped (ungrouped-imports)
pybirdai/entry_points/execute_datapoint.py:15:0: W0611: Unused import django (unused-import)
************* Module import_input_model
pybirdai/entry_points/import_input_model.py:18:0: E0401: Unable to import 'pybirdai.context.sdd_context_django' (import-error)
pybirdai/entry_points/import_input_model.py:40:8: E0401: Unable to import 'pybirdai.models.bird_meta_data_model' (import-error)
pybirdai/entry_points/import_input_model.py:40:8: C0415: Import outside toplevel (pybirdai.models.bird_meta_data_model.MAINTENANCE_AGENCY) (import-outside-toplevel)
pybirdai/entry_points/import_input_model.py:42:8: E0401: Unable to import 'pybirdai.process_steps.input_model.import_input_model' (import-error)
pybirdai/entry_points/import_input_model.py:42:8: C0415: Import outside toplevel (pybirdai.process_steps.input_model.import_input_model.ImportInputModel) (import-outside-toplevel)
pybirdai/entry_points/import_input_model.py:45:8: E0401: Unable to import 'pybirdai.context.context' (import-error)
pybirdai/entry_points/import_input_model.py:45:8: C0415: Import outside toplevel (pybirdai.context.context.Context) (import-outside-toplevel)
pybirdai/entry_points/import_input_model.py:40:8: W0611: Unused MAINTENANCE_AGENCY imported from pybirdai.models.bird_meta_data_model (unused-import)
pybirdai/entry_points/import_input_model.py:16:0: C0411: standard import "os" should be placed before third party import "django" (wrong-import-order)
pybirdai/entry_points/import_input_model.py:19:0: C0411: third party import "django.conf.settings" should be placed before first party import "pybirdai.context.sdd_context_django.SDDContext"  (wrong-import-order)
pybirdai/entry_points/import_input_model.py:19:0: C0412: Imports from package django are not grouped (ungrouped-imports)
************* Module import_dpm_data
pybirdai/entry_points/import_dpm_data.py:18:0: E0401: Unable to import 'pybirdai.context.sdd_context_django' (import-error)
pybirdai/entry_points/import_dpm_data.py:40:8: W0621: Redefining name 'settings' from outer scope (line 19) (redefined-outer-name)
pybirdai/entry_points/import_dpm_data.py:35:8: E0401: Unable to import 'pybirdai.process_steps.dpm_integration.dpm_integration_service' (import-error)
pybirdai/entry_points/import_dpm_data.py:35:8: C0415: Import outside toplevel (pybirdai.process_steps.dpm_integration.dpm_integration_service.DPMImporterService) (import-outside-toplevel)
pybirdai/entry_points/import_dpm_data.py:36:8: E0401: Unable to import 'pybirdai.process_steps.website_to_sddmodel.import_website_to_sdd_model_django' (import-error)
pybirdai/entry_points/import_dpm_data.py:36:8: C0415: Import outside toplevel (pybirdai.process_steps.website_to_sddmodel.import_website_to_sdd_model_django.ImportWebsiteToSDDModel) (import-outside-toplevel)
pybirdai/entry_points/import_dpm_data.py:39:8: E0401: Unable to import 'pybirdai.context.context' (import-error)
pybirdai/entry_points/import_dpm_data.py:39:8: C0415: Import outside toplevel (pybirdai.context.context.Context) (import-outside-toplevel)
pybirdai/entry_points/import_dpm_data.py:40:8: W0404: Reimport 'settings' (imported line 19) (reimported)
pybirdai/entry_points/import_dpm_data.py:40:8: C0415: Import outside toplevel (django.conf.settings) (import-outside-toplevel)
pybirdai/entry_points/import_dpm_data.py:16:0: C0411: standard import "os" should be placed before third party import "django" (wrong-import-order)
pybirdai/entry_points/import_dpm_data.py:19:0: C0411: third party import "django.conf.settings" should be placed before first party import "pybirdai.context.sdd_context_django.SDDContext"  (wrong-import-order)
pybirdai/entry_points/import_dpm_data.py:20:0: C0411: standard import "logging" should be placed before third party imports "django", "django.apps.AppConfig", "django.conf.settings" and first party import "pybirdai.context.sdd_context_django.SDDContext"  (wrong-import-order)
pybirdai/entry_points/import_dpm_data.py:19:0: C0412: Imports from package django are not grouped (ungrouped-imports)
pybirdai/entry_points/import_dpm_data.py:15:0: W0611: Unused import django (unused-import)
************* Module create_filters
pybirdai/entry_points/create_filters.py:83:0: W0311: Bad indentation. Found 8 spaces, expected 4 (bad-indentation)
pybirdai/entry_points/create_filters.py:44:8: E0401: Unable to import 'pybirdai.models.bird_meta_data_model' (import-error)
pybirdai/entry_points/create_filters.py:44:8: C0415: Import outside toplevel (pybirdai.models.bird_meta_data_model.MAINTENANCE_AGENCY) (import-outside-toplevel)
pybirdai/entry_points/create_filters.py:45:8: E0401: Unable to import 'pybirdai.process_steps.input_model.import_database_to_sdd_model' (import-error)
pybirdai/entry_points/create_filters.py:45:8: C0415: Import outside toplevel (pybirdai.process_steps.input_model.import_database_to_sdd_model.ImportDatabaseToSDDModel) (import-outside-toplevel)
pybirdai/entry_points/create_filters.py:48:8: E0401: Unable to import 'pybirdai.context.sdd_context_django' (import-error)
pybirdai/entry_points/create_filters.py:48:8: C0415: Import outside toplevel (pybirdai.context.sdd_context_django.SDDContext) (import-outside-toplevel)
pybirdai/entry_points/create_filters.py:49:8: E0401: Unable to import 'pybirdai.context.context' (import-error)
pybirdai/entry_points/create_filters.py:49:8: C0415: Import outside toplevel (pybirdai.context.context.Context) (import-outside-toplevel)
pybirdai/entry_points/create_filters.py:51:8: E0401: Unable to import 'pybirdai.process_steps.report_filters.create_output_layers' (import-error)
pybirdai/entry_points/create_filters.py:51:8: C0415: Import outside toplevel (pybirdai.process_steps.report_filters.create_output_layers.CreateOutputLayers) (import-outside-toplevel)
pybirdai/entry_points/create_filters.py:54:8: E0401: Unable to import 'pybirdai.process_steps.report_filters.create_report_filters' (import-error)
pybirdai/entry_points/create_filters.py:54:8: C0415: Import outside toplevel (pybirdai.process_steps.report_filters.create_report_filters.CreateReportFilters) (import-outside-toplevel)
pybirdai/entry_points/create_filters.py:57:8: E0401: Unable to import 'pybirdai.process_steps.input_model.import_database_to_sdd_model' (import-error)
pybirdai/entry_points/create_filters.py:57:8: W0404: Reimport 'ImportDatabaseToSDDModel' (imported line 45) (reimported)
pybirdai/entry_points/create_filters.py:57:8: C0415: Import outside toplevel (pybirdai.process_steps.input_model.import_database_to_sdd_model.ImportDatabaseToSDDModel) (import-outside-toplevel)
pybirdai/entry_points/create_filters.py:44:8: W0611: Unused MAINTENANCE_AGENCY imported from pybirdai.models.bird_meta_data_model (unused-import)
pybirdai/entry_points/create_filters.py:45:8: W0611: Unused ImportDatabaseToSDDModel imported from pybirdai.process_steps.input_model.import_database_to_sdd_model (unused-import)
pybirdai/entry_points/create_filters.py:15:0: W0611: Unused import django (unused-import)
************* Module import_export_mapping_join_metadata
pybirdai/entry_points/import_export_mapping_join_metadata.py:20:0: E0401: Unable to import 'pybirdai.process_steps.import_export_join_metadata.export_join_metadata' (import-error)
pybirdai/entry_points/import_export_mapping_join_metadata.py:21:0: E0401: Unable to import 'pybirdai.process_steps.import_export_join_metadata.import_join_metadata' (import-error)
pybirdai/entry_points/import_export_mapping_join_metadata.py:22:0: E0401: Unable to import 'pybirdai.process_steps.mapping_join_metadata_eil_ldm.mapping_join_eil_ldm' (import-error)
pybirdai/entry_points/import_export_mapping_join_metadata.py:15:0: W0611: Unused Path imported from pathlib (unused-import)
pybirdai/entry_points/import_export_mapping_join_metadata.py:17:0: W0611: Unused import django (unused-import)
************* Module import_hierarchy_analysis_from_website
pybirdai/entry_points/import_hierarchy_analysis_from_website.py:52:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/entry_points/import_hierarchy_analysis_from_website.py:69:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/entry_points/import_hierarchy_analysis_from_website.py:70:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/entry_points/import_hierarchy_analysis_from_website.py:18:0: E0401: Unable to import 'pybirdai.context.sdd_context_django' (import-error)
pybirdai/entry_points/import_hierarchy_analysis_from_website.py:39:8: E0401: Unable to import 'pybirdai.models.bird_meta_data_model' (import-error)
pybirdai/entry_points/import_hierarchy_analysis_from_website.py:39:8: C0415: Import outside toplevel (pybirdai.models.bird_meta_data_model.MAINTENANCE_AGENCY) (import-outside-toplevel)
pybirdai/entry_points/import_hierarchy_analysis_from_website.py:40:8: E0401: Unable to import 'pybirdai.process_steps.website_to_sddmodel.import_website_to_sdd_model_django' (import-error)
pybirdai/entry_points/import_hierarchy_analysis_from_website.py:40:8: C0415: Import outside toplevel (pybirdai.process_steps.website_to_sddmodel.import_website_to_sdd_model_django.ImportWebsiteToSDDModel) (import-outside-toplevel)
pybirdai/entry_points/import_hierarchy_analysis_from_website.py:43:8: E0401: Unable to import 'pybirdai.process_steps.input_model.import_database_to_sdd_model' (import-error)
pybirdai/entry_points/import_hierarchy_analysis_from_website.py:43:8: C0415: Import outside toplevel (pybirdai.process_steps.input_model.import_database_to_sdd_model.ImportDatabaseToSDDModel) (import-outside-toplevel)
pybirdai/entry_points/import_hierarchy_analysis_from_website.py:46:8: E0401: Unable to import 'pybirdai.context.context' (import-error)
pybirdai/entry_points/import_hierarchy_analysis_from_website.py:46:8: C0415: Import outside toplevel (pybirdai.context.context.Context) (import-outside-toplevel)
pybirdai/entry_points/import_hierarchy_analysis_from_website.py:39:8: W0611: Unused MAINTENANCE_AGENCY imported from pybirdai.models.bird_meta_data_model (unused-import)
pybirdai/entry_points/import_hierarchy_analysis_from_website.py:43:8: W0611: Unused ImportDatabaseToSDDModel imported from pybirdai.process_steps.input_model.import_database_to_sdd_model (unused-import)
pybirdai/entry_points/import_hierarchy_analysis_from_website.py:16:0: C0411: standard import "os" should be placed before third party import "django" (wrong-import-order)
pybirdai/entry_points/import_hierarchy_analysis_from_website.py:19:0: C0411: third party import "django.conf.settings" should be placed before first party import "pybirdai.context.sdd_context_django.SDDContext"  (wrong-import-order)
pybirdai/entry_points/import_hierarchy_analysis_from_website.py:19:0: C0412: Imports from package django are not grouped (ungrouped-imports)
************* Module ancrdt_transformation
pybirdai/entry_points/ancrdt_transformation.py:52:8: E0401: Unable to import 'pybirdai.utils.bird_ecb_website_fetcher' (import-error)
pybirdai/entry_points/ancrdt_transformation.py:52:8: C0415: Import outside toplevel (pybirdai.utils.bird_ecb_website_fetcher.BirdEcbWebsiteClient) (import-outside-toplevel)
pybirdai/entry_points/ancrdt_transformation.py:66:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/ancrdt_transformation.py:69:12: C0415: Import outside toplevel (traceback) (import-outside-toplevel)
pybirdai/entry_points/ancrdt_transformation.py:71:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/ancrdt_transformation.py:72:12: W0707: Consider explicitly re-raising using 'raise Exception(f"ANCRDT Step 0 failed: {str(e) or 'Unknown error occurred'}") from e' (raise-missing-from)
pybirdai/entry_points/ancrdt_transformation.py:72:12: W0719: Raising too general exception: Exception (broad-exception-raised)
pybirdai/entry_points/ancrdt_transformation.py:79:8: E0401: Unable to import 'pybirdai.process_steps.ancrdt_transformation.ancrdt_importer' (import-error)
pybirdai/entry_points/ancrdt_transformation.py:79:8: C0415: Import outside toplevel (pybirdai.process_steps.ancrdt_transformation.ancrdt_importer.RunANCRDTImport) (import-outside-toplevel)
pybirdai/entry_points/ancrdt_transformation.py:86:12: C0415: Import outside toplevel (traceback) (import-outside-toplevel)
pybirdai/entry_points/ancrdt_transformation.py:88:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/ancrdt_transformation.py:89:12: W0707: Consider explicitly re-raising using 'raise Exception(f"ANCRDT Step 1 failed: {str(e) or 'Unknown error occurred'}") from e' (raise-missing-from)
pybirdai/entry_points/ancrdt_transformation.py:89:12: W0719: Raising too general exception: Exception (broad-exception-raised)
pybirdai/entry_points/ancrdt_transformation.py:96:8: E0401: Unable to import 'pybirdai.process_steps.ancrdt_transformation.create_joins_meta_data_ancrdt' (import-error)
pybirdai/entry_points/ancrdt_transformation.py:96:8: C0415: Import outside toplevel (pybirdai.process_steps.ancrdt_transformation.create_joins_meta_data_ancrdt.JoinsMetaDataCreatorANCRDT) (import-outside-toplevel)
pybirdai/entry_points/ancrdt_transformation.py:106:12: C0415: Import outside toplevel (traceback) (import-outside-toplevel)
pybirdai/entry_points/ancrdt_transformation.py:108:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/ancrdt_transformation.py:109:12: W0707: Consider explicitly re-raising using 'raise Exception(f"ANCRDT Step 2 failed: {str(e) or 'Unknown error occurred'}") from e' (raise-missing-from)
pybirdai/entry_points/ancrdt_transformation.py:109:12: W0719: Raising too general exception: Exception (broad-exception-raised)
pybirdai/entry_points/ancrdt_transformation.py:116:8: E0401: Unable to import 'pybirdai.process_steps.ancrdt_transformation.create_executable_joins_ancrdt' (import-error)
pybirdai/entry_points/ancrdt_transformation.py:116:8: C0415: Import outside toplevel (pybirdai.process_steps.ancrdt_transformation.create_executable_joins_ancrdt.RunCreateExecutableJoins) (import-outside-toplevel)
pybirdai/entry_points/ancrdt_transformation.py:125:12: C0415: Import outside toplevel (traceback) (import-outside-toplevel)
pybirdai/entry_points/ancrdt_transformation.py:127:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/ancrdt_transformation.py:128:12: W0707: Consider explicitly re-raising using 'raise Exception(f"ANCRDT Step 3 failed: {str(e) or 'Unknown error occurred'}") from e' (raise-missing-from)
pybirdai/entry_points/ancrdt_transformation.py:128:12: W0719: Raising too general exception: Exception (broad-exception-raised)
pybirdai/entry_points/ancrdt_transformation.py:151:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/entry_points/ancrdt_transformation.py:20:0: C0411: standard import "logging" should be placed before third party imports "django", "django.apps.AppConfig", "django.conf.settings" (wrong-import-order)
pybirdai/entry_points/ancrdt_transformation.py:16:0: W0611: Unused import sys (unused-import)
pybirdai/entry_points/ancrdt_transformation.py:17:0: W0611: Unused import django (unused-import)
pybirdai/entry_points/ancrdt_transformation.py:1:0: R0801: Similar lines in 2 files
==delete_joins:[24:57]
==delete_joins_metadata:[24:57]
    path = os.path.join(settings.BASE_DIR, 'birds_nest')

    @staticmethod
    def run_joins():
        """Execute the process of creating generation rules when the app is ready."""

        from pybirdai.context.sdd_context_django import SDDContext
        from pybirdai.context.context import Context

        from pybirdai.process_steps.joins_meta_data.delete_joins_meta_data import (
            TransformationMetaDataDestroyer
        )


        base_dir = settings.BASE_DIR
        sdd_context = SDDContext()
        sdd_context.file_directory = os.path.join(base_dir, 'resources')
        sdd_context.output_directory = os.path.join(base_dir, 'results')

        context = Context()
        context.file_directory = sdd_context.file_directory
        context.output_directory = sdd_context.output_directory


        TransformationMetaDataDestroyer().delete_joins_meta_data(
            context,
            sdd_context,
            "FINREP_REF"
        )

def ready(self):
        # This method is still needed for Django's AppConfig
        pass (duplicate-code)
pybirdai/entry_points/ancrdt_transformation.py:1:0: R0801: Similar lines in 2 files
==delete_bird_metadata_database:[21:48]
==delete_joins:[21:48]
class RunDeleteBirdMetadataDatabase(AppConfig):
    """Django AppConfig for running the creation of generation rules."""

    path = os.path.join(settings.BASE_DIR, 'birds_nest')

    @staticmethod
    def run_delete_bird_metadata_database():
        """Execute the process of creating generation rules when the app is ready."""

        from pybirdai.context.sdd_context_django import SDDContext
        from pybirdai.context.context import Context

        from pybirdai.process_steps.joins_meta_data.delete_joins_meta_data import (
            TransformationMetaDataDestroyer
        )


        base_dir = settings.BASE_DIR
        sdd_context = SDDContext()
        sdd_context.file_directory = os.path.join(base_dir, 'resources')
        sdd_context.output_directory = os.path.join(base_dir, 'results')

        context = Context()
        context.file_directory = sdd_context.file_directory
        context.output_directory = sdd_context.output_directory

 (duplicate-code)
pybirdai/entry_points/ancrdt_transformation.py:1:0: R0801: Similar lines in 2 files
==create_filters:[27:73]
==import_input_model:[28:57]
    path = os.path.join(settings.BASE_DIR, 'birds_nest')
    #path = os.path.join(settings.BASE_DIR, 'birds_nest')


    def ready(self):
        """
        Prepare and execute the website to SDD model conversion process.

        This method sets up the necessary contexts, creates reference domains
        and variables, and imports the website data into the SDD model.
        """
        from pybirdai.models.bird_meta_data_model import MAINTENANCE_AGENCY

        from pybirdai.process_steps.input_model.import_input_model import (
            ImportInputModel
        )
        from pybirdai.context.context import Context
        # from pybirdai.context.context_ancrdt import Context

        base_dir = settings.BASE_DIR
        sdd_context = SDDContext()
        sdd_context.file_directory = os.path.join(base_dir, 'resources')
        sdd_context.output_directory = os.path.join(base_dir, 'results')

        context = Context()
        context.file_directory = sdd_context.file_directory
        context.output_directory = sdd_context.output_directory

        # Create reference domains, variables, and cubes (duplicate-code)
pybirdai/entry_points/ancrdt_transformation.py:1:0: R0801: Similar lines in 2 files
==convert_ldm_to_sdd_hierarchies:[23:44]
==execute_datapoint:[28:56]
    path = os.path.join(settings.BASE_DIR, 'birds_nest')
    #path = os.path.join(settings.BASE_DIR, 'birds_nest')

    @staticmethod
    def run_execute_data_point(data_point_id):
        """
        Prepare and execute the website to SDD model conversion process.

        This method sets up the necessary contexts, creates reference domains
        and variables, and imports the website data into the SDD model.
        """
        from pybirdai.models.bird_meta_data_model import MAINTENANCE_AGENCY

        from pybirdai.process_steps.pybird.execute_datapoint import (
            ExecuteDataPoint
        )
        from pybirdai.context.context import Context

        base_dir = settings.BASE_DIR
        sdd_context = SDDContext()
        sdd_context.file_directory = os.path.join(base_dir, 'resources')
        sdd_context.output_directory = os.path.join(base_dir, 'results')

        context = Context()
        context.file_directory = sdd_context.file_directory
        context.output_directory = sdd_context.output_directory

        # Create reference domains, variables, and cubes (duplicate-code)
pybirdai/entry_points/ancrdt_transformation.py:1:0: R0801: Similar lines in 2 files
==create_django_models:[35:49]
==import_semantic_integrations_from_website:[28:57]
    path = os.path.join(settings.BASE_DIR, 'birds_nest')


    def ready(self):
        """Prepare the context and run the import and conversion processes."""
        base_dir = settings.BASE_DIR

        sdd_context = SDDContext()
        sdd_context.file_directory = os.path.join(base_dir, 'resources')
        sdd_context.output_directory = os.path.join(base_dir, 'results')

        context = Context()
        context.file_directory = sdd_context.file_directory
        context.output_directory = sdd_context.output_directory (duplicate-code)
pybirdai/entry_points/ancrdt_transformation.py:1:0: R0801: Similar lines in 2 files
==run_create_executable_filters:[28:52]
==upload_technical_export_files:[28:54]
    path = os.path.join(settings.BASE_DIR, 'birds_nest')

    @staticmethod
    def run_create_executable_filters():
        from pybirdai.models.bird_meta_data_model import MAINTENANCE_AGENCY

        from pybirdai.process_steps.input_model.import_database_to_sdd_model import (
            ImportDatabaseToSDDModel
        )
        from pybirdai.process_steps.pybird.create_executable_filters import (
            CreateExecutableFilters
        )
        from pybirdai.context.context import Context

        base_dir = settings.BASE_DIR
        sdd_context = SDDContext()
        sdd_context.file_directory = os.path.join(base_dir, 'resources')
        sdd_context.output_directory = os.path.join(base_dir, 'results')

        context = Context()
        context.file_directory = sdd_context.file_directory
        context.output_directory = sdd_context.output_directory

        #ImportDatabaseToSDDModel().import_sdd(sdd_context) (duplicate-code)
pybirdai/entry_points/ancrdt_transformation.py:1:0: R0801: Similar lines in 2 files
==dpm_output_layer_creation:[30:69]
==import_dpm_data:[29:52]
    path = os.path.join(settings.BASE_DIR, 'birds_nest')

    @staticmethod
    def run_import(import_:bool):
        # Import and run the DPM integration service
        from pybirdai.process_steps.dpm_integration.dpm_integration_service import DPMImporterService
        from pybirdai.process_steps.website_to_sddmodel.import_website_to_sdd_model_django import (
            ImportWebsiteToSDDModel
        )
        from pybirdai.context.context import Context
        from django.conf import settings

        base_dir = settings.BASE_DIR
        sdd_context = SDDContext()
        sdd_context.file_directory = os.path.join(base_dir, 'results')
        sdd_context.output_directory = os.path.join(base_dir, 'results')


        context = Context()
        context.file_directory = sdd_context.output_directory
        context.output_directory = sdd_context.output_directory

        # Run DPM import service (duplicate-code)
pybirdai/entry_points/ancrdt_transformation.py:1:0: R0801: Similar lines in 2 files
==delete_joins_metadata:[24:48]
==import_report_templates_from_website:[31:51]
    path = os.path.join(settings.BASE_DIR, 'birds_nest')

    @staticmethod
    def run_import():
        # Move the content of the ready() method here
        from pybirdai.process_steps.website_to_sddmodel.import_website_to_sdd_model_django import (
            ImportWebsiteToSDDModel
        )
        from pybirdai.context.context import Context
        from django.conf import settings

        base_dir = settings.BASE_DIR
        sdd_context = SDDContext()
        sdd_context.file_directory = os.path.join(base_dir, 'resources')
        sdd_context.output_directory = os.path.join(base_dir, 'results')

        context = Context()
        context.file_directory = sdd_context.file_directory
        context.output_directory = sdd_context.output_directory
 (duplicate-code)
pybirdai/entry_points/ancrdt_transformation.py:1:0: R0801: Similar lines in 2 files
==delete_output_concepts:[24:48]
==upload_joins_configuration:[28:54]
    path = os.path.join(settings.BASE_DIR, 'birds_nest')

    @staticmethod
    def upload_joins_configuration(request):
        """
        Prepare and execute the website to SDD model conversion process.

        This method sets up the necessary contexts, creates reference domains
        and variables, and imports the website data into the SDD model.
        """
        from pybirdai.models.bird_meta_data_model import MAINTENANCE_AGENCY
        from pybirdai.process_steps.upload_files.file_uploader import (
            FileUploader
        )

        from pybirdai.context.context import Context

        base_dir = settings.BASE_DIR
        sdd_context = SDDContext()
        sdd_context.file_directory = os.path.join(base_dir, 'resources')
        sdd_context.output_directory = os.path.join(base_dir, 'results')

        context = Context()
        context.file_directory = sdd_context.file_directory
        context.output_directory = sdd_context.output_directory
 (duplicate-code)
pybirdai/entry_points/ancrdt_transformation.py:1:0: R0801: Similar lines in 2 files
==delete_bird_metadata_database:[24:48]
==upload_sqldev_eil_files:[28:54]
    path = os.path.join(settings.BASE_DIR, 'birds_nest')

    @staticmethod
    def upload_sqldev_eil_files(request):
        """
        Prepare and execute the website to SDD model conversion process.

        This method sets up the necessary contexts, creates reference domains
        and variables, and imports the website data into the SDD model.
        """
        from pybirdai.models.bird_meta_data_model import MAINTENANCE_AGENCY
        from pybirdai.process_steps.upload_files.file_uploader import (
            FileUploader
        )

        from pybirdai.context.context import Context

        base_dir = settings.BASE_DIR
        sdd_context = SDDContext()
        sdd_context.file_directory = os.path.join(base_dir, 'resources')
        sdd_context.output_directory = os.path.join(base_dir, 'results')

        context = Context()
        context.file_directory = sdd_context.file_directory
        context.output_directory = sdd_context.output_directory
 (duplicate-code)
pybirdai/entry_points/ancrdt_transformation.py:1:0: R0801: Similar lines in 2 files
==create_executable_joins:[62:74]
==run_create_executable_filters:[66:77]
        base_dir = settings.BASE_DIR
        sdd_context = SDDContext()
        sdd_context.file_directory = os.path.join(base_dir, 'resources')
        sdd_context.output_directory = os.path.join(base_dir, 'results')

        context = Context()
        context.file_directory = sdd_context.file_directory
        context.output_directory = sdd_context.output_directory

        # Only import the necessary tables for joins
        importer = ImportDatabaseToSDDModel()
 (duplicate-code)
pybirdai/entry_points/ancrdt_transformation.py:1:0: R0801: Similar lines in 2 files
==create_executable_joins:[24:48]
==upload_sqldev_eldm_files:[28:54]
    path = os.path.join(settings.BASE_DIR, 'birds_nest')

    @staticmethod
    def upload_sqldev_eldm_files(request):
        """
        Prepare and execute the website to SDD model conversion process.

        This method sets up the necessary contexts, creates reference domains
        and variables, and imports the website data into the SDD model.
        """
        from pybirdai.models.bird_meta_data_model import MAINTENANCE_AGENCY
        from pybirdai.process_steps.upload_files.file_uploader import (
            FileUploader
        )

        from pybirdai.context.context import Context

        base_dir = settings.BASE_DIR
        sdd_context = SDDContext()
        sdd_context.file_directory = os.path.join(base_dir, 'resources')
        sdd_context.output_directory = os.path.join(base_dir, 'results')

        context = Context()
        context.file_directory = sdd_context.file_directory
        context.output_directory = sdd_context.output_directory
 (duplicate-code)
pybirdai/entry_points/ancrdt_transformation.py:1:0: R0801: Similar lines in 2 files
==delete_joins:[24:48]
==delete_semantic_integrations:[24:48]
    path = os.path.join(settings.BASE_DIR, 'birds_nest')

    @staticmethod
    def run_delete_semantic_integrations():
        """Execute the process of creating generation rules when the app is ready."""

        from pybirdai.context.sdd_context_django import SDDContext
        from pybirdai.context.context import Context

        from pybirdai.process_steps.joins_meta_data.delete_joins_meta_data import (
            TransformationMetaDataDestroyer
        )


        base_dir = settings.BASE_DIR
        sdd_context = SDDContext()
        sdd_context.file_directory = os.path.join(base_dir, 'resources')
        sdd_context.output_directory = os.path.join(base_dir, 'results')

        context = Context()
        context.file_directory = sdd_context.file_directory
        context.output_directory = sdd_context.output_directory

 (duplicate-code)
pybirdai/entry_points/ancrdt_transformation.py:1:0: R0801: Similar lines in 2 files
==run_create_executable_filters:[66:76]
==upload_sqldev_eldm_files:[45:54]
        base_dir = settings.BASE_DIR
        sdd_context = SDDContext()
        sdd_context.file_directory = os.path.join(base_dir, 'resources')
        sdd_context.output_directory = os.path.join(base_dir, 'results')

        context = Context()
        context.file_directory = sdd_context.file_directory
        context.output_directory = sdd_context.output_directory
 (duplicate-code)
pybirdai/entry_points/ancrdt_transformation.py:1:0: R0801: Similar lines in 2 files
==create_executable_joins:[62:72]
==delete_joins:[38:48]
        base_dir = settings.BASE_DIR
        sdd_context = SDDContext()
        sdd_context.file_directory = os.path.join(base_dir, 'resources')
        sdd_context.output_directory = os.path.join(base_dir, 'results')

        context = Context()
        context.file_directory = sdd_context.file_directory
        context.output_directory = sdd_context.output_directory

        # Only import the necessary tables for joins (duplicate-code)
pybirdai/entry_points/ancrdt_transformation.py:1:0: R0801: Similar lines in 2 files
==create_joins_metadata:[46:57]
==delete_semantic_integrations:[38:48]
        base_dir = settings.BASE_DIR
        sdd_context = SDDContext()
        sdd_context.file_directory = os.path.join(base_dir, 'resources')
        sdd_context.output_directory = os.path.join(base_dir, 'results')

        context = Context()
        context.file_directory = sdd_context.file_directory
        context.output_directory = sdd_context.output_directory

        #ImportDatabaseToSDDModel().import_sdd(sdd_context)
 (duplicate-code)

-----------------------------------
Your code has been rated at 1.74/10


Running pylint on: pybirdai/process_steps/
-----------------------------------
************* Module pybirdai.process_steps.import_export_join_metadata.export_join_metadata
pybirdai/process_steps/import_export_join_metadata/export_join_metadata.py:16:0: W0611: Unused apps imported from django.apps (unused-import)
pybirdai/process_steps/import_export_join_metadata/export_join_metadata.py:17:0: W0611: Unused timezone imported from django.utils (unused-import)
************* Module pybirdai.process_steps.import_export_join_metadata.import_join_metadata
pybirdai/process_steps/import_export_join_metadata/import_join_metadata.py:68:4: R0914: Too many local variables (20/15) (too-many-locals)
pybirdai/process_steps/import_export_join_metadata/import_join_metadata.py:156:15: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/process_steps/import_export_join_metadata/import_join_metadata.py:150:31: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/process_steps/import_export_join_metadata/import_join_metadata.py:221:15: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/process_steps/import_export_join_metadata/import_join_metadata.py:215:31: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/process_steps/import_export_join_metadata/import_join_metadata.py:68:4: R0912: Too many branches (19/12) (too-many-branches)
pybirdai/process_steps/import_export_join_metadata/import_join_metadata.py:68:4: R0915: Too many statements (76/50) (too-many-statements)
pybirdai/process_steps/import_export_join_metadata/import_join_metadata.py:143:28: W0612: Unused variable 'obj' (unused-variable)
pybirdai/process_steps/import_export_join_metadata/import_join_metadata.py:143:33: W0612: Unused variable 'created' (unused-variable)
************* Module pybirdai.process_steps.upload_files.file_uploader
pybirdai/process_steps/upload_files/file_uploader.py:216:19: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/process_steps/upload_files/file_uploader.py:264:19: W0705: Catching previously caught exception type ValueError (duplicate-except)
pybirdai/process_steps/upload_files/file_uploader.py:269:19: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/process_steps/upload_files/file_uploader.py:318:19: W0705: Catching previously caught exception type ValueError (duplicate-except)
pybirdai/process_steps/upload_files/file_uploader.py:323:19: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/process_steps/upload_files/file_uploader.py:371:19: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/process_steps/upload_files/file_uploader.py:459:12: W0707: Consider explicitly re-raising using 'raise ValueError(f'Failed to save file: {str(e)}') from e' (raise-missing-from)
pybirdai/process_steps/upload_files/file_uploader.py:16:0: W0611: Unused import uuid (unused-import)
pybirdai/process_steps/upload_files/file_uploader.py:17:0: W0611: Unused Path imported from pathlib (unused-import)
************* Module pybirdai.process_steps.generate_test_data.enrich_ldm_with_il_links_from_fe
pybirdai/process_steps/generate_test_data/enrich_ldm_with_il_links_from_fe.py:58:0: C0325: Unnecessary parens after 'not' keyword (superfluous-parens)
pybirdai/process_steps/generate_test_data/enrich_ldm_with_il_links_from_fe.py:102:0: C0325: Unnecessary parens after 'not' keyword (superfluous-parens)
pybirdai/process_steps/generate_test_data/enrich_ldm_with_il_links_from_fe.py:20:0: R0205: Class 'InputLayerLinkEnricher' inherits from object, can be safely removed from bases in python3 (useless-object-inheritance)
pybirdai/process_steps/generate_test_data/enrich_ldm_with_il_links_from_fe.py:41:4: R0914: Too many local variables (24/15) (too-many-locals)
pybirdai/process_steps/generate_test_data/enrich_ldm_with_il_links_from_fe.py:48:12: R1702: Too many nested blocks (6/5) (too-many-nested-blocks)
pybirdai/process_steps/generate_test_data/enrich_ldm_with_il_links_from_fe.py:48:12: R1702: Too many nested blocks (7/5) (too-many-nested-blocks)
pybirdai/process_steps/generate_test_data/enrich_ldm_with_il_links_from_fe.py:41:4: R0912: Too many branches (16/12) (too-many-branches)
pybirdai/process_steps/generate_test_data/enrich_ldm_with_il_links_from_fe.py:41:4: R0915: Too many statements (60/50) (too-many-statements)
pybirdai/process_steps/generate_test_data/enrich_ldm_with_il_links_from_fe.py:48:12: R1702: Too many nested blocks (7/5) (too-many-nested-blocks)
pybirdai/process_steps/generate_test_data/enrich_ldm_with_il_links_from_fe.py:135:8: R1702: Too many nested blocks (14/5) (too-many-nested-blocks)
pybirdai/process_steps/generate_test_data/enrich_ldm_with_il_links_from_fe.py:134:4: R0912: Too many branches (20/12) (too-many-branches)
pybirdai/process_steps/generate_test_data/enrich_ldm_with_il_links_from_fe.py:135:8: R1702: Too many nested blocks (9/5) (too-many-nested-blocks)
pybirdai/process_steps/generate_test_data/enrich_ldm_with_il_links_from_fe.py:134:4: R1710: Either all return statements in a function should return an expression, or none of them should. (inconsistent-return-statements)
pybirdai/process_steps/generate_test_data/enrich_ldm_with_il_links_from_fe.py:164:8: R1702: Too many nested blocks (7/5) (too-many-nested-blocks)
pybirdai/process_steps/generate_test_data/enrich_ldm_with_il_links_from_fe.py:163:4: R1710: Either all return statements in a function should return an expression, or none of them should. (inconsistent-return-statements)
pybirdai/process_steps/generate_test_data/enrich_ldm_with_il_links_from_fe.py:16:0: C0411: standard import "os" should be placed before first party import "pybirdai.utils.utils.Utils"  (wrong-import-order)
pybirdai/process_steps/generate_test_data/enrich_ldm_with_il_links_from_fe.py:18:0: W0611: Unused ELEnum imported from pybirdai.regdna (unused-import)
pybirdai/process_steps/generate_test_data/enrich_ldm_with_il_links_from_fe.py:18:0: W0611: Unused ELEnumLiteral imported from pybirdai.regdna (unused-import)
pybirdai/process_steps/generate_test_data/enrich_ldm_with_il_links_from_fe.py:18:0: W0611: Unused ELOperation imported from pybirdai.regdna (unused-import)
************* Module pybirdai.process_steps.generate_test_data.ldm_utils
pybirdai/process_steps/generate_test_data/ldm_utils.py:54:0: C0325: Unnecessary parens after 'not' keyword (superfluous-parens)
pybirdai/process_steps/generate_test_data/ldm_utils.py:26:0: R0205: Class 'Utils' inherits from object, can be safely removed from bases in python3 (useless-object-inheritance)
pybirdai/process_steps/generate_test_data/ldm_utils.py:239:8: R1705: Unnecessary "else" after "return", remove the "else" and de-indent the code inside it (no-else-return)
pybirdai/process_steps/generate_test_data/ldm_utils.py:239:12: R1714: Consider merging these comparisons with 'in' by using 'adapted_enum_name in ('All_last_days_of_months___YYYY_MM', 'All_last_days_of_quarters___YYYY_MM', 'All_possible_dates_YYYY_MM_DD')'. Use a set instead if elements are hashable. (consider-using-in)
pybirdai/process_steps/generate_test_data/ldm_utils.py:297:12: W0612: Unused variable 'key' (unused-variable)
pybirdai/process_steps/generate_test_data/ldm_utils.py:21:0: W0611: Unused ResourceSet imported from pyecore.resources (unused-import)
pybirdai/process_steps/generate_test_data/ldm_utils.py:21:0: W0611: Unused URI imported from pyecore.resources (unused-import)
************* Module pybirdai.process_steps.generate_test_data.traverser
pybirdai/process_steps/generate_test_data/traverser.py:234:0: W0311: Bad indentation. Found 24 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/generate_test_data/traverser.py:244:0: W0311: Bad indentation. Found 24 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/generate_test_data/traverser.py:248:0: W0311: Bad indentation. Found 16 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/generate_test_data/traverser.py:249:0: W0311: Bad indentation. Found 16 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/generate_test_data/traverser.py:250:0: W0311: Bad indentation. Found 20 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/generate_test_data/traverser.py:251:0: W0311: Bad indentation. Found 16 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/generate_test_data/traverser.py:252:0: W0311: Bad indentation. Found 24 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/generate_test_data/traverser.py:253:0: W0311: Bad indentation. Found 16 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/generate_test_data/traverser.py:277:0: C0325: Unnecessary parens after 'not' keyword (superfluous-parens)
pybirdai/process_steps/generate_test_data/traverser.py:293:0: C0325: Unnecessary parens after 'not' keyword (superfluous-parens)
pybirdai/process_steps/generate_test_data/traverser.py:308:0: C0325: Unnecessary parens after 'not' keyword (superfluous-parens)
pybirdai/process_steps/generate_test_data/traverser.py:345:0: C0325: Unnecessary parens after 'if' keyword (superfluous-parens)
pybirdai/process_steps/generate_test_data/traverser.py:413:0: C0325: Unnecessary parens after 'not' keyword (superfluous-parens)
pybirdai/process_steps/generate_test_data/traverser.py:430:0: C0325: Unnecessary parens after 'not' keyword (superfluous-parens)
pybirdai/process_steps/generate_test_data/traverser.py:431:0: W0311: Bad indentation. Found 24 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/generate_test_data/traverser.py:442:0: C0325: Unnecessary parens after 'not' keyword (superfluous-parens)
pybirdai/process_steps/generate_test_data/traverser.py:472:0: C0325: Unnecessary parens after 'not' keyword (superfluous-parens)
pybirdai/process_steps/generate_test_data/traverser.py:531:0: C0325: Unnecessary parens after 'not' keyword (superfluous-parens)
pybirdai/process_steps/generate_test_data/traverser.py:537:0: W0301: Unnecessary semicolon (unnecessary-semicolon)
pybirdai/process_steps/generate_test_data/traverser.py:545:0: W0301: Unnecessary semicolon (unnecessary-semicolon)
pybirdai/process_steps/generate_test_data/traverser.py:553:0: C0325: Unnecessary parens after 'not' keyword (superfluous-parens)
pybirdai/process_steps/generate_test_data/traverser.py:558:0: C0325: Unnecessary parens after 'not' keyword (superfluous-parens)
pybirdai/process_steps/generate_test_data/traverser.py:576:0: C0325: Unnecessary parens after 'not' keyword (superfluous-parens)
pybirdai/process_steps/generate_test_data/traverser.py:581:0: C0325: Unnecessary parens after 'not' keyword (superfluous-parens)
pybirdai/process_steps/generate_test_data/traverser.py:611:0: C0325: Unnecessary parens after 'not' keyword (superfluous-parens)
pybirdai/process_steps/generate_test_data/traverser.py:18:0: R0205: Class 'SubtypeExploder' inherits from object, can be safely removed from bases in python3 (useless-object-inheritance)
pybirdai/process_steps/generate_test_data/traverser.py:58:4: R0914: Too many local variables (16/15) (too-many-locals)
pybirdai/process_steps/generate_test_data/traverser.py:58:4: R0912: Too many branches (29/12) (too-many-branches)
pybirdai/process_steps/generate_test_data/traverser.py:58:4: R0915: Too many statements (73/50) (too-many-statements)
pybirdai/process_steps/generate_test_data/traverser.py:91:8: R1702: Too many nested blocks (6/5) (too-many-nested-blocks)
pybirdai/process_steps/generate_test_data/traverser.py:135:16: R1732: Consider using 'with' for resource-allocating operations (consider-using-with)
pybirdai/process_steps/generate_test_data/traverser.py:178:8: W0612: Unused variable 'il_table_names' (unused-variable)
pybirdai/process_steps/generate_test_data/traverser.py:187:12: R1705: Unnecessary "else" after "return", remove the "else" and de-indent the code inside it (no-else-return)
pybirdai/process_steps/generate_test_data/traverser.py:193:12: R1705: Unnecessary "else" after "return", remove the "else" and de-indent the code inside it (no-else-return)
pybirdai/process_steps/generate_test_data/traverser.py:224:8: W0622: Redefining built-in 'map' (redefined-builtin)
pybirdai/process_steps/generate_test_data/traverser.py:260:4: R0914: Too many local variables (27/15) (too-many-locals)
pybirdai/process_steps/generate_test_data/traverser.py:260:4: R0912: Too many branches (18/12) (too-many-branches)
pybirdai/process_steps/generate_test_data/traverser.py:329:20: W0612: Unused variable 'each_entity' (unused-variable)
pybirdai/process_steps/generate_test_data/traverser.py:362:4: R0912: Too many branches (14/12) (too-many-branches)
pybirdai/process_steps/generate_test_data/traverser.py:391:16: W0612: Unused variable 'item' (unused-variable)
pybirdai/process_steps/generate_test_data/traverser.py:486:12: W0622: Redefining built-in 'type' (redefined-builtin)
pybirdai/process_steps/generate_test_data/traverser.py:488:16: R1705: Unnecessary "else" after "return", remove the "else" and de-indent the code inside it (no-else-return)
pybirdai/process_steps/generate_test_data/traverser.py:493:16: R1705: Unnecessary "elif" after "return", remove the leading "el" from "elif" (no-else-return)
pybirdai/process_steps/generate_test_data/traverser.py:481:4: R0911: Too many return statements (9/6) (too-many-return-statements)
pybirdai/process_steps/generate_test_data/traverser.py:564:8: R1705: Unnecessary "elif" after "return", remove the leading "el" from "elif" (no-else-return)
pybirdai/process_steps/generate_test_data/traverser.py:587:8: R1705: Unnecessary "elif" after "return", remove the leading "el" from "elif" (no-else-return)
pybirdai/process_steps/generate_test_data/traverser.py:643:4: R1710: Either all return statements in a function should return an expression, or none of them should. (inconsistent-return-statements)
pybirdai/process_steps/generate_test_data/traverser.py:18:0: R0904: Too many public methods (21/20) (too-many-public-methods)
************* Module pybirdai.process_steps.metadata_lineage.__init__
pybirdai/process_steps/metadata_lineage/__init__.py:12:0: C0304: Final newline missing (missing-final-newline)
************* Module pybirdai.process_steps.metadata_lineage.bpmn_metadata_lineage_processor
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:32:52: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:47:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:52:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:61:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:68:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:74:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:77:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:79:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:92:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:104:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:118:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:127:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:130:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:133:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:147:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:156:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:159:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:162:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:177:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:187:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:190:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:193:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:201:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:209:67: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:212:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:220:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:222:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:231:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:233:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:242:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:244:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:247:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:253:67: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:256:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:263:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:269:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:271:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:279:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:281:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:286:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:291:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:293:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:300:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:304:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:312:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:314:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:317:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:320:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:329:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:334:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:339:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:343:67: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:346:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:347:89: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:353:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:355:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:364:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:366:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:369:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:378:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:380:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:389:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:391:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:398:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:408:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:410:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:418:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:420:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:429:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:431:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:440:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:442:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:447:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:456:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:462:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:465:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:467:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:472:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:482:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:490:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:492:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:501:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:503:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:504:0: C0304: Final newline missing (missing-final-newline)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:70:8: W0612: Unused variable 'workflow_module' (unused-variable)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:80:40: W0622: Redefining built-in 'id' (redefined-builtin)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:105:39: W0622: Redefining built-in 'id' (redefined-builtin)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:134:42: W0622: Redefining built-in 'id' (redefined-builtin)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:163:43: W0622: Redefining built-in 'id' (redefined-builtin)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:208:8: C0415: Import outside toplevel (pybirdai.models.bird_meta_data_model.COMBINATION_ITEM, pybirdai.models.bird_meta_data_model.CUBE, pybirdai.models.bird_meta_data_model.CUBE_STRUCTURE_ITEM, pybirdai.models.bird_meta_data_model.CUBE_LINK, pybirdai.models.bird_meta_data_model.CUBE_STRUCTURE_ITEM_LINK) (import-outside-toplevel)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:208:8: W0611: Unused COMBINATION_ITEM imported from pybirdai.models.bird_meta_data_model (unused-import)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:208:8: W0611: Unused CUBE imported from pybirdai.models.bird_meta_data_model (unused-import)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:208:8: W0611: Unused CUBE_STRUCTURE_ITEM imported from pybirdai.models.bird_meta_data_model (unused-import)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:208:8: W0611: Unused CUBE_LINK imported from pybirdai.models.bird_meta_data_model (unused-import)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:208:8: W0611: Unused CUBE_STRUCTURE_ITEM_LINK imported from pybirdai.models.bird_meta_data_model (unused-import)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:248:4: R0914: Too many local variables (25/15) (too-many-locals)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:252:8: C0415: Import outside toplevel (pybirdai.models.bird_meta_data_model.COMBINATION_ITEM, pybirdai.models.bird_meta_data_model.CUBE, pybirdai.models.bird_meta_data_model.CUBE_STRUCTURE_ITEM, pybirdai.models.bird_meta_data_model.CUBE_LINK, pybirdai.models.bird_meta_data_model.CUBE_STRUCTURE_ITEM_LINK) (import-outside-toplevel)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:248:4: R0912: Too many branches (16/12) (too-many-branches)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:252:8: W0611: Unused CUBE_STRUCTURE_ITEM_LINK imported from pybirdai.models.bird_meta_data_model (unused-import)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:347:4: R0914: Too many local variables (19/15) (too-many-locals)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:352:8: C0415: Import outside toplevel (pybirdai.models.bird_meta_data_model.CUBE_STRUCTURE_ITEM) (import-outside-toplevel)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:461:8: C0415: Import outside toplevel (pybirdai.models.bird_meta_data_model.CUBE_STRUCTURE_ITEM) (import-outside-toplevel)
pybirdai/process_steps/metadata_lineage/bpmn_metadata_lineage_processor.py:30:0: W0611: Unused transaction imported from django.db (unused-import)
************* Module pybirdai.process_steps.input_model.import_database_to_sdd_model
pybirdai/process_steps/input_model/import_database_to_sdd_model.py:336:0: C0325: Unnecessary parens after 'not' keyword (superfluous-parens)
pybirdai/process_steps/input_model/import_database_to_sdd_model.py:14:0: W0401: Wildcard import pybirdai.models.bird_meta_data_model (wildcard-import)
pybirdai/process_steps/input_model/import_database_to_sdd_model.py:17:0: R0205: Class 'ImportDatabaseToSDDModel' inherits from object, can be safely removed from bases in python3 (useless-object-inheritance)
pybirdai/process_steps/input_model/import_database_to_sdd_model.py:17:0: R0904: Too many public methods (30/20) (too-many-public-methods)
pybirdai/process_steps/input_model/import_database_to_sdd_model.py:15:0: C0411: standard import "concurrent.futures.ThreadPoolExecutor" should be placed before first party import "pybirdai.models.bird_meta_data_model.*"  (wrong-import-order)
pybirdai/process_steps/input_model/import_database_to_sdd_model.py:16:0: C0411: standard import "datetime.datetime" should be placed before first party import "pybirdai.models.bird_meta_data_model.*"  (wrong-import-order)
pybirdai/process_steps/input_model/import_database_to_sdd_model.py:14:0: W0614: Unused import(s) SUBDOMAIN, SUBDOMAIN_ENUMERATION, FACET_COLLECTION, VARIABLE_SET, VARIABLE_SET_ENUMERATION, MEMBER_LINK, models, OperationalError and timezone from wildcard import of pybirdai.models.bird_meta_data_model (unused-wildcard-import)
************* Module pybirdai.process_steps.input_model.import_input_model
pybirdai/process_steps/input_model/import_input_model.py:14:0: W0401: Wildcard import pybirdai.models.bird_meta_data_model (wildcard-import)
pybirdai/process_steps/input_model/import_input_model.py:30:0: R0205: Class 'ImportInputModel' inherits from object, can be safely removed from bases in python3 (useless-object-inheritance)
pybirdai/process_steps/input_model/import_input_model.py:35:4: E0213: Method 'import_input_model' should have "self" as first argument (no-self-argument)
pybirdai/process_steps/input_model/import_input_model.py:56:8: W0621: Redefining name 'settings' from outer scope (line 25) (redefined-outer-name)
pybirdai/process_steps/input_model/import_input_model.py:44:34: R1735: Consider using '{}' instead of a call to 'dict'. (use-dict-literal)
pybirdai/process_steps/input_model/import_input_model.py:45:25: R1735: Consider using '{}' instead of a call to 'dict'. (use-dict-literal)
pybirdai/process_steps/input_model/import_input_model.py:46:37: R1735: Consider using '{}' instead of a call to 'dict'. (use-dict-literal)
pybirdai/process_steps/input_model/import_input_model.py:56:8: W0404: Reimport 'settings' (imported line 25) (reimported)
pybirdai/process_steps/input_model/import_input_model.py:56:8: C0415: Import outside toplevel (django.conf.settings) (import-outside-toplevel)
pybirdai/process_steps/input_model/import_input_model.py:67:4: E0213: Method '_create_maintenance_agency' should have "self" as first argument (no-self-argument)
pybirdai/process_steps/input_model/import_input_model.py:109:4: E0213: Method '_create_primitive_domains' should have "self" as first argument (no-self-argument)
pybirdai/process_steps/input_model/import_input_model.py:132:4: E0213: Method '_create_subdomain_to_domain_map' should have "self" as first argument (no-self-argument)
pybirdai/process_steps/input_model/import_input_model.py:149:4: E0213: Method '_process_models' should have "self" as first argument (no-self-argument)
pybirdai/process_steps/input_model/import_input_model.py:158:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/process_steps/input_model/import_input_model.py:158:21: W1309: Using an f-string that does not have any interpolated variables (f-string-without-interpolation)
pybirdai/process_steps/input_model/import_input_model.py:160:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/process_steps/input_model/import_input_model.py:165:4: E0213: Method '_create_cube_and_structure' should have "self" as first argument (no-self-argument)
pybirdai/process_steps/input_model/import_input_model.py:190:4: E0213: Method '_fetch_derived_fields_from_model' should have "self" as first argument (no-self-argument)
pybirdai/process_steps/input_model/import_input_model.py:191:8: C0415: Import outside toplevel (ast) (import-outside-toplevel)
pybirdai/process_steps/input_model/import_input_model.py:226:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/process_steps/input_model/import_input_model.py:229:4: E0213: Method '_provide_fields' should have "self" as first argument (no-self-argument)
pybirdai/process_steps/input_model/import_input_model.py:248:4: E0213: Method '_process_fields' should have "self" as first argument (no-self-argument)
pybirdai/process_steps/input_model/import_input_model.py:248:4: R0914: Too many local variables (16/15) (too-many-locals)
pybirdai/process_steps/input_model/import_input_model.py:325:8: R1705: Unnecessary "elif" after "return", remove the leading "el" from "elif" (no-else-return)
pybirdai/process_steps/input_model/import_input_model.py:338:4: E0213: Method '_create_domain_and_subdomain_if_needed' should have "self" as first argument (no-self-argument)
pybirdai/process_steps/input_model/import_input_model.py:338:4: R0912: Too many branches (13/12) (too-many-branches)
pybirdai/process_steps/input_model/import_input_model.py:44:8: W0201: Attribute 'csi_counter' defined outside __init__ (attribute-defined-outside-init)
pybirdai/process_steps/input_model/import_input_model.py:15:0: C0411: third party import "django.apps.apps" should be placed before first party import "pybirdai.models.bird_meta_data_model.*"  (wrong-import-order)
pybirdai/process_steps/input_model/import_input_model.py:16:0: C0411: third party import "django.db.models" should be placed before first party import "pybirdai.models.bird_meta_data_model.*"  (wrong-import-order)
pybirdai/process_steps/input_model/import_input_model.py:17:0: C0411: standard import "os" should be placed before third party imports "django.apps.apps", "django.db.models" and first party import "pybirdai.models.bird_meta_data_model.*"  (wrong-import-order)
pybirdai/process_steps/input_model/import_input_model.py:18:0: C0411: standard import "csv" should be placed before third party imports "django.apps.apps", "django.db.models" and first party import "pybirdai.models.bird_meta_data_model.*"  (wrong-import-order)
pybirdai/process_steps/input_model/import_input_model.py:20:0: C0411: third party import "django.db.models.fields.CharField" should be placed before first party imports "pybirdai.models.bird_meta_data_model.*", "pybirdai.context.csv_column_index_context.ColumnIndexes"  (wrong-import-order)
pybirdai/process_steps/input_model/import_input_model.py:21:0: C0411: third party import "django.db.transaction" should be placed before first party imports "pybirdai.models.bird_meta_data_model.*", "pybirdai.context.csv_column_index_context.ColumnIndexes"  (wrong-import-order)
pybirdai/process_steps/input_model/import_input_model.py:22:0: C0411: standard import "uuid.uuid4" should be placed before third party imports "django.apps.apps", "django.db.models", "django.db.models.fields.CharField", "django.db.transaction" and first party imports "pybirdai.models.bird_meta_data_model.*", "pybirdai.context.csv_column_index_context.ColumnIndexes"  (wrong-import-order)
pybirdai/process_steps/input_model/import_input_model.py:24:0: C0411: standard import "logging" should be placed before third party imports "django.apps.apps", "django.db.models", "django.db.models.fields.CharField", "django.db.transaction" and first party imports "pybirdai.models.bird_meta_data_model.*", "pybirdai.context.csv_column_index_context.ColumnIndexes", "pybirdai.views.load_variables_from_csv_file"  (wrong-import-order)
pybirdai/process_steps/input_model/import_input_model.py:25:0: C0411: third party import "django.conf.settings" should be placed before first party imports "pybirdai.models.bird_meta_data_model.*", "pybirdai.context.csv_column_index_context.ColumnIndexes", "pybirdai.views.load_variables_from_csv_file"  (wrong-import-order)
pybirdai/process_steps/input_model/import_input_model.py:26:0: C0411: standard import "copy" should be placed before third party imports "django.apps.apps", "django.db.models", "django.db.models.fields.CharField", "django.db.transaction", "django.conf.settings" and first party imports "pybirdai.models.bird_meta_data_model.*", "pybirdai.context.csv_column_index_context.ColumnIndexes", "pybirdai.views.load_variables_from_csv_file"  (wrong-import-order)
pybirdai/process_steps/input_model/import_input_model.py:19:0: C0412: Imports from package pybirdai are not grouped (ungrouped-imports)
pybirdai/process_steps/input_model/import_input_model.py:20:0: C0412: Imports from package django are not grouped (ungrouped-imports)
pybirdai/process_steps/input_model/import_input_model.py:23:0: C0412: Imports from package pybirdai are not grouped (ungrouped-imports)
pybirdai/process_steps/input_model/import_input_model.py:25:0: C0412: Imports from package django are not grouped (ungrouped-imports)
pybirdai/process_steps/input_model/import_input_model.py:21:0: W0611: Unused transaction imported from django.db (unused-import)
pybirdai/process_steps/input_model/import_input_model.py:22:0: W0611: Unused uuid4 imported from uuid (unused-import)
pybirdai/process_steps/input_model/import_input_model.py:14:0: W0614: Unused import(s) FACET_COLLECTION, MEMBER_HIERARCHY, MEMBER_HIERARCHY_NODE, VARIABLE_SET, VARIABLE_SET_ENUMERATION, FRAMEWORK, MEMBER_MAPPING, MEMBER_MAPPING_ITEM, VARIABLE_MAPPING_ITEM, VARIABLE_MAPPING, MAPPING_TO_CUBE, MAPPING_DEFINITION, AXIS, AXIS_ORDINATE, CELL_POSITION, ORDINATE_ITEM, TABLE, TABLE_CELL, CUBE_LINK, CUBE_STRUCTURE_ITEM_LINK, COMBINATION, COMBINATION_ITEM, CUBE_TO_COMBINATION, MEMBER_LINK, OperationalError and timezone from wildcard import of pybirdai.models.bird_meta_data_model (unused-wildcard-import)
************* Module pybirdai.process_steps.mapping_join_metadata_eil_ldm.mapping_join_eil_ldm
pybirdai/process_steps/mapping_join_metadata_eil_ldm/mapping_join_eil_ldm.py:90:0: W0311: Bad indentation. Found 13 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/mapping_join_metadata_eil_ldm/mapping_join_eil_ldm.py:134:0: W0311: Bad indentation. Found 13 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/mapping_join_metadata_eil_ldm/mapping_join_eil_ldm.py:135:0: W0311: Bad indentation. Found 13 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/mapping_join_metadata_eil_ldm/mapping_join_eil_ldm.py:184:0: W0311: Bad indentation. Found 13 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/mapping_join_metadata_eil_ldm/mapping_join_eil_ldm.py:185:0: W0311: Bad indentation. Found 13 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/mapping_join_metadata_eil_ldm/mapping_join_eil_ldm.py:221:0: W0311: Bad indentation. Found 13 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/mapping_join_metadata_eil_ldm/mapping_join_eil_ldm.py:222:0: W0311: Bad indentation. Found 13 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/mapping_join_metadata_eil_ldm/mapping_join_eil_ldm.py:252:0: W0311: Bad indentation. Found 17 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/mapping_join_metadata_eil_ldm/mapping_join_eil_ldm.py:17:0: R0902: Too many instance attributes (11/7) (too-many-instance-attributes)
pybirdai/process_steps/mapping_join_metadata_eil_ldm/mapping_join_eil_ldm.py:66:15: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/process_steps/mapping_join_metadata_eil_ldm/mapping_join_eil_ldm.py:91:15: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/process_steps/mapping_join_metadata_eil_ldm/mapping_join_eil_ldm.py:123:15: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/process_steps/mapping_join_metadata_eil_ldm/mapping_join_eil_ldm.py:211:4: R0914: Too many local variables (18/15) (too-many-locals)
************* Module pybirdai.process_steps.automode.database_setup_first_use
pybirdai/process_steps/automode/database_setup_first_use.py:48:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/process_steps/automode/database_setup_first_use.py:53:4: C0415: Import outside toplevel (pybirdai.entry_points.create_django_models.RunCreateDjangoModels) (import-outside-toplevel)
pybirdai/process_steps/automode/database_setup_first_use.py:69:4: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/process_steps/automode/database_setup_first_use.py:73:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/process_steps/automode/database_setup_first_use.py:75:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/process_steps/automode/database_setup_first_use.py:81:4: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/process_steps/automode/database_setup_first_use.py:85:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/process_steps/automode/database_setup_first_use.py:87:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/process_steps/automode/database_setup_first_use.py:93:4: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/process_steps/automode/database_setup_first_use.py:95:9: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/process_steps/automode/database_setup_first_use.py:97:13: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/process_steps/automode/database_setup_first_use.py:106:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/process_steps/automode/database_setup_first_use.py:111:13: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/process_steps/automode/database_setup_first_use.py:115:13: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/process_steps/automode/database_setup_first_use.py:119:13: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/process_steps/automode/database_setup_first_use.py:123:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/process_steps/automode/database_setup_first_use.py:125:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/process_steps/automode/database_setup_first_use.py:132:4: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/process_steps/automode/database_setup_first_use.py:135:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/process_steps/automode/database_setup_first_use.py:140:13: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/process_steps/automode/database_setup_first_use.py:144:13: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/process_steps/automode/database_setup_first_use.py:148:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/process_steps/automode/database_setup_first_use.py:150:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/process_steps/automode/database_setup_first_use.py:163:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/process_steps/automode/database_setup_first_use.py:172:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/process_steps/automode/database_setup_first_use.py:188:4: E1120: No value for argument 'app_name' in constructor call (no-value-for-parameter)
pybirdai/process_steps/automode/database_setup_first_use.py:188:4: E1120: No value for argument 'app_module' in constructor call (no-value-for-parameter)
pybirdai/process_steps/automode/database_setup_first_use.py:15:0: C0411: standard import "os" should be placed before third party import "django" (wrong-import-order)
pybirdai/process_steps/automode/database_setup_first_use.py:16:0: C0411: standard import "sys" should be placed before third party import "django" (wrong-import-order)
pybirdai/process_steps/automode/database_setup_first_use.py:19:0: C0411: standard import "logging" should be placed before third party imports "django", "django.apps.AppConfig", "django.conf.settings" (wrong-import-order)
pybirdai/process_steps/automode/database_setup_first_use.py:20:0: C0411: standard import "importlib.metadata" should be placed before third party imports "django", "django.apps.AppConfig", "django.conf.settings" (wrong-import-order)
pybirdai/process_steps/automode/database_setup_first_use.py:21:0: C0411: standard import "ast" should be placed before third party imports "django", "django.apps.AppConfig", "django.conf.settings" (wrong-import-order)
pybirdai/process_steps/automode/database_setup_first_use.py:18:0: W0611: Unused settings imported from django.conf (unused-import)
pybirdai/process_steps/automode/database_setup_first_use.py:20:0: W0611: Unused metadata imported from importlib (unused-import)
************* Module pybirdai.process_steps.website_to_sddmodel.import_website_to_sdd_model_django
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:162:0: C0325: Unnecessary parens after 'not' keyword (superfluous-parens)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:215:0: C0325: Unnecessary parens after 'not' keyword (superfluous-parens)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:232:0: C0325: Unnecessary parens after 'not' keyword (superfluous-parens)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:264:0: C0325: Unnecessary parens after 'not' keyword (superfluous-parens)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:337:0: C0325: Unnecessary parens after 'not' keyword (superfluous-parens)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:359:0: C0325: Unnecessary parens after 'not' keyword (superfluous-parens)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:476:0: C0325: Unnecessary parens after 'not' keyword (superfluous-parens)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:637:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:640:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:1:0: C0302: Too many lines in module (1354/1000) (too-many-lines)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:16:0: W0401: Wildcard import pybirdai.models.bird_meta_data_model (wildcard-import)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:23:0: R0205: Class 'ImportWebsiteToSDDModel' inherits from object, can be safely removed from bases in python3 (useless-object-inheritance)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:90:20: W0622: Redefining built-in 'id' (redefined-builtin)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:91:20: W0612: Unused variable 'name' (unused-variable)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:121:20: W0622: Redefining built-in 'id' (redefined-builtin)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:122:20: W0612: Unused variable 'name' (unused-variable)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:136:4: R0914: Too many local variables (19/15) (too-many-locals)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:176:47: R1719: The if expression can be replaced with 'bool(test)' (simplifiable-if-expression)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:177:46: R1719: The if expression can be replaced with 'bool(test)' (simplifiable-if-expression)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:152:20: W0612: Unused variable 'data_type' (unused-variable)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:188:4: R0914: Too many local variables (19/15) (too-many-locals)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:239:4: R0914: Too many local variables (20/15) (too-many-locals)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:282:31: R1714: Consider merging these comparisons with 'in' by using 'primary_concept in ('', None)'. Use a set instead if elements are hashable. (consider-using-in)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:282:59: C0121: Comparison 'primary_concept == None' should be 'primary_concept is None' (singleton-comparison)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:288:4: R0914: Too many local variables (18/15) (too-many-locals)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:288:4: R0912: Too many branches (16/12) (too-many-branches)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:288:4: R0915: Too many statements (51/50) (too-many-statements)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:380:16: W0622: Redefining built-in 'id' (redefined-builtin)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:410:4: E0213: Method 'save_missing_domains_to_csv' should have "self" as first argument (no-self-argument)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:412:13: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:416:4: E0213: Method 'save_missing_members_to_csv' should have "self" as first argument (no-self-argument)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:418:13: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:422:4: E0213: Method 'save_missing_variables_to_csv' should have "self" as first argument (no-self-argument)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:424:13: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:428:4: E0213: Method 'save_missing_children_to_csv' should have "self" as first argument (no-self-argument)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:430:13: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:437:4: R0914: Too many local variables (23/15) (too-many-locals)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:462:20: W0612: Unused variable 'valid_from' (unused-variable)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:463:20: W0612: Unused variable 'valid_to' (unused-variable)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:493:4: E0213: Method 'save_missing_hierarchies_to_csv' should have "self" as first argument (no-self-argument)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:495:13: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:504:4: R0914: Too many local variables (18/15) (too-many-locals)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:524:20: W0612: Unused variable 'valid_from' (unused-variable)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:525:20: W0612: Unused variable 'valid_to' (unused-variable)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:543:4: R0914: Too many local variables (16/15) (too-many-locals)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:559:20: W0612: Unused variable 'axis_order' (unused-variable)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:560:20: W0612: Unused variable 'axis_name' (unused-variable)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:563:20: W0612: Unused variable 'axis_is_open_axis' (unused-variable)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:578:4: R0914: Too many local variables (18/15) (too-many-locals)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:593:20: W0612: Unused variable 'axis_ordinate_is_abstract_header' (unused-variable)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:595:20: W0612: Unused variable 'axis_ordinate_order' (unused-variable)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:598:20: W0612: Unused variable 'axis_ordinate_parent_axis_ordinate_id' (unused-variable)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:617:4: R0914: Too many local variables (16/15) (too-many-locals)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:658:12: R1705: Unnecessary "elif" after "return", remove the leading "el" from "elif" (no-else-return)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:685:20: W0719: Raising too general exception: Exception (broad-exception-raised)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:717:27: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:720:24: E1121: Too many positional arguments for method call (too-many-function-args)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:726:16: E1121: Too many positional arguments for method call (too-many-function-args)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:617:4: R0912: Too many branches (14/12) (too-many-branches)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:617:4: R0915: Too many statements (51/50) (too-many-statements)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:738:8: W0621: Redefining name 'csv' from outer scope (line 14) (redefined-outer-name)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:738:8: W0404: Reimport 'csv' (imported line 14) (reimported)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:738:8: C0415: Import outside toplevel (csv) (import-outside-toplevel)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:749:54: E0602: Undefined variable 'table_name' (undefined-variable)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:738:8: W0611: Unused import csv (unused-import)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:740:8: W0612: Unused variable 'fallback_import_func' (unused-variable)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:764:4: R0914: Too many local variables (17/15) (too-many-locals)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:925:4: R0914: Too many local variables (22/15) (too-many-locals)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:925:4: R0912: Too many branches (16/12) (too-many-branches)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:990:4: E0213: Method 'save_missing_mapping_variables_to_csv' should have "self" as first argument (no-self-argument)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:992:13: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:998:4: E0213: Method 'save_missing_mapping_members_to_csv' should have "self" as first argument (no-self-argument)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:1000:13: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:1008:4: E0213: Method 'create_mappings_warnings_summary' should have "self" as first argument (no-self-argument)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:1015:13: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:1021:13: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:1229:8: W0702: No exception type(s) specified (bare-except)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:23:0: R0904: Too many public methods (50/20) (too-many-public-methods)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:18:0: C0411: standard import "pathlib.Path" should be placed before third party import "django.conf.settings" and first party imports "pybirdai.models.bird_meta_data_model.*", "pybirdai.context.csv_column_index_context.ColumnIndexes"  (wrong-import-order)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:19:0: C0411: third party import "django.db.connection" should be placed before first party imports "pybirdai.models.bird_meta_data_model.*", "pybirdai.context.csv_column_index_context.ColumnIndexes"  (wrong-import-order)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:20:0: C0411: standard import "subprocess" should be placed before third party imports "django.conf.settings", "django.db.connection" and first party imports "pybirdai.models.bird_meta_data_model.*", "pybirdai.context.csv_column_index_context.ColumnIndexes"  (wrong-import-order)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:21:0: C0411: standard import "platform" should be placed before third party imports "django.conf.settings", "django.db.connection" and first party imports "pybirdai.models.bird_meta_data_model.*", "pybirdai.context.csv_column_index_context.ColumnIndexes"  (wrong-import-order)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:19:0: C0412: Imports from package django are not grouped (ungrouped-imports)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:19:0: W0611: Unused transaction imported from django.db (unused-import)
pybirdai/process_steps/website_to_sddmodel/import_website_to_sdd_model_django.py:16:0: W0614: Unused import(s) SUBDOMAIN, SUBDOMAIN_ENUMERATION, FACET_COLLECTION, VARIABLE_SET, VARIABLE_SET_ENUMERATION, CUBE_STRUCTURE, CUBE_STRUCTURE_ITEM, CUBE, CUBE_LINK, CUBE_STRUCTURE_ITEM_LINK, COMBINATION, COMBINATION_ITEM, CUBE_TO_COMBINATION, MEMBER_LINK, models, OperationalError and timezone from wildcard import of pybirdai.models.bird_meta_data_model (unused-wildcard-import)
************* Module pybirdai.process_steps.sqldeveloper_import.import_sqldev_il_to_regdna
pybirdai/process_steps/sqldeveloper_import/import_sqldev_il_to_regdna.py:73:0: C0325: Unnecessary parens after 'if' keyword (superfluous-parens)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_il_to_regdna.py:124:0: C0325: Unnecessary parens after 'if' keyword (superfluous-parens)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_il_to_regdna.py:150:0: C0325: Unnecessary parens after 'if' keyword (superfluous-parens)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_il_to_regdna.py:194:0: C0325: Unnecessary parens after 'if' keyword (superfluous-parens)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_il_to_regdna.py:204:0: C0325: Unnecessary parens after 'if' keyword (superfluous-parens)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_il_to_regdna.py:208:0: C0325: Unnecessary parens after 'if' keyword (superfluous-parens)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_il_to_regdna.py:212:0: C0325: Unnecessary parens after 'if' keyword (superfluous-parens)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_il_to_regdna.py:216:0: C0325: Unnecessary parens after 'if' keyword (superfluous-parens)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_il_to_regdna.py:220:0: C0325: Unnecessary parens after 'if' keyword (superfluous-parens)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_il_to_regdna.py:224:0: C0325: Unnecessary parens after 'if' keyword (superfluous-parens)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_il_to_regdna.py:238:0: C0325: Unnecessary parens after 'if' keyword (superfluous-parens)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_il_to_regdna.py:253:0: C0325: Unnecessary parens after 'if' keyword (superfluous-parens)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_il_to_regdna.py:261:0: C0325: Unnecessary parens after 'if' keyword (superfluous-parens)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_il_to_regdna.py:270:0: C0325: Unnecessary parens after 'if' keyword (superfluous-parens)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_il_to_regdna.py:276:0: C0325: Unnecessary parens after 'elif' keyword (superfluous-parens)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_il_to_regdna.py:282:0: C0325: Unnecessary parens after 'elif' keyword (superfluous-parens)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_il_to_regdna.py:288:0: C0325: Unnecessary parens after 'elif' keyword (superfluous-parens)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_il_to_regdna.py:294:0: C0325: Unnecessary parens after 'elif' keyword (superfluous-parens)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_il_to_regdna.py:306:0: C0325: Unnecessary parens after 'elif' keyword (superfluous-parens)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_il_to_regdna.py:312:0: C0325: Unnecessary parens after 'elif' keyword (superfluous-parens)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_il_to_regdna.py:318:0: C0325: Unnecessary parens after 'elif' keyword (superfluous-parens)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_il_to_regdna.py:324:0: C0325: Unnecessary parens after 'elif' keyword (superfluous-parens)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_il_to_regdna.py:330:0: C0325: Unnecessary parens after 'elif' keyword (superfluous-parens)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_il_to_regdna.py:336:0: C0325: Unnecessary parens after 'elif' keyword (superfluous-parens)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_il_to_regdna.py:342:0: C0325: Unnecessary parens after 'elif' keyword (superfluous-parens)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_il_to_regdna.py:348:0: C0325: Unnecessary parens after 'elif' keyword (superfluous-parens)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_il_to_regdna.py:354:0: C0325: Unnecessary parens after 'elif' keyword (superfluous-parens)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_il_to_regdna.py:361:0: C0325: Unnecessary parens after 'elif' keyword (superfluous-parens)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_il_to_regdna.py:368:0: C0325: Unnecessary parens after 'elif' keyword (superfluous-parens)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_il_to_regdna.py:386:0: C0325: Unnecessary parens after 'if' keyword (superfluous-parens)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_il_to_regdna.py:456:0: C0325: Unnecessary parens after 'if' keyword (superfluous-parens)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_il_to_regdna.py:480:0: C0325: Unnecessary parens after 'not' keyword (superfluous-parens)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_il_to_regdna.py:482:0: C0325: Unnecessary parens after 'if' keyword (superfluous-parens)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_il_to_regdna.py:601:0: C0325: Unnecessary parens after 'if' keyword (superfluous-parens)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_il_to_regdna.py:617:0: C0325: Unnecessary parens after 'if' keyword (superfluous-parens)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_il_to_regdna.py:649:0: C0325: Unnecessary parens after 'if' keyword (superfluous-parens)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_il_to_regdna.py:652:0: C0325: Unnecessary parens after 'if' keyword (superfluous-parens)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_il_to_regdna.py:653:0: C0325: Unnecessary parens after 'if' keyword (superfluous-parens)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_il_to_regdna.py:672:0: C0325: Unnecessary parens after 'if' keyword (superfluous-parens)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_il_to_regdna.py:689:0: C0325: Unnecessary parens after 'not' keyword (superfluous-parens)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_il_to_regdna.py:41:0: R0205: Class 'SQLDeveloperILImport' inherits from object, can be safely removed from bases in python3 (useless-object-inheritance)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_il_to_regdna.py:129:20: W0612: Unused variable 'enum_name' (unused-variable)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_il_to_regdna.py:139:4: R0914: Too many local variables (16/15) (too-many-locals)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_il_to_regdna.py:228:4: R0914: Too many local variables (23/15) (too-many-locals)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_il_to_regdna.py:413:24: W0702: No exception type(s) specified (bare-except)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_il_to_regdna.py:228:4: R0912: Too many branches (28/12) (too-many-branches)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_il_to_regdna.py:228:4: R0915: Too many statements (158/50) (too-many-statements)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_il_to_regdna.py:248:20: W0612: Unused variable 'relationID' (unused-variable)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_il_to_regdna.py:252:20: W0612: Unused variable 'class_is_derived' (unused-variable)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_il_to_regdna.py:389:32: W0612: Unused variable 'datatype' (unused-variable)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_il_to_regdna.py:441:4: R0914: Too many local variables (23/15) (too-many-locals)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_il_to_regdna.py:585:24: W0702: No exception type(s) specified (bare-except)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_il_to_regdna.py:441:4: R0912: Too many branches (27/12) (too-many-branches)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_il_to_regdna.py:441:4: R0915: Too many statements (118/50) (too-many-statements)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_il_to_regdna.py:566:32: W0612: Unused variable 'datatype' (unused-variable)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_il_to_regdna.py:608:4: R0914: Too many local variables (20/15) (too-many-locals)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_il_to_regdna.py:608:4: R0912: Too many branches (14/12) (too-many-branches)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_il_to_regdna.py:608:4: R0915: Too many statements (64/50) (too-many-statements)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_il_to_regdna.py:651:20: W0612: Unused variable 'relational_attribute' (unused-variable)
************* Module pybirdai.process_steps.sqldeveloper_import.convert_regdna_to_xcore
pybirdai/process_steps/sqldeveloper_import/convert_regdna_to_xcore.py:29:4: R1705: Unnecessary "elif" after "return", remove the leading "el" from "elif" (no-else-return)
pybirdai/process_steps/sqldeveloper_import/convert_regdna_to_xcore.py:31:45: R1714: Consider merging these comparisons with 'in' by using 'element.upperBound in (1, 0)'. Use a set instead if elements are hashable. (consider-using-in)
pybirdai/process_steps/sqldeveloper_import/convert_regdna_to_xcore.py:73:8: W0612: Unused variable 'details' (unused-variable)
pybirdai/process_steps/sqldeveloper_import/convert_regdna_to_xcore.py:258:38: W0621: Redefining name 'context' from outer scope (line 282) (redefined-outer-name)
pybirdai/process_steps/sqldeveloper_import/convert_regdna_to_xcore.py:20:0: W0611: Unused ELPackage imported from pybirdai.regdna (unused-import)
pybirdai/process_steps/sqldeveloper_import/convert_regdna_to_xcore.py:20:0: W0611: Unused ELOperation imported from pybirdai.regdna (unused-import)
************* Module pybirdai.process_steps.sqldeveloper_import.import_sqldev_ldm_to_django
pybirdai/process_steps/sqldeveloper_import/import_sqldev_ldm_to_django.py:68:0: C0325: Unnecessary parens after 'not' keyword (superfluous-parens)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_ldm_to_django.py:18:0: R0205: Class 'RegDNAToDJango' inherits from object, can be safely removed from bases in python3 (useless-object-inheritance)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_ldm_to_django.py:42:22: R1732: Consider using 'with' for resource-allocating operations (consider-using-with)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_ldm_to_django.py:43:21: R1732: Consider using 'with' for resource-allocating operations (consider-using-with)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_ldm_to_django.py:79:8: R1705: Unnecessary "else" after "return", remove the "else" and de-indent the code inside it (no-else-return)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_ldm_to_django.py:85:16: W0702: No exception type(s) specified (bare-except)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_ldm_to_django.py:77:4: R0912: Too many branches (24/12) (too-many-branches)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_ldm_to_django.py:77:4: R0915: Too many statements (52/50) (too-many-statements)
************* Module pybirdai.process_steps.sqldeveloper_import.import_sqldev_ldm_to_regdna
pybirdai/process_steps/sqldeveloper_import/import_sqldev_ldm_to_regdna.py:289:0: C0325: Unnecessary parens after 'not' keyword (superfluous-parens)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_ldm_to_regdna.py:448:0: C0325: Unnecessary parens after 'not' keyword (superfluous-parens)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_ldm_to_regdna.py:506:0: C0325: Unnecessary parens after 'not' keyword (superfluous-parens)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_ldm_to_regdna.py:753:0: C0325: Unnecessary parens after 'if' keyword (superfluous-parens)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_ldm_to_regdna.py:840:0: C0325: Unnecessary parens after 'not' keyword (superfluous-parens)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_ldm_to_regdna.py:840:0: C0325: Unnecessary parens after 'not' keyword (superfluous-parens)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_ldm_to_regdna.py:902:0: C0325: Unnecessary parens after 'not' keyword (superfluous-parens)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_ldm_to_regdna.py:933:0: C0325: Unnecessary parens after 'not' keyword (superfluous-parens)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_ldm_to_regdna.py:968:0: C0325: Unnecessary parens after 'not' keyword (superfluous-parens)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_ldm_to_regdna.py:969:0: W0311: Bad indentation. Found 16 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_ldm_to_regdna.py:1:0: C0302: Too many lines in module (1009/1000) (too-many-lines)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_ldm_to_regdna.py:26:0: R0205: Class 'SQLDevLDMImport' inherits from object, can be safely removed from bases in python3 (useless-object-inheritance)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_ldm_to_regdna.py:54:4: R0915: Too many statements (187/50) (too-many-statements)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_ldm_to_regdna.py:262:4: R0914: Too many local variables (23/15) (too-many-locals)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_ldm_to_regdna.py:281:20: W0612: Unused variable 'engineering_type' (unused-variable)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_ldm_to_regdna.py:326:4: R0914: Too many local variables (26/15) (too-many-locals)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_ldm_to_regdna.py:326:4: R0915: Too many statements (71/50) (too-many-statements)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_ldm_to_regdna.py:350:20: W0612: Unused variable 'relation_name' (unused-variable)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_ldm_to_regdna.py:441:20: W0612: Unused variable 'relation_name' (unused-variable)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_ldm_to_regdna.py:442:20: W0612: Unused variable 'target_entity_name' (unused-variable)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_ldm_to_regdna.py:445:20: W0612: Unused variable 'arc_class' (unused-variable)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_ldm_to_regdna.py:458:4: R1710: Either all return statements in a function should return an expression, or none of them should. (inconsistent-return-statements)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_ldm_to_regdna.py:472:8: R1702: Too many nested blocks (6/5) (too-many-nested-blocks)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_ldm_to_regdna.py:528:20: W0612: Unused variable 'enum_name' (unused-variable)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_ldm_to_regdna.py:538:4: R0914: Too many local variables (16/15) (too-many-locals)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_ldm_to_regdna.py:623:4: R0914: Too many local variables (26/15) (too-many-locals)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_ldm_to_regdna.py:785:24: W0702: No exception type(s) specified (bare-except)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_ldm_to_regdna.py:623:4: R0912: Too many branches (27/12) (too-many-branches)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_ldm_to_regdna.py:623:4: R0915: Too many statements (127/50) (too-many-statements)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_ldm_to_regdna.py:649:20: W0612: Unused variable 'relation_id' (unused-variable)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_ldm_to_regdna.py:650:20: W0612: Unused variable 'primary_key_or_not' (unused-variable)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_ldm_to_regdna.py:651:20: W0612: Unused variable 'foreign_key_or_not' (unused-variable)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_ldm_to_regdna.py:791:4: R0914: Too many local variables (25/15) (too-many-locals)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_ldm_to_regdna.py:799:12: R1702: Too many nested blocks (6/5) (too-many-nested-blocks)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_ldm_to_regdna.py:791:4: R0912: Too many branches (18/12) (too-many-branches)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_ldm_to_regdna.py:791:4: R0915: Too many statements (76/50) (too-many-statements)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_ldm_to_regdna.py:806:20: W0612: Unused variable 'target_class_name' (unused-variable)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_ldm_to_regdna.py:814:20: W0612: Unused variable 'identifying' (unused-variable)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_ldm_to_regdna.py:1003:12: R1705: Unnecessary "else" after "return", remove the "else" and de-indent the code inside it (no-else-return)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_ldm_to_regdna.py:1003:15: C0121: Comparison 'return_value == False' should be 'return_value is False' if checking for the singleton value False, or 'not return_value' if testing for falsiness (singleton-comparison)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_ldm_to_regdna.py:26:0: R0904: Too many public methods (23/20) (too-many-public-methods)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_ldm_to_regdna.py:23:0: W0611: Unused ELPackage imported from pybirdai.regdna (unused-import)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_ldm_to_regdna.py:23:0: W0611: Unused ModuleList imported from pybirdai.regdna (unused-import)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_ldm_to_regdna.py:23:0: W0611: Unused GenerationRulesModule imported from pybirdai.regdna (unused-import)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_ldm_to_regdna.py:23:0: W0611: Unused ReportModule imported from pybirdai.regdna (unused-import)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_ldm_to_regdna.py:23:0: W0611: Unused ELAnnotationDirective imported from pybirdai.regdna (unused-import)
pybirdai/process_steps/sqldeveloper_import/import_sqldev_ldm_to_regdna.py:24:0: W0611: Unused EcoreLiteTypes imported from pybirdai.context.ecore_lite_types (unused-import)
************* Module pybirdai.process_steps.pybird.csv_converter
pybirdai/process_steps/pybird/csv_converter.py:22:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:23:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:26:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:27:30: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/csv_converter.py:27:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:28:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:29:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:30:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:31:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:32:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:32:0: C0325: Unnecessary parens after 'if' keyword (superfluous-parens)
pybirdai/process_steps/pybird/csv_converter.py:33:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:34:77: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/csv_converter.py:34:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:35:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:36:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:37:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:38:77: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/csv_converter.py:38:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:39:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:41:24: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/csv_converter.py:41:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:42:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:43:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:45:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:47:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:48:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:49:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:50:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:51:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:52:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:53:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:54:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:55:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/csv_converter.py:56:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:57:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:58:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:59:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:60:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:61:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:62:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:63:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:64:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:65:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:66:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:67:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/csv_converter.py:71:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:72:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:74:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:75:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:76:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:77:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:78:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:82:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:83:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/csv_converter.py:84:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:85:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:92:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:93:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:94:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:95:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:96:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:97:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:99:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/csv_converter.py:100:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:101:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:102:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:103:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:105:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:107:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/csv_converter.py:109:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:110:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:111:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:112:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:113:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:114:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:115:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:117:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:118:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:121:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:122:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:122:0: C0325: Unnecessary parens after 'not' keyword (superfluous-parens)
pybirdai/process_steps/pybird/csv_converter.py:123:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:124:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/csv_converter.py:125:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:125:0: C0325: Unnecessary parens after 'not' keyword (superfluous-parens)
pybirdai/process_steps/pybird/csv_converter.py:126:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:127:0: W0311: Bad indentation. Found 7 spaces, expected 28 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:128:0: W0311: Bad indentation. Found 7 spaces, expected 28 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:129:0: W0311: Bad indentation. Found 7 spaces, expected 28 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:130:0: W0311: Bad indentation. Found 8 spaces, expected 32 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:131:0: W0311: Bad indentation. Found 7 spaces, expected 28 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:132:0: W0311: Bad indentation. Found 7 spaces, expected 28 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:134:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:136:0: W0311: Bad indentation. Found 7 spaces, expected 28 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:138:0: W0311: Bad indentation. Found 7 spaces, expected 28 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:139:0: W0311: Bad indentation. Found 7 spaces, expected 28 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:140:0: W0311: Bad indentation. Found 8 spaces, expected 32 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:141:0: W0311: Bad indentation. Found 7 spaces, expected 28 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:142:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:145:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:145:0: C0325: Unnecessary parens after 'if' keyword (superfluous-parens)
pybirdai/process_steps/pybird/csv_converter.py:146:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:147:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:148:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:149:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:151:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:153:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:154:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:155:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:156:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:157:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/csv_converter.py:158:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:159:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:161:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:162:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:163:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:167:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:168:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:169:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:171:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:174:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:174:0: C0325: Unnecessary parens after 'if' keyword (superfluous-parens)
pybirdai/process_steps/pybird/csv_converter.py:175:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:175:0: C0325: Unnecessary parens after 'if' keyword (superfluous-parens)
pybirdai/process_steps/pybird/csv_converter.py:176:0: W0311: Bad indentation. Found 7 spaces, expected 28 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:177:0: W0311: Bad indentation. Found 7 spaces, expected 28 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:178:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:179:0: W0311: Bad indentation. Found 7 spaces, expected 28 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:180:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:181:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:182:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:185:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:186:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:187:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:188:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:190:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:191:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:193:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:195:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/csv_converter.py:196:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:197:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:199:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:200:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:201:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:202:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:203:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:203:0: C0325: Unnecessary parens after 'if' keyword (superfluous-parens)
pybirdai/process_steps/pybird/csv_converter.py:204:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:206:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:208:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:209:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:211:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:214:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:215:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:217:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/csv_converter.py:218:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:219:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/csv_converter.py:220:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:221:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:222:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:222:0: C0325: Unnecessary parens after 'if' keyword (superfluous-parens)
pybirdai/process_steps/pybird/csv_converter.py:223:0: W0311: Bad indentation. Found 7 spaces, expected 28 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:225:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:225:0: C0325: Unnecessary parens after 'if' keyword (superfluous-parens)
pybirdai/process_steps/pybird/csv_converter.py:226:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:228:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:229:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:230:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:232:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:232:0: C0325: Unnecessary parens after 'if' keyword (superfluous-parens)
pybirdai/process_steps/pybird/csv_converter.py:233:147: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/csv_converter.py:233:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:234:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:235:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:237:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:238:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:239:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:242:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:243:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:244:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:245:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:246:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/csv_converter.py:247:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:248:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:249:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:250:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:252:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:253:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/csv_converter.py:255:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:256:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:257:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:258:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/csv_converter.py:260:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:261:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:262:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:263:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:264:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/csv_converter.py:265:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:266:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:267:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/csv_converter.py:269:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:270:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/csv_converter.py:272:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:274:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:275:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/csv_converter.py:277:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:279:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:280:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:281:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:282:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:284:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:285:0: W0311: Bad indentation. Found 7 spaces, expected 28 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:286:0: W0311: Bad indentation. Found 8 spaces, expected 32 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:287:0: W0311: Bad indentation. Found 8 spaces, expected 32 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:288:0: W0311: Bad indentation. Found 9 spaces, expected 36 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:289:0: W0311: Bad indentation. Found 7 spaces, expected 28 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:290:0: W0311: Bad indentation. Found 8 spaces, expected 32 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:291:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:292:0: W0311: Bad indentation. Found 7 spaces, expected 28 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:293:0: W0311: Bad indentation. Found 7 spaces, expected 28 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:294:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/csv_converter.py:297:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:299:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:300:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:302:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:303:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/csv_converter.py:304:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:305:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/csv_converter.py:306:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:307:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:309:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/csv_converter.py:310:0: C0305: Trailing newlines (trailing-newlines)
pybirdai/process_steps/pybird/csv_converter.py:22:1: E0213: Method 'persist_object_as_csv' should have "self" as first argument (no-self-argument)
pybirdai/process_steps/pybird/csv_converter.py:41:9: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/process_steps/pybird/csv_converter.py:38:11: R1732: Consider using 'with' for resource-allocating operations (consider-using-with)
pybirdai/process_steps/pybird/csv_converter.py:47:1: E0213: Method 'get_table_name' should have "self" as first argument (no-self-argument)
pybirdai/process_steps/pybird/csv_converter.py:53:16: C0207: Use class_name.split('_Table', maxsplit=1)[0] instead (use-maxsplit-arg)
pybirdai/process_steps/pybird/csv_converter.py:56:1: E0213: Method 'createCSVStringForTable' should have "self" as first argument (no-self-argument)
pybirdai/process_steps/pybird/csv_converter.py:84:1: E0213: Method 'get_contained_objects' should have "self" as first argument (no-self-argument)
pybirdai/process_steps/pybird/csv_converter.py:96:2: W0702: No exception type(s) specified (bare-except)
pybirdai/process_steps/pybird/csv_converter.py:109:1: E0213: Method 'createCSVStringForRow' should have "self" as first argument (no-self-argument)
pybirdai/process_steps/pybird/csv_converter.py:114:2: R1702: Too many nested blocks (6/5) (too-many-nested-blocks)
pybirdai/process_steps/pybird/csv_converter.py:110:2: W0612: Unused variable 'clazz' (unused-variable)
pybirdai/process_steps/pybird/csv_converter.py:112:2: W0612: Unused variable 'eClass' (unused-variable)
pybirdai/process_steps/pybird/csv_converter.py:153:1: E0213: Method 'createCSVHeaderStringForRow' should have "self" as first argument (no-self-argument)
pybirdai/process_steps/pybird/csv_converter.py:154:2: W0612: Unused variable 'clazz' (unused-variable)
pybirdai/process_steps/pybird/csv_converter.py:156:2: W0612: Unused variable 'eClass' (unused-variable)
pybirdai/process_steps/pybird/csv_converter.py:196:1: E0213: Method 'getReferencedItemString' should have "self" as first argument (no-self-argument)
pybirdai/process_steps/pybird/csv_converter.py:196:1: R0912: Too many branches (13/12) (too-many-branches)
pybirdai/process_steps/pybird/csv_converter.py:211:4: W0612: Unused variable 'eClass' (unused-variable)
pybirdai/process_steps/pybird/csv_converter.py:239:4: W0612: Unused variable 'pattern' (unused-variable)
pybirdai/process_steps/pybird/csv_converter.py:306:9: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/process_steps/pybird/csv_converter.py:252:3: C0415: Import outside toplevel (pybirdai.annotations.decorators._lineage_context) (import-outside-toplevel)
pybirdai/process_steps/pybird/csv_converter.py:284:19: W0212: Access to a protected member _meta of a client class (protected-access)
pybirdai/process_steps/pybird/csv_converter.py:289:7: W0702: No exception type(s) specified (bare-except)
pybirdai/process_steps/pybird/csv_converter.py:272:3: R1702: Too many nested blocks (6/5) (too-many-nested-blocks)
pybirdai/process_steps/pybird/csv_converter.py:309:3: W0107: Unnecessary pass statement (unnecessary-pass)
pybirdai/process_steps/pybird/csv_converter.py:15:0: C0411: third party import "django.conf.settings" should be placed before first party import "pybirdai.context.sdd_context_django.SDDContext"  (wrong-import-order)
pybirdai/process_steps/pybird/csv_converter.py:16:0: C0411: third party import "django.apps.apps" should be placed before first party import "pybirdai.context.sdd_context_django.SDDContext"  (wrong-import-order)
pybirdai/process_steps/pybird/csv_converter.py:17:0: C0411: third party import "django.db.models.QuerySet" should be placed before first party import "pybirdai.context.sdd_context_django.SDDContext"  (wrong-import-order)
pybirdai/process_steps/pybird/csv_converter.py:18:0: C0411: third party import "django.db.models.fields.related.ReverseOneToOneDescriptor" should be placed before first party import "pybirdai.context.sdd_context_django.SDDContext"  (wrong-import-order)
pybirdai/process_steps/pybird/csv_converter.py:14:0: W0611: Unused SDDContext imported from pybirdai.context.sdd_context_django (unused-import)
************* Module pybirdai.process_steps.pybird.create_python_django_transformations
pybirdai/process_steps/pybird/create_python_django_transformations.py:14:0: W0401: Wildcard import pybirdai.models.bird_meta_data_model (wildcard-import)
pybirdai/process_steps/pybird/create_python_django_transformations.py:41:4: E0213: Method 'create_output_classes' should have "self" as first argument (no-self-argument)
pybirdai/process_steps/pybird/create_python_django_transformations.py:41:4: R0915: Too many statements (53/50) (too-many-statements)
pybirdai/process_steps/pybird/create_python_django_transformations.py:44:15: R1732: Consider using 'with' for resource-allocating operations (consider-using-with)
pybirdai/process_steps/pybird/create_python_django_transformations.py:48:23: W0612: Unused variable 'cube_links' (unused-variable)
pybirdai/process_steps/pybird/create_python_django_transformations.py:102:4: E0213: Method 'create_slice_classes' should have "self" as first argument (no-self-argument)
pybirdai/process_steps/pybird/create_python_django_transformations.py:102:4: R0914: Too many local variables (18/15) (too-many-locals)
pybirdai/process_steps/pybird/create_python_django_transformations.py:103:8: R1702: Too many nested blocks (6/5) (too-many-nested-blocks)
pybirdai/process_steps/pybird/create_python_django_transformations.py:103:8: R1702: Too many nested blocks (6/5) (too-many-nested-blocks)
pybirdai/process_steps/pybird/create_python_django_transformations.py:102:4: R0912: Too many branches (42/12) (too-many-branches)
pybirdai/process_steps/pybird/create_python_django_transformations.py:102:4: R0915: Too many statements (136/50) (too-many-statements)
pybirdai/process_steps/pybird/create_python_django_transformations.py:104:19: R1732: Consider using 'with' for resource-allocating operations (consider-using-with)
pybirdai/process_steps/pybird/create_python_django_transformations.py:276:4: E0213: Method 'delete_generated_python_join_files' should have "self" as first argument (no-self-argument)
pybirdai/process_steps/pybird/create_python_django_transformations.py:15:0: C0411: standard import "os" should be placed before first party imports "pybirdai.utils.utils.Utils", "pybirdai.models.bird_meta_data_model.*"  (wrong-import-order)
pybirdai/process_steps/pybird/create_python_django_transformations.py:16:0: C0411: third party import "django.conf.settings" should be placed before first party imports "pybirdai.utils.utils.Utils", "pybirdai.models.bird_meta_data_model.*"  (wrong-import-order)
pybirdai/process_steps/pybird/create_python_django_transformations.py:19:0: C0411: standard import "datetime.datetime" should be placed before third party import "django.conf.settings" and first party imports "pybirdai.utils.utils.Utils", "pybirdai.models.bird_meta_data_model.*", "pybirdai.process_steps.pybird.orchestration.Orchestration", "pybirdai.models.Trail"  (wrong-import-order)
pybirdai/process_steps/pybird/create_python_django_transformations.py:17:0: C0412: Imports from package pybirdai are not grouped (ungrouped-imports)
pybirdai/process_steps/pybird/create_python_django_transformations.py:13:0: W0611: Unused Utils imported from pybirdai.utils.utils (unused-import)
pybirdai/process_steps/pybird/create_python_django_transformations.py:18:0: W0611: Unused Trail imported from pybirdai.models (unused-import)
pybirdai/process_steps/pybird/create_python_django_transformations.py:18:0: W0611: Unused MetaDataTrail imported from pybirdai.models (unused-import)
pybirdai/process_steps/pybird/create_python_django_transformations.py:18:0: W0611: Unused DerivedTable imported from pybirdai.models (unused-import)
pybirdai/process_steps/pybird/create_python_django_transformations.py:18:0: W0611: Unused FunctionText imported from pybirdai.models (unused-import)
pybirdai/process_steps/pybird/create_python_django_transformations.py:18:0: W0611: Unused TableCreationFunction imported from pybirdai.models (unused-import)
pybirdai/process_steps/pybird/create_python_django_transformations.py:14:0: W0614: Unused import(s) SUBDOMAIN, SUBDOMAIN_ENUMERATION, DOMAIN, FACET_COLLECTION, MAINTENANCE_AGENCY, MEMBER, MEMBER_HIERARCHY, MEMBER_HIERARCHY_NODE, VARIABLE, VARIABLE_SET, VARIABLE_SET_ENUMERATION, FRAMEWORK, MEMBER_MAPPING, MEMBER_MAPPING_ITEM, VARIABLE_MAPPING_ITEM, VARIABLE_MAPPING, MAPPING_TO_CUBE, MAPPING_DEFINITION, AXIS, AXIS_ORDINATE, CELL_POSITION, ORDINATE_ITEM, TABLE, TABLE_CELL, CUBE_STRUCTURE, CUBE_STRUCTURE_ITEM, CUBE, CUBE_LINK, CUBE_STRUCTURE_ITEM_LINK, COMBINATION, COMBINATION_ITEM, CUBE_TO_COMBINATION, MEMBER_LINK, models, OperationalError and timezone from wildcard import of pybirdai.models.bird_meta_data_model (unused-wildcard-import)
************* Module pybirdai.process_steps.pybird.orchestration
pybirdai/process_steps/pybird/orchestration.py:31:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:34:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:35:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:36:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:37:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:38:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:39:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:40:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:41:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:46:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:47:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:49:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:50:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:51:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:52:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:56:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:59:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:60:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:63:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:65:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:66:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:67:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:69:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:70:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:71:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:73:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:75:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:76:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:77:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:78:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:80:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:83:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:85:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:88:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:89:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:92:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:93:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:94:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:97:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:100:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:101:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:102:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:104:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:105:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:107:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:111:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:112:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:113:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:114:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:115:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:116:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:118:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:122:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:123:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:124:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:125:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:126:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:129:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:130:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:132:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:133:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:135:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:136:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:139:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:140:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:141:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:147:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:148:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:155:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:156:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:158:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:162:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:164:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:168:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:169:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:170:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:173:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:176:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:177:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:180:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:184:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:185:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:186:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:189:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:190:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:191:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:194:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:195:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:196:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:197:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:198:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:199:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:200:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:201:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:202:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:203:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:205:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:208:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:209:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:214:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:215:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:217:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:218:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:219:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:221:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:224:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:225:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:228:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:230:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:231:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:233:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:234:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:236:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:237:0: W0311: Bad indentation. Found 7 spaces, expected 28 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:238:0: W0311: Bad indentation. Found 7 spaces, expected 28 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:239:0: W0311: Bad indentation. Found 7 spaces, expected 28 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:240:0: W0311: Bad indentation. Found 7 spaces, expected 28 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:241:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:242:0: W0311: Bad indentation. Found 7 spaces, expected 28 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:244:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:245:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:248:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:249:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:250:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:251:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:253:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:254:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:255:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:256:0: W0311: Bad indentation. Found 7 spaces, expected 28 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:257:0: W0311: Bad indentation. Found 7 spaces, expected 28 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:259:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:260:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:262:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:263:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:264:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:266:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:267:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:268:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:269:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:272:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:276:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:277:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:278:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:281:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:282:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:283:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:286:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:287:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:289:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:290:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:293:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:294:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:296:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:297:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:298:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:300:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:302:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:303:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:305:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:306:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:307:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:309:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:310:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:313:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:314:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:317:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:318:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:320:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:321:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:322:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:324:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:325:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:326:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:327:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:330:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:331:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:334:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:335:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:336:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:337:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:340:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:341:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:342:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:343:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:346:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:347:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:349:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:351:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:352:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:354:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:356:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:357:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:358:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:360:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:361:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:364:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:367:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:368:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:370:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:374:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:375:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:376:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:379:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:380:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:383:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:385:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:386:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:388:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:390:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:391:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:395:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:397:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:398:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:400:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:401:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:402:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:403:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:404:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:405:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:406:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:407:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:409:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:410:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:411:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:412:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:413:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:414:0: W0311: Bad indentation. Found 7 spaces, expected 28 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:416:0: W0311: Bad indentation. Found 7 spaces, expected 28 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:416:0: W0301: Unnecessary semicolon (unnecessary-semicolon)
pybirdai/process_steps/pybird/orchestration.py:417:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:419:0: W0311: Bad indentation. Found 7 spaces, expected 28 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:420:0: W0311: Bad indentation. Found 8 spaces, expected 32 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:422:0: W0311: Bad indentation. Found 9 spaces, expected 36 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:423:0: W0311: Bad indentation. Found 9 spaces, expected 36 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:426:0: W0311: Bad indentation. Found 10 spaces, expected 40 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:427:0: W0311: Bad indentation. Found 8 spaces, expected 32 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:428:0: W0311: Bad indentation. Found 9 spaces, expected 36 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:430:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:431:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:431:0: W0301: Unnecessary semicolon (unnecessary-semicolon)
pybirdai/process_steps/pybird/orchestration.py:433:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:436:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:437:0: W0311: Bad indentation. Found 7 spaces, expected 28 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:438:0: W0311: Bad indentation. Found 8 spaces, expected 32 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:439:0: W0311: Bad indentation. Found 9 spaces, expected 36 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:442:0: W0311: Bad indentation. Found 9 spaces, expected 36 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:443:0: W0311: Bad indentation. Found 9 spaces, expected 36 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:444:0: W0311: Bad indentation. Found 9 spaces, expected 36 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:449:0: W0311: Bad indentation. Found 10 spaces, expected 40 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:450:0: W0311: Bad indentation. Found 10 spaces, expected 40 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:451:0: W0311: Bad indentation. Found 10 spaces, expected 40 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:454:0: W0311: Bad indentation. Found 10 spaces, expected 40 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:455:0: W0311: Bad indentation. Found 11 spaces, expected 44 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:456:0: W0311: Bad indentation. Found 10 spaces, expected 40 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:457:0: W0311: Bad indentation. Found 11 spaces, expected 44 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:460:0: W0311: Bad indentation. Found 10 spaces, expected 40 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:461:0: W0311: Bad indentation. Found 10 spaces, expected 40 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:462:0: W0311: Bad indentation. Found 11 spaces, expected 44 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:464:0: W0311: Bad indentation. Found 12 spaces, expected 48 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:465:0: W0311: Bad indentation. Found 12 spaces, expected 48 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:468:0: W0311: Bad indentation. Found 13 spaces, expected 52 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:469:0: W0311: Bad indentation. Found 14 spaces, expected 56 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:470:0: W0311: Bad indentation. Found 13 spaces, expected 52 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:471:0: W0311: Bad indentation. Found 14 spaces, expected 56 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:473:0: W0311: Bad indentation. Found 8 spaces, expected 32 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:474:0: W0311: Bad indentation. Found 9 spaces, expected 36 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:476:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:478:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:479:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:480:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:484:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:485:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:487:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:488:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:489:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:498:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:500:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:501:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:502:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:503:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:504:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:505:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:505:0: W0301: Unnecessary semicolon (unnecessary-semicolon)
pybirdai/process_steps/pybird/orchestration.py:506:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:507:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:511:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:512:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:513:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:514:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:517:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:520:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:521:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:522:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:523:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:525:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:528:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:529:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:531:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:535:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:536:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:537:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:538:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:539:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:541:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:542:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:543:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:544:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:547:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:548:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:555:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:556:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:560:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:563:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:568:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:570:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:572:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:574:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:579:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:590:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:591:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:592:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:594:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:595:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:597:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:598:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:604:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:606:0: W0311: Bad indentation. Found 7 spaces, expected 28 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:612:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:613:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:615:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:617:79: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:617:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:618:46: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:620:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:621:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:622:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:628:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:630:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:631:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:632:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:633:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:634:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:635:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:637:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:640:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:641:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:642:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:644:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:648:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:649:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:650:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:651:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:652:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:654:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:655:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:656:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:657:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:660:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:661:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:668:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:669:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:673:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:676:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:681:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:683:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:684:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:686:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:687:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:688:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:690:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:695:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:700:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:703:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:704:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:706:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:707:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:709:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:710:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:716:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:718:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:723:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:724:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:726:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:728:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:729:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:730:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:732:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:733:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:735:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:736:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:738:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:740:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:741:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:742:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:743:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:745:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:747:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:748:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:749:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:750:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:753:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:756:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:757:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:758:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:761:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:764:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:765:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:766:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:767:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:768:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:771:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:772:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:773:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:775:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:777:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:780:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:784:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:786:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:788:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:790:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:796:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:803:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:804:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:806:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:807:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:808:0: W0311: Bad indentation. Found 7 spaces, expected 28 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:810:0: W0311: Bad indentation. Found 7 spaces, expected 28 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:816:0: W0311: Bad indentation. Found 7 spaces, expected 28 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:817:0: W0311: Bad indentation. Found 8 spaces, expected 32 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:825:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:826:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:827:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:829:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:830:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:833:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:840:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:841:0: W0311: Bad indentation. Found 7 spaces, expected 28 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:849:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:851:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:852:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:854:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:855:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:856:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:859:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:860:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:862:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:863:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:865:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:867:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:868:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:869:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:870:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:871:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:874:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:878:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:879:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:881:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:883:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:886:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:887:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:889:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:890:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:891:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:892:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:894:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:896:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:897:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:898:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:900:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:901:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:902:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:904:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:905:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:907:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:910:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:914:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:915:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:917:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:919:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:920:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:921:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:925:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:927:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:928:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:929:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:934:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:935:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:937:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:938:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:939:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:941:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:942:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:943:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:945:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:946:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:948:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:950:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:951:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:954:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:955:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:957:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:959:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:960:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:963:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:964:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:966:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:968:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:969:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:970:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:972:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:973:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:974:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:976:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:977:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:978:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:981:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:982:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:983:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:985:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:986:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:987:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:988:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:990:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:991:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:992:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:994:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:997:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:999:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1000:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1001:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1002:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1003:0: W0311: Bad indentation. Found 7 spaces, expected 28 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1006:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1008:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1009:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1010:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1011:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1012:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1013:0: W0311: Bad indentation. Found 7 spaces, expected 28 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1015:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1016:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1018:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1020:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1021:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1022:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1023:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1025:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1027:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1028:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1029:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1030:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1033:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1036:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1037:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1039:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1040:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1041:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1042:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1043:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1044:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1046:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1047:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1048:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1050:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1051:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1052:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1053:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1054:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1057:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1059:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1063:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1065:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1071:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1072:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1073:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1074:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1075:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1077:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1078:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1079:0: W0311: Bad indentation. Found 7 spaces, expected 28 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1080:0: W0311: Bad indentation. Found 7 spaces, expected 28 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1083:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1084:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1087:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1090:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1092:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1093:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1094:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1096:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1097:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1098:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1100:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1101:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1103:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1105:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1109:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1110:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1111:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1115:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1116:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1118:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1119:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1120:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1121:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1122:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1124:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1132:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1133:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1135:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1136:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1137:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1139:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1140:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1141:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1142:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1143:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1146:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1147:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1149:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1150:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1153:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1154:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1155:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1156:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1159:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1160:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1161:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1162:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1163:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1166:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1167:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1169:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1171:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1172:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1173:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1175:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1176:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1177:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1178:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1180:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1182:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1183:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1184:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1185:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1188:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1193:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1194:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1195:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1196:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1197:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1202:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1203:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1206:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1209:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1212:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1214:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1215:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1216:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1218:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1219:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1220:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1221:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1222:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1223:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1224:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1226:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1228:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1229:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1230:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1231:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1232:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1233:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1234:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1237:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1238:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1240:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1243:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1246:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1247:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1248:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1251:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1253:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1254:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1255:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1259:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1260:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1262:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1263:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1264:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1265:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1266:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1267:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1268:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1270:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1271:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1272:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1273:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1274:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1277:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1282:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1286:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1288:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1292:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1293:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1295:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1296:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1297:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1298:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1299:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1301:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1302:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1308:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1311:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1312:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1313:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1315:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1316:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1317:0: W0311: Bad indentation. Found 7 spaces, expected 28 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1322:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1324:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1327:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1330:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1332:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1333:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1334:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1336:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1337:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1338:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1339:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1341:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1343:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1344:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1345:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1348:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1352:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1353:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1354:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1357:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1360:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1362:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1363:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1364:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1365:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1367:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1369:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1371:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1377:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1378:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1386:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1391:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1392:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1395:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1398:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1401:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1402:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1403:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1404:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1406:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1410:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1411:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1414:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1415:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1417:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1418:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1419:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1421:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1422:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1424:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1425:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1428:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1429:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1432:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1434:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1435:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1436:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1437:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1439:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1441:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1442:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1443:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1447:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1451:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1453:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1454:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1455:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1456:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1457:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1458:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1459:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1460:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1461:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1462:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1463:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1464:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1465:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1466:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1467:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1468:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1470:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1471:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1472:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1473:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1475:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1477:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1478:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1481:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1482:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1484:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1485:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1487:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1488:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1493:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1495:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1499:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1500:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1501:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1502:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1503:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1504:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1506:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1507:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1509:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1510:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1511:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1512:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1513:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1515:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1518:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1519:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1520:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1521:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1523:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1527:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1529:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1530:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1531:0: W0311: Bad indentation. Found 7 spaces, expected 28 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1532:0: W0311: Bad indentation. Found 7 spaces, expected 28 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1533:0: W0311: Bad indentation. Found 8 spaces, expected 32 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1534:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1535:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1536:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1540:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1541:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1542:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1543:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1544:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1546:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1547:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1549:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1551:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1552:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1554:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1555:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1557:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1559:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1560:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1562:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1563:0: W0311: Bad indentation. Found 7 spaces, expected 28 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1568:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1570:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1574:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1575:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1576:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1577:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1579:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1580:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1582:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1583:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1585:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1588:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1589:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1590:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1591:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1593:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1597:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1598:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1599:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1602:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1603:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1606:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1607:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1608:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1609:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1610:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1611:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1612:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1613:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1614:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1615:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1616:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1618:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1624:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1625:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1626:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1632:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1633:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1634:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1635:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1636:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1637:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1638:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1639:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1640:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1641:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1642:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1643:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1645:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1646:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1648:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1650:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1651:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1653:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1654:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1655:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1656:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1657:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1658:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1660:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1661:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1662:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1663:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1664:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1665:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1667:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1668:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1669:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1671:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1672:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1673:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1674:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1675:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1677:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1678:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1679:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1680:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1681:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1683:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1684:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1685:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1686:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1687:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1688:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1690:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1691:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1692:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1693:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1694:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1695:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1696:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1697:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1698:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1699:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1700:0: W0311: Bad indentation. Found 7 spaces, expected 28 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1701:0: W0311: Bad indentation. Found 7 spaces, expected 28 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1702:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1703:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1704:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1705:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1706:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1708:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1709:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1710:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1711:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1712:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1713:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1714:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1715:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1716:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1717:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1719:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1725:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1726:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1727:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1731:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1732:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1733:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1742:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1743:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1744:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1745:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1746:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1747:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1748:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1749:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1750:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1751:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1755:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1756:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1757:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1758:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1759:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1760:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1761:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1762:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1763:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1767:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1768:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1769:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1770:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1771:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1772:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1773:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1774:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1775:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1776:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1777:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1779:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1780:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1781:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1782:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1783:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1784:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1786:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1787:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1789:0: W0311: Bad indentation. Found 7 spaces, expected 28 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1794:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1795:0: W0311: Bad indentation. Found 7 spaces, expected 28 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1796:0: W0311: Bad indentation. Found 8 spaces, expected 32 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1801:0: W0311: Bad indentation. Found 8 spaces, expected 32 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1802:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1804:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1805:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1806:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1808:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1809:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1810:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1812:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1813:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1814:0: W0311: Bad indentation. Found 7 spaces, expected 28 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1815:0: W0311: Bad indentation. Found 7 spaces, expected 28 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1816:0: W0311: Bad indentation. Found 8 spaces, expected 32 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1821:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1822:0: W0311: Bad indentation. Found 8 spaces, expected 32 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1823:0: W0311: Bad indentation. Found 9 spaces, expected 36 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1828:0: W0311: Bad indentation. Found 9 spaces, expected 36 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1829:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1830:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1831:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1832:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1833:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1834:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1835:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1836:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1837:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1838:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1840:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1841:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1842:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1845:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1846:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1847:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1848:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1849:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1850:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1851:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1852:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1853:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1854:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1855:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1857:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1858:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1859:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1860:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1861:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1862:0: W0311: Bad indentation. Found 7 spaces, expected 28 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1863:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1864:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1865:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1866:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1868:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1869:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1870:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1871:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1872:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1874:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1875:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1876:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1877:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1878:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1879:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1881:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1882:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1884:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1885:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1886:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1887:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1889:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1890:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1891:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1893:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1894:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1895:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1896:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1900:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1901:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1902:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1904:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1905:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1906:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1907:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1908:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1909:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1910:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1911:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1912:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1913:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1919:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1920:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1921:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1923:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1924:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1925:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1927:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1928:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1929:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1930:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1931:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1932:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1935:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1936:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1938:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1939:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1941:0: W0311: Bad indentation. Found 7 spaces, expected 28 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1944:0: W0311: Bad indentation. Found 7 spaces, expected 28 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1946:0: W0311: Bad indentation. Found 8 spaces, expected 32 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1947:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1948:49: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1949:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1950:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1951:0: W0311: Bad indentation. Found 7 spaces, expected 28 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1952:0: W0311: Bad indentation. Found 7 spaces, expected 28 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1953:0: W0311: Bad indentation. Found 8 spaces, expected 32 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1955:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1957:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1959:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1961:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1962:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1963:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1964:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1965:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1967:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1968:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1973:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration.py:1974:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1976:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1978:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1981:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1982:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1983:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1985:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1987:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1991:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1992:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1993:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1994:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1995:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1996:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1998:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2002:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2003:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2004:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2005:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2006:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2009:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2010:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2011:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2012:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2013:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2014:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2015:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2017:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2018:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2023:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2024:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2027:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2028:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2029:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2033:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2034:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2038:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2039:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2040:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2041:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2044:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2045:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2048:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2050:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2051:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2052:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2053:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2054:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2055:0: W0311: Bad indentation. Found 7 spaces, expected 28 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2056:0: W0311: Bad indentation. Found 8 spaces, expected 32 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2058:0: W0311: Bad indentation. Found 8 spaces, expected 32 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2059:0: W0311: Bad indentation. Found 9 spaces, expected 36 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2060:0: W0311: Bad indentation. Found 7 spaces, expected 28 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2061:0: W0311: Bad indentation. Found 8 spaces, expected 32 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2062:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2063:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2066:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2070:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2071:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2073:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2074:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2075:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2077:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2078:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2079:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2080:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2081:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2082:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2084:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2085:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2088:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2099:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2100:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2101:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2102:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2107:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2108:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2109:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2117:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2123:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2125:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2127:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2128:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2129:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2131:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2132:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2135:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2138:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2140:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2141:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2145:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2147:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2148:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2150:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2151:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2152:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2153:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2154:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2155:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2156:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2157:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2159:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2160:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2161:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2162:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2163:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2164:0: W0311: Bad indentation. Found 7 spaces, expected 28 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2165:0: W0311: Bad indentation. Found 7 spaces, expected 28 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2165:0: W0301: Unnecessary semicolon (unnecessary-semicolon)
pybirdai/process_steps/pybird/orchestration.py:2167:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2168:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2168:0: W0301: Unnecessary semicolon (unnecessary-semicolon)
pybirdai/process_steps/pybird/orchestration.py:2170:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2173:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2174:0: W0311: Bad indentation. Found 7 spaces, expected 28 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2175:0: W0311: Bad indentation. Found 8 spaces, expected 32 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2176:0: W0311: Bad indentation. Found 9 spaces, expected 36 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2177:0: W0311: Bad indentation. Found 8 spaces, expected 32 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2178:0: W0311: Bad indentation. Found 9 spaces, expected 36 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2180:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2182:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2183:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2184:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2188:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2189:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2191:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2192:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2193:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2202:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2204:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2205:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2206:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2207:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2208:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2209:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2209:0: W0301: Unnecessary semicolon (unnecessary-semicolon)
pybirdai/process_steps/pybird/orchestration.py:2210:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2211:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2216:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2220:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2222:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2223:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2224:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2225:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2226:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2227:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2232:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:2236:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/orchestration.py:1:0: C0302: Too many lines in module (2236/1000) (too-many-lines)
pybirdai/process_steps/pybird/orchestration.py:2115:3: W0511: TODO: Add edges based on function references and data flow (fixme)
pybirdai/process_steps/pybird/orchestration.py:75:1: R0912: Too many branches (19/12) (too-many-branches)
pybirdai/process_steps/pybird/orchestration.py:196:6: W0621: Redefining name 'apps' from outer scope (line 14) (redefined-outer-name)
pybirdai/process_steps/pybird/orchestration.py:214:9: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/process_steps/pybird/orchestration.py:200:12: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/process_steps/pybird/orchestration.py:196:6: W0404: Reimport 'apps' (imported line 14) (reimported)
pybirdai/process_steps/pybird/orchestration.py:196:6: C0415: Import outside toplevel (django.apps.apps) (import-outside-toplevel)
pybirdai/process_steps/pybird/orchestration.py:198:49: W0212: Access to a protected member _meta of a client class (protected-access)
pybirdai/process_steps/pybird/orchestration.py:209:5: W0612: Unused variable 'db_field' (unused-variable)
pybirdai/process_steps/pybird/orchestration.py:262:9: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/process_steps/pybird/orchestration.py:244:11: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/process_steps/pybird/orchestration.py:241:13: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/process_steps/pybird/orchestration.py:268:2: C0415: Import outside toplevel (inspect) (import-outside-toplevel)
pybirdai/process_steps/pybird/orchestration.py:289:10: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/process_steps/pybird/orchestration.py:268:2: W0611: Unused import inspect (unused-import)
pybirdai/process_steps/pybird/orchestration.py:321:2: W0702: No exception type(s) specified (bare-except)
pybirdai/process_steps/pybird/orchestration.py:351:9: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/process_steps/pybird/orchestration.py:327:3: C0415: Import outside toplevel (inspect) (import-outside-toplevel)
pybirdai/process_steps/pybird/orchestration.py:357:18: W0212: Access to a protected member _initialized_objects of a client class (protected-access)
pybirdai/process_steps/pybird/orchestration.py:364:2: W0212: Access to a protected member _initialized_objects of a client class (protected-access)
pybirdai/process_steps/pybird/orchestration.py:367:2: C0415: Import outside toplevel (pybirdai.annotations.decorators._lineage_context) (import-outside-toplevel)
pybirdai/process_steps/pybird/orchestration.py:380:4: W0212: Access to a protected member _track_object_initialization of a client class (protected-access)
pybirdai/process_steps/pybird/orchestration.py:385:3: W0212: Access to a protected member _ensure_references_set of a client class (protected-access)
pybirdai/process_steps/pybird/orchestration.py:390:1: R0914: Too many local variables (16/15) (too-many-locals)
pybirdai/process_steps/pybird/orchestration.py:401:5: W0621: Redefining name 'apps' from outer scope (line 14) (redefined-outer-name)
pybirdai/process_steps/pybird/orchestration.py:401:5: W0404: Reimport 'apps' (imported line 14) (reimported)
pybirdai/process_steps/pybird/orchestration.py:401:5: C0415: Import outside toplevel (django.apps.apps) (import-outside-toplevel)
pybirdai/process_steps/pybird/orchestration.py:427:15: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/process_steps/pybird/orchestration.py:397:2: R1702: Too many nested blocks (8/5) (too-many-nested-blocks)
pybirdai/process_steps/pybird/orchestration.py:473:15: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/process_steps/pybird/orchestration.py:442:9: C0415: Import outside toplevel (pybirdai.annotations.decorators._lineage_context) (import-outside-toplevel)
pybirdai/process_steps/pybird/orchestration.py:455:11: W0212: Access to a protected member _track_object_initialization of a client class (protected-access)
pybirdai/process_steps/pybird/orchestration.py:390:1: R0912: Too many branches (21/12) (too-many-branches)
pybirdai/process_steps/pybird/orchestration.py:390:1: R0915: Too many statements (52/50) (too-many-statements)
pybirdai/process_steps/pybird/orchestration.py:397:2: R1702: Too many nested blocks (12/5) (too-many-nested-blocks)
pybirdai/process_steps/pybird/orchestration.py:506:2: W0702: No exception type(s) specified (bare-except)
pybirdai/process_steps/pybird/orchestration.py:501:1: R1710: Either all return statements in a function should return an expression, or none of them should. (inconsistent-return-statements)
pybirdai/process_steps/pybird/orchestration.py:511:1: R0914: Too many local variables (21/15) (too-many-locals)
pybirdai/process_steps/pybird/orchestration.py:612:11: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/process_steps/pybird/orchestration.py:511:1: R0912: Too many branches (17/12) (too-many-branches)
pybirdai/process_steps/pybird/orchestration.py:511:1: R1710: Either all return statements in a function should return an expression, or none of them should. (inconsistent-return-statements)
pybirdai/process_steps/pybird/orchestration.py:617:1: R0914: Too many local variables (25/15) (too-many-locals)
pybirdai/process_steps/pybird/orchestration.py:723:10: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/process_steps/pybird/orchestration.py:617:1: R0912: Too many branches (16/12) (too-many-branches)
pybirdai/process_steps/pybird/orchestration.py:617:1: R1710: Either all return statements in a function should return an expression, or none of them should. (inconsistent-return-statements)
pybirdai/process_steps/pybird/orchestration.py:742:10: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/process_steps/pybird/orchestration.py:747:1: R0914: Too many local variables (26/15) (too-many-locals)
pybirdai/process_steps/pybird/orchestration.py:851:9: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/process_steps/pybird/orchestration.py:767:4: W0702: No exception type(s) specified (bare-except)
pybirdai/process_steps/pybird/orchestration.py:765:5: C0415: Import outside toplevel (inspect) (import-outside-toplevel)
pybirdai/process_steps/pybird/orchestration.py:749:2: R1702: Too many nested blocks (6/5) (too-many-nested-blocks)
pybirdai/process_steps/pybird/orchestration.py:830:6: C0415: Import outside toplevel (pybirdai.models.TableCreationFunctionColumn) (import-outside-toplevel)
pybirdai/process_steps/pybird/orchestration.py:896:9: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/process_steps/pybird/orchestration.py:870:3: C0415: Import outside toplevel (inspect) (import-outside-toplevel)
pybirdai/process_steps/pybird/orchestration.py:871:3: C0415: Import outside toplevel (re) (import-outside-toplevel)
pybirdai/process_steps/pybird/orchestration.py:937:9: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/process_steps/pybird/orchestration.py:910:3: C0415: Import outside toplevel (re) (import-outside-toplevel)
pybirdai/process_steps/pybird/orchestration.py:968:9: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/process_steps/pybird/orchestration.py:986:9: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/process_steps/pybird/orchestration.py:1015:9: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/process_steps/pybird/orchestration.py:999:8: W0612: Unused variable 'table_name' (unused-variable)
pybirdai/process_steps/pybird/orchestration.py:1092:9: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/process_steps/pybird/orchestration.py:1020:1: R0912: Too many branches (19/12) (too-many-branches)
pybirdai/process_steps/pybird/orchestration.py:1020:1: R1710: Either all return statements in a function should return an expression, or none of them should. (inconsistent-return-statements)
pybirdai/process_steps/pybird/orchestration.py:1132:9: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/process_steps/pybird/orchestration.py:1171:9: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/process_steps/pybird/orchestration.py:1153:4: R1724: Unnecessary "elif" after "continue", remove the leading "el" from "elif" (no-else-continue)
pybirdai/process_steps/pybird/orchestration.py:1214:9: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/process_steps/pybird/orchestration.py:1175:1: R1710: Either all return statements in a function should return an expression, or none of them should. (inconsistent-return-statements)
pybirdai/process_steps/pybird/orchestration.py:1218:1: R0914: Too many local variables (21/15) (too-many-locals)
pybirdai/process_steps/pybird/orchestration.py:1223:9: W1309: Using an f-string that does not have any interpolated variables (f-string-without-interpolation)
pybirdai/process_steps/pybird/orchestration.py:1332:9: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/process_steps/pybird/orchestration.py:1322:12: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/process_steps/pybird/orchestration.py:1218:1: R0911: Too many return statements (7/6) (too-many-return-statements)
pybirdai/process_steps/pybird/orchestration.py:1218:1: R0912: Too many branches (17/12) (too-many-branches)
pybirdai/process_steps/pybird/orchestration.py:1218:1: R0915: Too many statements (52/50) (too-many-statements)
pybirdai/process_steps/pybird/orchestration.py:1218:1: R1710: Either all return statements in a function should return an expression, or none of them should. (inconsistent-return-statements)
pybirdai/process_steps/pybird/orchestration.py:1247:3: W0612: Unused variable 'class_name' (unused-variable)
pybirdai/process_steps/pybird/orchestration.py:1248:3: W0612: Unused variable 'method_name' (unused-variable)
pybirdai/process_steps/pybird/orchestration.py:1417:9: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/process_steps/pybird/orchestration.py:1460:1: R0914: Too many local variables (22/15) (too-many-locals)
pybirdai/process_steps/pybird/orchestration.py:1636:9: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/process_steps/pybird/orchestration.py:1475:42: W0212: Access to a protected member _meta of a client class (protected-access)
pybirdai/process_steps/pybird/orchestration.py:1529:18: W0212: Access to a protected member _meta of a client class (protected-access)
pybirdai/process_steps/pybird/orchestration.py:1468:2: R1702: Too many nested blocks (6/5) (too-many-nested-blocks)
pybirdai/process_steps/pybird/orchestration.py:1638:3: C0415: Import outside toplevel (traceback) (import-outside-toplevel)
pybirdai/process_steps/pybird/orchestration.py:1460:1: R0912: Too many branches (26/12) (too-many-branches)
pybirdai/process_steps/pybird/orchestration.py:1460:1: R0915: Too many statements (77/50) (too-many-statements)
pybirdai/process_steps/pybird/orchestration.py:1626:4: W0612: Unused variable 'used_row' (unused-variable)
pybirdai/process_steps/pybird/orchestration.py:1657:9: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/process_steps/pybird/orchestration.py:1655:12: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/process_steps/pybird/orchestration.py:1743:9: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/process_steps/pybird/orchestration.py:1660:1: R0912: Too many branches (18/12) (too-many-branches)
pybirdai/process_steps/pybird/orchestration.py:1830:9: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/process_steps/pybird/orchestration.py:1772:2: R1702: Too many nested blocks (6/5) (too-many-nested-blocks)
pybirdai/process_steps/pybird/orchestration.py:1770:1: R0912: Too many branches (13/12) (too-many-branches)
pybirdai/process_steps/pybird/orchestration.py:1772:2: R1702: Too many nested blocks (7/5) (too-many-nested-blocks)
pybirdai/process_steps/pybird/orchestration.py:1849:9: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/process_steps/pybird/orchestration.py:1840:7: W0612: Unused variable 'table_name' (unused-variable)
pybirdai/process_steps/pybird/orchestration.py:1864:9: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/process_steps/pybird/orchestration.py:1886:9: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/process_steps/pybird/orchestration.py:1920:9: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/process_steps/pybird/orchestration.py:1955:9: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/process_steps/pybird/orchestration.py:1925:2: R1702: Too many nested blocks (6/5) (too-many-nested-blocks)
pybirdai/process_steps/pybird/orchestration.py:1925:2: R1702: Too many nested blocks (6/5) (too-many-nested-blocks)
pybirdai/process_steps/pybird/orchestration.py:1936:8: W0612: Unused variable 'table_name' (unused-variable)
pybirdai/process_steps/pybird/orchestration.py:1961:1: R0914: Too many local variables (19/15) (too-many-locals)
pybirdai/process_steps/pybird/orchestration.py:2070:9: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/process_steps/pybird/orchestration.py:2060:7: W0702: No exception type(s) specified (bare-except)
pybirdai/process_steps/pybird/orchestration.py:1961:1: R0912: Too many branches (24/12) (too-many-branches)
pybirdai/process_steps/pybird/orchestration.py:1961:1: R0915: Too many statements (60/50) (too-many-statements)
pybirdai/process_steps/pybird/orchestration.py:1974:2: R1702: Too many nested blocks (7/5) (too-many-nested-blocks)
pybirdai/process_steps/pybird/orchestration.py:1983:4: W0612: Unused variable 'table_exists' (unused-variable)
pybirdai/process_steps/pybird/orchestration.py:2151:5: W0621: Redefining name 'apps' from outer scope (line 14) (redefined-outer-name)
pybirdai/process_steps/pybird/orchestration.py:2151:5: W0404: Reimport 'apps' (imported line 14) (reimported)
pybirdai/process_steps/pybird/orchestration.py:2151:5: C0415: Import outside toplevel (django.apps.apps) (import-outside-toplevel)
pybirdai/process_steps/pybird/orchestration.py:2177:8: W0702: No exception type(s) specified (bare-except)
pybirdai/process_steps/pybird/orchestration.py:2147:2: R1702: Too many nested blocks (7/5) (too-many-nested-blocks)
pybirdai/process_steps/pybird/orchestration.py:2210:2: W0702: No exception type(s) specified (bare-except)
pybirdai/process_steps/pybird/orchestration.py:2205:1: R1710: Either all return statements in a function should return an expression, or none of them should. (inconsistent-return-statements)
pybirdai/process_steps/pybird/orchestration.py:2220:1: C0415: Import outside toplevel (pybirdai.context.context.Context) (import-outside-toplevel)
pybirdai/process_steps/pybird/orchestration.py:2222:1: R1705: Unnecessary "else" after "return", remove the "else" and de-indent the code inside it (no-else-return)
pybirdai/process_steps/pybird/orchestration.py:25:0: C0411: standard import "datetime.datetime" should be placed before third party import "django.apps.apps" and first party imports "pybirdai.process_steps.pybird.csv_converter.CSVConverter", "pybirdai.models.Trail"  (wrong-import-order)
pybirdai/process_steps/pybird/orchestration.py:26:0: C0411: third party import "django.contrib.contenttypes.models.ContentType" should be placed before first party imports "pybirdai.process_steps.pybird.csv_converter.CSVConverter", "pybirdai.models.Trail"  (wrong-import-order)
pybirdai/process_steps/pybird/orchestration.py:28:0: C0411: standard import "importlib" should be placed before third party imports "django.apps.apps", "django.contrib.contenttypes.models.ContentType" and first party imports "pybirdai.process_steps.pybird.csv_converter.CSVConverter", "pybirdai.models.Trail"  (wrong-import-order)
pybirdai/process_steps/pybird/orchestration.py:26:0: C0412: Imports from package django are not grouped (ungrouped-imports)
************* Module pybirdai.process_steps.pybird.test_orchestration
pybirdai/process_steps/pybird/test_orchestration.py:24:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/test_orchestration.py:28:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/test_orchestration.py:39:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/test_orchestration.py:42:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/test_orchestration.py:45:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/test_orchestration.py:48:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/test_orchestration.py:51:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/test_orchestration.py:54:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/test_orchestration.py:57:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/test_orchestration.py:60:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/test_orchestration.py:63:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/test_orchestration.py:70:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/test_orchestration.py:73:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/test_orchestration.py:76:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/test_orchestration.py:79:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/test_orchestration.py:82:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/test_orchestration.py:85:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/test_orchestration.py:88:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/test_orchestration.py:95:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/test_orchestration.py:100:0: C0304: Final newline missing (missing-final-newline)
pybirdai/process_steps/pybird/test_orchestration.py:17:4: R1711: Useless return at end of function or method (useless-return)
pybirdai/process_steps/pybird/test_orchestration.py:25:4: R1711: Useless return at end of function or method (useless-return)
************* Module pybirdai.process_steps.pybird.execute_datapoint
pybirdai/process_steps/pybird/execute_datapoint.py:22:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/execute_datapoint.py:27:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/execute_datapoint.py:31:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/execute_datapoint.py:39:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/execute_datapoint.py:48:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/execute_datapoint.py:59:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/execute_datapoint.py:63:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/execute_datapoint.py:69:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/execute_datapoint.py:73:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/execute_datapoint.py:77:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/execute_datapoint.py:82:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/execute_datapoint.py:86:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/execute_datapoint.py:100:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/execute_datapoint.py:106:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/execute_datapoint.py:113:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/execute_datapoint.py:12:0: E0401: Unable to import 'pybirdai.process_steps.filter_code.report_cells' (import-error)
pybirdai/process_steps/pybird/execute_datapoint.py:12:0: W0401: Wildcard import pybirdai.process_steps.filter_code.report_cells (wildcard-import)
pybirdai/process_steps/pybird/execute_datapoint.py:12:0: E0611: No name 'report_cells' in module 'pybirdai.process_steps.filter_code' (no-name-in-module)
pybirdai/process_steps/pybird/execute_datapoint.py:19:4: E0213: Method 'execute_data_point' should have "self" as first argument (no-self-argument)
pybirdai/process_steps/pybird/execute_datapoint.py:19:4: R0914: Too many local variables (28/15) (too-many-locals)
pybirdai/process_steps/pybird/execute_datapoint.py:24:8: C0415: Import outside toplevel (pybirdai.process_steps.pybird.orchestration.Orchestration, pybirdai.process_steps.pybird.orchestration.OrchestrationWithLineage) (import-outside-toplevel)
pybirdai/process_steps/pybird/execute_datapoint.py:25:8: C0415: Import outside toplevel (pybirdai.annotations.decorators.set_lineage_orchestration) (import-outside-toplevel)
pybirdai/process_steps/pybird/execute_datapoint.py:26:8: C0415: Import outside toplevel (pybirdai.context.context.Context) (import-outside-toplevel)
pybirdai/process_steps/pybird/execute_datapoint.py:41:12: C0415: Import outside toplevel (pybirdai.models.MetaDataTrail, pybirdai.models.Trail) (import-outside-toplevel)
pybirdai/process_steps/pybird/execute_datapoint.py:52:18: W1309: Using an f-string that does not have any interpolated variables (f-string-without-interpolation)
pybirdai/process_steps/pybird/execute_datapoint.py:56:18: W1309: Using an f-string that does not have any interpolated variables (f-string-without-interpolation)
pybirdai/process_steps/pybird/execute_datapoint.py:71:12: C0415: Import outside toplevel (pybirdai.debug_tracking.add_debug_to_orchestration) (import-outside-toplevel)
pybirdai/process_steps/pybird/execute_datapoint.py:79:12: C0415: Import outside toplevel (pybirdai.process_steps.filter_code.automatic_tracking_wrapper.create_smart_tracking_wrapper) (import-outside-toplevel)
pybirdai/process_steps/pybird/execute_datapoint.py:92:16: C0415: Import outside toplevel (pybirdai.models.DatabaseTable, pybirdai.models.PopulatedDataBaseTable, pybirdai.models.DatabaseField, pybirdai.models.DatabaseRow, pybirdai.models.CalculationUsedRow, pybirdai.models.CalculationUsedField) (import-outside-toplevel)
pybirdai/process_steps/pybird/execute_datapoint.py:19:4: R0915: Too many statements (58/50) (too-many-statements)
pybirdai/process_steps/pybird/execute_datapoint.py:26:8: W0611: Unused Context imported from pybirdai.context.context (unused-import)
pybirdai/process_steps/pybird/execute_datapoint.py:117:4: E0211: Method 'delete_lineage_data' has no argument (no-method-argument)
pybirdai/process_steps/pybird/execute_datapoint.py:13:0: C0411: standard import "importlib" should be placed before first party import "pybirdai.process_steps.filter_code.report_cells.*"  (wrong-import-order)
pybirdai/process_steps/pybird/execute_datapoint.py:14:0: C0411: standard import "os" should be placed before first party import "pybirdai.process_steps.filter_code.report_cells.*"  (wrong-import-order)
pybirdai/process_steps/pybird/execute_datapoint.py:15:0: C0411: standard import "datetime.datetime" should be placed before first party import "pybirdai.process_steps.filter_code.report_cells.*"  (wrong-import-order)
pybirdai/process_steps/pybird/execute_datapoint.py:16:0: C0411: third party import "django.conf.settings" should be placed before first party import "pybirdai.process_steps.filter_code.report_cells.*"  (wrong-import-order)
pybirdai/process_steps/pybird/execute_datapoint.py:13:0: W0611: Unused import importlib (unused-import)
************* Module pybirdai.process_steps.pybird.create_executable_filters
pybirdai/process_steps/pybird/create_executable_filters.py:116:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/create_executable_filters.py:132:62: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/create_executable_filters.py:134:88: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/create_executable_filters.py:136:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/create_executable_filters.py:217:0: C0325: Unnecessary parens after 'not' keyword (superfluous-parens)
pybirdai/process_steps/pybird/create_executable_filters.py:218:0: C0325: Unnecessary parens after 'not' keyword (superfluous-parens)
pybirdai/process_steps/pybird/create_executable_filters.py:220:0: C0325: Unnecessary parens after 'not' keyword (superfluous-parens)
pybirdai/process_steps/pybird/create_executable_filters.py:35:4: R0914: Too many local variables (18/15) (too-many-locals)
pybirdai/process_steps/pybird/create_executable_filters.py:108:20: W0702: No exception type(s) specified (bare-except)
pybirdai/process_steps/pybird/create_executable_filters.py:35:4: R0915: Too many statements (109/50) (too-many-statements)
pybirdai/process_steps/pybird/create_executable_filters.py:66:8: R1702: Too many nested blocks (7/5) (too-many-nested-blocks)
pybirdai/process_steps/pybird/create_executable_filters.py:45:15: R1732: Consider using 'with' for resource-allocating operations (consider-using-with)
pybirdai/process_steps/pybird/create_executable_filters.py:46:27: R1732: Consider using 'with' for resource-allocating operations (consider-using-with)
pybirdai/process_steps/pybird/create_executable_filters.py:70:31: R1732: Consider using 'with' for resource-allocating operations (consider-using-with)
pybirdai/process_steps/pybird/create_executable_filters.py:177:4: R0914: Too many local variables (21/15) (too-many-locals)
pybirdai/process_steps/pybird/create_executable_filters.py:232:8: W0702: No exception type(s) specified (bare-except)
pybirdai/process_steps/pybird/create_executable_filters.py:229:4: R1710: Either all return statements in a function should return an expression, or none of them should. (inconsistent-return-statements)
pybirdai/process_steps/pybird/create_executable_filters.py:276:8: R1702: Too many nested blocks (6/5) (too-many-nested-blocks)
pybirdai/process_steps/pybird/create_executable_filters.py:14:0: C0411: third party import "django.conf.settings" should be placed before first party import "pybirdai.utils.utils.Utils"  (wrong-import-order)
pybirdai/process_steps/pybird/create_executable_filters.py:17:0: C0411: standard import "datetime.datetime" should be placed before third party import "django.conf.settings" and first party imports "pybirdai.utils.utils.Utils", "pybirdai.process_steps.pybird.orchestration.Orchestration", "pybirdai.models.Trail"  (wrong-import-order)
pybirdai/process_steps/pybird/create_executable_filters.py:19:0: C0411: standard import "os" should be placed before third party import "django.conf.settings" and first party imports "pybirdai.utils.utils.Utils", "pybirdai.process_steps.pybird.orchestration.Orchestration", "pybirdai.models.Trail"  (wrong-import-order)
pybirdai/process_steps/pybird/create_executable_filters.py:15:0: C0412: Imports from package pybirdai are not grouped (ungrouped-imports)
pybirdai/process_steps/pybird/create_executable_filters.py:13:0: W0611: Unused Utils imported from pybirdai.utils.utils (unused-import)
pybirdai/process_steps/pybird/create_executable_filters.py:16:0: W0611: Unused Trail imported from pybirdai.models (unused-import)
pybirdai/process_steps/pybird/create_executable_filters.py:16:0: W0611: Unused MetaDataTrail imported from pybirdai.models (unused-import)
pybirdai/process_steps/pybird/create_executable_filters.py:16:0: W0611: Unused DerivedTable imported from pybirdai.models (unused-import)
pybirdai/process_steps/pybird/create_executable_filters.py:16:0: W0611: Unused FunctionText imported from pybirdai.models (unused-import)
pybirdai/process_steps/pybird/create_executable_filters.py:16:0: W0611: Unused TableCreationFunction imported from pybirdai.models (unused-import)
************* Module pybirdai.process_steps.pybird.orchestration_original
pybirdai/process_steps/pybird/orchestration_original.py:19:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/orchestration_original.py:20:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration_original.py:21:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/orchestration_original.py:23:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration_original.py:24:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration_original.py:25:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration_original.py:27:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration_original.py:28:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration_original.py:29:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration_original.py:31:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration_original.py:32:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration_original.py:34:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration_original.py:36:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/orchestration_original.py:37:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration_original.py:41:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration_original.py:43:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration_original.py:44:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration_original.py:46:0: W0311: Bad indentation. Found 4 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/pybird/orchestration_original.py:47:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration_original.py:48:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration_original.py:49:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration_original.py:50:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration_original.py:51:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration_original.py:52:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration_original.py:53:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration_original.py:55:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration_original.py:56:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration_original.py:57:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration_original.py:58:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration_original.py:59:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration_original.py:60:0: W0311: Bad indentation. Found 7 spaces, expected 28 (bad-indentation)
pybirdai/process_steps/pybird/orchestration_original.py:61:58: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration_original.py:61:0: W0311: Bad indentation. Found 7 spaces, expected 28 (bad-indentation)
pybirdai/process_steps/pybird/orchestration_original.py:61:0: W0301: Unnecessary semicolon (unnecessary-semicolon)
pybirdai/process_steps/pybird/orchestration_original.py:62:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration_original.py:63:0: W0311: Bad indentation. Found 5 spaces, expected 20 (bad-indentation)
pybirdai/process_steps/pybird/orchestration_original.py:64:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration_original.py:64:0: W0301: Unnecessary semicolon (unnecessary-semicolon)
pybirdai/process_steps/pybird/orchestration_original.py:65:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration_original.py:66:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration_original.py:68:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration_original.py:69:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration_original.py:70:0: W0311: Bad indentation. Found 7 spaces, expected 28 (bad-indentation)
pybirdai/process_steps/pybird/orchestration_original.py:71:0: W0311: Bad indentation. Found 8 spaces, expected 32 (bad-indentation)
pybirdai/process_steps/pybird/orchestration_original.py:72:0: W0311: Bad indentation. Found 9 spaces, expected 36 (bad-indentation)
pybirdai/process_steps/pybird/orchestration_original.py:73:0: W0311: Bad indentation. Found 8 spaces, expected 32 (bad-indentation)
pybirdai/process_steps/pybird/orchestration_original.py:74:0: W0311: Bad indentation. Found 9 spaces, expected 36 (bad-indentation)
pybirdai/process_steps/pybird/orchestration_original.py:76:0: W0311: Bad indentation. Found 6 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/pybird/orchestration_original.py:78:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/orchestration_original.py:79:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/orchestration_original.py:80:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration_original.py:84:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration_original.py:85:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration_original.py:86:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration_original.py:87:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/orchestration_original.py:88:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/orchestration_original.py:89:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration_original.py:98:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration_original.py:100:0: W0311: Bad indentation. Found 1 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/pybird/orchestration_original.py:101:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration_original.py:102:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration_original.py:103:21: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration_original.py:103:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration_original.py:104:21: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration_original.py:104:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration_original.py:104:0: W0301: Unnecessary semicolon (unnecessary-semicolon)
pybirdai/process_steps/pybird/orchestration_original.py:105:0: W0311: Bad indentation. Found 2 spaces, expected 8 (bad-indentation)
pybirdai/process_steps/pybird/orchestration_original.py:106:0: W0311: Bad indentation. Found 3 spaces, expected 12 (bad-indentation)
pybirdai/process_steps/pybird/orchestration_original.py:107:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration_original.py:108:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/pybird/orchestration_original.py:113:0: C0305: Trailing newlines (trailing-newlines)
pybirdai/process_steps/pybird/orchestration_original.py:47:5: C0415: Import outside toplevel (django.apps.apps) (import-outside-toplevel)
pybirdai/process_steps/pybird/orchestration_original.py:73:8: W0702: No exception type(s) specified (bare-except)
pybirdai/process_steps/pybird/orchestration_original.py:43:2: R1702: Too many nested blocks (7/5) (too-many-nested-blocks)
pybirdai/process_steps/pybird/orchestration_original.py:100:1: E0213: Method 'createObjectFromReferenceType' should have "self" as first argument (no-self-argument)
pybirdai/process_steps/pybird/orchestration_original.py:105:2: W0702: No exception type(s) specified (bare-except)
pybirdai/process_steps/pybird/orchestration_original.py:100:1: R1710: Either all return statements in a function should return an expression, or none of them should. (inconsistent-return-statements)
pybirdai/process_steps/pybird/orchestration_original.py:16:0: C0411: standard import "importlib" should be placed before first party import "pybirdai.process_steps.pybird.csv_converter.CSVConverter"  (wrong-import-order)
************* Module pybirdai.process_steps.report_filters.create_non_reference_output_layers
pybirdai/process_steps/report_filters/create_non_reference_output_layers.py:14:0: W0401: Wildcard import pybirdai.models.bird_meta_data_model (wildcard-import)
pybirdai/process_steps/report_filters/create_non_reference_output_layers.py:22:0: R0902: Too many instance attributes (13/7) (too-many-instance-attributes)
pybirdai/process_steps/report_filters/create_non_reference_output_layers.py:185:8: W0612: Unused variable 'framework_suffix' (unused-variable)
pybirdai/process_steps/report_filters/create_non_reference_output_layers.py:211:4: R0914: Too many local variables (17/15) (too-many-locals)
pybirdai/process_steps/report_filters/create_non_reference_output_layers.py:320:34: W1309: Using an f-string that does not have any interpolated variables (f-string-without-interpolation)
pybirdai/process_steps/report_filters/create_non_reference_output_layers.py:353:32: W1309: Using an f-string that does not have any interpolated variables (f-string-without-interpolation)
pybirdai/process_steps/report_filters/create_non_reference_output_layers.py:482:15: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/process_steps/report_filters/create_non_reference_output_layers.py:471:23: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/process_steps/report_filters/create_non_reference_output_layers.py:544:15: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/process_steps/report_filters/create_non_reference_output_layers.py:533:23: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/process_steps/report_filters/create_non_reference_output_layers.py:598:15: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/process_steps/report_filters/create_non_reference_output_layers.py:670:15: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/process_steps/report_filters/create_non_reference_output_layers.py:659:23: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/process_steps/report_filters/create_non_reference_output_layers.py:644:37: W1309: Using an f-string that does not have any interpolated variables (f-string-without-interpolation)
pybirdai/process_steps/report_filters/create_non_reference_output_layers.py:142:8: W0201: Attribute 'save_to_db' defined outside __init__ (attribute-defined-outside-init)
pybirdai/process_steps/report_filters/create_non_reference_output_layers.py:15:0: C0411: third party import "django.db.models.Q" should be placed before first party import "pybirdai.models.bird_meta_data_model.*"  (wrong-import-order)
pybirdai/process_steps/report_filters/create_non_reference_output_layers.py:16:0: C0411: standard import "re" should be placed before third party import "django.db.models.Q" and first party import "pybirdai.models.bird_meta_data_model.*"  (wrong-import-order)
pybirdai/process_steps/report_filters/create_non_reference_output_layers.py:17:0: C0411: standard import "os" should be placed before third party import "django.db.models.Q" and first party import "pybirdai.models.bird_meta_data_model.*"  (wrong-import-order)
pybirdai/process_steps/report_filters/create_non_reference_output_layers.py:18:0: C0411: standard import "csv" should be placed before third party import "django.db.models.Q" and first party import "pybirdai.models.bird_meta_data_model.*"  (wrong-import-order)
pybirdai/process_steps/report_filters/create_non_reference_output_layers.py:19:0: C0411: standard import "datetime.datetime" should be placed before third party import "django.db.models.Q" and first party import "pybirdai.models.bird_meta_data_model.*"  (wrong-import-order)
pybirdai/process_steps/report_filters/create_non_reference_output_layers.py:20:0: C0411: standard import "uuid" should be placed before third party import "django.db.models.Q" and first party import "pybirdai.models.bird_meta_data_model.*"  (wrong-import-order)
pybirdai/process_steps/report_filters/create_non_reference_output_layers.py:16:0: W0611: Unused import re (unused-import)
pybirdai/process_steps/report_filters/create_non_reference_output_layers.py:17:0: W0611: Unused import os (unused-import)
pybirdai/process_steps/report_filters/create_non_reference_output_layers.py:18:0: W0611: Unused import csv (unused-import)
pybirdai/process_steps/report_filters/create_non_reference_output_layers.py:20:0: W0611: Unused import uuid (unused-import)
pybirdai/process_steps/report_filters/create_non_reference_output_layers.py:14:0: W0614: Unused import(s) DOMAIN, FACET_COLLECTION, MAINTENANCE_AGENCY, MEMBER, MEMBER_HIERARCHY, MEMBER_HIERARCHY_NODE, VARIABLE, VARIABLE_SET, VARIABLE_SET_ENUMERATION, MEMBER_MAPPING, MEMBER_MAPPING_ITEM, VARIABLE_MAPPING_ITEM, VARIABLE_MAPPING, MAPPING_TO_CUBE, MAPPING_DEFINITION, AXIS, AXIS_ORDINATE, CUBE_LINK, CUBE_STRUCTURE_ITEM_LINK, MEMBER_LINK, models, OperationalError and timezone from wildcard import of pybirdai.models.bird_meta_data_model (unused-wildcard-import)
************* Module pybirdai.process_steps.report_filters.create_output_layers
pybirdai/process_steps/report_filters/create_output_layers.py:14:0: W0401: Wildcard import pybirdai.models.bird_meta_data_model (wildcard-import)
pybirdai/process_steps/report_filters/create_output_layers.py:16:0: C0411: standard import "os" should be placed before first party import "pybirdai.models.bird_meta_data_model.*"  (wrong-import-order)
pybirdai/process_steps/report_filters/create_output_layers.py:17:0: C0411: standard import "csv" should be placed before first party import "pybirdai.models.bird_meta_data_model.*"  (wrong-import-order)
pybirdai/process_steps/report_filters/create_output_layers.py:14:0: W0614: Unused import(s) SUBDOMAIN, SUBDOMAIN_ENUMERATION, DOMAIN, FACET_COLLECTION, MAINTENANCE_AGENCY, MEMBER, MEMBER_HIERARCHY, MEMBER_HIERARCHY_NODE, VARIABLE, VARIABLE_SET, VARIABLE_SET_ENUMERATION, FRAMEWORK, MEMBER_MAPPING, MEMBER_MAPPING_ITEM, VARIABLE_MAPPING_ITEM, VARIABLE_MAPPING, MAPPING_TO_CUBE, MAPPING_DEFINITION, AXIS, AXIS_ORDINATE, CELL_POSITION, ORDINATE_ITEM, TABLE, TABLE_CELL, CUBE_STRUCTURE_ITEM, CUBE_LINK, CUBE_STRUCTURE_ITEM_LINK, COMBINATION, COMBINATION_ITEM, CUBE_TO_COMBINATION, MEMBER_LINK, models, OperationalError and timezone from wildcard import of pybirdai.models.bird_meta_data_model (unused-wildcard-import)
************* Module pybirdai.process_steps.report_filters.create_report_filters
pybirdai/process_steps/report_filters/create_report_filters.py:49:0: W0311: Bad indentation. Found 15 spaces, expected 16 (bad-indentation)
pybirdai/process_steps/report_filters/create_report_filters.py:130:0: C0325: Unnecessary parens after 'not' keyword (superfluous-parens)
pybirdai/process_steps/report_filters/create_report_filters.py:153:0: C0325: Unnecessary parens after 'not' keyword (superfluous-parens)
pybirdai/process_steps/report_filters/create_report_filters.py:279:0: C0325: Unnecessary parens after 'if' keyword (superfluous-parens)
pybirdai/process_steps/report_filters/create_report_filters.py:14:0: W0401: Wildcard import pybirdai.models.bird_meta_data_model (wildcard-import)
pybirdai/process_steps/report_filters/create_report_filters.py:31:8: W0612: Unused variable 'in_scope_reports' (unused-variable)
pybirdai/process_steps/report_filters/create_report_filters.py:53:4: E0213: Method 'read_in_scope_reports' should have "self" as first argument (no-self-argument)
pybirdai/process_steps/report_filters/create_report_filters.py:66:4: E0213: Method 'create_cell_to_variable_member_map' should have "self" as first argument (no-self-argument)
pybirdai/process_steps/report_filters/create_report_filters.py:127:12: W0107: Unnecessary pass statement (unnecessary-pass)
pybirdai/process_steps/report_filters/create_report_filters.py:189:4: E0213: Method 'get_metric' should have "self" as first argument (no-self-argument)
pybirdai/process_steps/report_filters/create_report_filters.py:212:36: R1714: Consider merging these comparisons with 'in' by using 'item.is_source in ('false', 'False')'. Use a set instead if elements are hashable. (consider-using-in)
pybirdai/process_steps/report_filters/create_report_filters.py:239:16: W0612: Unused variable 'var_string' (unused-variable)
pybirdai/process_steps/report_filters/create_report_filters.py:244:16: W0612: Unused variable 'member_string' (unused-variable)
pybirdai/process_steps/report_filters/create_report_filters.py:252:4: E0213: Method 'get_reference_tuple_list' should have "self" as first argument (no-self-argument)
pybirdai/process_steps/report_filters/create_report_filters.py:290:4: E0213: Method 'create_member_mapping_item_row_dict' should have "self" as first argument (no-self-argument)
pybirdai/process_steps/report_filters/create_report_filters.py:309:4: E0213: Method 'create_variable_mapping_row_dict' should have "self" as first argument (no-self-argument)
pybirdai/process_steps/report_filters/create_report_filters.py:328:4: E0213: Method 'get_report_cube_mapping_id_for_table_id' should have "self" as first argument (no-self-argument)
pybirdai/process_steps/report_filters/create_report_filters.py:341:4: E0213: Method 'get_rol_cube_for_table_id' should have "self" as first argument (no-self-argument)
pybirdai/process_steps/report_filters/create_report_filters.py:355:18: E1136: Value 'table_id' is unsubscriptable (unsubscriptable-object)
pybirdai/process_steps/report_filters/create_report_filters.py:36:8: W0201: Attribute 'combinations_to_create' defined outside __init__ (attribute-defined-outside-init)
pybirdai/process_steps/report_filters/create_report_filters.py:37:8: W0201: Attribute 'combination_items_to_create' defined outside __init__ (attribute-defined-outside-init)
pybirdai/process_steps/report_filters/create_report_filters.py:38:8: W0201: Attribute 'cube_structure_items_to_create' defined outside __init__ (attribute-defined-outside-init)
pybirdai/process_steps/report_filters/create_report_filters.py:39:8: W0201: Attribute 'cube_to_combinations_to_create' defined outside __init__ (attribute-defined-outside-init)
pybirdai/process_steps/report_filters/create_report_filters.py:15:0: C0411: standard import "os" should be placed before first party imports "pybirdai.utils.utils.Utils", "pybirdai.models.bird_meta_data_model.*"  (wrong-import-order)
pybirdai/process_steps/report_filters/create_report_filters.py:16:0: C0411: standard import "csv" should be placed before first party imports "pybirdai.utils.utils.Utils", "pybirdai.models.bird_meta_data_model.*"  (wrong-import-order)
pybirdai/process_steps/report_filters/create_report_filters.py:17:0: C0411: standard import "uuid.uuid4" should be placed before first party imports "pybirdai.utils.utils.Utils", "pybirdai.models.bird_meta_data_model.*"  (wrong-import-order)
pybirdai/process_steps/report_filters/create_report_filters.py:17:0: W0611: Unused uuid4 imported from uuid (unused-import)
pybirdai/process_steps/report_filters/create_report_filters.py:14:0: W0614: Unused import(s) SUBDOMAIN, SUBDOMAIN_ENUMERATION, DOMAIN, FACET_COLLECTION, MAINTENANCE_AGENCY, MEMBER, MEMBER_HIERARCHY, MEMBER_HIERARCHY_NODE, VARIABLE, VARIABLE_SET, VARIABLE_SET_ENUMERATION, FRAMEWORK, MEMBER_MAPPING, MEMBER_MAPPING_ITEM, VARIABLE_MAPPING_ITEM, VARIABLE_MAPPING, MAPPING_TO_CUBE, MAPPING_DEFINITION, AXIS, AXIS_ORDINATE, CELL_POSITION, ORDINATE_ITEM, TABLE, TABLE_CELL, CUBE_STRUCTURE, CUBE, CUBE_LINK, CUBE_STRUCTURE_ITEM_LINK, MEMBER_LINK, models, OperationalError and timezone from wildcard import of pybirdai.models.bird_meta_data_model (unused-wildcard-import)
************* Module pybirdai.process_steps.ancrdt_transformation.csv_column_index_context_ancrdt
pybirdai/process_steps/ancrdt_transformation/csv_column_index_context_ancrdt.py:13:0: R0205: Class 'ColumnIndexes' inherits from object, can be safely removed from bases in python3 (useless-object-inheritance)
************* Module pybirdai.process_steps.ancrdt_transformation.create_joins_meta_data_ancrdt
pybirdai/process_steps/ancrdt_transformation/create_joins_meta_data_ancrdt.py:30:0: C0413: Import "import logging" should be placed at the top of the module (wrong-import-position)
pybirdai/process_steps/ancrdt_transformation/create_joins_meta_data_ancrdt.py:66:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/process_steps/ancrdt_transformation/create_joins_meta_data_ancrdt.py:105:24: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/process_steps/ancrdt_transformation/create_joins_meta_data_ancrdt.py:107:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/process_steps/ancrdt_transformation/create_joins_meta_data_ancrdt.py:109:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/process_steps/ancrdt_transformation/create_joins_meta_data_ancrdt.py:112:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/process_steps/ancrdt_transformation/create_joins_meta_data_ancrdt.py:115:4: R0914: Too many local variables (36/15) (too-many-locals)
pybirdai/process_steps/ancrdt_transformation/create_joins_meta_data_ancrdt.py:120:8: C0415: Import outside toplevel (pybirdai.models.bird_meta_data_model.CUBE, pybirdai.models.bird_meta_data_model.CUBE_STRUCTURE_ITEM, pybirdai.models.bird_meta_data_model.CUBE_STRUCTURE, pybirdai.models.bird_meta_data_model.DOMAIN, pybirdai.models.bird_meta_data_model.VARIABLE, pybirdai.models.bird_meta_data_model.CUBE_LINK, pybirdai.models.bird_meta_data_model.CUBE_STRUCTURE_ITEM_LINK, pybirdai.models.bird_meta_data_model.MAINTENANCE_AGENCY, pybirdai.models.bird_meta_data_model.MEMBER_LINK) (import-outside-toplevel)
pybirdai/process_steps/ancrdt_transformation/create_joins_meta_data_ancrdt.py:136:16: W0702: No exception type(s) specified (bare-except)
pybirdai/process_steps/ancrdt_transformation/create_joins_meta_data_ancrdt.py:137:20: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/process_steps/ancrdt_transformation/create_joins_meta_data_ancrdt.py:155:27: W0612: Unused variable 'exists' (unused-variable)
pybirdai/process_steps/ancrdt_transformation/create_joins_meta_data_ancrdt.py:185:24: W0612: Unused variable 'member_link' (unused-variable)
pybirdai/process_steps/ancrdt_transformation/create_joins_meta_data_ancrdt.py:120:8: W0611: Unused VARIABLE imported from pybirdai.models.bird_meta_data_model (unused-import)
pybirdai/process_steps/ancrdt_transformation/create_joins_meta_data_ancrdt.py:200:4: R0914: Too many local variables (20/15) (too-many-locals)
pybirdai/process_steps/ancrdt_transformation/create_joins_meta_data_ancrdt.py:201:8: C0415: Import outside toplevel (pybirdai.models.bird_meta_data_model.CUBE_LINK, pybirdai.models.bird_meta_data_model.CUBE_STRUCTURE_ITEM_LINK, pybirdai.models.bird_meta_data_model.MAINTENANCE_AGENCY, pybirdai.models.bird_meta_data_model.MEMBER) (import-outside-toplevel)
pybirdai/process_steps/ancrdt_transformation/create_joins_meta_data_ancrdt.py:203:28: R1735: Consider using '{}' instead of a call to 'dict'. (use-dict-literal)
pybirdai/process_steps/ancrdt_transformation/create_joins_meta_data_ancrdt.py:201:8: W0611: Unused CUBE_LINK imported from pybirdai.models.bird_meta_data_model (unused-import)
pybirdai/process_steps/ancrdt_transformation/create_joins_meta_data_ancrdt.py:201:8: W0611: Unused CUBE_STRUCTURE_ITEM_LINK imported from pybirdai.models.bird_meta_data_model (unused-import)
pybirdai/process_steps/ancrdt_transformation/create_joins_meta_data_ancrdt.py:201:8: W0611: Unused MAINTENANCE_AGENCY imported from pybirdai.models.bird_meta_data_model (unused-import)
pybirdai/process_steps/ancrdt_transformation/create_joins_meta_data_ancrdt.py:201:8: W0611: Unused MEMBER imported from pybirdai.models.bird_meta_data_model (unused-import)
pybirdai/process_steps/ancrdt_transformation/create_joins_meta_data_ancrdt.py:222:8: C0415: Import outside toplevel (pybirdai.models.bird_meta_data_model.MEMBER) (import-outside-toplevel)
pybirdai/process_steps/ancrdt_transformation/create_joins_meta_data_ancrdt.py:232:8: C0415: Import outside toplevel (pybirdai.models.bird_meta_data_model.CUBE, pybirdai.models.bird_meta_data_model.CUBE_STRUCTURE_ITEM, pybirdai.models.bird_meta_data_model.CUBE_STRUCTURE) (import-outside-toplevel)
pybirdai/process_steps/ancrdt_transformation/create_joins_meta_data_ancrdt.py:232:8: W0611: Unused CUBE imported from pybirdai.models.bird_meta_data_model (unused-import)
pybirdai/process_steps/ancrdt_transformation/create_joins_meta_data_ancrdt.py:251:4: W0612: Unused variable 'result' (unused-variable)
pybirdai/process_steps/ancrdt_transformation/create_joins_meta_data_ancrdt.py:30:0: C0411: standard import "logging" should be placed before third party imports "django", "django.db.models.Q" (wrong-import-order)
pybirdai/process_steps/ancrdt_transformation/create_joins_meta_data_ancrdt.py:20:0: W0611: Unused Q imported from django.db.models (unused-import)
************* Module pybirdai.process_steps.ancrdt_transformation.create_python_django_transformations_ancrdt
pybirdai/process_steps/ancrdt_transformation/create_python_django_transformations_ancrdt.py:15:0: W0401: Wildcard import pybirdai.models.bird_meta_data_model (wildcard-import)
pybirdai/process_steps/ancrdt_transformation/create_python_django_transformations_ancrdt.py:34:4: E0213: Method 'create_output_classes' should have "self" as first argument (no-self-argument)
pybirdai/process_steps/ancrdt_transformation/create_python_django_transformations_ancrdt.py:37:15: R1732: Consider using 'with' for resource-allocating operations (consider-using-with)
pybirdai/process_steps/ancrdt_transformation/create_python_django_transformations_ancrdt.py:95:4: E0213: Method 'create_slice_classes' should have "self" as first argument (no-self-argument)
pybirdai/process_steps/ancrdt_transformation/create_python_django_transformations_ancrdt.py:95:4: R0914: Too many local variables (30/15) (too-many-locals)
pybirdai/process_steps/ancrdt_transformation/create_python_django_transformations_ancrdt.py:102:8: R1702: Too many nested blocks (6/5) (too-many-nested-blocks)
pybirdai/process_steps/ancrdt_transformation/create_python_django_transformations_ancrdt.py:102:8: R1702: Too many nested blocks (6/5) (too-many-nested-blocks)
pybirdai/process_steps/ancrdt_transformation/create_python_django_transformations_ancrdt.py:339:39: R1735: Consider using '{}' instead of a call to 'dict'. (use-dict-literal)
pybirdai/process_steps/ancrdt_transformation/create_python_django_transformations_ancrdt.py:95:4: R0912: Too many branches (61/12) (too-many-branches)
pybirdai/process_steps/ancrdt_transformation/create_python_django_transformations_ancrdt.py:95:4: R0915: Too many statements (190/50) (too-many-statements)
pybirdai/process_steps/ancrdt_transformation/create_python_django_transformations_ancrdt.py:102:8: R1702: Too many nested blocks (6/5) (too-many-nested-blocks)
pybirdai/process_steps/ancrdt_transformation/create_python_django_transformations_ancrdt.py:103:19: R1732: Consider using 'with' for resource-allocating operations (consider-using-with)
pybirdai/process_steps/ancrdt_transformation/create_python_django_transformations_ancrdt.py:367:4: E0213: Method 'delete_generated_python_join_files' should have "self" as first argument (no-self-argument)
pybirdai/process_steps/ancrdt_transformation/create_python_django_transformations_ancrdt.py:16:0: C0411: standard import "os" should be placed before first party imports "pybirdai.utils.utils.Utils", "pybirdai.models.bird_meta_data_model.*"  (wrong-import-order)
pybirdai/process_steps/ancrdt_transformation/create_python_django_transformations_ancrdt.py:17:0: C0411: third party import "django.conf.settings" should be placed before first party imports "pybirdai.utils.utils.Utils", "pybirdai.models.bird_meta_data_model.*"  (wrong-import-order)
pybirdai/process_steps/ancrdt_transformation/create_python_django_transformations_ancrdt.py:18:0: C0411: standard import "ast" should be placed before third party import "django.conf.settings" and first party imports "pybirdai.utils.utils.Utils", "pybirdai.models.bird_meta_data_model.*"  (wrong-import-order)
pybirdai/process_steps/ancrdt_transformation/create_python_django_transformations_ancrdt.py:19:0: C0412: Imports from package pybirdai are not grouped (ungrouped-imports)
pybirdai/process_steps/ancrdt_transformation/create_python_django_transformations_ancrdt.py:14:0: W0611: Unused Utils imported from pybirdai.utils.utils (unused-import)
pybirdai/process_steps/ancrdt_transformation/create_python_django_transformations_ancrdt.py:18:0: W0611: Unused import ast (unused-import)
pybirdai/process_steps/ancrdt_transformation/create_python_django_transformations_ancrdt.py:15:0: W0614: Unused import(s) SUBDOMAIN, SUBDOMAIN_ENUMERATION, DOMAIN, FACET_COLLECTION, MAINTENANCE_AGENCY, MEMBER, MEMBER_HIERARCHY, MEMBER_HIERARCHY_NODE, VARIABLE, VARIABLE_SET, VARIABLE_SET_ENUMERATION, FRAMEWORK, MEMBER_MAPPING, MEMBER_MAPPING_ITEM, VARIABLE_MAPPING_ITEM, VARIABLE_MAPPING, MAPPING_TO_CUBE, MAPPING_DEFINITION, AXIS, AXIS_ORDINATE, CELL_POSITION, ORDINATE_ITEM, TABLE, TABLE_CELL, CUBE_STRUCTURE, CUBE_STRUCTURE_ITEM, CUBE, CUBE_LINK, COMBINATION, COMBINATION_ITEM, CUBE_TO_COMBINATION, MEMBER_LINK, models, OperationalError and timezone from wildcard import of pybirdai.models.bird_meta_data_model (unused-wildcard-import)
************* Module pybirdai.process_steps.ancrdt_transformation.import_website_to_sdd_model_django_ancrdt
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:176:0: C0325: Unnecessary parens after 'not' keyword (superfluous-parens)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:233:0: C0325: Unnecessary parens after 'not' keyword (superfluous-parens)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:257:0: C0325: Unnecessary parens after 'not' keyword (superfluous-parens)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:294:0: C0325: Unnecessary parens after 'not' keyword (superfluous-parens)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:361:0: C0325: Unnecessary parens after 'not' keyword (superfluous-parens)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:383:0: C0325: Unnecessary parens after 'not' keyword (superfluous-parens)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:497:0: C0325: Unnecessary parens after 'not' keyword (superfluous-parens)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:1:0: C0302: Too many lines in module (1371/1000) (too-many-lines)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:16:0: W0401: Wildcard import pybirdai.models.bird_meta_data_model (wildcard-import)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:22:0: R0205: Class 'ImportWebsiteToSDDModel' inherits from object, can be safely removed from bases in python3 (useless-object-inheritance)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:26:4: R0912: Too many branches (14/12) (too-many-branches)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:103:20: W0622: Redefining built-in 'id' (redefined-builtin)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:104:20: W0612: Unused variable 'name' (unused-variable)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:135:20: W0622: Redefining built-in 'id' (redefined-builtin)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:136:20: W0612: Unused variable 'name' (unused-variable)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:150:4: R0914: Too many local variables (20/15) (too-many-locals)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:191:51: R1719: The if expression can be replaced with 'bool(test)' (simplifiable-if-expression)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:192:50: R1719: The if expression can be replaced with 'bool(test)' (simplifiable-if-expression)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:200:24: W0707: Consider explicitly re-raising using 'except Exception as exc' and 'raise ValueError(f'{row} :: {traceback.format_exc()}') from exc' (raise-missing-from)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:166:20: W0612: Unused variable 'data_type' (unused-variable)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:206:4: R0914: Too many local variables (19/15) (too-many-locals)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:262:24: W0707: Consider explicitly re-raising using 'except Exception as exc' and 'raise ValueError(f'{row} :: {traceback.format_exc()}') from exc' (raise-missing-from)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:268:4: R0914: Too many local variables (21/15) (too-many-locals)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:312:31: R1714: Consider merging these comparisons with 'in' by using 'primary_concept in ('', None)'. Use a set instead if elements are hashable. (consider-using-in)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:312:59: C0121: Comparison 'primary_concept == None' should be 'primary_concept is None' (singleton-comparison)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:318:4: R0914: Too many local variables (16/15) (too-many-locals)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:318:4: R0912: Too many branches (13/12) (too-many-branches)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:404:16: W0622: Redefining built-in 'id' (redefined-builtin)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:434:4: E0213: Method 'save_missing_domains_to_csv' should have "self" as first argument (no-self-argument)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:436:13: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:440:4: E0213: Method 'save_missing_members_to_csv' should have "self" as first argument (no-self-argument)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:442:13: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:446:4: E0213: Method 'save_missing_variables_to_csv' should have "self" as first argument (no-self-argument)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:448:13: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:452:4: E0213: Method 'save_missing_children_to_csv' should have "self" as first argument (no-self-argument)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:454:13: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:461:4: R0914: Too many local variables (22/15) (too-many-locals)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:483:20: W0612: Unused variable 'valid_from' (unused-variable)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:484:20: W0612: Unused variable 'valid_to' (unused-variable)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:514:4: E0213: Method 'save_missing_hierarchies_to_csv' should have "self" as first argument (no-self-argument)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:516:13: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:525:4: R0914: Too many local variables (18/15) (too-many-locals)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:545:20: W0612: Unused variable 'valid_from' (unused-variable)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:546:20: W0612: Unused variable 'valid_to' (unused-variable)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:564:4: R0914: Too many local variables (16/15) (too-many-locals)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:580:20: W0612: Unused variable 'axis_order' (unused-variable)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:581:20: W0612: Unused variable 'axis_name' (unused-variable)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:584:20: W0612: Unused variable 'axis_is_open_axis' (unused-variable)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:599:4: R0914: Too many local variables (18/15) (too-many-locals)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:614:20: W0612: Unused variable 'axis_ordinate_is_abstract_header' (unused-variable)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:616:20: W0612: Unused variable 'axis_ordinate_order' (unused-variable)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:619:20: W0612: Unused variable 'axis_ordinate_parent_axis_ordinate_id' (unused-variable)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:638:4: R0914: Too many local variables (16/15) (too-many-locals)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:782:4: R0914: Too many local variables (21/15) (too-many-locals)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:782:4: R0912: Too many branches (15/12) (too-many-branches)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:844:4: E0213: Method 'save_missing_mapping_variables_to_csv' should have "self" as first argument (no-self-argument)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:846:13: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:852:4: E0213: Method 'save_missing_mapping_members_to_csv' should have "self" as first argument (no-self-argument)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:854:13: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:862:4: E0213: Method 'create_mappings_warnings_summary' should have "self" as first argument (no-self-argument)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:869:13: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:875:13: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:1065:15: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:1074:15: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:1090:15: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:1105:15: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:1114:15: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:1123:15: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:1132:15: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:1184:4: R0914: Too many local variables (30/15) (too-many-locals)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:1226:22: R1735: Consider using '{}' instead of a call to 'dict'. (use-dict-literal)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:1263:45: C0321: More than one statement on a single line (multiple-statements)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:1184:4: R0912: Too many branches (17/12) (too-many-branches)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:1184:4: R0915: Too many statements (86/50) (too-many-statements)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:1282:20: W0612: Unused variable 'cube_type' (unused-variable)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:1304:4: R0914: Too many local variables (20/15) (too-many-locals)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:1320:16: W0612: Unused variable 'code' (unused-variable)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:30:8: W0201: Attribute 'base_path' defined outside __init__ (attribute-defined-outside-init)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:22:0: R0904: Too many public methods (49/20) (too-many-public-methods)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:18:0: C0411: standard import "pathlib.Path" should be placed before third party import "django.conf.settings" and first party imports "pybirdai.models.bird_meta_data_model.*", "pybirdai.process_steps.ancrdt_transformation.csv_column_index_context_ancrdt.ColumnIndexes"  (wrong-import-order)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:19:0: C0411: standard import "traceback" should be placed before third party import "django.conf.settings" and first party imports "pybirdai.models.bird_meta_data_model.*", "pybirdai.process_steps.ancrdt_transformation.csv_column_index_context_ancrdt.ColumnIndexes"  (wrong-import-order)
pybirdai/process_steps/ancrdt_transformation/import_website_to_sdd_model_django_ancrdt.py:16:0: W0614: Unused import(s) FACET_COLLECTION, VARIABLE_SET, VARIABLE_SET_ENUMERATION, CUBE_LINK, CUBE_STRUCTURE_ITEM_LINK, COMBINATION, COMBINATION_ITEM, CUBE_TO_COMBINATION, MEMBER_LINK, models, OperationalError and timezone from wildcard import of pybirdai.models.bird_meta_data_model (unused-wildcard-import)
************* Module pybirdai.process_steps.ancrdt_transformation.ancrdt_importer
pybirdai/process_steps/ancrdt_transformation/ancrdt_importer.py:46:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/process_steps/ancrdt_transformation/ancrdt_importer.py:62:8: C0415: Import outside toplevel (pybirdai.process_steps.ancrdt_transformation.context_ancrdt.Context) (import-outside-toplevel)
pybirdai/process_steps/ancrdt_transformation/ancrdt_importer.py:63:8: C0415: Import outside toplevel (pybirdai.process_steps.ancrdt_transformation.sdd_context_django_ancrdt.SDDContext) (import-outside-toplevel)
pybirdai/process_steps/ancrdt_transformation/ancrdt_importer.py:64:8: C0415: Import outside toplevel (pybirdai.process_steps.ancrdt_transformation.import_website_to_sdd_model_django_ancrdt.ImportWebsiteToSDDModel) (import-outside-toplevel)
pybirdai/process_steps/ancrdt_transformation/ancrdt_importer.py:68:8: W0612: Unused variable 'path' (unused-variable)
pybirdai/process_steps/ancrdt_transformation/ancrdt_importer.py:15:0: C0411: standard import "os" should be placed before third party import "django" (wrong-import-order)
pybirdai/process_steps/ancrdt_transformation/ancrdt_importer.py:16:0: C0411: standard import "sys" should be placed before third party import "django" (wrong-import-order)
pybirdai/process_steps/ancrdt_transformation/ancrdt_importer.py:19:0: C0411: standard import "logging" should be placed before third party imports "django", "django.apps.AppConfig", "django.conf.settings" (wrong-import-order)
************* Module pybirdai.process_steps.ancrdt_transformation.sdd_context_django_ancrdt
pybirdai/process_steps/ancrdt_transformation/sdd_context_django_ancrdt.py:16:0: R0205: Class 'SDDContext' inherits from object, can be safely removed from bases in python3 (useless-object-inheritance)
************* Module pybirdai.process_steps.ancrdt_transformation.create_executable_joins_ancrdt
pybirdai/process_steps/ancrdt_transformation/create_executable_joins_ancrdt.py:58:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/process_steps/ancrdt_transformation/create_executable_joins_ancrdt.py:68:36: W0621: Redefining name 'logger' from outer scope (line 34) (redefined-outer-name)
pybirdai/process_steps/ancrdt_transformation/create_executable_joins_ancrdt.py:71:8: C0415: Import outside toplevel (pybirdai.process_steps.input_model.import_database_to_sdd_model.ImportDatabaseToSDDModel) (import-outside-toplevel)
pybirdai/process_steps/ancrdt_transformation/create_executable_joins_ancrdt.py:74:8: C0415: Import outside toplevel (pybirdai.context.sdd_context_django.SDDContext) (import-outside-toplevel)
pybirdai/process_steps/ancrdt_transformation/create_executable_joins_ancrdt.py:75:8: C0415: Import outside toplevel (pybirdai.context.context.Context) (import-outside-toplevel)
pybirdai/process_steps/ancrdt_transformation/create_executable_joins_ancrdt.py:76:8: C0415: Import outside toplevel (pybirdai.process_steps.ancrdt_transformation.create_python_django_transformations_ancrdt.CreatePythonTransformations) (import-outside-toplevel)
pybirdai/process_steps/ancrdt_transformation/create_executable_joins_ancrdt.py:22:0: C0411: standard import "logging" should be placed before third party imports "django", "django.apps.AppConfig", "django.conf.settings" (wrong-import-order)
pybirdai/process_steps/ancrdt_transformation/create_executable_joins_ancrdt.py:16:0: W0611: Unused Path imported from pathlib (unused-import)
************* Module pybirdai.process_steps.ancrdt_transformation.filter_buildr
pybirdai/process_steps/ancrdt_transformation/filter_buildr.py:20:0: W0404: Reimport 'models' (imported line 17) (reimported)
pybirdai/process_steps/ancrdt_transformation/filter_buildr.py:58:4: R0914: Too many local variables (16/15) (too-many-locals)
pybirdai/process_steps/ancrdt_transformation/filter_buildr.py:62:8: W0621: Redefining name 'CUBE_LINK' from outer scope (line 138) (redefined-outer-name)
pybirdai/process_steps/ancrdt_transformation/filter_buildr.py:62:8: W0621: Redefining name 'MEMBER_LINK' from outer scope (line 138) (redefined-outer-name)
pybirdai/process_steps/ancrdt_transformation/filter_buildr.py:62:8: W0621: Redefining name 'CUBE_STRUCTURE_ITEM_LINK' from outer scope (line 138) (redefined-outer-name)
pybirdai/process_steps/ancrdt_transformation/filter_buildr.py:62:8: C0415: Import outside toplevel (pybirdai.models.bird_meta_data_model.CUBE_LINK, pybirdai.models.bird_meta_data_model.MEMBER_LINK, pybirdai.models.bird_meta_data_model.CUBE_STRUCTURE_ITEM_LINK) (import-outside-toplevel)
pybirdai/process_steps/ancrdt_transformation/filter_buildr.py:62:8: W0611: Unused CUBE_LINK imported from pybirdai.models.bird_meta_data_model (unused-import)
pybirdai/process_steps/ancrdt_transformation/filter_buildr.py:112:8: W0621: Redefining name 'CUBE_LINK' from outer scope (line 138) (redefined-outer-name)
pybirdai/process_steps/ancrdt_transformation/filter_buildr.py:112:8: W0621: Redefining name 'MEMBER_LINK' from outer scope (line 138) (redefined-outer-name)
pybirdai/process_steps/ancrdt_transformation/filter_buildr.py:112:8: W0621: Redefining name 'CUBE_STRUCTURE_ITEM_LINK' from outer scope (line 138) (redefined-outer-name)
pybirdai/process_steps/ancrdt_transformation/filter_buildr.py:112:8: C0415: Import outside toplevel (pybirdai.models.bird_meta_data_model.CUBE_LINK, pybirdai.models.bird_meta_data_model.MEMBER_LINK, pybirdai.models.bird_meta_data_model.CUBE_STRUCTURE_ITEM_LINK) (import-outside-toplevel)
pybirdai/process_steps/ancrdt_transformation/filter_buildr.py:113:8: W0612: Unused variable 'conditions' (unused-variable)
pybirdai/process_steps/ancrdt_transformation/filter_buildr.py:112:8: W0611: Unused CUBE_LINK imported from pybirdai.models.bird_meta_data_model (unused-import)
pybirdai/process_steps/ancrdt_transformation/filter_buildr.py:17:0: W0611: Unused models imported from django.db (unused-import)
pybirdai/process_steps/ancrdt_transformation/filter_buildr.py:18:0: W0611: Unused Q imported from django.db.models (unused-import)
pybirdai/process_steps/ancrdt_transformation/filter_buildr.py:138:4: W0611: Unused CUBE_LINK imported from pybirdai.models.bird_meta_data_model (unused-import)
pybirdai/process_steps/ancrdt_transformation/filter_buildr.py:138:4: W0611: Unused MEMBER_LINK imported from pybirdai.models.bird_meta_data_model (unused-import)
pybirdai/process_steps/ancrdt_transformation/filter_buildr.py:138:4: W0611: Unused CUBE_STRUCTURE_ITEM_LINK imported from pybirdai.models.bird_meta_data_model (unused-import)
************* Module pybirdai.process_steps.ancrdt_transformation.context_ancrdt
pybirdai/process_steps/ancrdt_transformation/context_ancrdt.py:18:0: R0205: Class 'Context' inherits from object, can be safely removed from bases in python3 (useless-object-inheritance)
pybirdai/process_steps/ancrdt_transformation/context_ancrdt.py:15:0: W0611: Unused GenerationRulesModule imported from pybirdai.regdna (unused-import)
pybirdai/process_steps/ancrdt_transformation/context_ancrdt.py:15:0: W0611: Unused ReportModule imported from pybirdai.regdna (unused-import)
************* Module pybirdai.process_steps.hierarchy_conversion.convert_ldm_to_sdd_hierarchies
pybirdai/process_steps/hierarchy_conversion/convert_ldm_to_sdd_hierarchies.py:35:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/hierarchy_conversion/convert_ldm_to_sdd_hierarchies.py:41:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/hierarchy_conversion/convert_ldm_to_sdd_hierarchies.py:86:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/hierarchy_conversion/convert_ldm_to_sdd_hierarchies.py:89:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/hierarchy_conversion/convert_ldm_to_sdd_hierarchies.py:92:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/hierarchy_conversion/convert_ldm_to_sdd_hierarchies.py:97:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/hierarchy_conversion/convert_ldm_to_sdd_hierarchies.py:105:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/hierarchy_conversion/convert_ldm_to_sdd_hierarchies.py:125:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/hierarchy_conversion/convert_ldm_to_sdd_hierarchies.py:128:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/hierarchy_conversion/convert_ldm_to_sdd_hierarchies.py:131:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/hierarchy_conversion/convert_ldm_to_sdd_hierarchies.py:135:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/hierarchy_conversion/convert_ldm_to_sdd_hierarchies.py:176:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/hierarchy_conversion/convert_ldm_to_sdd_hierarchies.py:183:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/hierarchy_conversion/convert_ldm_to_sdd_hierarchies.py:195:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/hierarchy_conversion/convert_ldm_to_sdd_hierarchies.py:200:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/hierarchy_conversion/convert_ldm_to_sdd_hierarchies.py:206:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/hierarchy_conversion/convert_ldm_to_sdd_hierarchies.py:211:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/hierarchy_conversion/convert_ldm_to_sdd_hierarchies.py:237:0: C0304: Final newline missing (missing-final-newline)
pybirdai/process_steps/hierarchy_conversion/convert_ldm_to_sdd_hierarchies.py:99:21: W0212: Access to a protected member _meta of a client class (protected-access)
pybirdai/process_steps/hierarchy_conversion/convert_ldm_to_sdd_hierarchies.py:108:4: R0914: Too many local variables (33/15) (too-many-locals)
pybirdai/process_steps/hierarchy_conversion/convert_ldm_to_sdd_hierarchies.py:138:13: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/process_steps/hierarchy_conversion/convert_ldm_to_sdd_hierarchies.py:163:13: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/process_steps/hierarchy_conversion/convert_ldm_to_sdd_hierarchies.py:178:29: W0212: Access to a protected member _meta of a client class (protected-access)
pybirdai/process_steps/hierarchy_conversion/convert_ldm_to_sdd_hierarchies.py:198:30: W0212: Access to a protected member _meta of a client class (protected-access)
pybirdai/process_steps/hierarchy_conversion/convert_ldm_to_sdd_hierarchies.py:199:37: W0212: Access to a protected member _meta of a client class (protected-access)
pybirdai/process_steps/hierarchy_conversion/convert_ldm_to_sdd_hierarchies.py:226:17: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/process_steps/hierarchy_conversion/convert_ldm_to_sdd_hierarchies.py:19:0: C0411: standard import "difflib.get_close_matches" should be placed before third party imports "django.apps.apps", "django.db.models" (wrong-import-order)
************* Module pybirdai.process_steps.joins_meta_data.create_joins_meta_data
pybirdai/process_steps/joins_meta_data/create_joins_meta_data.py:272:0: C0325: Unnecessary parens after 'not' keyword (superfluous-parens)
pybirdai/process_steps/joins_meta_data/create_joins_meta_data.py:357:0: W0311: Bad indentation. Found 28 spaces, expected 24 (bad-indentation)
pybirdai/process_steps/joins_meta_data/create_joins_meta_data.py:14:0: W0401: Wildcard import pybirdai.models.bird_meta_data_model (wildcard-import)
pybirdai/process_steps/joins_meta_data/create_joins_meta_data.py:77:19: W0212: Access to a protected member _meta of a client class (protected-access)
pybirdai/process_steps/joins_meta_data/create_joins_meta_data.py:78:31: E1120: No value for argument 'memoization_parents_from_disjoint_subtyping_eldm_search' in unbound method call (no-value-for-parameter)
pybirdai/process_steps/joins_meta_data/create_joins_meta_data.py:88:4: R0914: Too many local variables (35/15) (too-many-locals)
pybirdai/process_steps/joins_meta_data/create_joins_meta_data.py:118:8: R1702: Too many nested blocks (8/5) (too-many-nested-blocks)
pybirdai/process_steps/joins_meta_data/create_joins_meta_data.py:118:8: R1702: Too many nested blocks (7/5) (too-many-nested-blocks)
pybirdai/process_steps/joins_meta_data/create_joins_meta_data.py:118:8: R1702: Too many nested blocks (7/5) (too-many-nested-blocks)
pybirdai/process_steps/joins_meta_data/create_joins_meta_data.py:118:8: R1702: Too many nested blocks (10/5) (too-many-nested-blocks)
pybirdai/process_steps/joins_meta_data/create_joins_meta_data.py:88:4: R0912: Too many branches (26/12) (too-many-branches)
pybirdai/process_steps/joins_meta_data/create_joins_meta_data.py:88:4: R0915: Too many statements (77/50) (too-many-statements)
pybirdai/process_steps/joins_meta_data/create_joins_meta_data.py:347:35: R1714: Consider merging these comparisons with 'in' by using 'target_domain.domain_id in ('String', 'Date', 'Integer', 'Boolean', 'Float')'. Use a set instead if elements are hashable. (consider-using-in)
pybirdai/process_steps/joins_meta_data/create_joins_meta_data.py:15:0: C0411: third party import "django.apps.apps" should be placed before first party import "pybirdai.models.bird_meta_data_model.*"  (wrong-import-order)
pybirdai/process_steps/joins_meta_data/create_joins_meta_data.py:16:0: C0411: third party import "django.db.models.fields.CharField" should be placed before first party import "pybirdai.models.bird_meta_data_model.*"  (wrong-import-order)
pybirdai/process_steps/joins_meta_data/create_joins_meta_data.py:17:0: C0411: standard import "os" should be placed before third party imports "django.apps.apps", "django.db.models.fields.CharField" and first party import "pybirdai.models.bird_meta_data_model.*"  (wrong-import-order)
pybirdai/process_steps/joins_meta_data/create_joins_meta_data.py:18:0: C0411: standard import "csv" should be placed before third party imports "django.apps.apps", "django.db.models.fields.CharField" and first party import "pybirdai.models.bird_meta_data_model.*"  (wrong-import-order)
pybirdai/process_steps/joins_meta_data/create_joins_meta_data.py:19:0: C0411: standard import "typing.List" should be placed before third party imports "django.apps.apps", "django.db.models.fields.CharField" and first party import "pybirdai.models.bird_meta_data_model.*"  (wrong-import-order)
pybirdai/process_steps/joins_meta_data/create_joins_meta_data.py:21:0: C0412: Imports from package pybirdai are not grouped (ungrouped-imports)
pybirdai/process_steps/joins_meta_data/create_joins_meta_data.py:16:0: W0611: Unused CharField imported from django.db.models.fields (unused-import)
pybirdai/process_steps/joins_meta_data/create_joins_meta_data.py:16:0: W0611: Unused DateTimeField imported from django.db.models.fields (unused-import)
pybirdai/process_steps/joins_meta_data/create_joins_meta_data.py:16:0: W0611: Unused BooleanField imported from django.db.models.fields (unused-import)
pybirdai/process_steps/joins_meta_data/create_joins_meta_data.py:16:0: W0611: Unused FloatField imported from django.db.models.fields (unused-import)
pybirdai/process_steps/joins_meta_data/create_joins_meta_data.py:16:0: W0611: Unused BigIntegerField imported from django.db.models.fields (unused-import)
pybirdai/process_steps/joins_meta_data/create_joins_meta_data.py:14:0: W0614: Unused import(s) SUBDOMAIN, SUBDOMAIN_ENUMERATION, DOMAIN, FACET_COLLECTION, MAINTENANCE_AGENCY, MEMBER, MEMBER_HIERARCHY, MEMBER_HIERARCHY_NODE, VARIABLE, VARIABLE_SET, VARIABLE_SET_ENUMERATION, FRAMEWORK, MEMBER_MAPPING, MEMBER_MAPPING_ITEM, VARIABLE_MAPPING_ITEM, VARIABLE_MAPPING, MAPPING_TO_CUBE, MAPPING_DEFINITION, AXIS, AXIS_ORDINATE, CELL_POSITION, ORDINATE_ITEM, TABLE, TABLE_CELL, CUBE_STRUCTURE, CUBE_STRUCTURE_ITEM, CUBE, COMBINATION, COMBINATION_ITEM, CUBE_TO_COMBINATION, MEMBER_LINK, models, OperationalError and timezone from wildcard import of pybirdai.models.bird_meta_data_model (unused-wildcard-import)
************* Module pybirdai.process_steps.joins_meta_data.delete_joins_meta_data
pybirdai/process_steps/joins_meta_data/delete_joins_meta_data.py:157:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/joins_meta_data/delete_joins_meta_data.py:14:0: W0401: Wildcard import pybirdai.models.bird_meta_data_model (wildcard-import)
pybirdai/process_steps/joins_meta_data/delete_joins_meta_data.py:21:0: W0404: Reimport 'connection' (imported line 16) (reimported)
pybirdai/process_steps/joins_meta_data/delete_joins_meta_data.py:50:16: W0612: Unused variable 'value' (unused-variable)
pybirdai/process_steps/joins_meta_data/delete_joins_meta_data.py:124:25: W0130: Duplicate value 'pybirdai_axis_ordinate' in set (duplicate-value)
pybirdai/process_steps/joins_meta_data/delete_joins_meta_data.py:170:4: R0915: Too many statements (89/50) (too-many-statements)
pybirdai/process_steps/joins_meta_data/delete_joins_meta_data.py:15:0: C0411: third party import "django.apps.apps" should be placed before first party imports "pybirdai.context.sdd_context_django.SDDContext", "pybirdai.models.bird_meta_data_model.*"  (wrong-import-order)
pybirdai/process_steps/joins_meta_data/delete_joins_meta_data.py:16:0: C0411: third party import "django.db.connection" should be placed before first party imports "pybirdai.context.sdd_context_django.SDDContext", "pybirdai.models.bird_meta_data_model.*"  (wrong-import-order)
pybirdai/process_steps/joins_meta_data/delete_joins_meta_data.py:17:0: C0411: third party import "django.db.models.fields.CharField" should be placed before first party imports "pybirdai.context.sdd_context_django.SDDContext", "pybirdai.models.bird_meta_data_model.*"  (wrong-import-order)
pybirdai/process_steps/joins_meta_data/delete_joins_meta_data.py:18:0: C0411: standard import "os" should be placed before third party imports "django.apps.apps", "django.db.connection", "django.db.models.fields.CharField" and first party imports "pybirdai.context.sdd_context_django.SDDContext", "pybirdai.models.bird_meta_data_model.*"  (wrong-import-order)
pybirdai/process_steps/joins_meta_data/delete_joins_meta_data.py:19:0: C0411: standard import "csv" should be placed before third party imports "django.apps.apps", "django.db.connection", "django.db.models.fields.CharField" and first party imports "pybirdai.context.sdd_context_django.SDDContext", "pybirdai.models.bird_meta_data_model.*"  (wrong-import-order)
pybirdai/process_steps/joins_meta_data/delete_joins_meta_data.py:20:0: C0411: standard import "typing.List" should be placed before third party imports "django.apps.apps", "django.db.connection", "django.db.models.fields.CharField" and first party imports "pybirdai.context.sdd_context_django.SDDContext", "pybirdai.models.bird_meta_data_model.*"  (wrong-import-order)
pybirdai/process_steps/joins_meta_data/delete_joins_meta_data.py:21:0: C0411: third party import "django.db.connection" should be placed before first party imports "pybirdai.context.sdd_context_django.SDDContext", "pybirdai.models.bird_meta_data_model.*"  (wrong-import-order)
pybirdai/process_steps/joins_meta_data/delete_joins_meta_data.py:23:0: C0412: Imports from package pybirdai are not grouped (ungrouped-imports)
pybirdai/process_steps/joins_meta_data/delete_joins_meta_data.py:15:0: W0611: Unused apps imported from django.apps (unused-import)
pybirdai/process_steps/joins_meta_data/delete_joins_meta_data.py:17:0: W0611: Unused CharField imported from django.db.models.fields (unused-import)
pybirdai/process_steps/joins_meta_data/delete_joins_meta_data.py:17:0: W0611: Unused DateTimeField imported from django.db.models.fields (unused-import)
pybirdai/process_steps/joins_meta_data/delete_joins_meta_data.py:17:0: W0611: Unused BooleanField imported from django.db.models.fields (unused-import)
pybirdai/process_steps/joins_meta_data/delete_joins_meta_data.py:17:0: W0611: Unused FloatField imported from django.db.models.fields (unused-import)
pybirdai/process_steps/joins_meta_data/delete_joins_meta_data.py:17:0: W0611: Unused BigIntegerField imported from django.db.models.fields (unused-import)
pybirdai/process_steps/joins_meta_data/delete_joins_meta_data.py:18:0: W0611: Unused import os (unused-import)
pybirdai/process_steps/joins_meta_data/delete_joins_meta_data.py:19:0: W0611: Unused import csv (unused-import)
pybirdai/process_steps/joins_meta_data/delete_joins_meta_data.py:20:0: W0611: Unused List imported from typing (unused-import)
pybirdai/process_steps/joins_meta_data/delete_joins_meta_data.py:23:0: W0611: Unused ELDMSearch imported from pybirdai.process_steps.joins_meta_data.ldm_search (unused-import)
pybirdai/process_steps/joins_meta_data/delete_joins_meta_data.py:14:0: W0614: Unused import(s) VARIABLE_SET, VARIABLE_SET_ENUMERATION, MEMBER_LINK, models, OperationalError and timezone from wildcard import of pybirdai.models.bird_meta_data_model (unused-wildcard-import)
************* Module pybirdai.process_steps.joins_meta_data.main_category_finder
pybirdai/process_steps/joins_meta_data/main_category_finder.py:162:0: C0325: Unnecessary parens after 'not' keyword (superfluous-parens)
pybirdai/process_steps/joins_meta_data/main_category_finder.py:183:0: C0325: Unnecessary parens after 'not' keyword (superfluous-parens)
pybirdai/process_steps/joins_meta_data/main_category_finder.py:202:0: C0325: Unnecessary parens after 'not' keyword (superfluous-parens)
pybirdai/process_steps/joins_meta_data/main_category_finder.py:321:0: C0325: Unnecessary parens after 'not' keyword (superfluous-parens)
pybirdai/process_steps/joins_meta_data/main_category_finder.py:348:0: C0325: Unnecessary parens after 'not' keyword (superfluous-parens)
pybirdai/process_steps/joins_meta_data/main_category_finder.py:33:0: R0205: Class 'MainCategoryFinder' inherits from object, can be safely removed from bases in python3 (useless-object-inheritance)
pybirdai/process_steps/joins_meta_data/main_category_finder.py:297:4: E0102: method already defined line 86 (function-redefined)
pybirdai/process_steps/joins_meta_data/main_category_finder.py:340:4: R0914: Too many local variables (17/15) (too-many-locals)
pybirdai/process_steps/joins_meta_data/main_category_finder.py:367:80: W0612: Unused variable 'comments' (unused-variable)
pybirdai/process_steps/joins_meta_data/main_category_finder.py:31:0: W0611: Unused ImportWebsiteToSDDModel imported from pybirdai.process_steps.website_to_sddmodel.import_website_to_sdd_model_django (unused-import)
************* Module pybirdai.process_steps.joins_meta_data.create_joins_meta_data_combinations
pybirdai/process_steps/joins_meta_data/create_joins_meta_data_combinations.py:16:0: W0401: Wildcard import pybirdai.models.bird_meta_data_model (wildcard-import)
pybirdai/process_steps/joins_meta_data/create_joins_meta_data_combinations.py:154:19: W0212: Access to a protected member _meta of a client class (protected-access)
pybirdai/process_steps/joins_meta_data/create_joins_meta_data_combinations.py:164:4: R0914: Too many local variables (35/15) (too-many-locals)
pybirdai/process_steps/joins_meta_data/create_joins_meta_data_combinations.py:199:8: R1702: Too many nested blocks (8/5) (too-many-nested-blocks)
pybirdai/process_steps/joins_meta_data/create_joins_meta_data_combinations.py:199:8: R1702: Too many nested blocks (7/5) (too-many-nested-blocks)
pybirdai/process_steps/joins_meta_data/create_joins_meta_data_combinations.py:199:8: R1702: Too many nested blocks (7/5) (too-many-nested-blocks)
pybirdai/process_steps/joins_meta_data/create_joins_meta_data_combinations.py:349:20: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/process_steps/joins_meta_data/create_joins_meta_data_combinations.py:351:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
pybirdai/process_steps/joins_meta_data/create_joins_meta_data_combinations.py:199:8: R1702: Too many nested blocks (9/5) (too-many-nested-blocks)
pybirdai/process_steps/joins_meta_data/create_joins_meta_data_combinations.py:164:4: R0912: Too many branches (25/12) (too-many-branches)
pybirdai/process_steps/joins_meta_data/create_joins_meta_data_combinations.py:164:4: R0915: Too many statements (69/50) (too-many-statements)
pybirdai/process_steps/joins_meta_data/create_joins_meta_data_combinations.py:367:4: R0914: Too many local variables (17/15) (too-many-locals)
pybirdai/process_steps/joins_meta_data/create_joins_meta_data_combinations.py:506:55: R1735: Consider using '{}' instead of a call to 'dict'. (use-dict-literal)
pybirdai/process_steps/joins_meta_data/create_joins_meta_data_combinations.py:517:4: R0914: Too many local variables (26/15) (too-many-locals)
pybirdai/process_steps/joins_meta_data/create_joins_meta_data_combinations.py:595:20: C0415: Import outside toplevel (django.db.models.Prefetch) (import-outside-toplevel)
pybirdai/process_steps/joins_meta_data/create_joins_meta_data_combinations.py:517:4: R0912: Too many branches (20/12) (too-many-branches)
pybirdai/process_steps/joins_meta_data/create_joins_meta_data_combinations.py:595:20: W0611: Unused Prefetch imported from django.db.models (unused-import)
pybirdai/process_steps/joins_meta_data/create_joins_meta_data_combinations.py:66:8: W0201: Attribute 'member_hierarchy_service' defined outside __init__ (attribute-defined-outside-init)
pybirdai/process_steps/joins_meta_data/create_joins_meta_data_combinations.py:17:0: C0411: third party import "django.apps.apps" should be placed before first party import "pybirdai.models.bird_meta_data_model.*"  (wrong-import-order)
pybirdai/process_steps/joins_meta_data/create_joins_meta_data_combinations.py:18:0: C0411: third party import "django.db.models.fields.CharField" should be placed before first party import "pybirdai.models.bird_meta_data_model.*"  (wrong-import-order)
pybirdai/process_steps/joins_meta_data/create_joins_meta_data_combinations.py:25:0: C0411: standard import "os" should be placed before third party imports "django.apps.apps", "django.db.models.fields.CharField" and first party import "pybirdai.models.bird_meta_data_model.*"  (wrong-import-order)
pybirdai/process_steps/joins_meta_data/create_joins_meta_data_combinations.py:26:0: C0411: standard import "json" should be placed before third party imports "django.apps.apps", "django.db.models.fields.CharField" and first party import "pybirdai.models.bird_meta_data_model.*"  (wrong-import-order)
pybirdai/process_steps/joins_meta_data/create_joins_meta_data_combinations.py:27:0: C0411: standard import "csv" should be placed before third party imports "django.apps.apps", "django.db.models.fields.CharField" and first party import "pybirdai.models.bird_meta_data_model.*"  (wrong-import-order)
pybirdai/process_steps/joins_meta_data/create_joins_meta_data_combinations.py:28:0: C0411: standard import "typing.List" should be placed before third party imports "django.apps.apps", "django.db.models.fields.CharField" and first party import "pybirdai.models.bird_meta_data_model.*"  (wrong-import-order)
pybirdai/process_steps/joins_meta_data/create_joins_meta_data_combinations.py:32:0: C0411: standard import "itertools" should be placed before third party imports "django.apps.apps", "django.db.models.fields.CharField" and first party imports "pybirdai.models.bird_meta_data_model.*", "pybirdai.process_steps.joins_meta_data.member_hierarchy_service.MemberHierarchyService"  (wrong-import-order)
pybirdai/process_steps/joins_meta_data/create_joins_meta_data_combinations.py:33:0: C0411: standard import "traceback" should be placed before third party imports "django.apps.apps", "django.db.models.fields.CharField" and first party imports "pybirdai.models.bird_meta_data_model.*", "pybirdai.process_steps.joins_meta_data.member_hierarchy_service.MemberHierarchyService"  (wrong-import-order)
pybirdai/process_steps/joins_meta_data/create_joins_meta_data_combinations.py:29:0: C0412: Imports from package pybirdai are not grouped (ungrouped-imports)
pybirdai/process_steps/joins_meta_data/create_joins_meta_data_combinations.py:18:0: W0611: Unused CharField imported from django.db.models.fields (unused-import)
pybirdai/process_steps/joins_meta_data/create_joins_meta_data_combinations.py:18:0: W0611: Unused DateTimeField imported from django.db.models.fields (unused-import)
pybirdai/process_steps/joins_meta_data/create_joins_meta_data_combinations.py:18:0: W0611: Unused BooleanField imported from django.db.models.fields (unused-import)
pybirdai/process_steps/joins_meta_data/create_joins_meta_data_combinations.py:18:0: W0611: Unused FloatField imported from django.db.models.fields (unused-import)
pybirdai/process_steps/joins_meta_data/create_joins_meta_data_combinations.py:18:0: W0611: Unused BigIntegerField imported from django.db.models.fields (unused-import)
pybirdai/process_steps/joins_meta_data/create_joins_meta_data_combinations.py:26:0: W0611: Unused import json (unused-import)
pybirdai/process_steps/joins_meta_data/create_joins_meta_data_combinations.py:33:0: W0611: Unused import traceback (unused-import)
pybirdai/process_steps/joins_meta_data/create_joins_meta_data_combinations.py:16:0: W0614: Unused import(s) SUBDOMAIN, DOMAIN, FACET_COLLECTION, MAINTENANCE_AGENCY, MEMBER_HIERARCHY, MEMBER_HIERARCHY_NODE, VARIABLE, VARIABLE_SET, VARIABLE_SET_ENUMERATION, FRAMEWORK, MEMBER_MAPPING, MEMBER_MAPPING_ITEM, VARIABLE_MAPPING_ITEM, VARIABLE_MAPPING, MAPPING_TO_CUBE, MAPPING_DEFINITION, AXIS, AXIS_ORDINATE, CELL_POSITION, ORDINATE_ITEM, TABLE, TABLE_CELL, CUBE_STRUCTURE, CUBE, COMBINATION_ITEM, CUBE_TO_COMBINATION, MEMBER_LINK, models, OperationalError and timezone from wildcard import of pybirdai.models.bird_meta_data_model (unused-wildcard-import)
************* Module pybirdai.process_steps.joins_meta_data.ldm_search
pybirdai/process_steps/joins_meta_data/ldm_search.py:40:4: E0213: Method '_get_associated_entities' should have "self" as first argument (no-self-argument)
pybirdai/process_steps/joins_meta_data/ldm_search.py:56:23: W0212: Access to a protected member _meta of a client class (protected-access)
pybirdai/process_steps/joins_meta_data/ldm_search.py:72:4: E0213: Method '_get_superclasses_and_associated_entities' should have "self" as first argument (no-self-argument)
pybirdai/process_steps/joins_meta_data/ldm_search.py:97:22: W0212: Access to a protected member _meta of a client class (protected-access)
pybirdai/process_steps/joins_meta_data/ldm_search.py:127:4: E0213: Method '_get_parents_from_disjoint_subtyping' should have "self" as first argument (no-self-argument)
pybirdai/process_steps/joins_meta_data/ldm_search.py:143:15: W0212: Access to a protected member _meta of a client class (protected-access)
pybirdai/process_steps/joins_meta_data/ldm_search.py:145:31: W0212: Access to a protected member _meta of a client class (protected-access)
************* Module pybirdai.process_steps.joins_meta_data.member_hierarchy_service
pybirdai/process_steps/joins_meta_data/member_hierarchy_service.py:64:8: R1702: Too many nested blocks (6/5) (too-many-nested-blocks)
pybirdai/process_steps/joins_meta_data/member_hierarchy_service.py:14:0: W0611: Unused MEMBER_LINK imported from pybirdai.models.bird_meta_data_model (unused-import)
************* Module pybirdai.process_steps.dpm_integration.dpm_integration_service
pybirdai/process_steps/dpm_integration/dpm_integration_service.py:92:15: W0718: Catching too general exception Exception (broad-exception-caught)
pybirdai/process_steps/dpm_integration/dpm_integration_service.py:77:12: E0401: Unable to import 'bs4' (import-error)
pybirdai/process_steps/dpm_integration/dpm_integration_service.py:77:12: C0415: Import outside toplevel (bs4.BeautifulSoup) (import-outside-toplevel)
pybirdai/process_steps/dpm_integration/dpm_integration_service.py:79:33: W3101: Missing timeout argument for method 'requests.get' can cause your program to hang indefinitely (missing-timeout)
pybirdai/process_steps/dpm_integration/dpm_integration_service.py:92:8: W0612: Unused variable 'e' (unused-variable)
pybirdai/process_steps/dpm_integration/dpm_integration_service.py:100:22: W3101: Missing timeout argument for method 'requests.get' can cause your program to hang indefinitely (missing-timeout)
pybirdai/process_steps/dpm_integration/dpm_integration_service.py:108:16: W0719: Raising too general exception: Exception (broad-exception-raised)
pybirdai/process_steps/dpm_integration/dpm_integration_service.py:140:13: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
pybirdai/process_steps/dpm_integration/dpm_integration_service.py:144:4: R0914: Too many local variables (26/15) (too-many-locals)
pybirdai/process_steps/dpm_integration/dpm_integration_service.py:145:8: C0415: Import outside toplevel (pybirdai.process_steps.dpm_integration.mapping_functions) (import-outside-toplevel)
pybirdai/process_steps/dpm_integration/dpm_integration_service.py:146:8: W0105: String statement has no effect (pointless-string-statement)
pybirdai/process_steps/dpm_integration/dpm_integration_service.py:177:8: W0105: String statement has no effect (pointless-string-statement)
pybirdai/process_steps/dpm_integration/dpm_integration_service.py:188:8: W0105: String statement has no effect (pointless-string-statement)
pybirdai/process_steps/dpm_integration/dpm_integration_service.py:173:30: W0612: Unused variable 'hierarchy_node_map' (unused-variable)
pybirdai/process_steps/dpm_integration/dpm_integration_service.py:207:30: W0612: Unused variable 'cell_position_map' (unused-variable)
pybirdai/process_steps/dpm_integration/dpm_integration_service.py:211:30: W0612: Unused variable 'ordinate_item_map' (unused-variable)
pybirdai/process_steps/dpm_integration/dpm_integration_service.py:14:0: C0411: standard import "os" should be placed before third party import "requests" (wrong-import-order)
pybirdai/process_steps/dpm_integration/dpm_integration_service.py:15:0: C0411: standard import "zipfile" should be placed before third party import "requests" (wrong-import-order)
pybirdai/process_steps/dpm_integration/dpm_integration_service.py:16:0: C0411: standard import "shutil" should be placed before third party import "requests" (wrong-import-order)
pybirdai/process_steps/dpm_integration/dpm_integration_service.py:17:0: C0411: standard import "platform" should be placed before third party import "requests" (wrong-import-order)
pybirdai/process_steps/dpm_integration/dpm_integration_service.py:18:0: C0411: standard import "logging" should be placed before third party import "requests" (wrong-import-order)
pybirdai/process_steps/dpm_integration/dpm_integration_service.py:20:0: C0411: standard import "csv" should be placed before third party imports "requests", "numpy" (wrong-import-order)
************* Module pybirdai.process_steps.dpm_integration.mapping_functions.table_cell
pybirdai/process_steps/dpm_integration/mapping_functions/table_cell.py:100:0: C0304: Final newline missing (missing-final-newline)
pybirdai/process_steps/dpm_integration/mapping_functions/table_cell.py:21:0: W0102: Dangerous default value {} as argument (dangerous-default-value)
pybirdai/process_steps/dpm_integration/mapping_functions/table_cell.py:21:0: W0102: Dangerous default value {} as argument (dangerous-default-value)
pybirdai/process_steps/dpm_integration/mapping_functions/table_cell.py:21:0: R0912: Too many branches (15/12) (too-many-branches)
************* Module pybirdai.process_steps.dpm_integration.mapping_functions.axis
pybirdai/process_steps/dpm_integration/mapping_functions/axis.py:101:0: C0304: Final newline missing (missing-final-newline)
pybirdai/process_steps/dpm_integration/mapping_functions/axis.py:21:0: W0102: Dangerous default value {} as argument (dangerous-default-value)
pybirdai/process_steps/dpm_integration/mapping_functions/axis.py:21:0: R0914: Too many local variables (18/15) (too-many-locals)
************* Module pybirdai.process_steps.dpm_integration.mapping_functions.datapoint_version
pybirdai/process_steps/dpm_integration/mapping_functions/datapoint_version.py:124:0: C0304: Final newline missing (missing-final-newline)
pybirdai/process_steps/dpm_integration/mapping_functions/datapoint_version.py:23:0: W0102: Dangerous default value {} as argument (dangerous-default-value)
pybirdai/process_steps/dpm_integration/mapping_functions/datapoint_version.py:23:0: W0102: Dangerous default value {} as argument (dangerous-default-value)
pybirdai/process_steps/dpm_integration/mapping_functions/datapoint_version.py:23:0: W0102: Dangerous default value {} as argument (dangerous-default-value)
pybirdai/process_steps/dpm_integration/mapping_functions/datapoint_version.py:23:0: R0914: Too many local variables (18/15) (too-many-locals)
pybirdai/process_steps/dpm_integration/mapping_functions/datapoint_version.py:60:20: R1735: Consider using '{}' instead of a call to 'dict'. (use-dict-literal)
pybirdai/process_steps/dpm_integration/mapping_functions/datapoint_version.py:63:12: W0612: Unused variable 'idx' (unused-variable)
pybirdai/process_steps/dpm_integration/mapping_functions/datapoint_version.py:23:0: R0915: Too many statements (51/50) (too-many-statements)
pybirdai/process_steps/dpm_integration/mapping_functions/datapoint_version.py:25:4: W0612: Unused variable 'types' (unused-variable)
pybirdai/process_steps/dpm_integration/mapping_functions/datapoint_version.py:51:4: W0612: Unused variable 'is_number' (unused-variable)
pybirdai/process_steps/dpm_integration/mapping_functions/datapoint_version.py:16:0: C0411: standard import "collections.defaultdict" should be placed before third party import "numpy" (wrong-import-order)
************* Module pybirdai.process_steps.dpm_integration.mapping_functions.members
pybirdai/process_steps/dpm_integration/mapping_functions/members.py:80:0: C0304: Final newline missing (missing-final-newline)
pybirdai/process_steps/dpm_integration/mapping_functions/members.py:22:0: W0102: Dangerous default value {} as argument (dangerous-default-value)
************* Module pybirdai.process_steps.dpm_integration.mapping_functions.dimensions
pybirdai/process_steps/dpm_integration/mapping_functions/dimensions.py:81:0: C0304: Final newline missing (missing-final-newline)
pybirdai/process_steps/dpm_integration/mapping_functions/dimensions.py:21:0: W0102: Dangerous default value {} as argument (dangerous-default-value)
************* Module pybirdai.process_steps.dpm_integration.mapping_functions.ordinate_categorisation
pybirdai/process_steps/dpm_integration/mapping_functions/ordinate_categorisation.py:153:0: C0304: Final newline missing (missing-final-newline)
pybirdai/process_steps/dpm_integration/mapping_functions/ordinate_categorisation.py:39:0: W0102: Dangerous default value {} as argument (dangerous-default-value)
pybirdai/process_steps/dpm_integration/mapping_functions/ordinate_categorisation.py:39:0: W0102: Dangerous default value {} as argument (dangerous-default-value)
pybirdai/process_steps/dpm_integration/mapping_functions/ordinate_categorisation.py:39:0: W0102: Dangerous default value {} as argument (dangerous-default-value)
pybirdai/process_steps/dpm_integration/mapping_functions/ordinate_categorisation.py:39:0: W0102: Dangerous default value {} as argument (dangerous-default-value)
pybirdai/process_steps/dpm_integration/mapping_functions/ordinate_categorisation.py:39:0: R0914: Too many local variables (24/15) (too-many-locals)
pybirdai/process_steps/dpm_integration/mapping_functions/ordinate_categorisation.py:39:0: R0912: Too many branches (24/12) (too-many-branches)
pybirdai/process_steps/dpm_integration/mapping_functions/ordinate_categorisation.py:39:0: R0915: Too many statements (54/50) (too-many-statements)
pybirdai/process_steps/dpm_integration/mapping_functions/ordinate_categorisation.py:15:0: W0611: Unused numpy imported as np (unused-import)
************* Module pybirdai.process_steps.dpm_integration.mapping_functions.domains
pybirdai/process_steps/dpm_integration/mapping_functions/domains.py:63:0: C0304: Final newline missing (missing-final-newline)
************* Module pybirdai.process_steps.dpm_integration.mapping_functions.axis_ordinate
pybirdai/process_steps/dpm_integration/mapping_functions/axis_ordinate.py:98:0: C0304: Final newline missing (missing-final-newline)
pybirdai/process_steps/dpm_integration/mapping_functions/axis_ordinate.py:22:0: W0102: Dangerous default value {} as argument (dangerous-default-value)
pybirdai/process_steps/dpm_integration/mapping_functions/axis_ordinate.py:22:0: R0914: Too many local variables (18/15) (too-many-locals)
pybirdai/process_steps/dpm_integration/mapping_functions/axis_ordinate.py:82:16: W0702: No exception type(s) specified (bare-except)
pybirdai/process_steps/dpm_integration/mapping_functions/axis_ordinate.py:24:4: W0612: Unused variable 'types' (unused-variable)
************* Module pybirdai.process_steps.dpm_integration.mapping_functions.__init__
pybirdai/process_steps/dpm_integration/mapping_functions/__init__.py:98:0: C0304: Final newline missing (missing-final-newline)
************* Module pybirdai.process_steps.dpm_integration.mapping_functions.hierarchy
pybirdai/process_steps/dpm_integration/mapping_functions/hierarchy.py:66:0: C0304: Final newline missing (missing-final-newline)
pybirdai/process_steps/dpm_integration/mapping_functions/hierarchy.py:21:0: W0102: Dangerous default value {} as argument (dangerous-default-value)
pybirdai/process_steps/dpm_integration/mapping_functions/hierarchy.py:15:0: W0611: Unused drop_fields imported from utils (unused-import)
************* Module pybirdai.process_steps.dpm_integration.mapping_functions.hierarchy_node
pybirdai/process_steps/dpm_integration/mapping_functions/hierarchy_node.py:106:0: C0304: Final newline missing (missing-final-newline)
pybirdai/process_steps/dpm_integration/mapping_functions/hierarchy_node.py:21:0: W0102: Dangerous default value {} as argument (dangerous-default-value)
pybirdai/process_steps/dpm_integration/mapping_functions/hierarchy_node.py:21:0: W0102: Dangerous default value {} as argument (dangerous-default-value)
pybirdai/process_steps/dpm_integration/mapping_functions/hierarchy_node.py:21:0: R0912: Too many branches (16/12) (too-many-branches)
************* Module pybirdai.process_steps.dpm_integration.mapping_functions.utils
pybirdai/process_steps/dpm_integration/mapping_functions/utils.py:216:0: C0304: Final newline missing (missing-final-newline)
pybirdai/process_steps/dpm_integration/mapping_functions/utils.py:69:15: E1136: Value 'arr.dtype' is unsubscriptable (unsubscriptable-object)
pybirdai/process_steps/dpm_integration/mapping_functions/utils.py:72:16: W0702: No exception type(s) specified (bare-except)
pybirdai/process_steps/dpm_integration/mapping_functions/utils.py:36:0: R0912: Too many branches (13/12) (too-many-branches)
pybirdai/process_steps/dpm_integration/mapping_functions/utils.py:148:0: R0912: Too many branches (15/12) (too-many-branches)
pybirdai/process_steps/dpm_integration/mapping_functions/utils.py:209:12: C0200: Consider using enumerate instead of iterating with range and len (consider-using-enumerate)
pybirdai/process_steps/dpm_integration/mapping_functions/utils.py:15:0: C0411: standard import "csv" should be placed before third party import "numpy" (wrong-import-order)
pybirdai/process_steps/dpm_integration/mapping_functions/utils.py:16:0: C0411: standard import "re" should be placed before third party import "numpy" (wrong-import-order)
pybirdai/process_steps/dpm_integration/mapping_functions/utils.py:17:0: C0411: standard import "collections.defaultdict" should be placed before third party import "numpy" (wrong-import-order)
pybirdai/process_steps/dpm_integration/mapping_functions/utils.py:17:0: W0611: Unused defaultdict imported from collections (unused-import)
************* Module pybirdai.process_steps.dpm_integration.mapping_functions.frameworks
pybirdai/process_steps/dpm_integration/mapping_functions/frameworks.py:67:0: C0304: Final newline missing (missing-final-newline)
************* Module pybirdai.process_steps.dpm_integration.mapping_functions.cell_position
pybirdai/process_steps/dpm_integration/mapping_functions/cell_position.py:58:0: C0304: Final newline missing (missing-final-newline)
pybirdai/process_steps/dpm_integration/mapping_functions/cell_position.py:21:0: W0102: Dangerous default value {} as argument (dangerous-default-value)
pybirdai/process_steps/dpm_integration/mapping_functions/cell_position.py:21:0: W0102: Dangerous default value {} as argument (dangerous-default-value)
************* Module pybirdai.process_steps.dpm_integration.mapping_functions.tables
pybirdai/process_steps/dpm_integration/mapping_functions/tables.py:180:0: C0304: Final newline missing (missing-final-newline)
pybirdai/process_steps/dpm_integration/mapping_functions/tables.py:58:0: W0102: Dangerous default value {} as argument (dangerous-default-value)
pybirdai/process_steps/dpm_integration/mapping_functions/tables.py:58:0: R0914: Too many local variables (26/15) (too-many-locals)
pybirdai/process_steps/dpm_integration/mapping_functions/tables.py:58:0: R0912: Too many branches (15/12) (too-many-branches)
pybirdai/process_steps/dpm_integration/mapping_functions/tables.py:58:0: R0915: Too many statements (66/50) (too-many-statements)
************* Module pybirdai.process_steps.dpm_integration.mapping_functions.context_definition
pybirdai/process_steps/dpm_integration/mapping_functions/context_definition.py:51:0: C0304: Final newline missing (missing-final-newline)
pybirdai/process_steps/dpm_integration/mapping_functions/context_definition.py:22:0: W0102: Dangerous default value {} as argument (dangerous-default-value)
pybirdai/process_steps/dpm_integration/mapping_functions/context_definition.py:22:0: W0102: Dangerous default value {} as argument (dangerous-default-value)
pybirdai/process_steps/dpm_integration/mapping_functions/context_definition.py:24:4: W0612: Unused variable 'types' (unused-variable)
************* Module pybirdai.process_steps.generate_etl.simple_context
pybirdai/process_steps/generate_etl/simple_context.py:48:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/simple_context.py:49:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/simple_context.py:51:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/simple_context.py:14:0: C0112: Empty class docstring (empty-docstring)
pybirdai/process_steps/generate_etl/simple_context.py:14:0: R0205: Class 'SimpleContext' inherits from object, can be safely removed from bases in python3 (useless-object-inheritance)
************* Module pybirdai.process_steps.generate_etl.generate_etl
pybirdai/process_steps/generate_etl/generate_etl.py:26:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:27:62: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:31:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:33:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:39:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:41:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:46:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:52:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:54:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:88:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:89:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:92:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:93:43: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:98:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:101:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:118:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:119:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:121:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:122:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:131:58: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:133:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:134:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:140:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:142:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:147:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:148:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:156:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:157:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:159:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:160:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:162:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:163:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:165:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:166:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:168:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:169:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:188:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:223:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:225:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:226:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:229:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:230:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:235:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:237:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:240:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:241:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:244:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:247:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:258:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:259:0: C0325: Unnecessary parens after 'if' keyword (superfluous-parens)
pybirdai/process_steps/generate_etl/generate_etl.py:264:0: C0325: Unnecessary parens after 'not' keyword (superfluous-parens)
pybirdai/process_steps/generate_etl/generate_etl.py:269:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:270:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:271:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:272:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:273:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:297:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:312:0: C0325: Unnecessary parens after 'if' keyword (superfluous-parens)
pybirdai/process_steps/generate_etl/generate_etl.py:315:0: C0325: Unnecessary parens after 'elif' keyword (superfluous-parens)
pybirdai/process_steps/generate_etl/generate_etl.py:328:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:329:33: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:343:45: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:343:0: C0325: Unnecessary parens after 'not' keyword (superfluous-parens)
pybirdai/process_steps/generate_etl/generate_etl.py:344:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:345:0: C0325: Unnecessary parens after 'not' keyword (superfluous-parens)
pybirdai/process_steps/generate_etl/generate_etl.py:351:0: C0325: Unnecessary parens after 'not' keyword (superfluous-parens)
pybirdai/process_steps/generate_etl/generate_etl.py:352:53: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:354:37: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:355:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:361:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:365:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:370:60: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:371:57: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:372:64: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:378:61: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:389:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:401:0: C0325: Unnecessary parens after 'if' keyword (superfluous-parens)
pybirdai/process_steps/generate_etl/generate_etl.py:404:0: C0325: Unnecessary parens after 'elif' keyword (superfluous-parens)
pybirdai/process_steps/generate_etl/generate_etl.py:410:0: C0325: Unnecessary parens after 'not' keyword (superfluous-parens)
pybirdai/process_steps/generate_etl/generate_etl.py:414:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:418:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:420:68: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:421:65: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:422:72: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:424:0: C0325: Unnecessary parens after 'not' keyword (superfluous-parens)
pybirdai/process_steps/generate_etl/generate_etl.py:426:45: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:435:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:437:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:457:0: C0325: Unnecessary parens after 'if' keyword (superfluous-parens)
pybirdai/process_steps/generate_etl/generate_etl.py:471:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:491:0: C0325: Unnecessary parens after 'if' keyword (superfluous-parens)
pybirdai/process_steps/generate_etl/generate_etl.py:509:55: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:514:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:516:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:521:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:522:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:523:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:525:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:526:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:533:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:535:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:538:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:539:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:541:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:542:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:546:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:550:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:558:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:572:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:573:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:592:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:593:47: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:594:44: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:602:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:617:0: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:619:22: C0303: Trailing whitespace (trailing-whitespace)
pybirdai/process_steps/generate_etl/generate_etl.py:619:0: W0311: Bad indentation. Found 3 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/generate_etl/generate_etl.py:620:0: C0304: Final newline missing (missing-final-newline)
pybirdai/process_steps/generate_etl/generate_etl.py:620:0: W0311: Bad indentation. Found 3 spaces, expected 4 (bad-indentation)
pybirdai/process_steps/generate_etl/generate_etl.py:21:0: R0205: Class 'GenerateETL' inherits from object, can be safely removed from bases in python3 (useless-object-inheritance)
pybirdai/process_steps/generate_etl/generate_etl.py:86:27: W1309: Using an f-string that does not have any interpolated variables (f-string-without-interpolation)
pybirdai/process_steps/generate_etl/generate_etl.py:127:4: R0914: Too many local variables (22/15) (too-many-locals)
pybirdai/process_steps/generate_etl/generate_etl.py:143:8: R1702: Too many nested blocks (6/5) (too-many-nested-blocks)
pybirdai/process_steps/generate_etl/generate_etl.py:136:12: W0612: Unused variable 'entity' (unused-variable)
pybirdai/process_steps/generate_etl/generate_etl.py:274:32: W0612: Unused variable 'has_leaf' (unused-variable)
pybirdai/process_steps/generate_etl/generate_etl.py:298:4: R0914: Too many local variables (20/15) (too-many-locals)
pybirdai/process_steps/generate_etl/generate_etl.py:350:32: C0201: Consider iterating the dictionary directly instead of calling .keys() (consider-iterating-dictionary)
pybirdai/process_steps/generate_etl/generate_etl.py:367:20: R1723: Unnecessary "else" after "break", remove the "else" and de-indent the code inside it (no-else-break)
pybirdai/process_steps/generate_etl/generate_etl.py:385:28: W0702: No exception type(s) specified (bare-except)
pybirdai/process_steps/generate_etl/generate_etl.py:362:8: R1702: Too many nested blocks (8/5) (too-many-nested-blocks)
pybirdai/process_steps/generate_etl/generate_etl.py:390:4: R0914: Too many local variables (16/15) (too-many-locals)
pybirdai/process_steps/generate_etl/generate_etl.py:405:20: W0107: Unnecessary pass statement (unnecessary-pass)
pybirdai/process_steps/generate_etl/generate_etl.py:400:12: R1702: Too many nested blocks (6/5) (too-many-nested-blocks)
pybirdai/process_steps/generate_etl/generate_etl.py:432:16: W0702: No exception type(s) specified (bare-except)
pybirdai/process_steps/generate_etl/generate_etl.py:511:12: R1732: Consider using 'with' for resource-allocating operations (consider-using-with)
pybirdai/process_steps/generate_etl/generate_etl.py:515:20: W0612: Unused variable 'hierarchy' (unused-variable)
pybirdai/process_steps/generate_etl/generate_etl.py:576:8: W0612: Unused variable 'output_directory' (unused-variable)
pybirdai/process_steps/generate_etl/generate_etl.py:593:15: R1732: Consider using 'with' for resource-allocating operations (consider-using-with)
pybirdai/process_steps/generate_etl/generate_etl.py:19:0: C0411: standard import "math" should be placed before local import "simple_context.SimpleContext" (wrong-import-order)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.ancrdt_transformation.import_website_to_sdd_model_django_ancrdt:[334:428]
==pybirdai.process_steps.website_to_sddmodel.import_website_to_sdd_model_django:[310:404]
                if not parent_member_id:
                    continue

                if hierarchy_id not in hierarchy_cache:
                    hierarchy_cache[hierarchy_id] = ImportWebsiteToSDDModel.find_member_hierarchy_with_id(self,hierarchy_id,context)

                hierarchy = hierarchy_cache[hierarchy_id]
                if hierarchy:
                    domain = hierarchy.domain_id
                    parent_members_child_triples.append((parent_member_id,member_id,domain))
                    parent_members.add(parent_member_id)

        # Process parent-child relationships in batches
        for parent_member_id, member_id, domain in parent_members_child_triples:
            if member_id in parent_members:
                if not any(parent_member_id in d for d in (context.members_that_are_nodes,
                                                         context.member_dictionary,
                                                         context.member_dictionary)):
                    parent_member = MEMBER(
                        name=ImportWebsiteToSDDModel.replace_dots(self, parent_member_id),
                        member_id=ImportWebsiteToSDDModel.replace_dots(self, parent_member_id),
                        maintenance_agency_id=ImportWebsiteToSDDModel.find_maintenance_agency_with_id(self,context,"NODE"),
                        domain_id=domain
                    )
                    parent_members_to_create.append(parent_member)
                    context.member_dictionary[parent_member.member_id] = parent_member
                    if not (parent_member.domain_id is None) and not (parent_member.domain_id == ""):
                        context.member_id_to_domain_map[parent_member] = domain
                        context.member_id_to_member_code_map[parent_member.member_id] = parent_member.member_id

                    context.members_that_are_nodes[parent_member_id] = parent_member
            else:
                member = ImportWebsiteToSDDModel.find_member_with_id(self,member_id,context)
                if member is None:
                    missing_children.append((parent_member_id,member_id))
                elif not any(parent_member_id in d for d in (context.members_that_are_nodes,
                                                          context.member_dictionary,
                                                          context.member_dictionary)):
                    parent_member = MEMBER(
                        name=ImportWebsiteToSDDModel.replace_dots(self, parent_member_id),
                        member_id=ImportWebsiteToSDDModel.replace_dots(self, parent_member_id),
                        maintenance_agency_id=ImportWebsiteToSDDModel.find_maintenance_agency_with_id(self,context,"NODE"),
                        domain_id=domain
                    )
                    parent_members_to_create.append(parent_member)
                    context.members_that_are_nodes[parent_member_id] = parent_member

                    context.member_dictionary[parent_member.member_id] = parent_member
                    if not (parent_member.domain_id is None) and not (parent_member.domain_id == ""):
                        context.member_id_to_domain_map[parent_member] = domain
                        context.member_id_to_member_code_map[parent_member.member_id] = parent_member.member_id

        if context.save_sdd_to_db and parent_members_to_create:
            MEMBER.objects.bulk_create(parent_members_to_create, batch_size=5000,ignore_conflicts=True)  # Increased batch size

        ImportWebsiteToSDDModel.save_missing_children_to_csv(context,missing_children)

    def create_all_member_hierarchies(self, context):
        '''
        Import all member hierarchies with batch processing
        '''
        missing_domains = set()  # Using set for faster lookups
        hierarchies_to_create = []

        with open(f"{context.file_directory}/technical_export/member_hierarchy.csv", encoding='utf-8') as csvfile:
            next(csvfile)  # Skip header more efficiently
            for row in csv.reader(csvfile):
                maintenance_agency_id = row[ColumnIndexes().member_hierarchy_maintenance_agency]
                code = row[ColumnIndexes().member_hierarchy_code]
                id = row[ColumnIndexes().member_hierarchy_id]
                domain_id = row[ColumnIndexes().member_hierarchy_domain_id]
                description = row[ColumnIndexes().member_hierarchy_description]

                maintenance_agency = ImportWebsiteToSDDModel.find_maintenance_agency_with_id(self,context,maintenance_agency_id)
                domain = ImportWebsiteToSDDModel.find_domain_with_id(self,context,domain_id)

                if domain is None:
                    missing_domains.add(domain_id)
                    continue

                hierarchy = MEMBER_HIERARCHY(
                    name=ImportWebsiteToSDDModel.replace_dots(self, id),
                    member_hierarchy_id=ImportWebsiteToSDDModel.replace_dots(self, id),
                    code=code,
                    description=description,
                    maintenance_agency_id=maintenance_agency,
                    domain_id=domain
                )

                if hierarchy.member_hierarchy_id not in context.member_hierarchy_dictionary:
                    hierarchies_to_create.append(hierarchy)
                    context.member_hierarchy_dictionary[hierarchy.member_hierarchy_id] = hierarchy

        if context.save_sdd_to_db and hierarchies_to_create: (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.sqldeveloper_import.import_sqldev_il_to_regdna:[483:562]
==pybirdai.process_steps.sqldeveloper_import.import_sqldev_ldm_to_regdna:[668:750]
                            the_enum = context.enum_map[enum_id]

                            attribute = ELAttribute()


                            attribute.lowerBound = 0
                            attribute.upperBound = 1
                            if the_enum.name == "String":
                                attribute.name = the_attribute_name
                                attribute.eType = context.types.e_string
                                attribute.eAttributeType = context.types.e_string
                            elif the_enum.name.startswith("String_"):
                                attribute.name = the_attribute_name
                                attribute.eType = context.types.e_string
                                attribute.eAttributeType = context.types.e_string
                            elif the_enum.name.startswith("STRNG_"):
                                attribute.name = the_attribute_name
                                attribute.eType = context.types.e_string
                                attribute.eAttributeType = context.types.e_string
                            elif the_enum.name == "Number":
                                attribute.name = the_attribute_name
                                attribute.eType = context.types.e_double
                                attribute.eAttributeType = context.types.e_double
                            elif the_enum.name == "RL_domain":
                                attribute.name = the_attribute_name
                                attribute.eType = context.types.e_double
                                attribute.eAttributeType = context.types.e_double
                            elif the_enum.name.startswith("RL"):
                                attribute.name = the_attribute_name
                                attribute.eType = context.types.e_double
                                attribute.eAttributeType = context.types.e_double
                            elif the_enum.name.startswith("Real_"):
                                attribute.name = the_attribute_name
                                attribute.eType = context.types.e_double
                                attribute.eAttributeType = context.types.e_double
                            elif the_enum.name.startswith("Monetary"):
                                attribute.name = the_attribute_name
                                attribute.eType = context.types.e_int
                                attribute.eAttributeType = context.types.e_int
                            elif the_enum.name.startswith("MNTRY_"):
                                attribute.name = the_attribute_name
                                attribute.eType = context.types.e_int
                                attribute.eAttributeType = context.types.e_int
                            elif the_enum.name.startswith("Non_negative_monetary_amounts_with_2_decimals"):
                                attribute.name = the_attribute_name
                                attribute.eType = context.types.e_int
                                attribute.eAttributeType = context.types.e_int
                            elif the_enum.name.startswith("INTGR"):
                                attribute.name = the_attribute_name
                                attribute.eType = context.types.e_int
                                attribute.eAttributeType = context.types.e_int
                            elif the_enum.name.startswith("YR"):
                                attribute.name = the_attribute_name
                                attribute.eType = context.types.e_int
                                attribute.eAttributeType = context.types.e_int
                            elif the_enum.name.startswith("Non_negative_integers"):
                                attribute.name = the_attribute_name
                                attribute.eType = context.types.e_int
                                attribute.eAttributeType = context.types.e_int
                            elif the_enum.name.startswith("All_possible_dates"):
                                attribute.name = the_attribute_name
                                attribute.eType = context.types.e_date
                                attribute.eAttributeType = context.types.e_date
                            elif the_enum.name.startswith("DT_FLL"):
                                attribute.name = the_attribute_name
                                attribute.eType = context.types.e_date
                                attribute.eAttributeType = context.types.e_date
                            elif the_enum.name.startswith("BLN"):
                                attribute.name = the_attribute_name
                                attribute.eType = context.types.e_date
                                attribute.eAttributeType = context.types.e_boolean



                            # This is a common domain used for String identifiers in BIRD
                            # in SQLDeveloper

                            else:
                                attribute.name = the_attribute_name
                                attribute.eType = the_enum
                                attribute.eAttributeType = the_enum
 (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.ancrdt_transformation.create_executable_joins_ancrdt:[24:60]
==pybirdai.process_steps.ancrdt_transformation.create_joins_meta_data_ancrdt:[32:69]
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("log.log"),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

class DjangoSetup:
    _initialized = False

    @classmethod
    def configure_django(cls):
        """Configure Django settings without starting the application"""
        if cls._initialized:
            return

        try:
            # Set up Django settings module for birds_nest in parent directory
            project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '../../..'))
            sys.path.insert(0, project_root)
            os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'birds_nest.settings')

            # This allows us to use Django models without running the server
            django.setup()

            logger.info("Django configured successfully with settings module: %s",
                       os.environ['DJANGO_SETTINGS_MODULE'])
            cls._initialized = True
        except Exception as e:
            logger.error(f"Django configuration failed: {str(e)}")
            raise
 (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.ancrdt_transformation.import_website_to_sdd_model_django_ancrdt:[485:508]
==pybirdai.process_steps.website_to_sddmodel.import_website_to_sdd_model_django:[464:487]
                    hierarchy = ImportWebsiteToSDDModel.find_member_hierarchy_with_id(self,hierarchy_id,context)
                    if hierarchy is None:
                        print(f"Hierarchy {hierarchy_id} not found")
                        missing_hierarchies.append(hierarchy_id)
                    else:
                        member = ImportWebsiteToSDDModel.find_member_with_id_for_hierarchy(self,member_id,hierarchy,context)
                        if member is None:
                            print(f"Member {member_id} not found in the database for hierarchy {hierarchy_id}")
                            missing_members.append((hierarchy_id,member_id))
                        else:
                            parent_member = ImportWebsiteToSDDModel.find_member_with_id(self,parent_member_id,context)
                            if not (parent_member is None):
                                hierarchy_node = MEMBER_HIERARCHY_NODE()
                                hierarchy_node.member_hierarchy_id = hierarchy
                                hierarchy_node.comparator = comparator
                                hierarchy_node.operator = operator
                                hierarchy_node.member_id = member
                                hierarchy_node.level = int(node_level)
                                hierarchy_node.parent_member_id = parent_member
                                nodes_to_create.append(hierarchy_node)
                                context.member_hierarchy_node_dictionary[hierarchy_id + ":" + member_id] = hierarchy_node

        if context.save_sdd_to_db and nodes_to_create: (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.ancrdt_transformation.import_website_to_sdd_model_django_ancrdt:[291:315]
==pybirdai.process_steps.website_to_sddmodel.import_website_to_sdd_model_django:[261:285]
                    if (ref) and (maintenence_agency == "ECB"):
                        include = True
                    if (not ref) and not (maintenence_agency == "ECB"):
                        include = True

                    if include:
                        variable = VARIABLE(name=ImportWebsiteToSDDModel.replace_dots(self, variable_id))
                        maintenance_agency_id = ImportWebsiteToSDDModel.find_maintenance_agency_with_id(self,context,maintenence_agency)
                        variable.code = code
                        variable.variable_id = ImportWebsiteToSDDModel.replace_dots(self, variable_id)
                        variable.name = name
                        domain = ImportWebsiteToSDDModel.find_domain_with_id(self, context, domain_id)
                        variable.domain_id = domain
                        variable.description = description
                        variable.maintenance_agency_id = maintenance_agency_id

                        variables_to_create.append(variable)
                        context.variable_dictionary[variable.variable_id] = variable
                        context.variable_to_domain_map[variable.variable_id] = domain
                        context.variable_to_long_names_map[variable.variable_id] = name
                        if not((primary_concept == "") or (primary_concept == None)):
                            context.variable_to_primary_concept_map[variable.variable_id] = primary_concept

        if context.save_sdd_to_db and variables_to_create: (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.pybird.orchestration:[2137:2167]
==pybirdai.process_steps.pybird.orchestration_original:[33:63]
		self._ensure_references_set(theObject)

	def _ensure_references_set(self, theObject):
		"""
		Ensure that all table references are properly set for the object.
		This is called both during full initialization and when initialization is skipped.
		"""
		references = [method for method in dir(theObject.__class__) if not callable(
		getattr(theObject.__class__, method)) and not method.startswith('__')]
		for eReference in references:
			if eReference.endswith("Table"):
				# Only set the reference if it's currently None
				if getattr(theObject, eReference) is None:
					from django.apps import apps
					table_name = eReference.split('_Table')[0]
					relevant_model = None
					try:
						relevant_model = apps.get_model('pybirdai',table_name)
					except LookupError:
						print("LookupError: " + table_name)

					if relevant_model:
						print("relevant_model: " + str(relevant_model))
						newObject = relevant_model.objects.all()
						print("newObject: " + str(newObject))
						if newObject:
							setattr(theObject,eReference,newObject)
							CSVConverter.persist_object_as_csv(newObject,True);

					else: (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.ancrdt_transformation.import_website_to_sdd_model_django_ancrdt:[91:116]
==pybirdai.process_steps.website_to_sddmodel.import_website_to_sdd_model_django:[78:102]
        header_skipped = False
        agencies_to_create = []

        with open(file_location, encoding='utf-8') as csvfile:
            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
            for row in filereader:
                if not header_skipped:
                    header_skipped = True
                else:
                    code = row[ColumnIndexes().maintenance_agency_code]
                    description = row[ColumnIndexes().maintenance_agency_description]
                    id = row[ColumnIndexes().maintenance_agency_id]
                    name = row[ColumnIndexes().maintenance_agency_name]

                    maintenance_agency = MAINTENANCE_AGENCY(
                        name=ImportWebsiteToSDDModel.replace_dots(self, id))
                    maintenance_agency.code = code
                    maintenance_agency.description = description
                    maintenance_agency.maintenance_agency_id = ImportWebsiteToSDDModel.replace_dots(self, id)

                    agencies_to_create.append(maintenance_agency)
                    context.agency_dictionary[id] = maintenance_agency


        if context.save_sdd_to_db and agencies_to_create: (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.pybird.orchestration:[387:418]
==pybirdai.process_steps.pybird.orchestration_original:[33:62]
			self._ensure_references_set(theObject)

	def _ensure_references_set(self, theObject):
		"""
		Ensure that all table references are properly set for the object.
		This is called both during full initialization and when initialization is skipped.
		"""
		references = [method for method in dir(theObject.__class__) if not callable(
		getattr(theObject.__class__, method)) and not method.startswith('__')]
		for eReference in references:
			if eReference.endswith("Table"):
				# Only set the reference if it's currently None
				if getattr(theObject, eReference) is None:
					from django.apps import apps
					table_name = eReference.split('_Table')[0]
					relevant_model = None
					try:
						relevant_model = apps.get_model('pybirdai',table_name)
					except LookupError:
						print("LookupError: " + table_name)

					if relevant_model:
						print("relevant_model: " + str(relevant_model))
						newObject = relevant_model.objects.all()
						print("newObject: " + str(newObject))
						if newObject:
							setattr(theObject,eReference,newObject)
							# Original CSV persistence
							CSVConverter.persist_object_as_csv(newObject,True);

							# Enhanced lineage tracking - track when tables are created but distinguish from usage tracking (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.sqldeveloper_import.import_sqldev_il_to_regdna:[150:173]
==pybirdai.process_steps.sqldeveloper_import.import_sqldev_ldm_to_regdna:[549:568]
                    header_skipped = True
                else:
                    try:
                        counter = counter+1
                        enum_id = row[0]
                        adapted_enum_name = Utils.make_valid_id_for_literal( row[3])
                        value = row[4]
                        adapted_value = Utils.make_valid_id( value)
                        try:
                            the_enum = context.enum_map[enum_id]
                            new_adapted_value = Utils.unique_value(
                                the_enum, adapted_value)
                            #new_adapted_value = Utils.special_cases(
                            #    new_adapted_value, counter)
                            new_adapted_name = Utils.unique_name(
                                the_enum, adapted_enum_name)

                            enum_literal = ELEnumLiteral()
                            enum_literal.name = new_adapted_value
                            enum_literal.literal = new_adapted_name
                            enum_literal.value = counter
                            the_enum.eLiterals.extend([enum_literal])
 (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.pybird.orchestration:[2169:2221]
==pybirdai.process_steps.pybird.orchestration_original:[65:106]
						operations = [method for method in dir(newObject.__class__) if callable(
							getattr(newObject.__class__, method)) and not method.startswith('__')]

						for operation in operations:
							if operation == "init":
								try:
									getattr(newObject, operation)()
								except:
									print (" could not call function called " + operation)

						setattr(theObject,eReference,newObject)

	@classmethod
	def reset_initialization(cls):
		"""
		Reset the initialization tracking.
		This can be useful for testing or when re-initialization is required.
		"""
		cls._initialized_objects.clear()
		print("Initialization tracking has been reset.")

	@classmethod
	def is_initialized(cls, obj):
		"""
		Check if an object has been initialized.

		Args:
			obj: The object to check

		Returns:
			bool: True if the object has been initialized, False otherwise
		"""
		return id(obj) in cls._initialized_objects

	@staticmethod
	def createObjectFromReferenceType(eReference):
		try:
			cls = getattr(importlib.import_module('pybirdai.process_steps.filter_code.output_tables'), eReference)
			new_object = cls()
			return new_object;
		except:
			print("Error: " + eReference)


# Factory function to create the appropriate Orchestration instance
def create_orchestration():
	"""
	Factory function that returns the appropriate Orchestration instance
	based on the context configuration.
	"""
	from pybirdai.context.context import Context
 (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.ancrdt_transformation.create_python_django_transformations_ancrdt:[56:75]
==pybirdai.process_steps.pybird.create_python_django_transformations:[61:80]
                domain = variable.domain_id.domain_id
                file.write('\t@lineage(dependencies={"unionOfLayers.'+ variable.variable_id +'"})\n')
                if domain == 'String':
                    file.write('\tdef ' + variable.variable_id + '(self) -> str:\n')
                elif domain == 'Integer':
                    file.write('\tdef ' + variable.variable_id + '(self) -> int:\n')
                elif domain == 'Date':
                    file.write('\tdef ' + variable.variable_id + '(self) -> datetime:\n')
                elif domain == 'Float':
                    file.write('\tdef ' + variable.variable_id + '(self) -> float:\n')
                elif domain == 'Boolean':
                    file.write('\tdef ' + variable.variable_id + '(self) -> bool:\n')
                else:
                    file.write('\tdef ' + variable.variable_id + '(self) -> str:\n')
                    file.write('\t\t\'\'\' return string from ' + domain + ' enumeration \'\'\'\n')

                file.write('\t\treturn self.unionOfLayers.' + variable.variable_id + '()\n')
                file.write('\n')
            file.write('\n') (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.joins_meta_data.create_joins_meta_data:[99:117]
==pybirdai.process_steps.joins_meta_data.create_joins_meta_data_combinations:[179:196]
        tables_for_main_category_map = (
            context.tables_for_main_category_map_finrep
            if framework == "FINREP_REF"
            else context.tables_for_main_category_map_ae
        )
        join_for_products_to_linked_tables_map = (
            context.join_for_products_to_linked_tables_map_finrep
            if framework == "FINREP_REF"
            else context.join_for_products_to_linked_tables_map_ae
        )
        table_and_part_tuple_map = (
            context.table_and_part_tuple_map_finrep
            if framework == "FINREP_REF"
            else context.table_and_part_tuple_map_ae
        )
        cube_links_to_create = []  # New list to collect CUBE_LINK objects
        cube_structure_item_links_to_create = []  # New list for CUBE_STRUCTURE_ITEM_LINK objects (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.joins_meta_data.create_joins_meta_data:[147:167]
==pybirdai.process_steps.joins_meta_data.create_joins_meta_data_combinations:[241:262]
                                except KeyError:
                                    pass

                                for extra_table in extra_linked_tables:
                                    if extra_table not in extra_tables:
                                        extra_tables.append(extra_table)

                            for extra_table in extra_tables:
                                if extra_table not in linked_tables_list:
                                    linked_tables_list.append(extra_table)

                            for the_table in linked_tables_list:
                                the_input_table = self.find_input_layer_cube(
                                    sdd_context, the_table, framework
                                )
                                if the_input_table:
                                    input_entity_list.append(the_input_table)

                            if join_for_product[0] == table:
                                for input_entity in input_entity_list:
                                    # print(f"input_entity:{input_entity}") (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.ancrdt_transformation.ancrdt_importer:[21:48]
==pybirdai.process_steps.ancrdt_transformation.create_joins_meta_data_ancrdt:[41:69]
logger = logging.getLogger(__name__)

class DjangoSetup:
    _initialized = False

    @classmethod
    def configure_django(cls):
        """Configure Django settings without starting the application"""
        if cls._initialized:
            return

        try:
            # Set up Django settings module for birds_nest in parent directory
            project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '../../..'))
            sys.path.insert(0, project_root)
            os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'birds_nest.settings')

            # This allows us to use Django models without running the server
            django.setup()

            logger.info("Django configured successfully with settings module: %s",
                       os.environ['DJANGO_SETTINGS_MODULE'])
            cls._initialized = True
        except Exception as e:
            logger.error(f"Django configuration failed: {str(e)}")
            raise

 (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.ancrdt_transformation.create_python_django_transformations_ancrdt:[152:171]
==pybirdai.process_steps.pybird.create_python_django_transformations:[155:174]
                domain = variable.domain_id.domain_id
                if domain == 'String':
                    file.write('\tdef ' + variable.variable_id + '() -> str:\n')
                elif domain == 'Integer':
                    file.write('\tdef ' + variable.variable_id + '() -> int:\n')
                elif domain == 'Date':
                    file.write('\tdef ' + variable.variable_id + '() -> datetime:\n')
                elif domain == 'Float':
                    file.write('\tdef ' + variable.variable_id + '() -> float:\n')
                elif domain == 'Boolean':
                    file.write('\tdef ' + variable.variable_id + '() -> bool:\n')
                else:
                    file.write('\tdef ' + variable.variable_id + '() -> str:\n')
                    file.write('\t\t\'\'\' return string from ' + domain + ' enumeration \'\'\'\n')

                file.write('\t\tpass')
                file.write('\n')

 (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.ancrdt_transformation.create_python_django_transformations_ancrdt:[259:276]
==pybirdai.process_steps.pybird.create_python_django_transformations:[258:276]
                    join_id = report_and_join[1]
                    file.write("\t" + join_id.replace(' ','_') + "s = []# " + join_id.replace(' ','_') + "[]\n")
                    file.write("\tdef calc_" + join_id.replace(' ','_') + "s(self) :\n")
                    file.write("\t\titems = [] # " + join_id.replace(' ','_') + "[\n")
                    file.write("\t\t# Join up any refered tables that you need to join\n")
                    file.write("\t\t# loop through the main table\n")
                    file.write("\t\t# set any references you want to on the new Item so that it can refer to themin operations\n")
                    file.write("\t\treturn items\n")
                    file.write("\tdef init(self):\n")
                    file.write("\t\tOrchestration().init(self)\n")
                    file.write("\t\tself." + join_id.replace(' ','_') + "s = []\n")
                    file.write("\t\tself." + join_id.replace(' ','_') + "s.extend(self.calc_" + join_id.replace(' ','_') + "s())\n")
                    file.write("\t\tCSVConverter.persist_object_as_csv(self,True)\n")

                    file.write("\t\treturn None\n")
                    file.write("\n")
 (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.ancrdt_transformation.create_python_django_transformations_ancrdt:[58:72]
==pybirdai.process_steps.pybird.create_python_django_transformations:[123:137]
                if domain == 'String':
                    file.write('\tdef ' + variable.variable_id + '(self) -> str:\n')
                elif domain == 'Integer':
                    file.write('\tdef ' + variable.variable_id + '(self) -> int:\n')
                elif domain == 'Date':
                    file.write('\tdef ' + variable.variable_id + '(self) -> datetime:\n')
                elif domain == 'Float':
                    file.write('\tdef ' + variable.variable_id + '(self) -> float:\n')
                elif domain == 'Boolean':
                    file.write('\tdef ' + variable.variable_id + '(self) -> bool:\n')
                else:
                    file.write('\tdef ' + variable.variable_id + '(self) -> str:\n')
                    file.write('\t\t\'\'\' return string from ' + domain + ' enumeration \'\'\'\n')
 (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.ancrdt_transformation.create_python_django_transformations_ancrdt:[121:135]
==pybirdai.process_steps.pybird.create_python_django_transformations:[63:77]
                if domain == 'String':
                    file.write('\tdef ' + variable.variable_id + '(self) -> str:\n')
                elif domain == 'Integer':
                    file.write('\tdef ' + variable.variable_id + '(self) -> int:\n')
                elif domain == 'Date':
                    file.write('\tdef ' + variable.variable_id + '(self) -> datetime:\n')
                elif domain == 'Float':
                    file.write('\tdef ' + variable.variable_id + '(self) -> float:\n')
                elif domain == 'Boolean':
                    file.write('\tdef ' + variable.variable_id + '(self) -> bool:\n')
                else:
                    file.write('\tdef ' + variable.variable_id + '(self) -> str:\n')
                    file.write('\t\t\'\'\' return string from ' + domain + ' enumeration \'\'\'\n')
 (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.joins_meta_data.create_joins_meta_data:[117:130]
==pybirdai.process_steps.joins_meta_data.create_joins_meta_data_combinations:[198:213]
        try:
            report_template = generated_output_layer.name
            main_categories = context.report_to_main_category_map[report_template]
            for mc in main_categories:
                try:
                    tables = tables_for_main_category_map[mc]
                    for table in tables:
                        inputLayerTable = self.find_input_layer_cube(
                            sdd_context, table, framework
                        )
                        join_for_products = table_and_part_tuple_map[mc]

                        for join_for_product in join_for_products: (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.dpm_integration.mapping_functions.cell_position:[45:57]
==pybirdai.process_steps.dpm_integration.mapping_functions.ordinate_categorisation:[129:141]
    if start_index_after_last and "ID" in data.dtype.names and len(data) > 0:
        max_id = max(int(float(row["ID"])) for row in data if str(row["ID"]) != 'nan')
        start_idx = max_id + 1 if max_id else 0
        ids = list(range(start_idx, start_idx + len(data)))
        for i, row in enumerate(data):
            data[i]["ID"] = ids[i]
    else:
        if "ID" in data.dtype.names:
            data = drop_fields(data, "ID")
        ids = list(range(len(data)))
        data = add_field(data, "ID", ids, dtype='i8')
 (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.ancrdt_transformation.create_python_django_transformations_ancrdt:[227:240]
==pybirdai.process_steps.pybird.create_python_django_transformations:[224:237]
                        primary_cubes_added = []
                        if len(cube_structure_item_links) == 0:
                            file.write("\tpass\n")
                        for cube_structure_item_link in cube_structure_item_links:
                            if cube_structure_item_link.cube_link_id.primary_cube_id.cube_id not in primary_cubes_added:
                                file.write("\t" + cube_structure_item_link.cube_link_id.primary_cube_id.cube_id  + " = None # " + cube_structure_item_link.cube_link_id.primary_cube_id.cube_id + "\n")
                                primary_cubes_added.append(cube_structure_item_link.cube_link_id.primary_cube_id.cube_id)
                        for cube_structure_item_link in cube_structure_item_links:
                            file.write('\t@lineage(dependencies={"'+ cube_structure_item_link.cube_link_id.primary_cube_id.cube_id + '.' + cube_structure_item_link.primary_cube_variable_code.variable_id.variable_id +'"})\n')
                            file.write("\tdef " + cube_structure_item_link.foreign_cube_variable_code.variable_id.variable_id + "(self):\n")
                            file.write("\t\treturn self." +  cube_structure_item_link.cube_link_id.primary_cube_id.cube_id + "." + cube_structure_item_link.primary_cube_variable_code.variable_id.variable_id + "\n")

 (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.ancrdt_transformation.import_website_to_sdd_model_django_ancrdt:[186:198]
==pybirdai.process_steps.website_to_sddmodel.import_website_to_sdd_model_django:[171:184]
                        domain.code = code
                        domain.description = description
                        domain.domain_id = ImportWebsiteToSDDModel.replace_dots(self, domain_id)
                        domain.name = domain_name
                        domain.is_enumerated = True if is_enumerated else False
                        domain.is_reference = True if is_reference else False

                        domains_to_create.append(domain)
                        if ref:
                            context.domain_dictionary[domain.domain_id] = domain
                        else:
                            context.domain_dictionary[domain.domain_id] = domain
 (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.ancrdt_transformation.import_website_to_sdd_model_django_ancrdt:[69:90]
==pybirdai.process_steps.website_to_sddmodel.import_website_to_sdd_model_django:[56:77]
        ImportWebsiteToSDDModel.delete_mapping_warnings_files(self, sdd_context)
        ImportWebsiteToSDDModel.create_all_variable_mappings(self, sdd_context)
        ImportWebsiteToSDDModel.create_all_variable_mapping_items(self, sdd_context)
        ImportWebsiteToSDDModel.create_member_mappings(self, sdd_context)
        ImportWebsiteToSDDModel.create_all_member_mappings_items(self, sdd_context)
        ImportWebsiteToSDDModel.create_all_mapping_definitions(self, sdd_context)
        ImportWebsiteToSDDModel.create_all_mapping_to_cubes(self, sdd_context)

    def import_hierarchies_from_sdd(self, sdd_context):
        '''
        Import hierarchies from CSV file
        '''
        ImportWebsiteToSDDModel.delete_hierarchy_warnings_files(self, sdd_context)
        ImportWebsiteToSDDModel.create_all_member_hierarchies(self, sdd_context)
        ImportWebsiteToSDDModel.create_all_parent_members_with_children_locally(self, sdd_context)
        ImportWebsiteToSDDModel.create_all_member_hierarchies_nodes(self, sdd_context)

    def create_maintenance_agencies(self, context):
        '''
        Import maintenance agencies from CSV file using bulk create
        ''' (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.pybird.orchestration:[475:512]
==pybirdai.process_steps.pybird.orchestration_original:[75:106]
						setattr(theObject,eReference,newObject)

	@classmethod
	def reset_initialization(cls):
		"""
		Reset the initialization tracking.
		This can be useful for testing or when re-initialization is required.
		"""
		cls._initialized_objects.clear()
		print("Initialization tracking has been reset.")

	@classmethod
	def is_initialized(cls, obj):
		"""
		Check if an object has been initialized.

		Args:
			obj: The object to check

		Returns:
			bool: True if the object has been initialized, False otherwise
		"""
		return id(obj) in cls._initialized_objects

	def createObjectFromReferenceType(eReference):
		try:
			cls = getattr(importlib.import_module('pybirdai.process_steps.filter_code.output_tables'), eReference)
			new_object = cls()
			return new_object;
		except:
			print("Error: " + eReference) (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.sqldeveloper_import.import_sqldev_il_to_regdna:[124:135]
==pybirdai.process_steps.sqldeveloper_import.import_sqldev_ldm_to_regdna:[523:534]
                    header_skipped = True
                else:
                    counter = counter+1
                    enum_id = row[0]
                    enum_name = row[1]
                    synonym = row[3]
                    adapted_enum_name = Utils.make_valid_id(synonym)
                    the_enum = ELEnum()
                    the_enum.name = adapted_enum_name + "_domain"
                    # maintain a map of enum IDS to ELEnum objects
                    context.enum_map[enum_id] = the_enum (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.joins_meta_data.member_hierarchy_service:[28:39]
==pybirdai.process_steps.pybird.create_executable_filters:[241:253]
        for node in sdd_context.member_hierarchy_node_dictionary.values():
            if node.parent_member_id and node.parent_member_id != '':
                sdd_context.members_that_are_nodes.add(node.parent_member_id)
                member_plus_hierarchy = f"{node.parent_member_id.member_id}:{node.member_hierarchy_id.member_hierarchy_id}"

                if member_plus_hierarchy not in sdd_context.member_plus_hierarchy_to_child_literals:
                    sdd_context.member_plus_hierarchy_to_child_literals[member_plus_hierarchy] = [node.member_id]
                else:
                    if node.member_id not in sdd_context.member_plus_hierarchy_to_child_literals[member_plus_hierarchy]:
                        sdd_context.member_plus_hierarchy_to_child_literals[member_plus_hierarchy].append(node.member_id)
 (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.ancrdt_transformation.import_website_to_sdd_model_django_ancrdt:[350:359]
==pybirdai.process_steps.website_to_sddmodel.import_website_to_sdd_model_django:[346:355]
                                                          context.member_dictionary,
                                                          context.member_dictionary)):
                    parent_member = MEMBER(
                        name=ImportWebsiteToSDDModel.replace_dots(self, parent_member_id),
                        member_id=ImportWebsiteToSDDModel.replace_dots(self, parent_member_id),
                        maintenance_agency_id=ImportWebsiteToSDDModel.find_maintenance_agency_with_id(self,context,"NODE"),
                        domain_id=domain
                    )
                    parent_members_to_create.append(parent_member) (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.ancrdt_transformation.import_website_to_sdd_model_django_ancrdt:[370:379]
==pybirdai.process_steps.website_to_sddmodel.import_website_to_sdd_model_django:[326:335]
                                                         context.member_dictionary,
                                                         context.member_dictionary)):
                    parent_member = MEMBER(
                        name=ImportWebsiteToSDDModel.replace_dots(self, parent_member_id),
                        member_id=ImportWebsiteToSDDModel.replace_dots(self, parent_member_id),
                        maintenance_agency_id=ImportWebsiteToSDDModel.find_maintenance_agency_with_id(self,context,"NODE"),
                        domain_id=domain
                    )
                    parent_members_to_create.append(parent_member) (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.ancrdt_transformation.create_joins_meta_data_ancrdt:[55:69]
==pybirdai.process_steps.automode.database_setup_first_use:[37:50]
            sys.path.insert(0, project_root)
            os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'birds_nest.settings')

            # This allows us to use Django models without running the server
            django.setup()

            logger.info("Django configured successfully with settings module: %s",
                       os.environ['DJANGO_SETTINGS_MODULE'])
            cls._initialized = True
        except Exception as e:
            logger.error(f"Django configuration failed: {str(e)}")
            raise
 (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.joins_meta_data.create_joins_meta_data:[173:181]
==pybirdai.process_steps.joins_meta_data.create_joins_meta_data_combinations:[269:278]
                                    if primary_cube:
                                        cube_link.primary_cube_id = primary_cube
                                        cube_link.cube_link_id = (
                                            f"{report_template}:"
                                            f"{input_entity.cube_structure_id}:{join_for_product[1]}"
                                        )
                                    else:
                                        cube_link.cube_link_id = f"{input_entity.cube_structure_id}:{join_for_product[1]}"
                                        # print(f"cube_link.primary_cube_id not found for {table}") (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.joins_meta_data.member_hierarchy_service:[74:84]
==pybirdai.process_steps.pybird.create_executable_filters:[286:296]
        self._member_list_cache[cache_key] = return_list
        return return_list.copy()

    def get_member_list_considering_hierarchy(self, sdd_context, member, hierarchy, member_list):
        key = f"{member.member_id}:{hierarchy}"
        try:
            child_members = sdd_context.member_plus_hierarchy_to_child_literals[key]
            for item in child_members:
                if item is not None and item not in member_list:
                    if not self.is_member_a_node(sdd_context, item): (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.sqldeveloper_import.import_sqldev_il_to_regdna:[636:646]
==pybirdai.process_steps.sqldeveloper_import.import_sqldev_ldm_to_regdna:[825:835]
                        try:
                            the_class = context.classes_map[source_id]
                        except KeyError:
                            print("missing class1: " + source_id)

                        try:
                            target_class = context.classes_map[target_id]
                        except KeyError:
                            print("missing target class: " + target_id)
 (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.ancrdt_transformation.import_website_to_sdd_model_django_ancrdt:[249:260]
==pybirdai.process_steps.website_to_sddmodel.import_website_to_sdd_model_django:[224:235]
                        member.maintenance_agency_id = maintenance_agency
                        domain = ImportWebsiteToSDDModel.find_domain_with_id(self, context, domain_id)
                        member.domain_id = domain

                        members_to_create.append(member)
                        context.member_dictionary[member.member_id] = member

                        if not (domain_id is None) and not (domain_id == ""):
                            context.member_id_to_domain_map[member] = domain
                            context.member_id_to_member_code_map[member.member_id] = code
 (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.ancrdt_transformation.create_python_django_transformations_ancrdt:[198:206]
==pybirdai.process_steps.pybird.create_python_django_transformations:[195:203]
                            file.write("\t\t\tnewItem.base = item\n")
                            file.write("\t\t\titems.append(newItem)\n")
                            join_ids_added.append(cube_link.join_identifier)
            file.write("\t\treturn items\n")
            file.write("\n")

            file.write("\tdef init(self):\n")
            file.write("\t\tOrchestration().init(self)\n") (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.sqldeveloper_import.import_sqldev_il_to_regdna:[431:438]
==pybirdai.process_steps.sqldeveloper_import.import_sqldev_ldm_to_regdna:[313:320]
                            attribute = ELAttribute()
                            attribute.name = pk_name
                            attribute.eType = context.types.e_string
                            attribute.eAttributeType = context.types.e_string
                            attribute.iD = True
                            attribute.lowerBound = 0
                            attribute.upperBound = 1 (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.ancrdt_transformation.import_website_to_sdd_model_django_ancrdt:[318:328]
==pybirdai.process_steps.website_to_sddmodel.import_website_to_sdd_model_django:[288:298]
        print("Creating all parent members with children locally")
        parent_members = set()  # Using set for faster lookups
        parent_members_to_create = []
        parent_members_child_triples = []
        missing_children = []

        # Pre-fetch all hierarchies for faster lookup
        hierarchy_cache = {}

        with open(f"{context.file_directory}/technical_export/member_hierarchy_node.csv", encoding='utf-8') as csvfile: (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.ancrdt_transformation.import_website_to_sdd_model_django_ancrdt:[164:172]
==pybirdai.process_steps.website_to_sddmodel.import_website_to_sdd_model_django:[150:158]
                    code = row[ColumnIndexes().domain_domain_id_index]
                    data_type = row[ColumnIndexes().domain_domain_data_type]
                    description = row[ColumnIndexes().domain_domain_description]
                    domain_id = row[ColumnIndexes().domain_domain_true_id]
                    is_enumerated = row[ColumnIndexes().domain_domain_is_enumerated]
                    is_reference = row[ColumnIndexes().domain_domain_is_reference]
                    domain_name = row[ColumnIndexes().domain_domain_name_index]
 (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.generate_test_data.enrich_ldm_with_il_links_from_fe:[43:51]
==pybirdai.process_steps.sqldeveloper_import.import_sqldev_ldm_to_regdna:[250:257]
        header_skipped = False
        with open(file_location,  encoding='utf-8') as csvfile:
            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
            for row in filereader:
                if not header_skipped:
                    header_skipped = True
                else: (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.ancrdt_transformation.ancrdt_importer:[60:73]
==pybirdai.process_steps.ancrdt_transformation.create_executable_joins_ancrdt:[63:84]
    DjangoSetup.configure_django()
    path = os.path.join(settings.BASE_DIR, 'birds_nest')

    @staticmethod
    def create_python_joins_from_db(logger=logger):
        """Execute the process of creating generation rules from the database when the app is ready."""

        from pybirdai.process_steps.input_model.import_database_to_sdd_model import (
            ImportDatabaseToSDDModel
        )
        from pybirdai.context.sdd_context_django import SDDContext
        from pybirdai.context.context import Context
        from pybirdai.process_steps.ancrdt_transformation.create_python_django_transformations_ancrdt import (
            CreatePythonTransformations
        )

        base_dir = settings.BASE_DIR
        sdd_context = SDDContext()
        sdd_context.file_directory = os.path.join(base_dir, 'resources')
        sdd_context.output_directory = os.path.join(base_dir, 'results')
 (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.pybird.orchestration:[432:442]
==pybirdai.process_steps.pybird.orchestration_original:[65:72]
						operations = [method for method in dir(newObject.__class__) if callable(
							getattr(newObject.__class__, method)) and not method.startswith('__')]

						for operation in operations:
							if operation == "init":
								try:
									getattr(newObject, operation)()

									# Check if lineage tracking is enabled and track data after initialization
									from pybirdai.annotations.decorators import _lineage_context (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.ancrdt_transformation.create_python_django_transformations_ancrdt:[119:139]
==pybirdai.process_steps.pybird.create_python_django_transformations:[121:141]
                domain = variable.domain_id.domain_id
                file.write('\t@lineage(dependencies={"base.'+ variable.variable_id +'"})\n')
                if domain == 'String':
                    file.write('\tdef ' + variable.variable_id + '(self) -> str:\n')
                elif domain == 'Integer':
                    file.write('\tdef ' + variable.variable_id + '(self) -> int:\n')
                elif domain == 'Date':
                    file.write('\tdef ' + variable.variable_id + '(self) -> datetime:\n')
                elif domain == 'Float':
                    file.write('\tdef ' + variable.variable_id + '(self) -> float:\n')
                elif domain == 'Boolean':
                    file.write('\tdef ' + variable.variable_id + '(self) -> bool:\n')
                else:
                    file.write('\tdef ' + variable.variable_id + '(self) -> str:\n')
                    file.write('\t\t\'\'\' return string from ' + domain + ' enumeration \'\'\'\n')

                file.write('\t\treturn self.base.' + variable.variable_id + '()')
                file.write('\n')

 (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.sqldeveloper_import.import_sqldev_il_to_regdna:[393:402]
==pybirdai.process_steps.sqldeveloper_import.import_sqldev_ldm_to_regdna:[760:767]
                                attribute.eType = Utils.get_ecore_datatype_for_datatype(
                                    self)
                                attribute.eAttributeType = Utils.get_ecore_datatype_for_datatype(
                                    self)

                            except KeyError:
                                print("missing datatype: ") (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.sqldeveloper_import.import_sqldev_il_to_regdna:[173:188]
==pybirdai.process_steps.sqldeveloper_import.import_sqldev_ldm_to_regdna:[569:584]
                        except KeyError:
                            print("missing domain: " + enum_id)

                    except IndexError:
                        print(
                            "row in DM_Domain_AVT.csv skipped  due to improper formatting at row number")
                        print(counter)

    def create_il_types_map(self, context):
        '''
        for each type in the IL, create a map of typeID to type name
        '''
        # for each logicalDatatype for orcle 12c, make a Datatype if we have an
        # equivalent
 (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.ancrdt_transformation.import_website_to_sdd_model_django_ancrdt:[603:635]
==pybirdai.process_steps.website_to_sddmodel.import_website_to_sdd_model_django:[582:614]
        header_skipped = False
        ordinates_to_create = []

        with open(file_location, encoding='utf-8') as csvfile:
            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
            for row in filereader:
                if not header_skipped:
                    header_skipped = True
                else:
                    axis_ordinate_id = row[ColumnIndexes().axis_ordinate_axis_ordinate_id]
                    axis_ordinate_is_abstract_header = row[ColumnIndexes().axis_ordinate_is_abstract_header]
                    axis_ordinate_code = row[ColumnIndexes().axis_ordinate_code]
                    axis_ordinate_order = row[ColumnIndexes().axis_ordinate_order]
                    axis_ordinate_path = row[ColumnIndexes().axis_ordinate_path]
                    axis_ordinate_axis_id = row[ColumnIndexes().axis_ordinate_axis_id]
                    axis_ordinate_parent_axis_ordinate_id = row[ColumnIndexes().axis_ordinate_parent_axis_ordinate_id]
                    axis_ordinate_name = row[ColumnIndexes().axis_ordinate_name]
                    axis_ordinate_description = row[ColumnIndexes().axis_ordinate_description]

                    axis_ordinate = AXIS_ORDINATE(
                        name=ImportWebsiteToSDDModel.replace_dots(self, axis_ordinate_id))
                    axis_ordinate.axis_ordinate_id = ImportWebsiteToSDDModel.replace_dots(self, axis_ordinate_id)
                    axis_ordinate.code = axis_ordinate_code
                    axis_ordinate.path = axis_ordinate_path
                    axis_ordinate.axis_id = ImportWebsiteToSDDModel.find_axis_with_id(self, context, ImportWebsiteToSDDModel.replace_dots(self,axis_ordinate_axis_id))
                    axis_ordinate.name = axis_ordinate_name
                    axis_ordinate.description = axis_ordinate_description

                    ordinates_to_create.append(axis_ordinate)
                    context.axis_ordinate_dictionary[axis_ordinate.axis_ordinate_id] = axis_ordinate

        if context.save_sdd_to_db and ordinates_to_create: (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.ancrdt_transformation.import_website_to_sdd_model_django_ancrdt:[568:596]
==pybirdai.process_steps.website_to_sddmodel.import_website_to_sdd_model_django:[547:575]
        header_skipped = False
        axes_to_create = []

        with open(file_location, encoding='utf-8') as csvfile:
            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
            for row in filereader:
                if not header_skipped:
                    header_skipped = True
                else:
                    axis_id = row[ColumnIndexes().axis_id]
                    axis_orientation = row[ColumnIndexes().axis_orientation]
                    axis_order = row[ColumnIndexes().axis_order]
                    axis_name = row[ColumnIndexes().axis_name]
                    axis_description = row[ColumnIndexes().axis_description]
                    axis_table_id = row[ColumnIndexes().axis_table_id]
                    axis_is_open_axis = row[ColumnIndexes().axis_is_open_axis]

                    axis = AXIS(
                        name=ImportWebsiteToSDDModel.replace_dots(self, axis_id))
                    axis.axis_id = ImportWebsiteToSDDModel.replace_dots(self, axis_id)
                    axis.orientation = axis_orientation
                    axis.description = axis_description
                    axis.table_id = ImportWebsiteToSDDModel.find_table_with_id(self, context, axis_table_id)

                    axes_to_create.append(axis)
                    context.axis_dictionary[axis.axis_id] = axis

        if context.save_sdd_to_db and axes_to_create: (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.ancrdt_transformation.import_website_to_sdd_model_django_ancrdt:[529:561]
==pybirdai.process_steps.website_to_sddmodel.import_website_to_sdd_model_django:[508:540]
        header_skipped = False
        tables_to_create = []

        with open(file_location, encoding='utf-8') as csvfile:
            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
            for row in filereader:
                if not header_skipped:
                    header_skipped = True
                else:
                    table_id = row[ColumnIndexes().table_table_id]
                    display_name = row[ColumnIndexes().table_table_name]
                    code = row[ColumnIndexes().table_code]
                    description = row[ColumnIndexes().table_description]
                    maintenance_agency_id = row[ColumnIndexes().table_maintenance_agency_id]
                    version = row[ColumnIndexes().table_version]
                    valid_from = row[ColumnIndexes().table_valid_from]
                    valid_to = row[ColumnIndexes().table_valid_to]

                    table = TABLE(
                        name=ImportWebsiteToSDDModel.replace_dots(self, table_id))
                    table.table_id = ImportWebsiteToSDDModel.replace_dots(self, table_id)
                    table.name = display_name
                    table.code = code
                    table.description = description
                    maintenance_agency = ImportWebsiteToSDDModel.find_maintenance_agency_with_id(self,context,maintenance_agency_id)
                    table.maintenance_agency_id = maintenance_agency
                    table.version = version

                    tables_to_create.append(table)
                    context.report_tables_dictionary[table.table_id] = table

        if context.save_sdd_to_db and tables_to_create: (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.ancrdt_transformation.import_website_to_sdd_model_django_ancrdt:[515:528]
==pybirdai.process_steps.website_to_sddmodel.import_website_to_sdd_model_django:[494:507]
        with open(filename, 'w', newline='') as csvfile:
            writer = csv.writer(csvfile)
            writer.writerows(missing_hierarchies)

    def find_member_with_id_for_hierarchy(self,member_id,hierarchy,context):
        domain = hierarchy.domain_id
        member = MEMBER.objects.filter(domain_id=domain,member_id=member_id).first()
        return member

    def create_report_tables(self, context):
        '''
        Import all tables from the rendering package CSV file using bulk create
        ''' (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.ancrdt_transformation.import_website_to_sdd_model_django_ancrdt:[465:472]
==pybirdai.process_steps.website_to_sddmodel.import_website_to_sdd_model_django:[441:448]
        header_skipped = False
        missing_members = []
        missing_hierarchies = []
        nodes_to_create = []

        with open(file_location, encoding='utf-8') as csvfile:
            filereader = csv.reader(csvfile, delimiter=',', quotechar='"') (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.ancrdt_transformation.import_website_to_sdd_model_django_ancrdt:[272:289]
==pybirdai.process_steps.website_to_sddmodel.import_website_to_sdd_model_django:[243:260]
        header_skipped = False
        variables_to_create = []

        with open(file_location, encoding='utf-8') as csvfile:
            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
            for row in filereader:
                if not header_skipped:
                    header_skipped = True
                else:
                    maintenence_agency = row[ColumnIndexes().variable_variable_maintenence_agency]
                    code = row[ColumnIndexes().variable_code_index]
                    description = row[ColumnIndexes().variable_variable_description]
                    domain_id = row[ColumnIndexes().variable_domain_index]
                    name = row[ColumnIndexes().variable_long_name_index]
                    variable_id = row[ColumnIndexes().variable_variable_true_id]
                    primary_concept = row[ColumnIndexes().variable_primary_concept]
 (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.ancrdt_transformation.import_website_to_sdd_model_django_ancrdt:[210:224]
==pybirdai.process_steps.website_to_sddmodel.import_website_to_sdd_model_django:[192:206]
        header_skipped = False
        members_to_create = []

        with open(file_location, encoding='utf-8') as csvfile:
            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
            for row in filereader:
                if not header_skipped:
                    header_skipped = True
                else:
                    code = row[ColumnIndexes().member_member_code_index]
                    description = row[ColumnIndexes().member_member_descriptions]
                    domain_id = row[ColumnIndexes().member_domain_id_index]
                    member_id = row[ColumnIndexes().member_member_id_index]
                    member_name = row[ColumnIndexes().member_member_name_index] (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.ancrdt_transformation.import_website_to_sdd_model_django_ancrdt:[154:163]
==pybirdai.process_steps.website_to_sddmodel.import_website_to_sdd_model_django:[140:149]
        header_skipped = False
        domains_to_create = []

        with open(file_location, encoding='utf-8') as csvfile:
            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
            for row in filereader:
                if not header_skipped:
                    header_skipped = True
                else: (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.ancrdt_transformation.import_website_to_sdd_model_django_ancrdt:[123:147]
==pybirdai.process_steps.website_to_sddmodel.import_website_to_sdd_model_django:[109:133]
        header_skipped = False
        frameworks_to_create = []

        with open(file_location, encoding='utf-8') as csvfile:
            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
            for row in filereader:
                if not header_skipped:
                    header_skipped = True
                else:
                    code = row[ColumnIndexes().framework_code]
                    description = row[ColumnIndexes().framework_description]
                    id = row[ColumnIndexes().framework_id]
                    name = row[ColumnIndexes().framework_name]

                    framework = FRAMEWORK(
                        name=ImportWebsiteToSDDModel.replace_dots(self, id))
                    framework.code = code
                    framework.description = description
                    framework.framework_id = ImportWebsiteToSDDModel.replace_dots(self, id)

                    frameworks_to_create.append(framework)
                    context.framework_dictionary[ImportWebsiteToSDDModel.replace_dots(self, id)] = framework

        if context.save_sdd_to_db and frameworks_to_create: (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.ancrdt_transformation.import_website_to_sdd_model_django_ancrdt:[532:538]
==pybirdai.process_steps.website_to_sddmodel.import_website_to_sdd_model_django:[585:591]
        with open(file_location, encoding='utf-8') as csvfile:
            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
            for row in filereader:
                if not header_skipped:
                    header_skipped = True
                else: (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.ancrdt_transformation.import_website_to_sdd_model_django_ancrdt:[470:476]
==pybirdai.process_steps.website_to_sddmodel.import_website_to_sdd_model_django:[550:556]
        with open(file_location, encoding='utf-8') as csvfile:
            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
            for row in filereader:
                if not header_skipped:
                    header_skipped = True
                else: (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.ancrdt_transformation.import_website_to_sdd_model_django_ancrdt:[275:281]
==pybirdai.process_steps.website_to_sddmodel.import_website_to_sdd_model_django:[511:517]
        with open(file_location, encoding='utf-8') as csvfile:
            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
            for row in filereader:
                if not header_skipped:
                    header_skipped = True
                else: (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.ancrdt_transformation.import_website_to_sdd_model_django_ancrdt:[213:219]
==pybirdai.process_steps.website_to_sddmodel.import_website_to_sdd_model_django:[246:252]
        with open(file_location, encoding='utf-8') as csvfile:
            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
            for row in filereader:
                if not header_skipped:
                    header_skipped = True
                else: (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.ancrdt_transformation.import_website_to_sdd_model_django_ancrdt:[157:163]
==pybirdai.process_steps.website_to_sddmodel.import_website_to_sdd_model_django:[195:201]
        with open(file_location, encoding='utf-8') as csvfile:
            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
            for row in filereader:
                if not header_skipped:
                    header_skipped = True
                else: (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.ancrdt_transformation.import_website_to_sdd_model_django_ancrdt:[126:132]
==pybirdai.process_steps.website_to_sddmodel.import_website_to_sdd_model_django:[143:149]
        with open(file_location, encoding='utf-8') as csvfile:
            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
            for row in filereader:
                if not header_skipped:
                    header_skipped = True
                else: (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.ancrdt_transformation.import_website_to_sdd_model_django_ancrdt:[94:100]
==pybirdai.process_steps.website_to_sddmodel.import_website_to_sdd_model_django:[112:118]
        with open(file_location, encoding='utf-8') as csvfile:
            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
            for row in filereader:
                if not header_skipped:
                    header_skipped = True
                else: (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.ancrdt_transformation.create_joins_meta_data_ancrdt:[41:54]
==pybirdai.process_steps.automode.database_setup_first_use:[23:36]
logger = logging.getLogger(__name__)

class DjangoSetup:
    _initialized = False

    @classmethod
    def configure_django(cls):
        """Configure Django settings without starting the application"""
        if cls._initialized:
            return

        try:
            # Set up Django settings module for birds_nest in parent directory (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.input_model.import_input_model:[136:142]
==pybirdai.process_steps.website_to_sddmodel.import_website_to_sdd_model_django:[81:87]
        with open(file_location, encoding='utf-8') as csvfile:
            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
            for row in filereader:
                if not header_skipped:
                    header_skipped = True
                else: (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.generate_test_data.enrich_ldm_with_il_links_from_fe:[45:51]
==pybirdai.process_steps.sqldeveloper_import.import_sqldev_ldm_to_regdna:[340:347]
        with open(file_location,  encoding='utf-8') as csvfile:
            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
            for row in filereader:
                # skip the first line which is the header.
                if not header_skipped:
                    header_skipped = True
                else: (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.dpm_integration.mapping_functions.cell_position:[38:45]
==pybirdai.process_steps.dpm_integration.mapping_functions.ordinate_categorisation:[75:83]
    ordinate_ids = []
    for row in data:
        ordinate_ids.append(ordinate_map.get(str(row["ORDINATE_ID"]), str(row["ORDINATE_ID"])))

    for i, row in enumerate(data):
        data[i]["ORDINATE_ID"] = ordinate_ids[i]
 (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.joins_meta_data.member_hierarchy_service:[63:68]
==pybirdai.process_steps.pybird.create_executable_filters:[275:280]
        if member:
            for domain, hierarchy_list in sdd_context.domain_to_hierarchy_dictionary.items():
                if domain.domain_id == member.domain_id.domain_id:
                    for hierarchy in hierarchy_list:
                        hierarchy_id = hierarchy.member_hierarchy_id (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.ancrdt_transformation.create_python_django_transformations_ancrdt:[252:258]
==pybirdai.process_steps.pybird.create_python_django_transformations:[250:257]
                        primary_cubes_added = []
                        for cube_structure_item_link in cube_structure_item_links:
                            if cube_structure_item_link.cube_link_id.primary_cube_id.cube_id not in primary_cubes_added:
                                file.write("\t" + cube_structure_item_link.cube_link_id.primary_cube_id.cube_id  + "_Table = None # " + cube_structure_item_link.cube_link_id.primary_cube_id.cube_id + "\n")
                                primary_cubes_added.append(cube_structure_item_link.cube_link_id.primary_cube_id.cube_id)

 (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.ancrdt_transformation.create_python_django_transformations_ancrdt:[245:250]
==pybirdai.process_steps.pybird.create_python_django_transformations:[243:248]
                    for cube_link in cube_links:
                        cube_structure_item_links = []
                        try:
                            cube_structure_item_links = sdd_context.cube_structure_item_link_to_cube_link_map[cube_link.cube_link_id]
                        except KeyError: (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.ancrdt_transformation.create_python_django_transformations_ancrdt:[220:226]
==pybirdai.process_steps.pybird.create_python_django_transformations:[217:223]
                            class_header_is_written = True

                        cube_structure_item_links = []
                        try:
                            cube_structure_item_links = sdd_context.cube_structure_item_link_to_cube_link_map[cube_link.cube_link_id]
                        except KeyError: (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.ancrdt_transformation.import_website_to_sdd_model_django_ancrdt:[127:132]
==pybirdai.process_steps.sqldeveloper_import.import_sqldev_ldm_to_regdna:[797:802]
            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
            for row in filereader:
                if not header_skipped:
                    header_skipped = True
                else: (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.ancrdt_transformation.import_website_to_sdd_model_django_ancrdt:[95:100]
==pybirdai.process_steps.sqldeveloper_import.import_sqldev_ldm_to_regdna:[635:640]
            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
            for row in filereader:
                if not header_skipped:
                    header_skipped = True
                else: (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.sqldeveloper_import.import_sqldev_il_to_regdna:[143:149]
==pybirdai.process_steps.sqldeveloper_import.import_sqldev_ldm_to_regdna:[542:548]
        header_skipped = False
        counter = 0
        # Add the members of a domain as literals of the related Enum
        with open(file_location,  encoding='utf-8') as csvfile:
            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
            for row in filereader: (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.sqldeveloper_import.import_sqldev_il_to_regdna:[117:123]
==pybirdai.process_steps.sqldeveloper_import.import_sqldev_ldm_to_regdna:[516:522]
        header_skipped = False
        counter = 0
        # Create an ELEnum for each domain, and add it to the ELPackage
        with open(file_location,  encoding='utf-8') as csvfile:
            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
            for row in filereader: (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.ancrdt_transformation.import_website_to_sdd_model_django_ancrdt:[244:249]
==pybirdai.process_steps.website_to_sddmodel.import_website_to_sdd_model_django:[218:223]
                            member = MEMBER(name=ImportWebsiteToSDDModel.replace_dots(self, member_id))
                            member.member_id = ImportWebsiteToSDDModel.replace_dots(self, member_id)
                            member.code = code
                            member.description = description
                            member.name = member_name (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.ancrdt_transformation.import_website_to_sdd_model_django_ancrdt:[291:297]
==pybirdai.process_steps.website_to_sddmodel.import_website_to_sdd_model_django:[159:165]
                    if (ref) and (maintenence_agency == "ECB"):
                        include = True
                    if (not ref) and not (maintenence_agency == "ECB"):
                        include = True

                    if include: (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.sqldeveloper_import.import_sqldev_ldm_to_regdna:[587:592]
==pybirdai.process_steps.website_to_sddmodel.import_website_to_sdd_model_django:[586:591]
            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
            for row in filereader:
                if not header_skipped:
                    header_skipped = True
                else: (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.sqldeveloper_import.import_sqldev_ldm_to_regdna:[546:551]
==pybirdai.process_steps.website_to_sddmodel.import_website_to_sdd_model_django:[551:556]
            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
            for row in filereader:
                if not header_skipped:
                    header_skipped = True
                else: (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.sqldeveloper_import.import_sqldev_ldm_to_regdna:[520:525]
==pybirdai.process_steps.website_to_sddmodel.import_website_to_sdd_model_django:[512:517]
            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
            for row in filereader:
                if not header_skipped:
                    header_skipped = True
                else: (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.sqldeveloper_import.import_sqldev_ldm_to_regdna:[491:497]
==pybirdai.process_steps.website_to_sddmodel.import_website_to_sdd_model_django:[247:252]
            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
            for row in filereader:
                # skip the first line which is the header.
                if not header_skipped:
                    header_skipped = True
                else: (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.sqldeveloper_import.import_sqldev_ldm_to_regdna:[432:438]
==pybirdai.process_steps.website_to_sddmodel.import_website_to_sdd_model_django:[196:201]
            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
            for row in filereader:
                # skip the first line which is the header.
                if not header_skipped:
                    header_skipped = True
                else: (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.sqldeveloper_import.import_sqldev_ldm_to_regdna:[341:347]
==pybirdai.process_steps.website_to_sddmodel.import_website_to_sdd_model_django:[144:149]
            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
            for row in filereader:
                if not header_skipped:
                    header_skipped = True
                else: (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.sqldeveloper_import.import_sqldev_ldm_to_regdna:[271:278]
==pybirdai.process_steps.website_to_sddmodel.import_website_to_sdd_model_django:[113:118]
            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
            for row in filereader:
                # skip the first line which is the header.
                if not header_skipped:
                    header_skipped = True
                else:
 (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.sqldeveloper_import.import_sqldev_ldm_to_regdna:[252:257]
==pybirdai.process_steps.website_to_sddmodel.import_website_to_sdd_model_django:[82:87]
            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
            for row in filereader:
                if not header_skipped:
                    header_skipped = True
                else: (duplicate-code)
pybirdai/process_steps/generate_etl/generate_etl.py:1:0: R0801: Similar lines in 2 files
==pybirdai.process_steps.generate_test_data.enrich_ldm_with_il_links_from_fe:[46:51]
==pybirdai.process_steps.input_model.import_input_model:[137:142]
            filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
            for row in filereader:
                if not header_skipped:
                    header_skipped = True
                else: (duplicate-code)

-----------------------------------
Your code has been rated at 6.88/10


End of code quality check results
